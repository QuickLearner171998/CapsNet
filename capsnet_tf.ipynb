{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "5247d8f9-50de-49c3-bd1a-1fd351c618ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "40839b41-31c1-4eee-99d1-e898ead813ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   logdir\t    tensorboard.ipynb   Try-12_BEST_83.63\n",
            " capsnet_tf.py\t     logdir_try8    Try-1\t        Try-3\n",
            " data\t\t     results\t    Try-10\t        Try-4\n",
            " graph.pbtxt\t     results_best  'Try-11_Best _83%'   try-5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "outputId": "0463fa54-2bfe-4153-88ee-8661754461f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "import random\n",
        "import skimage.io\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3O76C7FuuN",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAbq_6IFxbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_aug(images,labels,angle,resize_rate, populate):\n",
        "\n",
        "\n",
        "  new_img = []\n",
        "  new_label = []\n",
        "  print(\"\\nStarting Data Augmentation\")\n",
        "  for img,label in zip(images,labels):\n",
        "\n",
        "    image = img\n",
        "    #label1 = label.reshape(1,1)\n",
        "    #flip = random.randint(0, 1)\n",
        "    size = image.shape[0]\n",
        "\n",
        "    sh = random.random()/2-0.25\n",
        "    rotate_angle = random.random()/180*np.pi*angle\n",
        "    # Create Afine transform\n",
        "    afine_tf = transform.AffineTransform(shear=sh,rotation=rotate_angle)\n",
        "    # Apply transform to image data\n",
        "    image = transform.warp(image, inverse_map=afine_tf,mode='edge')\n",
        "    \n",
        "    new_img.append(image)\n",
        "    new_label.append(label)\n",
        "  \n",
        "  print(\"\\nFinished Augmentation\")\n",
        "  if(populate):\n",
        "\n",
        "    final_trX = np.asarray(images + new_img)\n",
        "    final_labels = np.asarray(labels + new_label)\n",
        "    return final_trX.astype('float32'), final_labels.astype('int32')\n",
        "  return (np.array(new_img)).astype('float32'), (np.array(labels,dtype='int32').astype('int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "        ind = np.arange(trainX.shape[0])\n",
        "        np.random.shuffle(ind)\n",
        "        trainX = trainX[ind]\n",
        "        trainY = trainY[ind]\n",
        "        \n",
        "        trX = trainX[:45000] / 255.\n",
        "        trY = trainY[:45000]   \n",
        "        \n",
        "        trX, trY = data_aug(list(trX),list(trY),angle=5,resize_rate=0.9,populate=True)\n",
        "\n",
        "        valX = trainX[45000:, ] / 255.\n",
        "        valY = trainY[45000:]\n",
        "\n",
        "        num_tr_batch = trX.shape[0] // batch_size\n",
        "        num_val_batch = valX.shape[0] // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.7, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 15, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.392,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "# ############################\n",
        "# #   distributed setting    #\n",
        "# ############################\n",
        "# flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "# flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "# flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                # if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                  best_val_loss = val_loss\n",
        "                  best_val_acc = val_acc\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######  NOT SAVING MODEL  #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "outputId": "4aa94767-61dd-4e08-eaea-99c6586fca5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cfg.is_training=True\n",
        "# try:\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n",
        "\n",
        "# except:\n",
        "  # print(\"\\nBeginning Eval\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:17.101628 139840769816448 <ipython-input-10-deb5ab324590>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "WARNING:tensorflow:From <ipython-input-5-581c0053bd3b>:60: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.588781 139840769816448 deprecation.py:323] From <ipython-input-5-581c0053bd3b>:60: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.954898 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.962146 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.965891 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.971181 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.975576 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-581c0053bd3b>:65: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:24.991814 139840769816448 deprecation.py:323] From <ipython-input-5-581c0053bd3b>:65: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:25.016910 139840769816448 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I1112 12:41:25.263425 139840769816448 utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:25.600031 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:25.802269 139840769816448 deprecation.py:323] From <ipython-input-7-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:26.135010 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:26.777517 139840769816448 <ipython-input-7-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:26.779954 139840769816448 <ipython-input-10-deb5ab324590>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-deb5ab324590>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:41:26.781951 139840769816448 deprecation.py:323] From <ipython-input-10-deb5ab324590>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:27.347967 139840769816448 <ipython-input-10-deb5ab324590>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:35.755777 139840769816448 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:41:35.785772 139840769816448 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:42:33.771164 139840769816448 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:42:35.075195 139840769816448 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/15:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:42:37.632375 139837015512832 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:42:41.121099 139837023905536 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.77546895\n",
            "\n",
            "Global step: 1,loss: 0.7727701\n",
            "\n",
            "Global step: 2,loss: 0.7942305\n",
            "\n",
            "Global step: 3,loss: 0.7381582\n",
            "\n",
            "Global step: 4,loss: 0.721784\n",
            "\n",
            "Global step: 5,loss: 0.70034796\n",
            "\n",
            "Global step: 6,loss: 0.6437747\n",
            "\n",
            "Global step: 7,loss: 0.57904667\n",
            "\n",
            "Global step: 8,loss: 0.5311852\n",
            "\n",
            "Global step: 9,loss: 0.5092462\n",
            "\n",
            "Global step: 10,loss: 0.48494154\n",
            "\n",
            "Global step: 11,loss: 0.45746133\n",
            "\n",
            "Global step: 12,loss: 0.4623311\n",
            "\n",
            "Global step: 13,loss: 0.3976965\n",
            "\n",
            "Global step: 14,loss: 0.3923256\n",
            "\n",
            "Global step: 15,loss: 0.37671578\n",
            "\n",
            "Global step: 16,loss: 0.35089248\n",
            "\n",
            "Global step: 17,loss: 0.32780915\n",
            "\n",
            "Global step: 18,loss: 0.36199775\n",
            "\n",
            "Global step: 19,loss: 0.3541008\n",
            "\n",
            "Global step: 20,loss: 0.30199432\n",
            "\n",
            "Global step: 21,loss: 0.31172147\n",
            "\n",
            "Global step: 22,loss: 0.30333474\n",
            "\n",
            "Global step: 23,loss: 0.27991083\n",
            "\n",
            "Global step: 24,loss: 0.27399838\n",
            "\n",
            "Global step: 25,loss: 0.27822745\n",
            "\n",
            "Global step: 26,loss: 0.263017\n",
            "\n",
            "Global step: 27,loss: 0.2780303\n",
            "\n",
            "Global step: 28,loss: 0.25772423\n",
            "\n",
            "Global step: 29,loss: 0.2431389\n",
            "\n",
            "Global step: 30,loss: 0.25933096\n",
            "\n",
            "Global step: 31,loss: 0.22614643\n",
            "\n",
            "Global step: 32,loss: 0.23494974\n",
            "\n",
            "Global step: 33,loss: 0.20619166\n",
            "\n",
            "Global step: 34,loss: 0.19745281\n",
            "\n",
            "Global step: 35,loss: 0.22865543\n",
            "\n",
            "Global step: 36,loss: 0.19155414\n",
            "\n",
            "Global step: 37,loss: 0.21183091\n",
            "\n",
            "Global step: 38,loss: 0.19844879\n",
            "\n",
            "Global step: 39,loss: 0.1990635\n",
            "\n",
            "Global step: 40,loss: 0.21512944\n",
            "\n",
            "Global step: 41,loss: 0.18401162\n",
            "\n",
            "Global step: 42,loss: 0.17525263\n",
            "\n",
            "Global step: 43,loss: 0.1984186\n",
            "\n",
            "Global step: 44,loss: 0.14203289\n",
            "\n",
            "Global step: 45,loss: 0.16090132\n",
            "\n",
            "Global step: 46,loss: 0.1603432\n",
            "\n",
            "Global step: 47,loss: 0.13988894\n",
            "\n",
            "Global step: 48,loss: 0.15851882\n",
            "\n",
            "Global step: 49,loss: 0.16169624\n",
            "\n",
            "Global step: 50,loss: 0.17617679\n",
            "\n",
            "Global step: 51,loss: 0.15062717\n",
            "\n",
            "Global step: 52,loss: 0.13384871\n",
            "\n",
            "Global step: 53,loss: 0.1399025\n",
            "\n",
            "Global step: 54,loss: 0.13546875\n",
            "\n",
            "Global step: 55,loss: 0.13314472\n",
            "\n",
            "Global step: 56,loss: 0.15125391\n",
            "\n",
            "Global step: 57,loss: 0.14902477\n",
            "\n",
            "Global step: 58,loss: 0.124103144\n",
            "\n",
            "Global step: 59,loss: 0.118742466\n",
            "\n",
            "Global step: 60,loss: 0.116336174\n",
            "\n",
            "Global step: 61,loss: 0.116039485\n",
            "\n",
            "Global step: 62,loss: 0.11335059\n",
            "\n",
            "Global step: 63,loss: 0.11340795\n",
            "\n",
            "Global step: 64,loss: 0.14149751\n",
            "\n",
            "Global step: 65,loss: 0.12745938\n",
            "\n",
            "Global step: 66,loss: 0.1124438\n",
            "\n",
            "Global step: 67,loss: 0.09928869\n",
            "\n",
            "Global step: 68,loss: 0.097406015\n",
            "\n",
            "Global step: 69,loss: 0.10798545\n",
            "\n",
            "Global step: 70,loss: 0.12013678\n",
            "\n",
            "Global step: 71,loss: 0.113065645\n",
            "\n",
            "Global step: 72,loss: 0.10498032\n",
            "\n",
            "Global step: 73,loss: 0.13252264\n",
            "\n",
            "Global step: 74,loss: 0.09923631\n",
            "\n",
            "Global step: 75,loss: 0.09088527\n",
            "\n",
            "Global step: 76,loss: 0.10330267\n",
            "\n",
            "Global step: 77,loss: 0.12075494\n",
            "\n",
            "Global step: 78,loss: 0.10319489\n",
            "\n",
            "Global step: 79,loss: 0.09536223\n",
            "\n",
            "Global step: 80,loss: 0.098763674\n",
            "\n",
            "Global step: 81,loss: 0.093919784\n",
            "\n",
            "Global step: 82,loss: 0.10650418\n",
            "\n",
            "Global step: 83,loss: 0.07919252\n",
            "\n",
            "Global step: 84,loss: 0.084251955\n",
            "\n",
            "Global step: 85,loss: 0.09615482\n",
            "\n",
            "Global step: 86,loss: 0.08648897\n",
            "\n",
            "Global step: 87,loss: 0.091855906\n",
            "\n",
            "Global step: 88,loss: 0.0872156\n",
            "\n",
            "Global step: 89,loss: 0.09427921\n",
            "\n",
            "Global step: 90,loss: 0.08557382\n",
            "\n",
            "Global step: 91,loss: 0.09236576\n",
            "\n",
            "Global step: 92,loss: 0.08036715\n",
            "\n",
            "Global step: 93,loss: 0.08858176\n",
            "\n",
            "Global step: 94,loss: 0.07412352\n",
            "\n",
            "Global step: 95,loss: 0.07447734\n",
            "\n",
            "Global step: 96,loss: 0.08209688\n",
            "\n",
            "Global step: 97,loss: 0.06203024\n",
            "\n",
            "Global step: 98,loss: 0.07842443\n",
            "\n",
            "Global step: 99,loss: 0.072840884\n",
            "\n",
            "Global step: 100,loss: 0.083296396\n",
            "\n",
            "Global step: 101,loss: 0.07390814\n",
            "\n",
            "Global step: 102,loss: 0.060928497\n",
            "\n",
            "Global step: 103,loss: 0.072082065\n",
            "\n",
            "Global step: 104,loss: 0.07992708\n",
            "\n",
            "Global step: 105,loss: 0.07279065\n",
            "\n",
            "Global step: 106,loss: 0.060239255\n",
            "\n",
            "Global step: 107,loss: 0.07453715\n",
            "\n",
            "Global step: 108,loss: 0.072456926\n",
            "\n",
            "Global step: 109,loss: 0.069421746\n",
            "\n",
            "Global step: 110,loss: 0.058051813\n",
            "\n",
            "Global step: 111,loss: 0.07609607\n",
            "\n",
            "Global step: 112,loss: 0.066772304\n",
            "\n",
            "Global step: 113,loss: 0.07743542\n",
            "\n",
            "Global step: 114,loss: 0.077793695\n",
            "\n",
            "Global step: 115,loss: 0.069425344\n",
            "\n",
            "Global step: 116,loss: 0.07360324\n",
            "\n",
            "Global step: 117,loss: 0.08291944\n",
            "\n",
            "Global step: 118,loss: 0.099515244\n",
            "\n",
            "Global step: 119,loss: 0.09489733\n",
            "\n",
            "Global step: 120,loss: 0.058041357\n",
            "\n",
            "Global step: 121,loss: 0.07149931\n",
            "\n",
            "Global step: 122,loss: 0.08820903\n",
            "\n",
            "Global step: 123,loss: 0.099872455\n",
            "\n",
            "Global step: 124,loss: 0.062106058\n",
            "\n",
            "Global step: 125,loss: 0.06562458\n",
            "\n",
            "Global step: 126,loss: 0.06960408\n",
            "\n",
            "Global step: 127,loss: 0.057015516\n",
            "\n",
            "Global step: 128,loss: 0.079554595\n",
            "\n",
            "Global step: 129,loss: 0.055434033\n",
            "\n",
            "Global step: 130,loss: 0.068522476\n",
            "\n",
            "Global step: 131,loss: 0.07313572\n",
            "\n",
            "Global step: 132,loss: 0.07292329\n",
            "\n",
            "Global step: 133,loss: 0.06968513\n",
            "\n",
            "Global step: 134,loss: 0.07268198\n",
            "\n",
            "Global step: 135,loss: 0.06938711\n",
            "\n",
            "Global step: 136,loss: 0.0621068\n",
            "\n",
            "Global step: 137,loss: 0.0549382\n",
            "\n",
            "Global step: 138,loss: 0.08571292\n",
            "\n",
            "Global step: 139,loss: 0.07620288\n",
            "\n",
            "Global step: 140,loss: 0.0796856\n",
            "\n",
            "Global step: 141,loss: 0.086928\n",
            "\n",
            "Global step: 142,loss: 0.09888749\n",
            "\n",
            "Global step: 143,loss: 0.07325545\n",
            "\n",
            "Global step: 144,loss: 0.06472111\n",
            "\n",
            "Global step: 145,loss: 0.07708083\n",
            "\n",
            "Global step: 146,loss: 0.060886122\n",
            "\n",
            "Global step: 147,loss: 0.078036636\n",
            "\n",
            "Global step: 148,loss: 0.052596334\n",
            "\n",
            "Global step: 149,loss: 0.07110912\n",
            "\n",
            "Global step: 150,loss: 0.061455097\n",
            "\n",
            "Global step: 151,loss: 0.07479362\n",
            "\n",
            "Global step: 152,loss: 0.060787573\n",
            "\n",
            "Global step: 153,loss: 0.067970365\n",
            "\n",
            "Global step: 154,loss: 0.058408357\n",
            "\n",
            "Global step: 155,loss: 0.061552227\n",
            "\n",
            "Global step: 156,loss: 0.07264752\n",
            "\n",
            "Global step: 157,loss: 0.060289398\n",
            "\n",
            "Global step: 158,loss: 0.05327122\n",
            "\n",
            "Global step: 159,loss: 0.06727768\n",
            "\n",
            "Global step: 160,loss: 0.054113284\n",
            "\n",
            "Global step: 161,loss: 0.057122305\n",
            "\n",
            "Global step: 162,loss: 0.06544768\n",
            "\n",
            "Global step: 163,loss: 0.07277597\n",
            "\n",
            "Global step: 164,loss: 0.073187515\n",
            "\n",
            "Global step: 165,loss: 0.06561322\n",
            "\n",
            "Global step: 166,loss: 0.07638137\n",
            "\n",
            "Global step: 167,loss: 0.07216936\n",
            "\n",
            "Global step: 168,loss: 0.08342038\n",
            "\n",
            "Global step: 169,loss: 0.053773306\n",
            "\n",
            "Global step: 170,loss: 0.05089997\n",
            "\n",
            "Global step: 171,loss: 0.06127193\n",
            "\n",
            "Global step: 172,loss: 0.06356294\n",
            "\n",
            "Global step: 173,loss: 0.055371\n",
            "\n",
            "Global step: 174,loss: 0.067320466\n",
            "\n",
            "Global step: 175,loss: 0.06145359\n",
            "\n",
            "Global step: 176,loss: 0.07171181\n",
            "\n",
            "Global step: 177,loss: 0.07103001\n",
            "\n",
            "Global step: 178,loss: 0.039379917\n",
            "\n",
            "Global step: 179,loss: 0.0724506\n",
            "\n",
            "Global step: 180,loss: 0.05405309\n",
            "\n",
            "Global step: 181,loss: 0.06807599\n",
            "\n",
            "Global step: 182,loss: 0.045365\n",
            "\n",
            "Global step: 183,loss: 0.054342624\n",
            "\n",
            "Global step: 184,loss: 0.05617837\n",
            "\n",
            "Global step: 185,loss: 0.049688496\n",
            "\n",
            "Global step: 186,loss: 0.051888384\n",
            "\n",
            "Global step: 187,loss: 0.061547257\n",
            "\n",
            "Global step: 188,loss: 0.052665275\n",
            "\n",
            "Global step: 189,loss: 0.073565125\n",
            "\n",
            "Global step: 190,loss: 0.060123134\n",
            "\n",
            "Global step: 191,loss: 0.057667665\n",
            "\n",
            "Global step: 192,loss: 0.07299608\n",
            "\n",
            "Global step: 193,loss: 0.07470179\n",
            "\n",
            "Global step: 194,loss: 0.048871186\n",
            "\n",
            "Global step: 195,loss: 0.058062848\n",
            "\n",
            "Global step: 196,loss: 0.04686608\n",
            "\n",
            "Global step: 197,loss: 0.05070162\n",
            "\n",
            "Global step: 198,loss: 0.042168207\n",
            "\n",
            "Global step: 199,loss: 0.05809572\n",
            "\n",
            "Global step: 200,loss: 0.0684247\n",
            "\n",
            "Global step: 201,loss: 0.040845923\n",
            "\n",
            "Global step: 202,loss: 0.074590474\n",
            "\n",
            "Global step: 203,loss: 0.059714064\n",
            "\n",
            "Global step: 204,loss: 0.039321072\n",
            "\n",
            "Global step: 205,loss: 0.047851775\n",
            "\n",
            "Global step: 206,loss: 0.045515765\n",
            "\n",
            "Global step: 207,loss: 0.04174867\n",
            "\n",
            "Global step: 208,loss: 0.05201029\n",
            "\n",
            "Global step: 209,loss: 0.039292153\n",
            "\n",
            "Global step: 210,loss: 0.047967635\n",
            "\n",
            "Global step: 211,loss: 0.044392668\n",
            "\n",
            "Global step: 212,loss: 0.03876169\n",
            "\n",
            "Global step: 213,loss: 0.044368464\n",
            "\n",
            "Global step: 214,loss: 0.052150775\n",
            "\n",
            "Global step: 215,loss: 0.04763647\n",
            "\n",
            "Global step: 216,loss: 0.048533197\n",
            "\n",
            "Global step: 217,loss: 0.054276954\n",
            "\n",
            "Global step: 218,loss: 0.044009034\n",
            "\n",
            "Global step: 219,loss: 0.054051265\n",
            "\n",
            "Global step: 220,loss: 0.050608426\n",
            "\n",
            "Global step: 221,loss: 0.04483497\n",
            "\n",
            "Global step: 222,loss: 0.065469086\n",
            "\n",
            "Global step: 223,loss: 0.06125381\n",
            "\n",
            "Global step: 224,loss: 0.056143418\n",
            "\n",
            "Global step: 225,loss: 0.046218373\n",
            "\n",
            "Global step: 226,loss: 0.052978903\n",
            "\n",
            "Global step: 227,loss: 0.06290602\n",
            "\n",
            "Global step: 228,loss: 0.044348765\n",
            "\n",
            "Global step: 229,loss: 0.066527106\n",
            "\n",
            "Global step: 230,loss: 0.052590568\n",
            "\n",
            "Global step: 231,loss: 0.05255472\n",
            "\n",
            "Global step: 232,loss: 0.0453682\n",
            "\n",
            "Global step: 233,loss: 0.051172663\n",
            "\n",
            "Global step: 234,loss: 0.038634773\n",
            "\n",
            "Global step: 235,loss: 0.05236403\n",
            "\n",
            "Global step: 236,loss: 0.054177005\n",
            "\n",
            "Global step: 237,loss: 0.033419132\n",
            "\n",
            "Global step: 238,loss: 0.039742976\n",
            "\n",
            "Global step: 239,loss: 0.037077695\n",
            "\n",
            "Global step: 240,loss: 0.037408292\n",
            "\n",
            "Global step: 241,loss: 0.04268325\n",
            "\n",
            "Global step: 242,loss: 0.050126813\n",
            "\n",
            "Global step: 243,loss: 0.04332915\n",
            "\n",
            "Global step: 244,loss: 0.051351488\n",
            "\n",
            "Global step: 245,loss: 0.051405005\n",
            "\n",
            "Global step: 246,loss: 0.045915317\n",
            "\n",
            "Global step: 247,loss: 0.042074952\n",
            "\n",
            "Global step: 248,loss: 0.05298809\n",
            "\n",
            "Global step: 249,loss: 0.05614198\n",
            "\n",
            "Global step: 250,loss: 0.042817798\n",
            "\n",
            "Global step: 251,loss: 0.047881335\n",
            "\n",
            "Global step: 252,loss: 0.042777233\n",
            "\n",
            "Global step: 253,loss: 0.04575683\n",
            "\n",
            "Global step: 254,loss: 0.0491355\n",
            "\n",
            "Global step: 255,loss: 0.056522176\n",
            "\n",
            "Global step: 256,loss: 0.046005245\n",
            "\n",
            "Global step: 257,loss: 0.041690037\n",
            "\n",
            "Global step: 258,loss: 0.052519154\n",
            "\n",
            "Global step: 259,loss: 0.044488054\n",
            "\n",
            "Global step: 260,loss: 0.041912105\n",
            "\n",
            "Global step: 261,loss: 0.039289013\n",
            "\n",
            "Global step: 262,loss: 0.0320218\n",
            "\n",
            "Global step: 263,loss: 0.0571034\n",
            "\n",
            "Global step: 264,loss: 0.057250995\n",
            "\n",
            "Global step: 265,loss: 0.051162343\n",
            "\n",
            "Global step: 266,loss: 0.037535425\n",
            "\n",
            "Global step: 267,loss: 0.036763843\n",
            "\n",
            "Global step: 268,loss: 0.03392968\n",
            "\n",
            "Global step: 269,loss: 0.057942785\n",
            "\n",
            "Global step: 270,loss: 0.050638422\n",
            "\n",
            "Global step: 271,loss: 0.04698383\n",
            "\n",
            "Global step: 272,loss: 0.043010265\n",
            "\n",
            "Global step: 273,loss: 0.04290042\n",
            "\n",
            "Global step: 274,loss: 0.056947917\n",
            "\n",
            "Global step: 275,loss: 0.047702875\n",
            "\n",
            "Global step: 276,loss: 0.032829776\n",
            "\n",
            "Global step: 277,loss: 0.045639988\n",
            "\n",
            "Global step: 278,loss: 0.053833604\n",
            "\n",
            "Global step: 279,loss: 0.04627938\n",
            "\n",
            "Global step: 280,loss: 0.043468192\n",
            "\n",
            "Global step: 281,loss: 0.04048141\n",
            "\n",
            "Global step: 282,loss: 0.035273522\n",
            "\n",
            "Global step: 283,loss: 0.04313439\n",
            "\n",
            "Global step: 284,loss: 0.041203536\n",
            "\n",
            "Global step: 285,loss: 0.035013273\n",
            "\n",
            "Global step: 286,loss: 0.034236073\n",
            "\n",
            "Global step: 287,loss: 0.033785798\n",
            "\n",
            "Global step: 288,loss: 0.03392204\n",
            "\n",
            "Global step: 289,loss: 0.03592859\n",
            "\n",
            "Global step: 290,loss: 0.031374488\n",
            "\n",
            "Global step: 291,loss: 0.056081\n",
            "\n",
            "Global step: 292,loss: 0.04440672\n",
            "\n",
            "Global step: 293,loss: 0.046525866\n",
            "\n",
            "Global step: 294,loss: 0.05592476\n",
            "\n",
            "Global step: 295,loss: 0.03890283\n",
            "\n",
            "Global step: 296,loss: 0.05468226\n",
            "\n",
            "Global step: 297,loss: 0.03342642\n",
            "\n",
            "Global step: 298,loss: 0.03802819\n",
            "\n",
            "Global step: 299,loss: 0.04594854\n",
            "\n",
            "Global step: 300,loss: 0.035066642\n",
            "\n",
            "Global step: 301,loss: 0.04195887\n",
            "\n",
            "Global step: 302,loss: 0.040828716\n",
            "\n",
            "Global step: 303,loss: 0.03611228\n",
            "\n",
            "Global step: 304,loss: 0.04585385\n",
            "\n",
            "Global step: 305,loss: 0.03423108\n",
            "\n",
            "Global step: 306,loss: 0.052853517\n",
            "\n",
            "Global step: 307,loss: 0.043747477\n",
            "\n",
            "Global step: 308,loss: 0.042430624\n",
            "\n",
            "Global step: 309,loss: 0.036368087\n",
            "\n",
            "Global step: 310,loss: 0.048698902\n",
            "\n",
            "Global step: 311,loss: 0.043220058\n",
            "\n",
            "Global step: 312,loss: 0.044183023\n",
            "\n",
            "Global step: 313,loss: 0.035939515\n",
            "\n",
            "Global step: 314,loss: 0.030161673\n",
            "\n",
            "Global step: 315,loss: 0.036132585\n",
            "\n",
            "Global step: 316,loss: 0.043477003\n",
            "\n",
            "Global step: 317,loss: 0.03154645\n",
            "\n",
            "Global step: 318,loss: 0.04165036\n",
            "\n",
            "Global step: 319,loss: 0.055037588\n",
            "\n",
            "Global step: 320,loss: 0.039104406\n",
            "\n",
            "Global step: 321,loss: 0.04373494\n",
            "\n",
            "Global step: 322,loss: 0.032859456\n",
            "\n",
            "Global step: 323,loss: 0.064917274\n",
            "\n",
            "Global step: 324,loss: 0.044257484\n",
            "\n",
            "Global step: 325,loss: 0.05103076\n",
            "\n",
            "Global step: 326,loss: 0.03641852\n",
            "\n",
            "Global step: 327,loss: 0.04783662\n",
            "\n",
            "Global step: 328,loss: 0.046098687\n",
            "\n",
            "Global step: 329,loss: 0.041566737\n",
            "\n",
            "Global step: 330,loss: 0.045616724\n",
            "\n",
            "Global step: 331,loss: 0.024290647\n",
            "\n",
            "Global step: 332,loss: 0.025939144\n",
            "\n",
            "Global step: 333,loss: 0.053057786\n",
            "\n",
            "Global step: 334,loss: 0.03824582\n",
            "\n",
            "Global step: 335,loss: 0.03962183\n",
            "\n",
            "Global step: 336,loss: 0.03459916\n",
            "\n",
            "Global step: 337,loss: 0.04824719\n",
            "\n",
            "Global step: 338,loss: 0.02934495\n",
            "\n",
            "Global step: 339,loss: 0.05208013\n",
            "\n",
            "Global step: 340,loss: 0.03909433\n",
            "\n",
            "Global step: 341,loss: 0.033268306\n",
            "\n",
            "Global step: 342,loss: 0.03897204\n",
            "\n",
            "Global step: 343,loss: 0.026130974\n",
            "\n",
            "Global step: 344,loss: 0.032388333\n",
            "\n",
            "Global step: 345,loss: 0.032838773\n",
            "\n",
            "Global step: 346,loss: 0.029024508\n",
            "\n",
            "Global step: 347,loss: 0.042145837\n",
            "\n",
            "Global step: 348,loss: 0.04745472\n",
            "\n",
            "Global step: 349,loss: 0.039707705\n",
            "\n",
            "Global step: 350,loss: 0.052645687\n",
            "\n",
            "Global step: 351,loss: 0.041475154\n",
            "\n",
            "Global step: 352,loss: 0.03382024\n",
            "\n",
            "Global step: 353,loss: 0.042952415\n",
            "\n",
            "Global step: 354,loss: 0.04995984\n",
            "\n",
            "Global step: 355,loss: 0.03620187\n",
            "\n",
            "Global step: 356,loss: 0.05882915\n",
            "\n",
            "Global step: 357,loss: 0.03737989\n",
            "\n",
            "Global step: 358,loss: 0.04286914\n",
            "\n",
            "Global step: 359,loss: 0.0387896\n",
            "\n",
            "Global step: 360,loss: 0.028399387\n",
            "\n",
            "Global step: 361,loss: 0.032558396\n",
            "\n",
            "Global step: 362,loss: 0.02623863\n",
            "\n",
            "Global step: 363,loss: 0.029141083\n",
            "\n",
            "Global step: 364,loss: 0.0435345\n",
            "\n",
            "Global step: 365,loss: 0.040573627\n",
            "\n",
            "Global step: 366,loss: 0.03783869\n",
            "\n",
            "Global step: 367,loss: 0.067941636\n",
            "\n",
            "Global step: 368,loss: 0.04464162\n",
            "\n",
            "Global step: 369,loss: 0.045269992\n",
            "\n",
            "Global step: 370,loss: 0.040225178\n",
            "\n",
            "Global step: 371,loss: 0.034368813\n",
            "\n",
            "Global step: 372,loss: 0.036746446\n",
            "\n",
            "Global step: 373,loss: 0.06026141\n",
            "\n",
            "Global step: 374,loss: 0.031885944\n",
            "\n",
            "Global step: 375,loss: 0.047013253\n",
            "\n",
            "Global step: 376,loss: 0.038387734\n",
            "\n",
            "Global step: 377,loss: 0.05011054\n",
            "\n",
            "Global step: 378,loss: 0.04451686\n",
            "\n",
            "Global step: 379,loss: 0.03952743\n",
            "\n",
            "Global step: 380,loss: 0.032951444\n",
            "\n",
            "Global step: 381,loss: 0.040046334\n",
            "\n",
            "Global step: 382,loss: 0.046051025\n",
            "\n",
            "Global step: 383,loss: 0.0356517\n",
            "\n",
            "Global step: 384,loss: 0.038247395\n",
            "\n",
            "Global step: 385,loss: 0.050182275\n",
            "\n",
            "Global step: 386,loss: 0.04062965\n",
            "\n",
            "Global step: 387,loss: 0.05237459\n",
            "\n",
            "Global step: 388,loss: 0.058013946\n",
            "\n",
            "Global step: 389,loss: 0.028125767\n",
            "\n",
            "Global step: 390,loss: 0.025927532\n",
            "\n",
            "Global step: 391,loss: 0.02991458\n",
            "\n",
            "Global step: 392,loss: 0.028032815\n",
            "\n",
            "Global step: 393,loss: 0.035991445\n",
            "\n",
            "Global step: 394,loss: 0.0338546\n",
            "\n",
            "Global step: 395,loss: 0.04375302\n",
            "\n",
            "Global step: 396,loss: 0.039082497\n",
            "\n",
            "Global step: 397,loss: 0.041093394\n",
            "\n",
            "Global step: 398,loss: 0.041190147\n",
            "\n",
            "Global step: 399,loss: 0.03560554\n",
            "\n",
            "Global step: 400,loss: 0.042363577\n",
            "\n",
            "Global step: 401,loss: 0.028835995\n",
            "\n",
            "Global step: 402,loss: 0.034136277\n",
            "\n",
            "Global step: 403,loss: 0.036683887\n",
            "\n",
            "Global step: 404,loss: 0.0340105\n",
            "\n",
            "Global step: 405,loss: 0.043846685\n",
            "\n",
            "Global step: 406,loss: 0.038047463\n",
            "\n",
            "Global step: 407,loss: 0.028984757\n",
            "\n",
            "Global step: 408,loss: 0.032062083\n",
            "\n",
            "Global step: 409,loss: 0.039116222\n",
            "\n",
            "Global step: 410,loss: 0.04748767\n",
            "\n",
            "Global step: 411,loss: 0.037718154\n",
            "\n",
            "Global step: 412,loss: 0.03800431\n",
            "\n",
            "Global step: 413,loss: 0.03839291\n",
            "\n",
            "Global step: 414,loss: 0.048410334\n",
            "\n",
            "Global step: 415,loss: 0.040707793\n",
            "\n",
            "Global step: 416,loss: 0.03879842\n",
            "\n",
            "Global step: 417,loss: 0.029639691\n",
            "\n",
            "Global step: 418,loss: 0.03377764\n",
            "\n",
            "Global step: 419,loss: 0.045516856\n",
            "\n",
            "Global step: 420,loss: 0.028481688\n",
            "\n",
            "Global step: 421,loss: 0.031819813\n",
            "\n",
            "Global step: 422,loss: 0.03618221\n",
            "\n",
            "Global step: 423,loss: 0.026183672\n",
            "\n",
            "Global step: 424,loss: 0.054236863\n",
            "\n",
            "Global step: 425,loss: 0.033923566\n",
            "\n",
            "Global step: 426,loss: 0.033959083\n",
            "\n",
            "Global step: 427,loss: 0.031993564\n",
            "\n",
            "Global step: 428,loss: 0.055803735\n",
            "\n",
            "Global step: 429,loss: 0.028778162\n",
            "\n",
            "Global step: 430,loss: 0.045401044\n",
            "\n",
            "Global step: 431,loss: 0.04693064\n",
            "\n",
            "Global step: 432,loss: 0.031690776\n",
            "\n",
            "Global step: 433,loss: 0.036124233\n",
            "\n",
            "Global step: 434,loss: 0.04006155\n",
            "\n",
            "Global step: 435,loss: 0.03798653\n",
            "\n",
            "Global step: 436,loss: 0.030931206\n",
            "\n",
            "Global step: 437,loss: 0.05272535\n",
            "\n",
            "Global step: 438,loss: 0.03528941\n",
            "\n",
            "Global step: 439,loss: 0.0442622\n",
            "\n",
            "Global step: 440,loss: 0.033945147\n",
            "\n",
            "Global step: 441,loss: 0.03339513\n",
            "\n",
            "Global step: 442,loss: 0.0390416\n",
            "\n",
            "Global step: 443,loss: 0.039363973\n",
            "\n",
            "Global step: 444,loss: 0.028166406\n",
            "\n",
            "Global step: 445,loss: 0.03635364\n",
            "\n",
            "Global step: 446,loss: 0.032659706\n",
            "\n",
            "Global step: 447,loss: 0.037219666\n",
            "\n",
            "Global step: 448,loss: 0.032781545\n",
            "\n",
            "Global step: 449,loss: 0.03564124\n",
            "\n",
            "Global step: 450,loss: 0.050195314\n",
            "\n",
            "Global step: 451,loss: 0.045033798\n",
            "\n",
            "Global step: 452,loss: 0.03479417\n",
            "\n",
            "Global step: 453,loss: 0.030801527\n",
            "\n",
            "Global step: 454,loss: 0.037096187\n",
            "\n",
            "Global step: 455,loss: 0.035535857\n",
            "\n",
            "Global step: 456,loss: 0.03605124\n",
            "\n",
            "Global step: 457,loss: 0.028878901\n",
            "\n",
            "Global step: 458,loss: 0.026022516\n",
            "\n",
            "Global step: 459,loss: 0.047827136\n",
            "\n",
            "Global step: 460,loss: 0.032098167\n",
            "\n",
            "Global step: 461,loss: 0.031921122\n",
            "\n",
            "Global step: 462,loss: 0.02759596\n",
            "\n",
            "Global step: 463,loss: 0.02762367\n",
            "\n",
            "Global step: 464,loss: 0.04089816\n",
            "\n",
            "Global step: 465,loss: 0.029156338\n",
            "\n",
            "Global step: 466,loss: 0.033753652\n",
            "\n",
            "Global step: 467,loss: 0.0674413\n",
            "\n",
            "Global step: 468,loss: 0.050222997\n",
            "\n",
            "Global step: 469,loss: 0.040882763\n",
            "\n",
            "Global step: 470,loss: 0.03984718\n",
            "\n",
            "Global step: 471,loss: 0.031837795\n",
            "\n",
            "Global step: 472,loss: 0.027562916\n",
            "\n",
            "Global step: 473,loss: 0.03121952\n",
            "\n",
            "Global step: 474,loss: 0.035536464\n",
            "\n",
            "Global step: 475,loss: 0.025546946\n",
            "\n",
            "Global step: 476,loss: 0.03450756\n",
            "\n",
            "Global step: 477,loss: 0.04273793\n",
            "\n",
            "Global step: 478,loss: 0.030315883\n",
            "\n",
            "Global step: 479,loss: 0.024828278\n",
            "\n",
            "Global step: 480,loss: 0.037067853\n",
            "\n",
            "Global step: 481,loss: 0.029605962\n",
            "\n",
            "Global step: 482,loss: 0.03706342\n",
            "\n",
            "Global step: 483,loss: 0.03724446\n",
            "\n",
            "Global step: 484,loss: 0.031888086\n",
            "\n",
            "Global step: 485,loss: 0.039351463\n",
            "\n",
            "Global step: 486,loss: 0.03581568\n",
            "\n",
            "Global step: 487,loss: 0.038787108\n",
            "\n",
            "Global step: 488,loss: 0.029802997\n",
            "\n",
            "Global step: 489,loss: 0.03982243\n",
            "\n",
            "Global step: 490,loss: 0.04552022\n",
            "\n",
            "Global step: 491,loss: 0.029251218\n",
            "\n",
            "Global step: 492,loss: 0.037576273\n",
            "\n",
            "Global step: 493,loss: 0.03527256\n",
            "\n",
            "Global step: 494,loss: 0.042812422\n",
            "\n",
            "Global step: 495,loss: 0.026017135\n",
            "\n",
            "Global step: 496,loss: 0.037822075\n",
            "\n",
            "Global step: 497,loss: 0.03422892\n",
            "\n",
            "Global step: 498,loss: 0.022496333\n",
            "\n",
            "Global step: 499,loss: 0.039918695\n",
            "\n",
            "Global step: 500,loss: 0.030445747\n",
            "\n",
            "Global step: 501,loss: 0.062331986\n",
            "\n",
            "Global step: 502,loss: 0.036296315\n",
            "\n",
            "Global step: 503,loss: 0.03252059\n",
            "\n",
            "Global step: 504,loss: 0.022691706\n",
            "\n",
            "Global step: 505,loss: 0.032546073\n",
            "\n",
            "Global step: 506,loss: 0.043380465\n",
            "\n",
            "Global step: 507,loss: 0.028212782\n",
            "\n",
            "Global step: 508,loss: 0.037576534\n",
            "\n",
            "Global step: 509,loss: 0.04548527\n",
            "\n",
            "Global step: 510,loss: 0.040593367\n",
            "\n",
            "Global step: 511,loss: 0.027071072\n",
            "\n",
            "Global step: 512,loss: 0.035065398\n",
            "\n",
            "Global step: 513,loss: 0.034528367\n",
            "\n",
            "Global step: 514,loss: 0.031717055\n",
            "\n",
            "Global step: 515,loss: 0.04473969\n",
            "\n",
            "Global step: 516,loss: 0.05200647\n",
            "\n",
            "Global step: 517,loss: 0.039080657\n",
            "\n",
            "Global step: 518,loss: 0.035167877\n",
            "\n",
            "Global step: 519,loss: 0.04099546\n",
            "\n",
            "Global step: 520,loss: 0.031363264\n",
            "\n",
            "Global step: 521,loss: 0.031052565\n",
            "\n",
            "Global step: 522,loss: 0.032639723\n",
            "\n",
            "Global step: 523,loss: 0.0271203\n",
            "\n",
            "Global step: 524,loss: 0.050107263\n",
            "\n",
            "Global step: 525,loss: 0.03965936\n",
            "\n",
            "Global step: 526,loss: 0.032536283\n",
            "\n",
            "Global step: 527,loss: 0.044892073\n",
            "\n",
            "Global step: 528,loss: 0.035594404\n",
            "\n",
            "Global step: 529,loss: 0.028221708\n",
            "\n",
            "Global step: 530,loss: 0.033833522\n",
            "\n",
            "Global step: 531,loss: 0.03237929\n",
            "\n",
            "Global step: 532,loss: 0.024327766\n",
            "\n",
            "Global step: 533,loss: 0.03540145\n",
            "\n",
            "Global step: 534,loss: 0.046197962\n",
            "\n",
            "Global step: 535,loss: 0.043394566\n",
            "\n",
            "Global step: 536,loss: 0.026151313\n",
            "\n",
            "Global step: 537,loss: 0.03787593\n",
            "\n",
            "Global step: 538,loss: 0.03606202\n",
            "\n",
            "Global step: 539,loss: 0.02802473\n",
            "\n",
            "Global step: 540,loss: 0.035749093\n",
            "\n",
            "Global step: 541,loss: 0.029724482\n",
            "\n",
            "Global step: 542,loss: 0.033362526\n",
            "\n",
            "Global step: 543,loss: 0.027001437\n",
            "\n",
            "Global step: 544,loss: 0.030657731\n",
            "\n",
            "Global step: 545,loss: 0.02820104\n",
            "\n",
            "Global step: 546,loss: 0.028789785\n",
            "\n",
            "Global step: 547,loss: 0.028750308\n",
            "\n",
            "Global step: 548,loss: 0.036354948\n",
            "\n",
            "Global step: 549,loss: 0.0300557\n",
            "\n",
            "Global step: 550,loss: 0.028597714\n",
            "\n",
            "Global step: 551,loss: 0.03584054\n",
            "\n",
            "Global step: 552,loss: 0.036970124\n",
            "\n",
            "Global step: 553,loss: 0.035407074\n",
            "\n",
            "Global step: 554,loss: 0.034052722\n",
            "\n",
            "Global step: 555,loss: 0.029568253\n",
            "\n",
            "Global step: 556,loss: 0.04491584\n",
            "\n",
            "Global step: 557,loss: 0.027096275\n",
            "\n",
            "Global step: 558,loss: 0.036510985\n",
            "\n",
            "Global step: 559,loss: 0.0301855\n",
            "\n",
            "Global step: 560,loss: 0.038559765\n",
            "\n",
            "Global step: 561,loss: 0.027949587\n",
            "\n",
            "Global step: 562,loss: 0.027087044\n",
            "\n",
            "Global step: 563,loss: 0.035828687\n",
            "\n",
            "Global step: 564,loss: 0.025170578\n",
            "\n",
            "Global step: 565,loss: 0.028857443\n",
            "\n",
            "Global step: 566,loss: 0.034800116\n",
            "\n",
            "Global step: 567,loss: 0.03058956\n",
            "\n",
            "Global step: 568,loss: 0.025432434\n",
            "\n",
            "Global step: 569,loss: 0.035647936\n",
            "\n",
            "Global step: 570,loss: 0.04277407\n",
            "\n",
            "Global step: 571,loss: 0.025574198\n",
            "\n",
            "Global step: 572,loss: 0.037900947\n",
            "\n",
            "Global step: 573,loss: 0.031119237\n",
            "\n",
            "Global step: 574,loss: 0.034927897\n",
            "\n",
            "Global step: 575,loss: 0.04223252\n",
            "\n",
            "Global step: 576,loss: 0.03180427\n",
            "\n",
            "Global step: 577,loss: 0.043601476\n",
            "\n",
            "Global step: 578,loss: 0.039862625\n",
            "\n",
            "Global step: 579,loss: 0.031353295\n",
            "\n",
            "Global step: 580,loss: 0.045186352\n",
            "\n",
            "Global step: 581,loss: 0.040764105\n",
            "\n",
            "Global step: 582,loss: 0.034292288\n",
            "\n",
            "Global step: 583,loss: 0.029012896\n",
            "\n",
            "Global step: 584,loss: 0.036438562\n",
            "\n",
            "Global step: 585,loss: 0.031204566\n",
            "\n",
            "Global step: 586,loss: 0.039199464\n",
            "\n",
            "Global step: 587,loss: 0.028330984\n",
            "\n",
            "Global step: 588,loss: 0.028459895\n",
            "\n",
            "Global step: 589,loss: 0.027681496\n",
            "\n",
            "Global step: 590,loss: 0.036721483\n",
            "\n",
            "Global step: 591,loss: 0.026460253\n",
            "\n",
            "Global step: 592,loss: 0.029074151\n",
            "\n",
            "Global step: 593,loss: 0.046806972\n",
            "\n",
            "Global step: 594,loss: 0.02859197\n",
            "\n",
            "Global step: 595,loss: 0.024866734\n",
            "\n",
            "Global step: 596,loss: 0.040836263\n",
            "\n",
            "Global step: 597,loss: 0.033112183\n",
            "\n",
            "Global step: 598,loss: 0.029820634\n",
            "\n",
            "Global step: 599,loss: 0.030511338\n",
            "\n",
            "Global step: 600,loss: 0.028297998\n",
            "\n",
            "Global step: 601,loss: 0.021810453\n",
            "\n",
            "Global step: 602,loss: 0.03332734\n",
            "\n",
            "Global step: 603,loss: 0.036850445\n",
            "\n",
            "Global step: 604,loss: 0.029247306\n",
            "\n",
            "Global step: 605,loss: 0.029736595\n",
            "\n",
            "Global step: 606,loss: 0.043763332\n",
            "\n",
            "Global step: 607,loss: 0.036235627\n",
            "\n",
            "Global step: 608,loss: 0.030985419\n",
            "\n",
            "Global step: 609,loss: 0.040529456\n",
            "\n",
            "Global step: 610,loss: 0.035425805\n",
            "\n",
            "Global step: 611,loss: 0.031906728\n",
            "\n",
            "Global step: 612,loss: 0.025854293\n",
            "\n",
            "Global step: 613,loss: 0.0399433\n",
            "\n",
            "Global step: 614,loss: 0.02895286\n",
            "\n",
            "Global step: 615,loss: 0.02406973\n",
            "\n",
            "Global step: 616,loss: 0.034934167\n",
            "\n",
            "Global step: 617,loss: 0.032366652\n",
            "\n",
            "Global step: 618,loss: 0.03540993\n",
            "\n",
            "Global step: 619,loss: 0.048764236\n",
            "\n",
            "Global step: 620,loss: 0.02880799\n",
            "\n",
            "Global step: 621,loss: 0.026644053\n",
            "\n",
            "Global step: 622,loss: 0.025992073\n",
            "\n",
            "Global step: 623,loss: 0.027858362\n",
            "\n",
            "Global step: 624,loss: 0.031081382\n",
            "\n",
            "Global step: 625,loss: 0.023340538\n",
            "\n",
            "Global step: 626,loss: 0.03348368\n",
            "\n",
            "Global step: 627,loss: 0.03453823\n",
            "\n",
            "Global step: 628,loss: 0.026688699\n",
            "\n",
            "Global step: 629,loss: 0.035556532\n",
            "\n",
            "Global step: 630,loss: 0.025496554\n",
            "\n",
            "Global step: 631,loss: 0.033545386\n",
            "\n",
            "Global step: 632,loss: 0.023618575\n",
            "\n",
            "Global step: 633,loss: 0.030397408\n",
            "\n",
            "Global step: 634,loss: 0.045173693\n",
            "\n",
            "Global step: 635,loss: 0.027185693\n",
            "\n",
            "Global step: 636,loss: 0.025052644\n",
            "\n",
            "Global step: 637,loss: 0.021945542\n",
            "\n",
            "Global step: 638,loss: 0.041953415\n",
            "\n",
            "Global step: 639,loss: 0.021587325\n",
            "\n",
            "Global step: 640,loss: 0.04180559\n",
            "\n",
            "Global step: 641,loss: 0.023057517\n",
            "\n",
            "Global step: 642,loss: 0.02585993\n",
            "\n",
            "Global step: 643,loss: 0.027648158\n",
            "\n",
            "Global step: 644,loss: 0.0359475\n",
            "\n",
            "Global step: 645,loss: 0.024581695\n",
            "\n",
            "Global step: 646,loss: 0.021006078\n",
            "\n",
            "Global step: 647,loss: 0.030713513\n",
            "\n",
            "Global step: 648,loss: 0.040454622\n",
            "\n",
            "Global step: 649,loss: 0.024533406\n",
            "\n",
            "Global step: 650,loss: 0.028339569\n",
            "\n",
            "Global step: 651,loss: 0.03127714\n",
            "\n",
            "Global step: 652,loss: 0.020035736\n",
            "\n",
            "Global step: 653,loss: 0.021981223\n",
            "\n",
            "Global step: 654,loss: 0.023582008\n",
            "\n",
            "Global step: 655,loss: 0.025859505\n",
            "\n",
            "Global step: 656,loss: 0.024606563\n",
            "\n",
            "Global step: 657,loss: 0.029257158\n",
            "\n",
            "Global step: 658,loss: 0.037664246\n",
            "\n",
            "Global step: 659,loss: 0.027720604\n",
            "\n",
            "Global step: 660,loss: 0.026241923\n",
            "\n",
            "Global step: 661,loss: 0.025160037\n",
            "\n",
            "Global step: 662,loss: 0.03663941\n",
            "\n",
            "Global step: 663,loss: 0.03346256\n",
            "\n",
            "Global step: 664,loss: 0.035356883\n",
            "\n",
            "Global step: 665,loss: 0.034085933\n",
            "\n",
            "Global step: 666,loss: 0.025369003\n",
            "\n",
            "Global step: 667,loss: 0.03845498\n",
            "\n",
            "Global step: 668,loss: 0.03785877\n",
            "\n",
            "Global step: 669,loss: 0.036277317\n",
            "\n",
            "Global step: 670,loss: 0.02707677\n",
            "\n",
            "Global step: 671,loss: 0.033478193\n",
            "\n",
            "Global step: 672,loss: 0.024681289\n",
            "\n",
            "Global step: 673,loss: 0.02806395\n",
            "\n",
            "Global step: 674,loss: 0.02922703\n",
            "\n",
            "Global step: 675,loss: 0.02145265\n",
            "\n",
            "Global step: 676,loss: 0.033475958\n",
            "\n",
            "Global step: 677,loss: 0.023857987\n",
            "\n",
            "Global step: 678,loss: 0.0406394\n",
            "\n",
            "Global step: 679,loss: 0.02697426\n",
            "\n",
            "Global step: 680,loss: 0.03023795\n",
            "\n",
            "Global step: 681,loss: 0.034408834\n",
            "\n",
            "Global step: 682,loss: 0.02375192\n",
            "\n",
            "Global step: 683,loss: 0.039510213\n",
            "\n",
            "Global step: 684,loss: 0.042141996\n",
            "\n",
            "Global step: 685,loss: 0.032801762\n",
            "\n",
            "Global step: 686,loss: 0.0322463\n",
            "\n",
            "Global step: 687,loss: 0.026345287\n",
            "\n",
            "Global step: 688,loss: 0.035313092\n",
            "\n",
            "Global step: 689,loss: 0.031858042\n",
            "\n",
            "Global step: 690,loss: 0.027088854\n",
            "\n",
            "Global step: 691,loss: 0.03101321\n",
            "\n",
            "Global step: 692,loss: 0.024273057\n",
            "\n",
            "Global step: 693,loss: 0.023213394\n",
            "\n",
            "Global step: 694,loss: 0.027365793\n",
            "\n",
            "Global step: 695,loss: 0.029346183\n",
            "\n",
            "Global step: 696,loss: 0.038474217\n",
            "\n",
            "Global step: 697,loss: 0.039081447\n",
            "\n",
            "Global step: 698,loss: 0.020146526\n",
            "\n",
            "Global step: 699,loss: 0.029022356\n",
            "\n",
            "Global step: 700,loss: 0.028798401\n",
            "\n",
            "Global step: 701,loss: 0.032237813\n",
            "\n",
            "Global step: 702,loss: 0.026332587\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 702,Val_Loss: 0.025535805134946465,  Val_acc: 0.9949252136752137 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:44:34.359354 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 703.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:44:35.125630 139837023905536 supervisor.py:1050] Recording summary at step 703.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 5.97839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:44:35.222535 139837015512832 supervisor.py:1099] global_step/sec: 5.97839\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/15:\n",
            "Global step: 703,loss: 0.034599535\n",
            "\n",
            "Global step: 704,loss: 0.031189509\n",
            "\n",
            "Global step: 705,loss: 0.032731146\n",
            "\n",
            "Global step: 706,loss: 0.027869467\n",
            "\n",
            "Global step: 707,loss: 0.024190629\n",
            "\n",
            "Global step: 708,loss: 0.022527095\n",
            "\n",
            "Global step: 709,loss: 0.027529906\n",
            "\n",
            "Global step: 710,loss: 0.02857206\n",
            "\n",
            "Global step: 711,loss: 0.02942364\n",
            "\n",
            "Global step: 712,loss: 0.030150764\n",
            "\n",
            "Global step: 713,loss: 0.032665752\n",
            "\n",
            "Global step: 714,loss: 0.03155825\n",
            "\n",
            "Global step: 715,loss: 0.029017368\n",
            "\n",
            "Global step: 716,loss: 0.02296122\n",
            "\n",
            "Global step: 717,loss: 0.025105894\n",
            "\n",
            "Global step: 718,loss: 0.03540928\n",
            "\n",
            "Global step: 719,loss: 0.027959213\n",
            "\n",
            "Global step: 720,loss: 0.037962254\n",
            "\n",
            "Global step: 721,loss: 0.033577994\n",
            "\n",
            "Global step: 722,loss: 0.02433258\n",
            "\n",
            "Global step: 723,loss: 0.021584671\n",
            "\n",
            "Global step: 724,loss: 0.024047446\n",
            "\n",
            "Global step: 725,loss: 0.030040098\n",
            "\n",
            "Global step: 726,loss: 0.019704696\n",
            "\n",
            "Global step: 727,loss: 0.028418262\n",
            "\n",
            "Global step: 728,loss: 0.03652336\n",
            "\n",
            "Global step: 729,loss: 0.026059698\n",
            "\n",
            "Global step: 730,loss: 0.022471663\n",
            "\n",
            "Global step: 731,loss: 0.038120724\n",
            "\n",
            "Global step: 732,loss: 0.026342755\n",
            "\n",
            "Global step: 733,loss: 0.030985676\n",
            "\n",
            "Global step: 734,loss: 0.034761503\n",
            "\n",
            "Global step: 735,loss: 0.019317484\n",
            "\n",
            "Global step: 736,loss: 0.026802998\n",
            "\n",
            "Global step: 737,loss: 0.026314419\n",
            "\n",
            "Global step: 738,loss: 0.020720873\n",
            "\n",
            "Global step: 739,loss: 0.026365798\n",
            "\n",
            "Global step: 740,loss: 0.02170409\n",
            "\n",
            "Global step: 741,loss: 0.027977146\n",
            "\n",
            "Global step: 742,loss: 0.020481512\n",
            "\n",
            "Global step: 743,loss: 0.031956077\n",
            "\n",
            "Global step: 744,loss: 0.026463687\n",
            "\n",
            "Global step: 745,loss: 0.032268766\n",
            "\n",
            "Global step: 746,loss: 0.025070183\n",
            "\n",
            "Global step: 747,loss: 0.031963106\n",
            "\n",
            "Global step: 748,loss: 0.02908216\n",
            "\n",
            "Global step: 749,loss: 0.027178723\n",
            "\n",
            "Global step: 750,loss: 0.021015087\n",
            "\n",
            "Global step: 751,loss: 0.025716303\n",
            "\n",
            "Global step: 752,loss: 0.032010242\n",
            "\n",
            "Global step: 753,loss: 0.027002525\n",
            "\n",
            "Global step: 754,loss: 0.019689737\n",
            "\n",
            "Global step: 755,loss: 0.033305418\n",
            "\n",
            "Global step: 756,loss: 0.02196809\n",
            "\n",
            "Global step: 757,loss: 0.023402505\n",
            "\n",
            "Global step: 758,loss: 0.028048247\n",
            "\n",
            "Global step: 759,loss: 0.029861812\n",
            "\n",
            "Global step: 760,loss: 0.02110153\n",
            "\n",
            "Global step: 761,loss: 0.022498854\n",
            "\n",
            "Global step: 762,loss: 0.021764899\n",
            "\n",
            "Global step: 763,loss: 0.019390691\n",
            "\n",
            "Global step: 764,loss: 0.025743\n",
            "\n",
            "Global step: 765,loss: 0.03411106\n",
            "\n",
            "Global step: 766,loss: 0.02724186\n",
            "\n",
            "Global step: 767,loss: 0.023574773\n",
            "\n",
            "Global step: 768,loss: 0.022130745\n",
            "\n",
            "Global step: 769,loss: 0.02769927\n",
            "\n",
            "Global step: 770,loss: 0.019234976\n",
            "\n",
            "Global step: 771,loss: 0.018499542\n",
            "\n",
            "Global step: 772,loss: 0.020605544\n",
            "\n",
            "Global step: 773,loss: 0.025838854\n",
            "\n",
            "Global step: 774,loss: 0.030939879\n",
            "\n",
            "Global step: 775,loss: 0.03508621\n",
            "\n",
            "Global step: 776,loss: 0.020932106\n",
            "\n",
            "Global step: 777,loss: 0.027412154\n",
            "\n",
            "Global step: 778,loss: 0.029132819\n",
            "\n",
            "Global step: 779,loss: 0.019840978\n",
            "\n",
            "Global step: 780,loss: 0.029409843\n",
            "\n",
            "Global step: 781,loss: 0.022776533\n",
            "\n",
            "Global step: 782,loss: 0.035361666\n",
            "\n",
            "Global step: 783,loss: 0.02619085\n",
            "\n",
            "Global step: 784,loss: 0.030917512\n",
            "\n",
            "Global step: 785,loss: 0.030890394\n",
            "\n",
            "Global step: 786,loss: 0.0323449\n",
            "\n",
            "Global step: 787,loss: 0.026819352\n",
            "\n",
            "Global step: 788,loss: 0.024112277\n",
            "\n",
            "Global step: 789,loss: 0.023092676\n",
            "\n",
            "Global step: 790,loss: 0.024728792\n",
            "\n",
            "Global step: 791,loss: 0.018490076\n",
            "\n",
            "Global step: 792,loss: 0.032763682\n",
            "\n",
            "Global step: 793,loss: 0.040590614\n",
            "\n",
            "Global step: 794,loss: 0.022898924\n",
            "\n",
            "Global step: 795,loss: 0.019011816\n",
            "\n",
            "Global step: 796,loss: 0.023413744\n",
            "\n",
            "Global step: 797,loss: 0.022884805\n",
            "\n",
            "Global step: 798,loss: 0.024810355\n",
            "\n",
            "Global step: 799,loss: 0.022738215\n",
            "\n",
            "Global step: 800,loss: 0.022688288\n",
            "\n",
            "Global step: 801,loss: 0.02266911\n",
            "\n",
            "Global step: 802,loss: 0.025269179\n",
            "\n",
            "Global step: 803,loss: 0.02640665\n",
            "\n",
            "Global step: 804,loss: 0.031253934\n",
            "\n",
            "Global step: 805,loss: 0.03077557\n",
            "\n",
            "Global step: 806,loss: 0.02275048\n",
            "\n",
            "Global step: 807,loss: 0.03156743\n",
            "\n",
            "Global step: 808,loss: 0.030720074\n",
            "\n",
            "Global step: 809,loss: 0.01953815\n",
            "\n",
            "Global step: 810,loss: 0.01914984\n",
            "\n",
            "Global step: 811,loss: 0.021251861\n",
            "\n",
            "Global step: 812,loss: 0.043256316\n",
            "\n",
            "Global step: 813,loss: 0.031398088\n",
            "\n",
            "Global step: 814,loss: 0.027863197\n",
            "\n",
            "Global step: 815,loss: 0.024955481\n",
            "\n",
            "Global step: 816,loss: 0.025953528\n",
            "\n",
            "Global step: 817,loss: 0.03133133\n",
            "\n",
            "Global step: 818,loss: 0.023455681\n",
            "\n",
            "Global step: 819,loss: 0.023555648\n",
            "\n",
            "Global step: 820,loss: 0.023566883\n",
            "\n",
            "Global step: 821,loss: 0.019280614\n",
            "\n",
            "Global step: 822,loss: 0.02146506\n",
            "\n",
            "Global step: 823,loss: 0.023783896\n",
            "\n",
            "Global step: 824,loss: 0.025442146\n",
            "\n",
            "Global step: 825,loss: 0.026577001\n",
            "\n",
            "Global step: 826,loss: 0.020937651\n",
            "\n",
            "Global step: 827,loss: 0.036513425\n",
            "\n",
            "Global step: 828,loss: 0.02347992\n",
            "\n",
            "Global step: 829,loss: 0.021853952\n",
            "\n",
            "Global step: 830,loss: 0.017815676\n",
            "\n",
            "Global step: 831,loss: 0.021639291\n",
            "\n",
            "Global step: 832,loss: 0.028081004\n",
            "\n",
            "Global step: 833,loss: 0.028157543\n",
            "\n",
            "Global step: 834,loss: 0.026690284\n",
            "\n",
            "Global step: 835,loss: 0.01977127\n",
            "\n",
            "Global step: 836,loss: 0.020389248\n",
            "\n",
            "Global step: 837,loss: 0.026136149\n",
            "\n",
            "Global step: 838,loss: 0.029047232\n",
            "\n",
            "Global step: 839,loss: 0.023900378\n",
            "\n",
            "Global step: 840,loss: 0.02378209\n",
            "\n",
            "Global step: 841,loss: 0.02382311\n",
            "\n",
            "Global step: 842,loss: 0.028319292\n",
            "\n",
            "Global step: 843,loss: 0.025796317\n",
            "\n",
            "Global step: 844,loss: 0.025720123\n",
            "\n",
            "Global step: 845,loss: 0.030465893\n",
            "\n",
            "Global step: 846,loss: 0.022144966\n",
            "\n",
            "Global step: 847,loss: 0.045050334\n",
            "\n",
            "Global step: 848,loss: 0.02209447\n",
            "\n",
            "Global step: 849,loss: 0.039767336\n",
            "\n",
            "Global step: 850,loss: 0.04145976\n",
            "\n",
            "Global step: 851,loss: 0.025624178\n",
            "\n",
            "Global step: 852,loss: 0.03507386\n",
            "\n",
            "Global step: 853,loss: 0.027168157\n",
            "\n",
            "Global step: 854,loss: 0.027196016\n",
            "\n",
            "Global step: 855,loss: 0.028642341\n",
            "\n",
            "Global step: 856,loss: 0.032999843\n",
            "\n",
            "Global step: 857,loss: 0.018589366\n",
            "\n",
            "Global step: 858,loss: 0.020788286\n",
            "\n",
            "Global step: 859,loss: 0.022010263\n",
            "\n",
            "Global step: 860,loss: 0.03151737\n",
            "\n",
            "Global step: 861,loss: 0.034509204\n",
            "\n",
            "Global step: 862,loss: 0.024948716\n",
            "\n",
            "Global step: 863,loss: 0.024088223\n",
            "\n",
            "Global step: 864,loss: 0.03556013\n",
            "\n",
            "Global step: 865,loss: 0.027145693\n",
            "\n",
            "Global step: 866,loss: 0.028634\n",
            "\n",
            "Global step: 867,loss: 0.023839181\n",
            "\n",
            "Global step: 868,loss: 0.027350776\n",
            "\n",
            "Global step: 869,loss: 0.019736098\n",
            "\n",
            "Global step: 870,loss: 0.02432751\n",
            "\n",
            "Global step: 871,loss: 0.024550045\n",
            "\n",
            "Global step: 872,loss: 0.022413362\n",
            "\n",
            "Global step: 873,loss: 0.029479906\n",
            "\n",
            "Global step: 874,loss: 0.035137184\n",
            "\n",
            "Global step: 875,loss: 0.030810189\n",
            "\n",
            "Global step: 876,loss: 0.021944422\n",
            "\n",
            "Global step: 877,loss: 0.024430852\n",
            "\n",
            "Global step: 878,loss: 0.028588168\n",
            "\n",
            "Global step: 879,loss: 0.020077325\n",
            "\n",
            "Global step: 880,loss: 0.028067715\n",
            "\n",
            "Global step: 881,loss: 0.01922898\n",
            "\n",
            "Global step: 882,loss: 0.022846296\n",
            "\n",
            "Global step: 883,loss: 0.031152248\n",
            "\n",
            "Global step: 884,loss: 0.022435408\n",
            "\n",
            "Global step: 885,loss: 0.017552074\n",
            "\n",
            "Global step: 886,loss: 0.02019815\n",
            "\n",
            "Global step: 887,loss: 0.028998284\n",
            "\n",
            "Global step: 888,loss: 0.017491935\n",
            "\n",
            "Global step: 889,loss: 0.026577495\n",
            "\n",
            "Global step: 890,loss: 0.026109077\n",
            "\n",
            "Global step: 891,loss: 0.025978599\n",
            "\n",
            "Global step: 892,loss: 0.021354709\n",
            "\n",
            "Global step: 893,loss: 0.020238595\n",
            "\n",
            "Global step: 894,loss: 0.022648025\n",
            "\n",
            "Global step: 895,loss: 0.028586622\n",
            "\n",
            "Global step: 896,loss: 0.025310734\n",
            "\n",
            "Global step: 897,loss: 0.019710114\n",
            "\n",
            "Global step: 898,loss: 0.02605195\n",
            "\n",
            "Global step: 899,loss: 0.016372722\n",
            "\n",
            "Global step: 900,loss: 0.025213268\n",
            "\n",
            "Global step: 901,loss: 0.0253251\n",
            "\n",
            "Global step: 902,loss: 0.028340155\n",
            "\n",
            "Global step: 903,loss: 0.02662924\n",
            "\n",
            "Global step: 904,loss: 0.021689542\n",
            "\n",
            "Global step: 905,loss: 0.024009522\n",
            "\n",
            "Global step: 906,loss: 0.025380539\n",
            "\n",
            "Global step: 907,loss: 0.022630502\n",
            "\n",
            "Global step: 908,loss: 0.019798495\n",
            "\n",
            "Global step: 909,loss: 0.028866015\n",
            "\n",
            "Global step: 910,loss: 0.025571497\n",
            "\n",
            "Global step: 911,loss: 0.023731219\n",
            "\n",
            "Global step: 912,loss: 0.021610644\n",
            "\n",
            "Global step: 913,loss: 0.02105967\n",
            "\n",
            "Global step: 914,loss: 0.026733823\n",
            "\n",
            "Global step: 915,loss: 0.022633769\n",
            "\n",
            "Global step: 916,loss: 0.023291983\n",
            "\n",
            "Global step: 917,loss: 0.030249164\n",
            "\n",
            "Global step: 918,loss: 0.02812144\n",
            "\n",
            "Global step: 919,loss: 0.023964379\n",
            "\n",
            "Global step: 920,loss: 0.025603533\n",
            "\n",
            "Global step: 921,loss: 0.036132194\n",
            "\n",
            "Global step: 922,loss: 0.020248\n",
            "\n",
            "Global step: 923,loss: 0.026729431\n",
            "\n",
            "Global step: 924,loss: 0.02700206\n",
            "\n",
            "Global step: 925,loss: 0.019227654\n",
            "\n",
            "Global step: 926,loss: 0.032130614\n",
            "\n",
            "Global step: 927,loss: 0.018702678\n",
            "\n",
            "Global step: 928,loss: 0.029845435\n",
            "\n",
            "Global step: 929,loss: 0.019410014\n",
            "\n",
            "Global step: 930,loss: 0.031117762\n",
            "\n",
            "Global step: 931,loss: 0.023607768\n",
            "\n",
            "Global step: 932,loss: 0.03492237\n",
            "\n",
            "Global step: 933,loss: 0.025398087\n",
            "\n",
            "Global step: 934,loss: 0.037594777\n",
            "\n",
            "Global step: 935,loss: 0.038701974\n",
            "\n",
            "Global step: 936,loss: 0.025043074\n",
            "\n",
            "Global step: 937,loss: 0.025685746\n",
            "\n",
            "Global step: 938,loss: 0.018632079\n",
            "\n",
            "Global step: 939,loss: 0.029623205\n",
            "\n",
            "Global step: 940,loss: 0.01859019\n",
            "\n",
            "Global step: 941,loss: 0.029208813\n",
            "\n",
            "Global step: 942,loss: 0.01812489\n",
            "\n",
            "Global step: 943,loss: 0.038043372\n",
            "\n",
            "Global step: 944,loss: 0.022279773\n",
            "\n",
            "Global step: 945,loss: 0.0286407\n",
            "\n",
            "Global step: 946,loss: 0.020806532\n",
            "\n",
            "Global step: 947,loss: 0.02450164\n",
            "\n",
            "Global step: 948,loss: 0.03025845\n",
            "\n",
            "Global step: 949,loss: 0.02567553\n",
            "\n",
            "Global step: 950,loss: 0.023804374\n",
            "\n",
            "Global step: 951,loss: 0.02891792\n",
            "\n",
            "Global step: 952,loss: 0.018088756\n",
            "\n",
            "Global step: 953,loss: 0.020641327\n",
            "\n",
            "Global step: 954,loss: 0.021875264\n",
            "\n",
            "Global step: 955,loss: 0.018750846\n",
            "\n",
            "Global step: 956,loss: 0.027553648\n",
            "\n",
            "Global step: 957,loss: 0.029115058\n",
            "\n",
            "Global step: 958,loss: 0.03248846\n",
            "\n",
            "Global step: 959,loss: 0.022358414\n",
            "\n",
            "Global step: 960,loss: 0.028352406\n",
            "\n",
            "Global step: 961,loss: 0.027003562\n",
            "\n",
            "Global step: 962,loss: 0.028210323\n",
            "\n",
            "Global step: 963,loss: 0.021761682\n",
            "\n",
            "Global step: 964,loss: 0.031821422\n",
            "\n",
            "Global step: 965,loss: 0.018330183\n",
            "\n",
            "Global step: 966,loss: 0.023701381\n",
            "\n",
            "Global step: 967,loss: 0.03334649\n",
            "\n",
            "Global step: 968,loss: 0.034525424\n",
            "\n",
            "Global step: 969,loss: 0.022558369\n",
            "\n",
            "Global step: 970,loss: 0.025437996\n",
            "\n",
            "Global step: 971,loss: 0.024451137\n",
            "\n",
            "Global step: 972,loss: 0.028721929\n",
            "\n",
            "Global step: 973,loss: 0.020378796\n",
            "\n",
            "Global step: 974,loss: 0.020000711\n",
            "\n",
            "Global step: 975,loss: 0.030979775\n",
            "\n",
            "Global step: 976,loss: 0.020844419\n",
            "\n",
            "Global step: 977,loss: 0.023753695\n",
            "\n",
            "Global step: 978,loss: 0.021645602\n",
            "\n",
            "Global step: 979,loss: 0.026280697\n",
            "\n",
            "Global step: 980,loss: 0.020260325\n",
            "\n",
            "Global step: 981,loss: 0.023508348\n",
            "\n",
            "Global step: 982,loss: 0.02235736\n",
            "\n",
            "Global step: 983,loss: 0.03361313\n",
            "\n",
            "Global step: 984,loss: 0.024254221\n",
            "\n",
            "Global step: 985,loss: 0.025895167\n",
            "\n",
            "Global step: 986,loss: 0.025503412\n",
            "\n",
            "Global step: 987,loss: 0.017303072\n",
            "\n",
            "Global step: 988,loss: 0.028189376\n",
            "\n",
            "Global step: 989,loss: 0.01862532\n",
            "\n",
            "Global step: 990,loss: 0.03181288\n",
            "\n",
            "Global step: 991,loss: 0.02230382\n",
            "\n",
            "Global step: 992,loss: 0.023065144\n",
            "\n",
            "Global step: 993,loss: 0.019034967\n",
            "\n",
            "Global step: 994,loss: 0.027067997\n",
            "\n",
            "Global step: 995,loss: 0.015528788\n",
            "\n",
            "Global step: 996,loss: 0.016473925\n",
            "\n",
            "Global step: 997,loss: 0.020176277\n",
            "\n",
            "Global step: 998,loss: 0.03044186\n",
            "\n",
            "Global step: 999,loss: 0.02201334\n",
            "\n",
            "Global step: 1000,loss: 0.031746775\n",
            "\n",
            "Global step: 1001,loss: 0.026107673\n",
            "\n",
            "Global step: 1002,loss: 0.016839594\n",
            "\n",
            "Global step: 1003,loss: 0.029090537\n",
            "\n",
            "Global step: 1004,loss: 0.019306365\n",
            "\n",
            "Global step: 1005,loss: 0.024779454\n",
            "\n",
            "Global step: 1006,loss: 0.01912095\n",
            "\n",
            "Global step: 1007,loss: 0.017847195\n",
            "\n",
            "Global step: 1008,loss: 0.025988705\n",
            "\n",
            "Global step: 1009,loss: 0.02894565\n",
            "\n",
            "Global step: 1010,loss: 0.022735577\n",
            "\n",
            "Global step: 1011,loss: 0.028242104\n",
            "\n",
            "Global step: 1012,loss: 0.022774301\n",
            "\n",
            "Global step: 1013,loss: 0.020867083\n",
            "\n",
            "Global step: 1014,loss: 0.025269523\n",
            "\n",
            "Global step: 1015,loss: 0.021262767\n",
            "\n",
            "Global step: 1016,loss: 0.023800913\n",
            "\n",
            "Global step: 1017,loss: 0.028071217\n",
            "\n",
            "Global step: 1018,loss: 0.020143382\n",
            "\n",
            "Global step: 1019,loss: 0.028039716\n",
            "\n",
            "Global step: 1020,loss: 0.022458345\n",
            "\n",
            "Global step: 1021,loss: 0.018525597\n",
            "\n",
            "Global step: 1022,loss: 0.022887468\n",
            "\n",
            "Global step: 1023,loss: 0.019034447\n",
            "\n",
            "Global step: 1024,loss: 0.019612253\n",
            "\n",
            "Global step: 1025,loss: 0.026606528\n",
            "\n",
            "Global step: 1026,loss: 0.023755368\n",
            "\n",
            "Global step: 1027,loss: 0.01903871\n",
            "\n",
            "Global step: 1028,loss: 0.030056767\n",
            "\n",
            "Global step: 1029,loss: 0.01898855\n",
            "\n",
            "Global step: 1030,loss: 0.028136134\n",
            "\n",
            "Global step: 1031,loss: 0.015801687\n",
            "\n",
            "Global step: 1032,loss: 0.024356082\n",
            "\n",
            "Global step: 1033,loss: 0.01859248\n",
            "\n",
            "Global step: 1034,loss: 0.023844017\n",
            "\n",
            "Global step: 1035,loss: 0.034766536\n",
            "\n",
            "Global step: 1036,loss: 0.02167913\n",
            "\n",
            "Global step: 1037,loss: 0.032091197\n",
            "\n",
            "Global step: 1038,loss: 0.02776966\n",
            "\n",
            "Global step: 1039,loss: 0.019900382\n",
            "\n",
            "Global step: 1040,loss: 0.026867906\n",
            "\n",
            "Global step: 1041,loss: 0.021062894\n",
            "\n",
            "Global step: 1042,loss: 0.019316114\n",
            "\n",
            "Global step: 1043,loss: 0.027137887\n",
            "\n",
            "Global step: 1044,loss: 0.021448791\n",
            "\n",
            "Global step: 1045,loss: 0.01971773\n",
            "\n",
            "Global step: 1046,loss: 0.02046537\n",
            "\n",
            "Global step: 1047,loss: 0.028701931\n",
            "\n",
            "Global step: 1048,loss: 0.018243512\n",
            "\n",
            "Global step: 1049,loss: 0.022505816\n",
            "\n",
            "Global step: 1050,loss: 0.034898885\n",
            "\n",
            "Global step: 1051,loss: 0.03494893\n",
            "\n",
            "Global step: 1052,loss: 0.051530946\n",
            "\n",
            "Global step: 1053,loss: 0.032688953\n",
            "\n",
            "Global step: 1054,loss: 0.030609563\n",
            "\n",
            "Global step: 1055,loss: 0.029778231\n",
            "\n",
            "Global step: 1056,loss: 0.037672598\n",
            "\n",
            "Global step: 1057,loss: 0.021729304\n",
            "\n",
            "Global step: 1058,loss: 0.018781934\n",
            "\n",
            "Global step: 1059,loss: 0.019549306\n",
            "\n",
            "Global step: 1060,loss: 0.032604165\n",
            "\n",
            "Global step: 1061,loss: 0.03552652\n",
            "\n",
            "Global step: 1062,loss: 0.03860209\n",
            "\n",
            "Global step: 1063,loss: 0.024514414\n",
            "\n",
            "Global step: 1064,loss: 0.032162495\n",
            "\n",
            "Global step: 1065,loss: 0.025510365\n",
            "\n",
            "Global step: 1066,loss: 0.030899003\n",
            "\n",
            "Global step: 1067,loss: 0.018657852\n",
            "\n",
            "Global step: 1068,loss: 0.023406193\n",
            "\n",
            "Global step: 1069,loss: 0.031047467\n",
            "\n",
            "Global step: 1070,loss: 0.02547704\n",
            "\n",
            "Global step: 1071,loss: 0.02688485\n",
            "\n",
            "Global step: 1072,loss: 0.021182025\n",
            "\n",
            "Global step: 1073,loss: 0.031305574\n",
            "\n",
            "Global step: 1074,loss: 0.028551236\n",
            "\n",
            "Global step: 1075,loss: 0.019564882\n",
            "\n",
            "Global step: 1076,loss: 0.028683882\n",
            "\n",
            "Global step: 1077,loss: 0.023039173\n",
            "\n",
            "Global step: 1078,loss: 0.033616796\n",
            "\n",
            "Global step: 1079,loss: 0.025524704\n",
            "\n",
            "Global step: 1080,loss: 0.016232276\n",
            "\n",
            "Global step: 1081,loss: 0.043137815\n",
            "\n",
            "Global step: 1082,loss: 0.024250712\n",
            "\n",
            "Global step: 1083,loss: 0.019022852\n",
            "\n",
            "Global step: 1084,loss: 0.016006608\n",
            "\n",
            "Global step: 1085,loss: 0.02944887\n",
            "\n",
            "Global step: 1086,loss: 0.03004371\n",
            "\n",
            "Global step: 1087,loss: 0.022596603\n",
            "\n",
            "Global step: 1088,loss: 0.030059678\n",
            "\n",
            "Global step: 1089,loss: 0.03376561\n",
            "\n",
            "Global step: 1090,loss: 0.026059354\n",
            "\n",
            "Global step: 1091,loss: 0.030388694\n",
            "\n",
            "Global step: 1092,loss: 0.035068594\n",
            "\n",
            "Global step: 1093,loss: 0.02507915\n",
            "\n",
            "Global step: 1094,loss: 0.027138855\n",
            "\n",
            "Global step: 1095,loss: 0.019796051\n",
            "\n",
            "Global step: 1096,loss: 0.025052922\n",
            "\n",
            "Global step: 1097,loss: 0.018809214\n",
            "\n",
            "Global step: 1098,loss: 0.0214566\n",
            "\n",
            "Global step: 1099,loss: 0.018975837\n",
            "\n",
            "Global step: 1100,loss: 0.02158751\n",
            "\n",
            "Global step: 1101,loss: 0.03282\n",
            "\n",
            "Global step: 1102,loss: 0.022182833\n",
            "\n",
            "Global step: 1103,loss: 0.021766152\n",
            "\n",
            "Global step: 1104,loss: 0.019422472\n",
            "\n",
            "Global step: 1105,loss: 0.018124819\n",
            "\n",
            "Global step: 1106,loss: 0.018845167\n",
            "\n",
            "Global step: 1107,loss: 0.023574557\n",
            "\n",
            "Global step: 1108,loss: 0.023123974\n",
            "\n",
            "Global step: 1109,loss: 0.016047766\n",
            "\n",
            "Global step: 1110,loss: 0.02028143\n",
            "\n",
            "Global step: 1111,loss: 0.031795584\n",
            "\n",
            "Global step: 1112,loss: 0.021442503\n",
            "\n",
            "Global step: 1113,loss: 0.01647779\n",
            "\n",
            "Global step: 1114,loss: 0.026085652\n",
            "\n",
            "Global step: 1115,loss: 0.023961972\n",
            "\n",
            "Global step: 1116,loss: 0.026172418\n",
            "\n",
            "Global step: 1117,loss: 0.02708975\n",
            "\n",
            "Global step: 1118,loss: 0.020303592\n",
            "\n",
            "Global step: 1119,loss: 0.021522615\n",
            "\n",
            "Global step: 1120,loss: 0.02734143\n",
            "\n",
            "Global step: 1121,loss: 0.020601053\n",
            "\n",
            "Global step: 1122,loss: 0.026932836\n",
            "\n",
            "Global step: 1123,loss: 0.019189358\n",
            "\n",
            "Global step: 1124,loss: 0.029401835\n",
            "\n",
            "Global step: 1125,loss: 0.029089186\n",
            "\n",
            "Global step: 1126,loss: 0.017338311\n",
            "\n",
            "Global step: 1127,loss: 0.022642512\n",
            "\n",
            "Global step: 1128,loss: 0.027676715\n",
            "\n",
            "Global step: 1129,loss: 0.021459393\n",
            "\n",
            "Global step: 1130,loss: 0.03013423\n",
            "\n",
            "Global step: 1131,loss: 0.021761626\n",
            "\n",
            "Global step: 1132,loss: 0.022817638\n",
            "\n",
            "Global step: 1133,loss: 0.022386443\n",
            "\n",
            "Global step: 1134,loss: 0.018005146\n",
            "\n",
            "Global step: 1135,loss: 0.025339372\n",
            "\n",
            "Global step: 1136,loss: 0.02106757\n",
            "\n",
            "Global step: 1137,loss: 0.026132913\n",
            "\n",
            "Global step: 1138,loss: 0.024256991\n",
            "\n",
            "Global step: 1139,loss: 0.019732121\n",
            "\n",
            "Global step: 1140,loss: 0.026881702\n",
            "\n",
            "Global step: 1141,loss: 0.029348547\n",
            "\n",
            "Global step: 1142,loss: 0.01966262\n",
            "\n",
            "Global step: 1143,loss: 0.027036771\n",
            "\n",
            "Global step: 1144,loss: 0.027945325\n",
            "\n",
            "Global step: 1145,loss: 0.023953967\n",
            "\n",
            "Global step: 1146,loss: 0.018879667\n",
            "\n",
            "Global step: 1147,loss: 0.02061963\n",
            "\n",
            "Global step: 1148,loss: 0.03017421\n",
            "\n",
            "Global step: 1149,loss: 0.019664366\n",
            "\n",
            "Global step: 1150,loss: 0.025128677\n",
            "\n",
            "Global step: 1151,loss: 0.01531135\n",
            "\n",
            "Global step: 1152,loss: 0.030511253\n",
            "\n",
            "Global step: 1153,loss: 0.022593569\n",
            "\n",
            "Global step: 1154,loss: 0.022348514\n",
            "\n",
            "Global step: 1155,loss: 0.01982197\n",
            "\n",
            "Global step: 1156,loss: 0.023174172\n",
            "\n",
            "Global step: 1157,loss: 0.022685582\n",
            "\n",
            "Global step: 1158,loss: 0.020483714\n",
            "\n",
            "Global step: 1159,loss: 0.02812723\n",
            "\n",
            "Global step: 1160,loss: 0.033793017\n",
            "\n",
            "Global step: 1161,loss: 0.01645768\n",
            "\n",
            "Global step: 1162,loss: 0.030131824\n",
            "\n",
            "Global step: 1163,loss: 0.024614014\n",
            "\n",
            "Global step: 1164,loss: 0.026259542\n",
            "\n",
            "Global step: 1165,loss: 0.028990258\n",
            "\n",
            "Global step: 1166,loss: 0.02129936\n",
            "\n",
            "Global step: 1167,loss: 0.025391784\n",
            "\n",
            "Global step: 1168,loss: 0.027335618\n",
            "\n",
            "Global step: 1169,loss: 0.021991797\n",
            "\n",
            "Global step: 1170,loss: 0.029998064\n",
            "\n",
            "Global step: 1171,loss: 0.028887887\n",
            "\n",
            "Global step: 1172,loss: 0.019908488\n",
            "\n",
            "Global step: 1173,loss: 0.022836227\n",
            "\n",
            "Global step: 1174,loss: 0.024388723\n",
            "\n",
            "Global step: 1175,loss: 0.037381068\n",
            "\n",
            "Global step: 1176,loss: 0.01644163\n",
            "\n",
            "Global step: 1177,loss: 0.021777028\n",
            "\n",
            "Global step: 1178,loss: 0.019947764\n",
            "\n",
            "Global step: 1179,loss: 0.035305314\n",
            "\n",
            "Global step: 1180,loss: 0.025035007\n",
            "\n",
            "Global step: 1181,loss: 0.022276007\n",
            "\n",
            "Global step: 1182,loss: 0.027570643\n",
            "\n",
            "Global step: 1183,loss: 0.01631782\n",
            "\n",
            "Global step: 1184,loss: 0.01765506\n",
            "\n",
            "Global step: 1185,loss: 0.03217082\n",
            "\n",
            "Global step: 1186,loss: 0.016791455\n",
            "\n",
            "Global step: 1187,loss: 0.01960677\n",
            "\n",
            "Global step: 1188,loss: 0.029382322\n",
            "\n",
            "Global step: 1189,loss: 0.03174339\n",
            "\n",
            "Global step: 1190,loss: 0.024868056\n",
            "\n",
            "Global step: 1191,loss: 0.024960816\n",
            "\n",
            "Global step: 1192,loss: 0.023019105\n",
            "\n",
            "Global step: 1193,loss: 0.024434011\n",
            "\n",
            "Global step: 1194,loss: 0.029515717\n",
            "\n",
            "Global step: 1195,loss: 0.017618574\n",
            "\n",
            "Global step: 1196,loss: 0.023145761\n",
            "\n",
            "Global step: 1197,loss: 0.016899403\n",
            "\n",
            "Global step: 1198,loss: 0.016253881\n",
            "\n",
            "Global step: 1199,loss: 0.022700869\n",
            "\n",
            "Global step: 1200,loss: 0.01632991\n",
            "\n",
            "Global step: 1201,loss: 0.019915326\n",
            "\n",
            "Global step: 1202,loss: 0.029661695\n",
            "\n",
            "Global step: 1203,loss: 0.024542749\n",
            "\n",
            "Global step: 1204,loss: 0.019498337\n",
            "\n",
            "Global step: 1205,loss: 0.02473668\n",
            "\n",
            "Global step: 1206,loss: 0.031884752\n",
            "\n",
            "Global step: 1207,loss: 0.018337132\n",
            "\n",
            "Global step: 1208,loss: 0.028064163\n",
            "\n",
            "Global step: 1209,loss: 0.020929331\n",
            "\n",
            "Global step: 1210,loss: 0.023277622\n",
            "\n",
            "Global step: 1211,loss: 0.019667227\n",
            "\n",
            "Global step: 1212,loss: 0.015654389\n",
            "\n",
            "Global step: 1213,loss: 0.02344148\n",
            "\n",
            "Global step: 1214,loss: 0.01699081\n",
            "\n",
            "Global step: 1215,loss: 0.028163359\n",
            "\n",
            "Global step: 1216,loss: 0.024771625\n",
            "\n",
            "Global step: 1217,loss: 0.01898582\n",
            "\n",
            "Global step: 1218,loss: 0.021076363\n",
            "\n",
            "Global step: 1219,loss: 0.024538895\n",
            "\n",
            "Global step: 1220,loss: 0.025033709\n",
            "\n",
            "Global step: 1221,loss: 0.01817273\n",
            "\n",
            "Global step: 1222,loss: 0.021648306\n",
            "\n",
            "Global step: 1223,loss: 0.02471631\n",
            "\n",
            "Global step: 1224,loss: 0.021985162\n",
            "\n",
            "Global step: 1225,loss: 0.020330336\n",
            "\n",
            "Global step: 1226,loss: 0.028107816\n",
            "\n",
            "Global step: 1227,loss: 0.022359828\n",
            "\n",
            "Global step: 1228,loss: 0.016803578\n",
            "\n",
            "Global step: 1229,loss: 0.023612458\n",
            "\n",
            "Global step: 1230,loss: 0.020419404\n",
            "\n",
            "Global step: 1231,loss: 0.022105724\n",
            "\n",
            "Global step: 1232,loss: 0.016212068\n",
            "\n",
            "Global step: 1233,loss: 0.01869196\n",
            "\n",
            "Global step: 1234,loss: 0.02461546\n",
            "\n",
            "Global step: 1235,loss: 0.01919308\n",
            "\n",
            "Global step: 1236,loss: 0.018307038\n",
            "\n",
            "Global step: 1237,loss: 0.019006152\n",
            "\n",
            "Global step: 1238,loss: 0.016884515\n",
            "\n",
            "Global step: 1239,loss: 0.017836016\n",
            "\n",
            "Global step: 1240,loss: 0.021255288\n",
            "\n",
            "Global step: 1241,loss: 0.024103023\n",
            "\n",
            "Global step: 1242,loss: 0.021950025\n",
            "\n",
            "Global step: 1243,loss: 0.022208411\n",
            "\n",
            "Global step: 1244,loss: 0.025341496\n",
            "\n",
            "Global step: 1245,loss: 0.025002373\n",
            "\n",
            "Global step: 1246,loss: 0.020940961\n",
            "\n",
            "Global step: 1247,loss: 0.022404935\n",
            "\n",
            "Global step: 1248,loss: 0.022011919\n",
            "\n",
            "Global step: 1249,loss: 0.018825956\n",
            "\n",
            "Global step: 1250,loss: 0.028872956\n",
            "\n",
            "Global step: 1251,loss: 0.025312927\n",
            "\n",
            "Global step: 1252,loss: 0.019414436\n",
            "\n",
            "Global step: 1253,loss: 0.0193752\n",
            "\n",
            "Global step: 1254,loss: 0.018883903\n",
            "\n",
            "Global step: 1255,loss: 0.015598858\n",
            "\n",
            "Global step: 1256,loss: 0.01873463\n",
            "\n",
            "Global step: 1257,loss: 0.019647934\n",
            "\n",
            "Global step: 1258,loss: 0.020409383\n",
            "\n",
            "Global step: 1259,loss: 0.019666558\n",
            "\n",
            "Global step: 1260,loss: 0.02984386\n",
            "\n",
            "Global step: 1261,loss: 0.015988437\n",
            "\n",
            "Global step: 1262,loss: 0.018185608\n",
            "\n",
            "Global step: 1263,loss: 0.01628438\n",
            "\n",
            "Global step: 1264,loss: 0.022890488\n",
            "\n",
            "Global step: 1265,loss: 0.017797314\n",
            "\n",
            "Global step: 1266,loss: 0.020106442\n",
            "\n",
            "Global step: 1267,loss: 0.016463764\n",
            "\n",
            "Global step: 1268,loss: 0.018579323\n",
            "\n",
            "Global step: 1269,loss: 0.027880417\n",
            "\n",
            "Global step: 1270,loss: 0.022519436\n",
            "\n",
            "Global step: 1271,loss: 0.017890068\n",
            "\n",
            "Global step: 1272,loss: 0.021414377\n",
            "\n",
            "Global step: 1273,loss: 0.024307463\n",
            "\n",
            "Global step: 1274,loss: 0.02126872\n",
            "\n",
            "Global step: 1275,loss: 0.023246638\n",
            "\n",
            "Global step: 1276,loss: 0.015644103\n",
            "\n",
            "Global step: 1277,loss: 0.01962524\n",
            "\n",
            "Global step: 1278,loss: 0.020442028\n",
            "\n",
            "Global step: 1279,loss: 0.019266816\n",
            "\n",
            "Global step: 1280,loss: 0.016996661\n",
            "\n",
            "Global step: 1281,loss: 0.018518023\n",
            "\n",
            "Global step: 1282,loss: 0.016826596\n",
            "\n",
            "Global step: 1283,loss: 0.01827911\n",
            "\n",
            "Global step: 1284,loss: 0.031228542\n",
            "\n",
            "Global step: 1285,loss: 0.020913871\n",
            "\n",
            "Global step: 1286,loss: 0.037189417\n",
            "\n",
            "Global step: 1287,loss: 0.02637731\n",
            "\n",
            "Global step: 1288,loss: 0.018340051\n",
            "\n",
            "Global step: 1289,loss: 0.01534454\n",
            "\n",
            "Global step: 1290,loss: 0.027239049\n",
            "\n",
            "Global step: 1291,loss: 0.027788598\n",
            "\n",
            "Global step: 1292,loss: 0.024596388\n",
            "\n",
            "Global step: 1293,loss: 0.018611979\n",
            "\n",
            "Global step: 1294,loss: 0.015455881\n",
            "\n",
            "Global step: 1295,loss: 0.022731436\n",
            "\n",
            "Global step: 1296,loss: 0.020030972\n",
            "\n",
            "Global step: 1297,loss: 0.018955031\n",
            "\n",
            "Global step: 1298,loss: 0.026984327\n",
            "\n",
            "Global step: 1299,loss: 0.019678295\n",
            "\n",
            "Global step: 1300,loss: 0.021695077\n",
            "\n",
            "Global step: 1301,loss: 0.029822322\n",
            "\n",
            "Global step: 1302,loss: 0.0353366\n",
            "\n",
            "Global step: 1303,loss: 0.019431587\n",
            "\n",
            "Global step: 1304,loss: 0.01649141\n",
            "\n",
            "Global step: 1305,loss: 0.015517818\n",
            "\n",
            "Global step: 1306,loss: 0.02220406\n",
            "\n",
            "Global step: 1307,loss: 0.02251202\n",
            "\n",
            "Global step: 1308,loss: 0.023001935\n",
            "\n",
            "Global step: 1309,loss: 0.017581604\n",
            "\n",
            "Global step: 1310,loss: 0.022967037\n",
            "\n",
            "Global step: 1311,loss: 0.020706795\n",
            "\n",
            "Global step: 1312,loss: 0.025291216\n",
            "\n",
            "Global step: 1313,loss: 0.019915815\n",
            "\n",
            "Global step: 1314,loss: 0.018069565\n",
            "\n",
            "Global step: 1315,loss: 0.020736853\n",
            "\n",
            "Global step: 1316,loss: 0.02052889\n",
            "\n",
            "Global step: 1317,loss: 0.017953562\n",
            "\n",
            "Global step: 1318,loss: 0.034210857\n",
            "\n",
            "Global step: 1319,loss: 0.015895009\n",
            "\n",
            "Global step: 1320,loss: 0.029439531\n",
            "\n",
            "Global step: 1321,loss: 0.025161736\n",
            "\n",
            "Global step: 1322,loss: 0.018460657\n",
            "\n",
            "Global step: 1323,loss: 0.016509023\n",
            "\n",
            "Global step: 1324,loss: 0.021599058\n",
            "\n",
            "Global step: 1325,loss: 0.019893918\n",
            "\n",
            "Global step: 1326,loss: 0.026929751\n",
            "\n",
            "Global step: 1327,loss: 0.017898152\n",
            "\n",
            "Global step: 1328,loss: 0.021300012\n",
            "\n",
            "Global step: 1329,loss: 0.021314286\n",
            "\n",
            "Global step: 1330,loss: 0.019009778\n",
            "\n",
            "Global step: 1331,loss: 0.016882237\n",
            "\n",
            "Global step: 1332,loss: 0.015944935\n",
            "\n",
            "Global step: 1333,loss: 0.021180442\n",
            "\n",
            "Global step: 1334,loss: 0.023448912\n",
            "\n",
            "Global step: 1335,loss: 0.018770248\n",
            "\n",
            "Global step: 1336,loss: 0.03216362\n",
            "\n",
            "Global step: 1337,loss: 0.029490372\n",
            "\n",
            "Global step: 1338,loss: 0.019747488\n",
            "\n",
            "Global step: 1339,loss: 0.0312809\n",
            "\n",
            "Global step: 1340,loss: 0.021717511\n",
            "\n",
            "Global step: 1341,loss: 0.032947514\n",
            "\n",
            "Global step: 1342,loss: 0.028026404\n",
            "\n",
            "Global step: 1343,loss: 0.020460634\n",
            "\n",
            "Global step: 1344,loss: 0.019174073\n",
            "\n",
            "Global step: 1345,loss: 0.025707282\n",
            "\n",
            "Global step: 1346,loss: 0.027849719\n",
            "\n",
            "Global step: 1347,loss: 0.020691682\n",
            "\n",
            "Global step: 1348,loss: 0.03324386\n",
            "\n",
            "Global step: 1349,loss: 0.02369316\n",
            "\n",
            "Global step: 1350,loss: 0.019362003\n",
            "\n",
            "Global step: 1351,loss: 0.016956132\n",
            "\n",
            "Global step: 1352,loss: 0.018415073\n",
            "\n",
            "Global step: 1353,loss: 0.021624334\n",
            "\n",
            "Global step: 1354,loss: 0.029021304\n",
            "\n",
            "Global step: 1355,loss: 0.021246102\n",
            "\n",
            "Global step: 1356,loss: 0.016364492\n",
            "\n",
            "Global step: 1357,loss: 0.021465965\n",
            "\n",
            "Global step: 1358,loss: 0.020841619\n",
            "\n",
            "Global step: 1359,loss: 0.014809892\n",
            "\n",
            "Global step: 1360,loss: 0.018616188\n",
            "\n",
            "Global step: 1361,loss: 0.026942216\n",
            "\n",
            "Global step: 1362,loss: 0.01527697\n",
            "\n",
            "Global step: 1363,loss: 0.018141933\n",
            "\n",
            "Global step: 1364,loss: 0.016378397\n",
            "\n",
            "Global step: 1365,loss: 0.017358549\n",
            "\n",
            "Global step: 1366,loss: 0.01770291\n",
            "\n",
            "Global step: 1367,loss: 0.019732513\n",
            "\n",
            "Global step: 1368,loss: 0.022040458\n",
            "\n",
            "Global step: 1369,loss: 0.026369665\n",
            "\n",
            "Global step: 1370,loss: 0.034566186\n",
            "\n",
            "Global step: 1371,loss: 0.014357762\n",
            "\n",
            "Global step: 1372,loss: 0.02016012\n",
            "\n",
            "Global step: 1373,loss: 0.032743096\n",
            "\n",
            "Global step: 1374,loss: 0.02360114\n",
            "\n",
            "Global step: 1375,loss: 0.02013692\n",
            "\n",
            "Global step: 1376,loss: 0.023349576\n",
            "\n",
            "Global step: 1377,loss: 0.01814411\n",
            "\n",
            "Global step: 1378,loss: 0.021494523\n",
            "\n",
            "Global step: 1379,loss: 0.015591591\n",
            "\n",
            "Global step: 1380,loss: 0.018080566\n",
            "\n",
            "Global step: 1381,loss: 0.016307006\n",
            "\n",
            "Global step: 1382,loss: 0.013735265\n",
            "\n",
            "Global step: 1383,loss: 0.019277377\n",
            "\n",
            "Global step: 1384,loss: 0.03329809\n",
            "\n",
            "Global step: 1385,loss: 0.01560525\n",
            "\n",
            "Global step: 1386,loss: 0.032818966\n",
            "\n",
            "Global step: 1387,loss: 0.020880193\n",
            "\n",
            "Global step: 1388,loss: 0.024203347\n",
            "\n",
            "Global step: 1389,loss: 0.01728741\n",
            "\n",
            "Global step: 1390,loss: 0.015962256\n",
            "\n",
            "Global step: 1391,loss: 0.02773353\n",
            "\n",
            "Global step: 1392,loss: 0.017847093\n",
            "\n",
            "Global step: 1393,loss: 0.016272686\n",
            "\n",
            "Global step: 1394,loss: 0.020177893\n",
            "\n",
            "Global step: 1395,loss: 0.016565643\n",
            "\n",
            "Global step: 1396,loss: 0.023468342\n",
            "\n",
            "Global step: 1397,loss: 0.018986188\n",
            "\n",
            "Global step: 1398,loss: 0.022467136\n",
            "\n",
            "Global step: 1399,loss: 0.020463478\n",
            "\n",
            "Global step: 1400,loss: 0.019602422\n",
            "\n",
            "Global step: 1401,loss: 0.028740544\n",
            "\n",
            "Global step: 1402,loss: 0.017700229\n",
            "\n",
            "Global step: 1403,loss: 0.016171575\n",
            "\n",
            "Global step: 1404,loss: 0.018966159\n",
            "\n",
            "Global step: 1405,loss: 0.015563279\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1405,Val_Loss: 0.019136186068256695,  Val_acc: 0.9962606837606838 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:46:23.128675 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/15:\n",
            "Global step: 1406,loss: 0.021252993\n",
            "\n",
            "Global step: 1407,loss: 0.025403969\n",
            "\n",
            "Global step: 1408,loss: 0.015596617\n",
            "\n",
            "Global step: 1409,loss: 0.0173623\n",
            "\n",
            "Global step: 1410,loss: 0.015532533\n",
            "\n",
            "Global step: 1411,loss: 0.024644937\n",
            "\n",
            "Global step: 1412,loss: 0.015815796\n",
            "\n",
            "Global step: 1413,loss: 0.018279338\n",
            "\n",
            "Global step: 1414,loss: 0.017577795\n",
            "\n",
            "Global step: 1415,loss: 0.018759191\n",
            "\n",
            "Global step: 1416,loss: 0.023949645\n",
            "\n",
            "Global step: 1417,loss: 0.014383392\n",
            "\n",
            "Global step: 1418,loss: 0.022890422\n",
            "\n",
            "Global step: 1419,loss: 0.018345343\n",
            "\n",
            "Global step: 1420,loss: 0.014857932\n",
            "\n",
            "Global step: 1421,loss: 0.01703914\n",
            "\n",
            "Global step: 1422,loss: 0.01961622\n",
            "\n",
            "Global step: 1423,loss: 0.015528699\n",
            "\n",
            "Global step: 1424,loss: 0.023434754\n",
            "\n",
            "Global step: 1425,loss: 0.018274948\n",
            "\n",
            "Global step: 1426,loss: 0.016210869\n",
            "\n",
            "Global step: 1427,loss: 0.018244727\n",
            "\n",
            "Global step: 1428,loss: 0.01881475\n",
            "\n",
            "Global step: 1429,loss: 0.014119632\n",
            "\n",
            "Global step: 1430,loss: 0.017941173\n",
            "\n",
            "Global step: 1431,loss: 0.01606912\n",
            "\n",
            "Global step: 1432,loss: 0.020246133\n",
            "\n",
            "Global step: 1433,loss: 0.021534111\n",
            "\n",
            "Global step: 1434,loss: 0.0134170335\n",
            "\n",
            "Global step: 1435,loss: 0.017508931\n",
            "\n",
            "Global step: 1436,loss: 0.0228661\n",
            "\n",
            "Global step: 1437,loss: 0.01665036\n",
            "\n",
            "Global step: 1438,loss: 0.02035788\n",
            "\n",
            "Global step: 1439,loss: 0.016541101\n",
            "\n",
            "Global step: 1440,loss: 0.017351385\n",
            "\n",
            "Global step: 1441,loss: 0.017812088\n",
            "\n",
            "Global step: 1442,loss: 0.019085625\n",
            "\n",
            "Global step: 1443,loss: 0.014273034\n",
            "\n",
            "Global step: 1444,loss: 0.01661098\n",
            "\n",
            "Global step: 1445,loss: 0.015311157\n",
            "\n",
            "Global step: 1446,loss: 0.021130746\n",
            "\n",
            "Global step: 1447,loss: 0.016367992\n",
            "\n",
            "Global step: 1448,loss: 0.014470635\n",
            "\n",
            "Global step: 1449,loss: 0.021598859\n",
            "\n",
            "Global step: 1450,loss: 0.017144646\n",
            "\n",
            "Global step: 1451,loss: 0.019858886\n",
            "\n",
            "Global step: 1452,loss: 0.0216665\n",
            "\n",
            "Global step: 1453,loss: 0.015712261\n",
            "\n",
            "Global step: 1454,loss: 0.015090646\n",
            "\n",
            "Global step: 1455,loss: 0.02890847\n",
            "\n",
            "Global step: 1456,loss: 0.021055173\n",
            "\n",
            "Global step: 1457,loss: 0.014158171\n",
            "\n",
            "Global step: 1458,loss: 0.021680664\n",
            "\n",
            "Global step: 1459,loss: 0.01986232\n",
            "\n",
            "Global step: 1460,loss: 0.016127445\n",
            "\n",
            "Global step: 1461,loss: 0.019511843\n",
            "\n",
            "Global step: 1462,loss: 0.01858471\n",
            "\n",
            "Global step: 1463,loss: 0.014871414\n",
            "\n",
            "Global step: 1464,loss: 0.0138821285\n",
            "\n",
            "Global step: 1465,loss: 0.014925251\n",
            "\n",
            "Global step: 1466,loss: 0.017969994\n",
            "\n",
            "Global step: 1467,loss: 0.020406988\n",
            "\n",
            "Global step: 1468,loss: 0.01591426\n",
            "\n",
            "Global step: 1469,loss: 0.025270652\n",
            "\n",
            "Global step: 1470,loss: 0.016570646\n",
            "\n",
            "Global step: 1471,loss: 0.0147643015\n",
            "\n",
            "Global step: 1472,loss: 0.017710892\n",
            "\n",
            "Global step: 1473,loss: 0.021240916\n",
            "\n",
            "Global step: 1474,loss: 0.01892944\n",
            "\n",
            "Global step: 1475,loss: 0.015264\n",
            "\n",
            "Global step: 1476,loss: 0.017283883\n",
            "\n",
            "Global step: 1477,loss: 0.017415997\n",
            "\n",
            "Global step: 1478,loss: 0.016001983\n",
            "\n",
            "Global step: 1479,loss: 0.017182797\n",
            "\n",
            "Global step: 1480,loss: 0.02063042\n",
            "\n",
            "Global step: 1481,loss: 0.01672492\n",
            "\n",
            "Global step: 1482,loss: 0.016541354\n",
            "\n",
            "Global step: 1483,loss: 0.017643515\n",
            "\n",
            "Global step: 1484,loss: 0.013644488\n",
            "\n",
            "Global step: 1485,loss: 0.018290242\n",
            "\n",
            "Global step: 1486,loss: 0.017254036\n",
            "\n",
            "Global step: 1487,loss: 0.015239311\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.54164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:46:35.222821 139837015512832 supervisor.py:1099] global_step/sec: 6.54164\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 1488.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:46:35.323211 139837023905536 supervisor.py:1050] Recording summary at step 1488.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1488,loss: 0.020108435\n",
            "\n",
            "Global step: 1489,loss: 0.01804442\n",
            "\n",
            "Global step: 1490,loss: 0.015870107\n",
            "\n",
            "Global step: 1491,loss: 0.015407154\n",
            "\n",
            "Global step: 1492,loss: 0.016966254\n",
            "\n",
            "Global step: 1493,loss: 0.017553538\n",
            "\n",
            "Global step: 1494,loss: 0.01618515\n",
            "\n",
            "Global step: 1495,loss: 0.014978983\n",
            "\n",
            "Global step: 1496,loss: 0.017524526\n",
            "\n",
            "Global step: 1497,loss: 0.014850864\n",
            "\n",
            "Global step: 1498,loss: 0.0131621435\n",
            "\n",
            "Global step: 1499,loss: 0.015541826\n",
            "\n",
            "Global step: 1500,loss: 0.018400434\n",
            "\n",
            "Global step: 1501,loss: 0.023237007\n",
            "\n",
            "Global step: 1502,loss: 0.017136443\n",
            "\n",
            "Global step: 1503,loss: 0.019669004\n",
            "\n",
            "Global step: 1504,loss: 0.019847976\n",
            "\n",
            "Global step: 1505,loss: 0.018369768\n",
            "\n",
            "Global step: 1506,loss: 0.014640457\n",
            "\n",
            "Global step: 1507,loss: 0.018852541\n",
            "\n",
            "Global step: 1508,loss: 0.014658537\n",
            "\n",
            "Global step: 1509,loss: 0.026891898\n",
            "\n",
            "Global step: 1510,loss: 0.022472791\n",
            "\n",
            "Global step: 1511,loss: 0.0146475565\n",
            "\n",
            "Global step: 1512,loss: 0.027724463\n",
            "\n",
            "Global step: 1513,loss: 0.018641382\n",
            "\n",
            "Global step: 1514,loss: 0.018647952\n",
            "\n",
            "Global step: 1515,loss: 0.025130797\n",
            "\n",
            "Global step: 1516,loss: 0.024616214\n",
            "\n",
            "Global step: 1517,loss: 0.02612761\n",
            "\n",
            "Global step: 1518,loss: 0.016335364\n",
            "\n",
            "Global step: 1519,loss: 0.023477226\n",
            "\n",
            "Global step: 1520,loss: 0.014069862\n",
            "\n",
            "Global step: 1521,loss: 0.035531946\n",
            "\n",
            "Global step: 1522,loss: 0.022514842\n",
            "\n",
            "Global step: 1523,loss: 0.022857696\n",
            "\n",
            "Global step: 1524,loss: 0.021132085\n",
            "\n",
            "Global step: 1525,loss: 0.024168096\n",
            "\n",
            "Global step: 1526,loss: 0.015749834\n",
            "\n",
            "Global step: 1527,loss: 0.014086159\n",
            "\n",
            "Global step: 1528,loss: 0.018400446\n",
            "\n",
            "Global step: 1529,loss: 0.022081867\n",
            "\n",
            "Global step: 1530,loss: 0.0173163\n",
            "\n",
            "Global step: 1531,loss: 0.017528769\n",
            "\n",
            "Global step: 1532,loss: 0.014044883\n",
            "\n",
            "Global step: 1533,loss: 0.015593117\n",
            "\n",
            "Global step: 1534,loss: 0.015216872\n",
            "\n",
            "Global step: 1535,loss: 0.01464493\n",
            "\n",
            "Global step: 1536,loss: 0.034494765\n",
            "\n",
            "Global step: 1537,loss: 0.018978864\n",
            "\n",
            "Global step: 1538,loss: 0.026101235\n",
            "\n",
            "Global step: 1539,loss: 0.021781059\n",
            "\n",
            "Global step: 1540,loss: 0.019090127\n",
            "\n",
            "Global step: 1541,loss: 0.01668553\n",
            "\n",
            "Global step: 1542,loss: 0.016846694\n",
            "\n",
            "Global step: 1543,loss: 0.024329845\n",
            "\n",
            "Global step: 1544,loss: 0.016995635\n",
            "\n",
            "Global step: 1545,loss: 0.02352162\n",
            "\n",
            "Global step: 1546,loss: 0.022966191\n",
            "\n",
            "Global step: 1547,loss: 0.029913738\n",
            "\n",
            "Global step: 1548,loss: 0.017802818\n",
            "\n",
            "Global step: 1549,loss: 0.023143705\n",
            "\n",
            "Global step: 1550,loss: 0.0132859545\n",
            "\n",
            "Global step: 1551,loss: 0.017519321\n",
            "\n",
            "Global step: 1552,loss: 0.02476542\n",
            "\n",
            "Global step: 1553,loss: 0.016625946\n",
            "\n",
            "Global step: 1554,loss: 0.023578424\n",
            "\n",
            "Global step: 1555,loss: 0.021388631\n",
            "\n",
            "Global step: 1556,loss: 0.023863658\n",
            "\n",
            "Global step: 1557,loss: 0.015647888\n",
            "\n",
            "Global step: 1558,loss: 0.027385175\n",
            "\n",
            "Global step: 1559,loss: 0.016848579\n",
            "\n",
            "Global step: 1560,loss: 0.01984572\n",
            "\n",
            "Global step: 1561,loss: 0.013909734\n",
            "\n",
            "Global step: 1562,loss: 0.018319203\n",
            "\n",
            "Global step: 1563,loss: 0.012803473\n",
            "\n",
            "Global step: 1564,loss: 0.015707444\n",
            "\n",
            "Global step: 1565,loss: 0.018856611\n",
            "\n",
            "Global step: 1566,loss: 0.014655334\n",
            "\n",
            "Global step: 1567,loss: 0.01873406\n",
            "\n",
            "Global step: 1568,loss: 0.018881349\n",
            "\n",
            "Global step: 1569,loss: 0.020024318\n",
            "\n",
            "Global step: 1570,loss: 0.016469749\n",
            "\n",
            "Global step: 1571,loss: 0.02492232\n",
            "\n",
            "Global step: 1572,loss: 0.02272221\n",
            "\n",
            "Global step: 1573,loss: 0.023021277\n",
            "\n",
            "Global step: 1574,loss: 0.015875436\n",
            "\n",
            "Global step: 1575,loss: 0.014312927\n",
            "\n",
            "Global step: 1576,loss: 0.019941278\n",
            "\n",
            "Global step: 1577,loss: 0.017304592\n",
            "\n",
            "Global step: 1578,loss: 0.019086711\n",
            "\n",
            "Global step: 1579,loss: 0.0136204185\n",
            "\n",
            "Global step: 1580,loss: 0.016818192\n",
            "\n",
            "Global step: 1581,loss: 0.016646963\n",
            "\n",
            "Global step: 1582,loss: 0.015579206\n",
            "\n",
            "Global step: 1583,loss: 0.015857827\n",
            "\n",
            "Global step: 1584,loss: 0.014046867\n",
            "\n",
            "Global step: 1585,loss: 0.023926407\n",
            "\n",
            "Global step: 1586,loss: 0.022877391\n",
            "\n",
            "Global step: 1587,loss: 0.015437868\n",
            "\n",
            "Global step: 1588,loss: 0.017476453\n",
            "\n",
            "Global step: 1589,loss: 0.017667875\n",
            "\n",
            "Global step: 1590,loss: 0.029001288\n",
            "\n",
            "Global step: 1591,loss: 0.018270157\n",
            "\n",
            "Global step: 1592,loss: 0.01692408\n",
            "\n",
            "Global step: 1593,loss: 0.017094135\n",
            "\n",
            "Global step: 1594,loss: 0.0154824\n",
            "\n",
            "Global step: 1595,loss: 0.023658147\n",
            "\n",
            "Global step: 1596,loss: 0.014083781\n",
            "\n",
            "Global step: 1597,loss: 0.017161768\n",
            "\n",
            "Global step: 1598,loss: 0.020028573\n",
            "\n",
            "Global step: 1599,loss: 0.014516067\n",
            "\n",
            "Global step: 1600,loss: 0.02154814\n",
            "\n",
            "Global step: 1601,loss: 0.018212\n",
            "\n",
            "Global step: 1602,loss: 0.014131472\n",
            "\n",
            "Global step: 1603,loss: 0.025764104\n",
            "\n",
            "Global step: 1604,loss: 0.014643284\n",
            "\n",
            "Global step: 1605,loss: 0.017006114\n",
            "\n",
            "Global step: 1606,loss: 0.014074074\n",
            "\n",
            "Global step: 1607,loss: 0.014481834\n",
            "\n",
            "Global step: 1608,loss: 0.01497221\n",
            "\n",
            "Global step: 1609,loss: 0.018330667\n",
            "\n",
            "Global step: 1610,loss: 0.014251762\n",
            "\n",
            "Global step: 1611,loss: 0.022629455\n",
            "\n",
            "Global step: 1612,loss: 0.02015985\n",
            "\n",
            "Global step: 1613,loss: 0.01384499\n",
            "\n",
            "Global step: 1614,loss: 0.016465295\n",
            "\n",
            "Global step: 1615,loss: 0.026871212\n",
            "\n",
            "Global step: 1616,loss: 0.019614283\n",
            "\n",
            "Global step: 1617,loss: 0.027875878\n",
            "\n",
            "Global step: 1618,loss: 0.024713334\n",
            "\n",
            "Global step: 1619,loss: 0.018759217\n",
            "\n",
            "Global step: 1620,loss: 0.01748744\n",
            "\n",
            "Global step: 1621,loss: 0.019540727\n",
            "\n",
            "Global step: 1622,loss: 0.027577268\n",
            "\n",
            "Global step: 1623,loss: 0.016004678\n",
            "\n",
            "Global step: 1624,loss: 0.016678857\n",
            "\n",
            "Global step: 1625,loss: 0.020125777\n",
            "\n",
            "Global step: 1626,loss: 0.016806774\n",
            "\n",
            "Global step: 1627,loss: 0.017745845\n",
            "\n",
            "Global step: 1628,loss: 0.018673636\n",
            "\n",
            "Global step: 1629,loss: 0.020012189\n",
            "\n",
            "Global step: 1630,loss: 0.021939585\n",
            "\n",
            "Global step: 1631,loss: 0.023570357\n",
            "\n",
            "Global step: 1632,loss: 0.023878805\n",
            "\n",
            "Global step: 1633,loss: 0.020623107\n",
            "\n",
            "Global step: 1634,loss: 0.0167145\n",
            "\n",
            "Global step: 1635,loss: 0.015550837\n",
            "\n",
            "Global step: 1636,loss: 0.01569097\n",
            "\n",
            "Global step: 1637,loss: 0.018182587\n",
            "\n",
            "Global step: 1638,loss: 0.022358976\n",
            "\n",
            "Global step: 1639,loss: 0.01721285\n",
            "\n",
            "Global step: 1640,loss: 0.02194177\n",
            "\n",
            "Global step: 1641,loss: 0.016278952\n",
            "\n",
            "Global step: 1642,loss: 0.015102261\n",
            "\n",
            "Global step: 1643,loss: 0.01899926\n",
            "\n",
            "Global step: 1644,loss: 0.016236309\n",
            "\n",
            "Global step: 1645,loss: 0.022037871\n",
            "\n",
            "Global step: 1646,loss: 0.015317513\n",
            "\n",
            "Global step: 1647,loss: 0.016850997\n",
            "\n",
            "Global step: 1648,loss: 0.01357289\n",
            "\n",
            "Global step: 1649,loss: 0.017256314\n",
            "\n",
            "Global step: 1650,loss: 0.016767051\n",
            "\n",
            "Global step: 1651,loss: 0.016101453\n",
            "\n",
            "Global step: 1652,loss: 0.017364455\n",
            "\n",
            "Global step: 1653,loss: 0.018705651\n",
            "\n",
            "Global step: 1654,loss: 0.025464436\n",
            "\n",
            "Global step: 1655,loss: 0.012656223\n",
            "\n",
            "Global step: 1656,loss: 0.019675858\n",
            "\n",
            "Global step: 1657,loss: 0.014234049\n",
            "\n",
            "Global step: 1658,loss: 0.013792779\n",
            "\n",
            "Global step: 1659,loss: 0.013717829\n",
            "\n",
            "Global step: 1660,loss: 0.018452737\n",
            "\n",
            "Global step: 1661,loss: 0.016583674\n",
            "\n",
            "Global step: 1662,loss: 0.018517867\n",
            "\n",
            "Global step: 1663,loss: 0.014723861\n",
            "\n",
            "Global step: 1664,loss: 0.023895284\n",
            "\n",
            "Global step: 1665,loss: 0.024350692\n",
            "\n",
            "Global step: 1666,loss: 0.022806268\n",
            "\n",
            "Global step: 1667,loss: 0.018076815\n",
            "\n",
            "Global step: 1668,loss: 0.017704286\n",
            "\n",
            "Global step: 1669,loss: 0.022881769\n",
            "\n",
            "Global step: 1670,loss: 0.019630667\n",
            "\n",
            "Global step: 1671,loss: 0.018740844\n",
            "\n",
            "Global step: 1672,loss: 0.014457662\n",
            "\n",
            "Global step: 1673,loss: 0.014129985\n",
            "\n",
            "Global step: 1674,loss: 0.015446434\n",
            "\n",
            "Global step: 1675,loss: 0.015677083\n",
            "\n",
            "Global step: 1676,loss: 0.016945105\n",
            "\n",
            "Global step: 1677,loss: 0.017317377\n",
            "\n",
            "Global step: 1678,loss: 0.013069477\n",
            "\n",
            "Global step: 1679,loss: 0.01791964\n",
            "\n",
            "Global step: 1680,loss: 0.016497187\n",
            "\n",
            "Global step: 1681,loss: 0.020208884\n",
            "\n",
            "Global step: 1682,loss: 0.0141369235\n",
            "\n",
            "Global step: 1683,loss: 0.0126322815\n",
            "\n",
            "Global step: 1684,loss: 0.016743377\n",
            "\n",
            "Global step: 1685,loss: 0.020151861\n",
            "\n",
            "Global step: 1686,loss: 0.020840578\n",
            "\n",
            "Global step: 1687,loss: 0.021874089\n",
            "\n",
            "Global step: 1688,loss: 0.0151483705\n",
            "\n",
            "Global step: 1689,loss: 0.021055285\n",
            "\n",
            "Global step: 1690,loss: 0.014396445\n",
            "\n",
            "Global step: 1691,loss: 0.017431946\n",
            "\n",
            "Global step: 1692,loss: 0.0130789075\n",
            "\n",
            "Global step: 1693,loss: 0.016093973\n",
            "\n",
            "Global step: 1694,loss: 0.029470127\n",
            "\n",
            "Global step: 1695,loss: 0.017072126\n",
            "\n",
            "Global step: 1696,loss: 0.017010558\n",
            "\n",
            "Global step: 1697,loss: 0.023163434\n",
            "\n",
            "Global step: 1698,loss: 0.017236644\n",
            "\n",
            "Global step: 1699,loss: 0.019210193\n",
            "\n",
            "Global step: 1700,loss: 0.017206678\n",
            "\n",
            "Global step: 1701,loss: 0.013362222\n",
            "\n",
            "Global step: 1702,loss: 0.028039413\n",
            "\n",
            "Global step: 1703,loss: 0.013518634\n",
            "\n",
            "Global step: 1704,loss: 0.014446016\n",
            "\n",
            "Global step: 1705,loss: 0.026469432\n",
            "\n",
            "Global step: 1706,loss: 0.01593872\n",
            "\n",
            "Global step: 1707,loss: 0.017032322\n",
            "\n",
            "Global step: 1708,loss: 0.012048967\n",
            "\n",
            "Global step: 1709,loss: 0.016552286\n",
            "\n",
            "Global step: 1710,loss: 0.015771775\n",
            "\n",
            "Global step: 1711,loss: 0.021944255\n",
            "\n",
            "Global step: 1712,loss: 0.022271689\n",
            "\n",
            "Global step: 1713,loss: 0.015430819\n",
            "\n",
            "Global step: 1714,loss: 0.014808063\n",
            "\n",
            "Global step: 1715,loss: 0.013553751\n",
            "\n",
            "Global step: 1716,loss: 0.016282894\n",
            "\n",
            "Global step: 1717,loss: 0.01550596\n",
            "\n",
            "Global step: 1718,loss: 0.016576175\n",
            "\n",
            "Global step: 1719,loss: 0.02550817\n",
            "\n",
            "Global step: 1720,loss: 0.022960216\n",
            "\n",
            "Global step: 1721,loss: 0.01680753\n",
            "\n",
            "Global step: 1722,loss: 0.013760734\n",
            "\n",
            "Global step: 1723,loss: 0.016199816\n",
            "\n",
            "Global step: 1724,loss: 0.016507952\n",
            "\n",
            "Global step: 1725,loss: 0.025666676\n",
            "\n",
            "Global step: 1726,loss: 0.015396068\n",
            "\n",
            "Global step: 1727,loss: 0.012718046\n",
            "\n",
            "Global step: 1728,loss: 0.019540608\n",
            "\n",
            "Global step: 1729,loss: 0.016516423\n",
            "\n",
            "Global step: 1730,loss: 0.013320658\n",
            "\n",
            "Global step: 1731,loss: 0.019112224\n",
            "\n",
            "Global step: 1732,loss: 0.014754356\n",
            "\n",
            "Global step: 1733,loss: 0.0135222655\n",
            "\n",
            "Global step: 1734,loss: 0.019983172\n",
            "\n",
            "Global step: 1735,loss: 0.014380077\n",
            "\n",
            "Global step: 1736,loss: 0.016150102\n",
            "\n",
            "Global step: 1737,loss: 0.017223617\n",
            "\n",
            "Global step: 1738,loss: 0.015262235\n",
            "\n",
            "Global step: 1739,loss: 0.02469137\n",
            "\n",
            "Global step: 1740,loss: 0.012591248\n",
            "\n",
            "Global step: 1741,loss: 0.014019276\n",
            "\n",
            "Global step: 1742,loss: 0.01421722\n",
            "\n",
            "Global step: 1743,loss: 0.026269903\n",
            "\n",
            "Global step: 1744,loss: 0.03240236\n",
            "\n",
            "Global step: 1745,loss: 0.023734532\n",
            "\n",
            "Global step: 1746,loss: 0.025843237\n",
            "\n",
            "Global step: 1747,loss: 0.015974423\n",
            "\n",
            "Global step: 1748,loss: 0.016539432\n",
            "\n",
            "Global step: 1749,loss: 0.019172184\n",
            "\n",
            "Global step: 1750,loss: 0.015164813\n",
            "\n",
            "Global step: 1751,loss: 0.015232075\n",
            "\n",
            "Global step: 1752,loss: 0.011853058\n",
            "\n",
            "Global step: 1753,loss: 0.020135168\n",
            "\n",
            "Global step: 1754,loss: 0.013749075\n",
            "\n",
            "Global step: 1755,loss: 0.021893723\n",
            "\n",
            "Global step: 1756,loss: 0.016601864\n",
            "\n",
            "Global step: 1757,loss: 0.017621515\n",
            "\n",
            "Global step: 1758,loss: 0.013891596\n",
            "\n",
            "Global step: 1759,loss: 0.018633317\n",
            "\n",
            "Global step: 1760,loss: 0.023020755\n",
            "\n",
            "Global step: 1761,loss: 0.02618086\n",
            "\n",
            "Global step: 1762,loss: 0.014045032\n",
            "\n",
            "Global step: 1763,loss: 0.015804425\n",
            "\n",
            "Global step: 1764,loss: 0.015552322\n",
            "\n",
            "Global step: 1765,loss: 0.015558265\n",
            "\n",
            "Global step: 1766,loss: 0.017634023\n",
            "\n",
            "Global step: 1767,loss: 0.013478544\n",
            "\n",
            "Global step: 1768,loss: 0.015399991\n",
            "\n",
            "Global step: 1769,loss: 0.0136238\n",
            "\n",
            "Global step: 1770,loss: 0.015769558\n",
            "\n",
            "Global step: 1771,loss: 0.012553389\n",
            "\n",
            "Global step: 1772,loss: 0.016444653\n",
            "\n",
            "Global step: 1773,loss: 0.018630482\n",
            "\n",
            "Global step: 1774,loss: 0.014768222\n",
            "\n",
            "Global step: 1775,loss: 0.01813975\n",
            "\n",
            "Global step: 1776,loss: 0.019766875\n",
            "\n",
            "Global step: 1777,loss: 0.01591679\n",
            "\n",
            "Global step: 1778,loss: 0.018892063\n",
            "\n",
            "Global step: 1779,loss: 0.024555298\n",
            "\n",
            "Global step: 1780,loss: 0.014462277\n",
            "\n",
            "Global step: 1781,loss: 0.01763606\n",
            "\n",
            "Global step: 1782,loss: 0.019739494\n",
            "\n",
            "Global step: 1783,loss: 0.015034176\n",
            "\n",
            "Global step: 1784,loss: 0.013023501\n",
            "\n",
            "Global step: 1785,loss: 0.016475853\n",
            "\n",
            "Global step: 1786,loss: 0.01272694\n",
            "\n",
            "Global step: 1787,loss: 0.017079525\n",
            "\n",
            "Global step: 1788,loss: 0.016851215\n",
            "\n",
            "Global step: 1789,loss: 0.021143144\n",
            "\n",
            "Global step: 1790,loss: 0.015903242\n",
            "\n",
            "Global step: 1791,loss: 0.017509416\n",
            "\n",
            "Global step: 1792,loss: 0.014398967\n",
            "\n",
            "Global step: 1793,loss: 0.016010102\n",
            "\n",
            "Global step: 1794,loss: 0.016793162\n",
            "\n",
            "Global step: 1795,loss: 0.01855224\n",
            "\n",
            "Global step: 1796,loss: 0.015815567\n",
            "\n",
            "Global step: 1797,loss: 0.02059944\n",
            "\n",
            "Global step: 1798,loss: 0.018056337\n",
            "\n",
            "Global step: 1799,loss: 0.015364315\n",
            "\n",
            "Global step: 1800,loss: 0.013436209\n",
            "\n",
            "Global step: 1801,loss: 0.014269948\n",
            "\n",
            "Global step: 1802,loss: 0.019579245\n",
            "\n",
            "Global step: 1803,loss: 0.014422954\n",
            "\n",
            "Global step: 1804,loss: 0.020669151\n",
            "\n",
            "Global step: 1805,loss: 0.017814802\n",
            "\n",
            "Global step: 1806,loss: 0.0141572775\n",
            "\n",
            "Global step: 1807,loss: 0.015954591\n",
            "\n",
            "Global step: 1808,loss: 0.024076575\n",
            "\n",
            "Global step: 1809,loss: 0.015681269\n",
            "\n",
            "Global step: 1810,loss: 0.015900956\n",
            "\n",
            "Global step: 1811,loss: 0.025822408\n",
            "\n",
            "Global step: 1812,loss: 0.021438971\n",
            "\n",
            "Global step: 1813,loss: 0.01589671\n",
            "\n",
            "Global step: 1814,loss: 0.014672762\n",
            "\n",
            "Global step: 1815,loss: 0.016032265\n",
            "\n",
            "Global step: 1816,loss: 0.01573309\n",
            "\n",
            "Global step: 1817,loss: 0.015696764\n",
            "\n",
            "Global step: 1818,loss: 0.013039762\n",
            "\n",
            "Global step: 1819,loss: 0.02406837\n",
            "\n",
            "Global step: 1820,loss: 0.024653241\n",
            "\n",
            "Global step: 1821,loss: 0.015066943\n",
            "\n",
            "Global step: 1822,loss: 0.03568633\n",
            "\n",
            "Global step: 1823,loss: 0.033655345\n",
            "\n",
            "Global step: 1824,loss: 0.013145357\n",
            "\n",
            "Global step: 1825,loss: 0.020947345\n",
            "\n",
            "Global step: 1826,loss: 0.012656084\n",
            "\n",
            "Global step: 1827,loss: 0.014936249\n",
            "\n",
            "Global step: 1828,loss: 0.021500662\n",
            "\n",
            "Global step: 1829,loss: 0.013478985\n",
            "\n",
            "Global step: 1830,loss: 0.030685103\n",
            "\n",
            "Global step: 1831,loss: 0.023099782\n",
            "\n",
            "Global step: 1832,loss: 0.014572628\n",
            "\n",
            "Global step: 1833,loss: 0.0134362215\n",
            "\n",
            "Global step: 1834,loss: 0.026019301\n",
            "\n",
            "Global step: 1835,loss: 0.022269238\n",
            "\n",
            "Global step: 1836,loss: 0.028407149\n",
            "\n",
            "Global step: 1837,loss: 0.03136527\n",
            "\n",
            "Global step: 1838,loss: 0.017027691\n",
            "\n",
            "Global step: 1839,loss: 0.015020305\n",
            "\n",
            "Global step: 1840,loss: 0.0143944295\n",
            "\n",
            "Global step: 1841,loss: 0.015358647\n",
            "\n",
            "Global step: 1842,loss: 0.014252735\n",
            "\n",
            "Global step: 1843,loss: 0.012179453\n",
            "\n",
            "Global step: 1844,loss: 0.017172284\n",
            "\n",
            "Global step: 1845,loss: 0.018474631\n",
            "\n",
            "Global step: 1846,loss: 0.024747401\n",
            "\n",
            "Global step: 1847,loss: 0.016168332\n",
            "\n",
            "Global step: 1848,loss: 0.018506294\n",
            "\n",
            "Global step: 1849,loss: 0.028221715\n",
            "\n",
            "Global step: 1850,loss: 0.01910016\n",
            "\n",
            "Global step: 1851,loss: 0.016068453\n",
            "\n",
            "Global step: 1852,loss: 0.019974317\n",
            "\n",
            "Global step: 1853,loss: 0.024567924\n",
            "\n",
            "Global step: 1854,loss: 0.016624961\n",
            "\n",
            "Global step: 1855,loss: 0.013910406\n",
            "\n",
            "Global step: 1856,loss: 0.017903203\n",
            "\n",
            "Global step: 1857,loss: 0.015861282\n",
            "\n",
            "Global step: 1858,loss: 0.017228454\n",
            "\n",
            "Global step: 1859,loss: 0.018144445\n",
            "\n",
            "Global step: 1860,loss: 0.029754251\n",
            "\n",
            "Global step: 1861,loss: 0.014920207\n",
            "\n",
            "Global step: 1862,loss: 0.015071734\n",
            "\n",
            "Global step: 1863,loss: 0.019817047\n",
            "\n",
            "Global step: 1864,loss: 0.029809546\n",
            "\n",
            "Global step: 1865,loss: 0.018117037\n",
            "\n",
            "Global step: 1866,loss: 0.016567487\n",
            "\n",
            "Global step: 1867,loss: 0.01890625\n",
            "\n",
            "Global step: 1868,loss: 0.015282709\n",
            "\n",
            "Global step: 1869,loss: 0.014924115\n",
            "\n",
            "Global step: 1870,loss: 0.015929133\n",
            "\n",
            "Global step: 1871,loss: 0.016899314\n",
            "\n",
            "Global step: 1872,loss: 0.016773865\n",
            "\n",
            "Global step: 1873,loss: 0.015606664\n",
            "\n",
            "Global step: 1874,loss: 0.01337402\n",
            "\n",
            "Global step: 1875,loss: 0.013091903\n",
            "\n",
            "Global step: 1876,loss: 0.026548076\n",
            "\n",
            "Global step: 1877,loss: 0.020682033\n",
            "\n",
            "Global step: 1878,loss: 0.017195366\n",
            "\n",
            "Global step: 1879,loss: 0.013652326\n",
            "\n",
            "Global step: 1880,loss: 0.028175449\n",
            "\n",
            "Global step: 1881,loss: 0.020236567\n",
            "\n",
            "Global step: 1882,loss: 0.014367955\n",
            "\n",
            "Global step: 1883,loss: 0.017807389\n",
            "\n",
            "Global step: 1884,loss: 0.022650555\n",
            "\n",
            "Global step: 1885,loss: 0.0148832705\n",
            "\n",
            "Global step: 1886,loss: 0.013738966\n",
            "\n",
            "Global step: 1887,loss: 0.017006595\n",
            "\n",
            "Global step: 1888,loss: 0.023967233\n",
            "\n",
            "Global step: 1889,loss: 0.024865787\n",
            "\n",
            "Global step: 1890,loss: 0.019185854\n",
            "\n",
            "Global step: 1891,loss: 0.017311092\n",
            "\n",
            "Global step: 1892,loss: 0.012712968\n",
            "\n",
            "Global step: 1893,loss: 0.012557596\n",
            "\n",
            "Global step: 1894,loss: 0.016560491\n",
            "\n",
            "Global step: 1895,loss: 0.014624795\n",
            "\n",
            "Global step: 1896,loss: 0.015229909\n",
            "\n",
            "Global step: 1897,loss: 0.014231658\n",
            "\n",
            "Global step: 1898,loss: 0.013427778\n",
            "\n",
            "Global step: 1899,loss: 0.020329421\n",
            "\n",
            "Global step: 1900,loss: 0.01845334\n",
            "\n",
            "Global step: 1901,loss: 0.015186444\n",
            "\n",
            "Global step: 1902,loss: 0.015592359\n",
            "\n",
            "Global step: 1903,loss: 0.012604582\n",
            "\n",
            "Global step: 1904,loss: 0.013004085\n",
            "\n",
            "Global step: 1905,loss: 0.018094128\n",
            "\n",
            "Global step: 1906,loss: 0.018229859\n",
            "\n",
            "Global step: 1907,loss: 0.020207338\n",
            "\n",
            "Global step: 1908,loss: 0.020024292\n",
            "\n",
            "Global step: 1909,loss: 0.01719779\n",
            "\n",
            "Global step: 1910,loss: 0.021758338\n",
            "\n",
            "Global step: 1911,loss: 0.014106172\n",
            "\n",
            "Global step: 1912,loss: 0.013621455\n",
            "\n",
            "Global step: 1913,loss: 0.015820473\n",
            "\n",
            "Global step: 1914,loss: 0.0140679525\n",
            "\n",
            "Global step: 1915,loss: 0.016296918\n",
            "\n",
            "Global step: 1916,loss: 0.017904727\n",
            "\n",
            "Global step: 1917,loss: 0.017643876\n",
            "\n",
            "Global step: 1918,loss: 0.024037212\n",
            "\n",
            "Global step: 1919,loss: 0.019205045\n",
            "\n",
            "Global step: 1920,loss: 0.01607726\n",
            "\n",
            "Global step: 1921,loss: 0.016966838\n",
            "\n",
            "Global step: 1922,loss: 0.014784906\n",
            "\n",
            "Global step: 1923,loss: 0.01706184\n",
            "\n",
            "Global step: 1924,loss: 0.014998607\n",
            "\n",
            "Global step: 1925,loss: 0.018291088\n",
            "\n",
            "Global step: 1926,loss: 0.024390444\n",
            "\n",
            "Global step: 1927,loss: 0.02099159\n",
            "\n",
            "Global step: 1928,loss: 0.022496276\n",
            "\n",
            "Global step: 1929,loss: 0.014211148\n",
            "\n",
            "Global step: 1930,loss: 0.017348701\n",
            "\n",
            "Global step: 1931,loss: 0.014041347\n",
            "\n",
            "Global step: 1932,loss: 0.019496791\n",
            "\n",
            "Global step: 1933,loss: 0.01762962\n",
            "\n",
            "Global step: 1934,loss: 0.012999697\n",
            "\n",
            "Global step: 1935,loss: 0.027090315\n",
            "\n",
            "Global step: 1936,loss: 0.017822372\n",
            "\n",
            "Global step: 1937,loss: 0.017368466\n",
            "\n",
            "Global step: 1938,loss: 0.01689169\n",
            "\n",
            "Global step: 1939,loss: 0.01310502\n",
            "\n",
            "Global step: 1940,loss: 0.014333327\n",
            "\n",
            "Global step: 1941,loss: 0.012039295\n",
            "\n",
            "Global step: 1942,loss: 0.013099406\n",
            "\n",
            "Global step: 1943,loss: 0.012708331\n",
            "\n",
            "Global step: 1944,loss: 0.013468701\n",
            "\n",
            "Global step: 1945,loss: 0.020578343\n",
            "\n",
            "Global step: 1946,loss: 0.016654741\n",
            "\n",
            "Global step: 1947,loss: 0.012981904\n",
            "\n",
            "Global step: 1948,loss: 0.031183776\n",
            "\n",
            "Global step: 1949,loss: 0.019463256\n",
            "\n",
            "Global step: 1950,loss: 0.01408161\n",
            "\n",
            "Global step: 1951,loss: 0.012824632\n",
            "\n",
            "Global step: 1952,loss: 0.015805248\n",
            "\n",
            "Global step: 1953,loss: 0.013431555\n",
            "\n",
            "Global step: 1954,loss: 0.012953874\n",
            "\n",
            "Global step: 1955,loss: 0.013281733\n",
            "\n",
            "Global step: 1956,loss: 0.016368236\n",
            "\n",
            "Global step: 1957,loss: 0.018142968\n",
            "\n",
            "Global step: 1958,loss: 0.015176782\n",
            "\n",
            "Global step: 1959,loss: 0.015216988\n",
            "\n",
            "Global step: 1960,loss: 0.017266529\n",
            "\n",
            "Global step: 1961,loss: 0.020901639\n",
            "\n",
            "Global step: 1962,loss: 0.013415753\n",
            "\n",
            "Global step: 1963,loss: 0.011665612\n",
            "\n",
            "Global step: 1964,loss: 0.0149645\n",
            "\n",
            "Global step: 1965,loss: 0.012033938\n",
            "\n",
            "Global step: 1966,loss: 0.013254693\n",
            "\n",
            "Global step: 1967,loss: 0.019135758\n",
            "\n",
            "Global step: 1968,loss: 0.015280385\n",
            "\n",
            "Global step: 1969,loss: 0.016243115\n",
            "\n",
            "Global step: 1970,loss: 0.014301203\n",
            "\n",
            "Global step: 1971,loss: 0.020002652\n",
            "\n",
            "Global step: 1972,loss: 0.023564754\n",
            "\n",
            "Global step: 1973,loss: 0.014974112\n",
            "\n",
            "Global step: 1974,loss: 0.015148988\n",
            "\n",
            "Global step: 1975,loss: 0.014131965\n",
            "\n",
            "Global step: 1976,loss: 0.015009247\n",
            "\n",
            "Global step: 1977,loss: 0.01483121\n",
            "\n",
            "Global step: 1978,loss: 0.013588002\n",
            "\n",
            "Global step: 1979,loss: 0.014935254\n",
            "\n",
            "Global step: 1980,loss: 0.015194627\n",
            "\n",
            "Global step: 1981,loss: 0.020616531\n",
            "\n",
            "Global step: 1982,loss: 0.023210302\n",
            "\n",
            "Global step: 1983,loss: 0.017403647\n",
            "\n",
            "Global step: 1984,loss: 0.015124571\n",
            "\n",
            "Global step: 1985,loss: 0.016865898\n",
            "\n",
            "Global step: 1986,loss: 0.014260645\n",
            "\n",
            "Global step: 1987,loss: 0.019213289\n",
            "\n",
            "Global step: 1988,loss: 0.014993263\n",
            "\n",
            "Global step: 1989,loss: 0.029308164\n",
            "\n",
            "Global step: 1990,loss: 0.012626817\n",
            "\n",
            "Global step: 1991,loss: 0.013145547\n",
            "\n",
            "Global step: 1992,loss: 0.015830034\n",
            "\n",
            "Global step: 1993,loss: 0.02206936\n",
            "\n",
            "Global step: 1994,loss: 0.025959618\n",
            "\n",
            "Global step: 1995,loss: 0.024369117\n",
            "\n",
            "Global step: 1996,loss: 0.025338102\n",
            "\n",
            "Global step: 1997,loss: 0.018196419\n",
            "\n",
            "Global step: 1998,loss: 0.013607209\n",
            "\n",
            "Global step: 1999,loss: 0.013833435\n",
            "\n",
            "Global step: 2000,loss: 0.013304723\n",
            "\n",
            "Global step: 2001,loss: 0.012772279\n",
            "\n",
            "Global step: 2002,loss: 0.028499052\n",
            "\n",
            "Global step: 2003,loss: 0.01465462\n",
            "\n",
            "Global step: 2004,loss: 0.018178305\n",
            "\n",
            "Global step: 2005,loss: 0.023915084\n",
            "\n",
            "Global step: 2006,loss: 0.015304692\n",
            "\n",
            "Global step: 2007,loss: 0.015026207\n",
            "\n",
            "Global step: 2008,loss: 0.022695845\n",
            "\n",
            "Global step: 2009,loss: 0.01290139\n",
            "\n",
            "Global step: 2010,loss: 0.013626853\n",
            "\n",
            "Global step: 2011,loss: 0.0125985\n",
            "\n",
            "Global step: 2012,loss: 0.014767608\n",
            "\n",
            "Global step: 2013,loss: 0.01277923\n",
            "\n",
            "Global step: 2014,loss: 0.014116467\n",
            "\n",
            "Global step: 2015,loss: 0.023719428\n",
            "\n",
            "Global step: 2016,loss: 0.013141256\n",
            "\n",
            "Global step: 2017,loss: 0.011407651\n",
            "\n",
            "Global step: 2018,loss: 0.020882811\n",
            "\n",
            "Global step: 2019,loss: 0.014631152\n",
            "\n",
            "Global step: 2020,loss: 0.017033461\n",
            "\n",
            "Global step: 2021,loss: 0.01564902\n",
            "\n",
            "Global step: 2022,loss: 0.012125488\n",
            "\n",
            "Global step: 2023,loss: 0.013364774\n",
            "\n",
            "Global step: 2024,loss: 0.020227317\n",
            "\n",
            "Global step: 2025,loss: 0.021756021\n",
            "\n",
            "Global step: 2026,loss: 0.015018668\n",
            "\n",
            "Global step: 2027,loss: 0.022887122\n",
            "\n",
            "Global step: 2028,loss: 0.026464254\n",
            "\n",
            "Global step: 2029,loss: 0.018843057\n",
            "\n",
            "Global step: 2030,loss: 0.021802679\n",
            "\n",
            "Global step: 2031,loss: 0.016290877\n",
            "\n",
            "Global step: 2032,loss: 0.021025613\n",
            "\n",
            "Global step: 2033,loss: 0.012625657\n",
            "\n",
            "Global step: 2034,loss: 0.021807116\n",
            "\n",
            "Global step: 2035,loss: 0.014839152\n",
            "\n",
            "Global step: 2036,loss: 0.018606294\n",
            "\n",
            "Global step: 2037,loss: 0.014790184\n",
            "\n",
            "Global step: 2038,loss: 0.016453195\n",
            "\n",
            "Global step: 2039,loss: 0.017841803\n",
            "\n",
            "Global step: 2040,loss: 0.019282984\n",
            "\n",
            "Global step: 2041,loss: 0.020296222\n",
            "\n",
            "Global step: 2042,loss: 0.018114682\n",
            "\n",
            "Global step: 2043,loss: 0.0139470715\n",
            "\n",
            "Global step: 2044,loss: 0.014749603\n",
            "\n",
            "Global step: 2045,loss: 0.014308901\n",
            "\n",
            "Global step: 2046,loss: 0.022160457\n",
            "\n",
            "Global step: 2047,loss: 0.016010977\n",
            "\n",
            "Global step: 2048,loss: 0.018859485\n",
            "\n",
            "Global step: 2049,loss: 0.014574591\n",
            "\n",
            "Global step: 2050,loss: 0.01307451\n",
            "\n",
            "Global step: 2051,loss: 0.022122035\n",
            "\n",
            "Global step: 2052,loss: 0.014067368\n",
            "\n",
            "Global step: 2053,loss: 0.025197022\n",
            "\n",
            "Global step: 2054,loss: 0.014627374\n",
            "\n",
            "Global step: 2055,loss: 0.013558226\n",
            "\n",
            "Global step: 2056,loss: 0.021019682\n",
            "\n",
            "Global step: 2057,loss: 0.0149693405\n",
            "\n",
            "Global step: 2058,loss: 0.017096885\n",
            "\n",
            "Global step: 2059,loss: 0.0142411245\n",
            "\n",
            "Global step: 2060,loss: 0.01577973\n",
            "\n",
            "Global step: 2061,loss: 0.023835897\n",
            "\n",
            "Global step: 2062,loss: 0.014317683\n",
            "\n",
            "Global step: 2063,loss: 0.022142429\n",
            "\n",
            "Global step: 2064,loss: 0.0143294325\n",
            "\n",
            "Global step: 2065,loss: 0.028797258\n",
            "\n",
            "Global step: 2066,loss: 0.017145917\n",
            "\n",
            "Global step: 2067,loss: 0.022256171\n",
            "\n",
            "Global step: 2068,loss: 0.012551797\n",
            "\n",
            "Global step: 2069,loss: 0.030979011\n",
            "\n",
            "Global step: 2070,loss: 0.013990703\n",
            "\n",
            "Global step: 2071,loss: 0.013825823\n",
            "\n",
            "Global step: 2072,loss: 0.01440271\n",
            "\n",
            "Global step: 2073,loss: 0.012674151\n",
            "\n",
            "Global step: 2074,loss: 0.015745211\n",
            "\n",
            "Global step: 2075,loss: 0.017456379\n",
            "\n",
            "Global step: 2076,loss: 0.022144813\n",
            "\n",
            "Global step: 2077,loss: 0.023937853\n",
            "\n",
            "Global step: 2078,loss: 0.019719075\n",
            "\n",
            "Global step: 2079,loss: 0.01593399\n",
            "\n",
            "Global step: 2080,loss: 0.017150111\n",
            "\n",
            "Global step: 2081,loss: 0.024761148\n",
            "\n",
            "Global step: 2082,loss: 0.017638277\n",
            "\n",
            "Global step: 2083,loss: 0.016709229\n",
            "\n",
            "Global step: 2084,loss: 0.01623385\n",
            "\n",
            "Global step: 2085,loss: 0.014920621\n",
            "\n",
            "Global step: 2086,loss: 0.0120306425\n",
            "\n",
            "Global step: 2087,loss: 0.016190436\n",
            "\n",
            "Global step: 2088,loss: 0.01927054\n",
            "\n",
            "Global step: 2089,loss: 0.016239505\n",
            "\n",
            "Global step: 2090,loss: 0.018339973\n",
            "\n",
            "Global step: 2091,loss: 0.0133780595\n",
            "\n",
            "Global step: 2092,loss: 0.01978308\n",
            "\n",
            "Global step: 2093,loss: 0.012944507\n",
            "\n",
            "Global step: 2094,loss: 0.015242389\n",
            "\n",
            "Global step: 2095,loss: 0.012942671\n",
            "\n",
            "Global step: 2096,loss: 0.01573623\n",
            "\n",
            "Global step: 2097,loss: 0.0152561385\n",
            "\n",
            "Global step: 2098,loss: 0.015868884\n",
            "\n",
            "Global step: 2099,loss: 0.01707555\n",
            "\n",
            "Global step: 2100,loss: 0.012676055\n",
            "\n",
            "Global step: 2101,loss: 0.016976852\n",
            "\n",
            "Global step: 2102,loss: 0.013795501\n",
            "\n",
            "Global step: 2103,loss: 0.016451692\n",
            "\n",
            "Global step: 2104,loss: 0.014409648\n",
            "\n",
            "Global step: 2105,loss: 0.013539847\n",
            "\n",
            "Global step: 2106,loss: 0.014348808\n",
            "\n",
            "Global step: 2107,loss: 0.018220177\n",
            "\n",
            "Global step: 2108,loss: 0.016393349\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2108,Val_Loss: 0.01644381432610954,  Val_acc: 0.9969951923076923 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:48:11.010537 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/15:\n",
            "Global step: 2109,loss: 0.021613207\n",
            "\n",
            "Global step: 2110,loss: 0.021722307\n",
            "\n",
            "Global step: 2111,loss: 0.016429741\n",
            "\n",
            "Global step: 2112,loss: 0.012741616\n",
            "\n",
            "Global step: 2113,loss: 0.017038656\n",
            "\n",
            "Global step: 2114,loss: 0.021365827\n",
            "\n",
            "Global step: 2115,loss: 0.013971011\n",
            "\n",
            "Global step: 2116,loss: 0.013681343\n",
            "\n",
            "Global step: 2117,loss: 0.013600836\n",
            "\n",
            "Global step: 2118,loss: 0.023261497\n",
            "\n",
            "Global step: 2119,loss: 0.022476725\n",
            "\n",
            "Global step: 2120,loss: 0.013305159\n",
            "\n",
            "Global step: 2121,loss: 0.02031884\n",
            "\n",
            "Global step: 2122,loss: 0.019305103\n",
            "\n",
            "Global step: 2123,loss: 0.015908536\n",
            "\n",
            "Global step: 2124,loss: 0.014331512\n",
            "\n",
            "Global step: 2125,loss: 0.022654647\n",
            "\n",
            "Global step: 2126,loss: 0.013436332\n",
            "\n",
            "Global step: 2127,loss: 0.014079737\n",
            "\n",
            "Global step: 2128,loss: 0.01493326\n",
            "\n",
            "Global step: 2129,loss: 0.013831964\n",
            "\n",
            "Global step: 2130,loss: 0.01581227\n",
            "\n",
            "Global step: 2131,loss: 0.013197328\n",
            "\n",
            "Global step: 2132,loss: 0.021990418\n",
            "\n",
            "Global step: 2133,loss: 0.015774684\n",
            "\n",
            "Global step: 2134,loss: 0.015144803\n",
            "\n",
            "Global step: 2135,loss: 0.020217005\n",
            "\n",
            "Global step: 2136,loss: 0.014796641\n",
            "\n",
            "Global step: 2137,loss: 0.013007147\n",
            "\n",
            "Global step: 2138,loss: 0.012682196\n",
            "\n",
            "Global step: 2139,loss: 0.013197478\n",
            "\n",
            "Global step: 2140,loss: 0.012448996\n",
            "\n",
            "Global step: 2141,loss: 0.0146371685\n",
            "\n",
            "Global step: 2142,loss: 0.013177242\n",
            "\n",
            "Global step: 2143,loss: 0.01264557\n",
            "\n",
            "Global step: 2144,loss: 0.014711502\n",
            "\n",
            "Global step: 2145,loss: 0.015356498\n",
            "\n",
            "Global step: 2146,loss: 0.013339384\n",
            "\n",
            "Global step: 2147,loss: 0.019079942\n",
            "\n",
            "Global step: 2148,loss: 0.013780747\n",
            "\n",
            "Global step: 2149,loss: 0.0126214195\n",
            "\n",
            "Global step: 2150,loss: 0.017146476\n",
            "\n",
            "Global step: 2151,loss: 0.013484998\n",
            "\n",
            "Global step: 2152,loss: 0.016099961\n",
            "\n",
            "Global step: 2153,loss: 0.014735304\n",
            "\n",
            "Global step: 2154,loss: 0.012580836\n",
            "\n",
            "Global step: 2155,loss: 0.015521131\n",
            "\n",
            "Global step: 2156,loss: 0.03450419\n",
            "\n",
            "Global step: 2157,loss: 0.01862755\n",
            "\n",
            "Global step: 2158,loss: 0.013491619\n",
            "\n",
            "Global step: 2159,loss: 0.014328429\n",
            "\n",
            "Global step: 2160,loss: 0.03311521\n",
            "\n",
            "Global step: 2161,loss: 0.01698491\n",
            "\n",
            "Global step: 2162,loss: 0.011780169\n",
            "\n",
            "Global step: 2163,loss: 0.013422627\n",
            "\n",
            "Global step: 2164,loss: 0.012850203\n",
            "\n",
            "Global step: 2165,loss: 0.020169795\n",
            "\n",
            "Global step: 2166,loss: 0.016243959\n",
            "\n",
            "Global step: 2167,loss: 0.014615856\n",
            "\n",
            "Global step: 2168,loss: 0.01566007\n",
            "\n",
            "Global step: 2169,loss: 0.012495446\n",
            "\n",
            "Global step: 2170,loss: 0.015104448\n",
            "\n",
            "Global step: 2171,loss: 0.013612271\n",
            "\n",
            "Global step: 2172,loss: 0.013495955\n",
            "\n",
            "Global step: 2173,loss: 0.015389537\n",
            "\n",
            "Global step: 2174,loss: 0.013448306\n",
            "\n",
            "Global step: 2175,loss: 0.0149939265\n",
            "\n",
            "Global step: 2176,loss: 0.017635727\n",
            "\n",
            "Global step: 2177,loss: 0.015201241\n",
            "\n",
            "Global step: 2178,loss: 0.012698425\n",
            "\n",
            "Global step: 2179,loss: 0.017890893\n",
            "\n",
            "Global step: 2180,loss: 0.012422722\n",
            "\n",
            "Global step: 2181,loss: 0.016628416\n",
            "\n",
            "Global step: 2182,loss: 0.012409849\n",
            "\n",
            "Global step: 2183,loss: 0.0110996\n",
            "\n",
            "Global step: 2184,loss: 0.0128253745\n",
            "\n",
            "Global step: 2185,loss: 0.014876504\n",
            "\n",
            "Global step: 2186,loss: 0.015590347\n",
            "\n",
            "Global step: 2187,loss: 0.0196157\n",
            "\n",
            "Global step: 2188,loss: 0.016439069\n",
            "\n",
            "Global step: 2189,loss: 0.014633851\n",
            "\n",
            "Global step: 2190,loss: 0.011937972\n",
            "\n",
            "Global step: 2191,loss: 0.013558023\n",
            "\n",
            "Global step: 2192,loss: 0.01626847\n",
            "\n",
            "Global step: 2193,loss: 0.010969752\n",
            "\n",
            "Global step: 2194,loss: 0.021065678\n",
            "\n",
            "Global step: 2195,loss: 0.011148192\n",
            "\n",
            "Global step: 2196,loss: 0.014295839\n",
            "\n",
            "Global step: 2197,loss: 0.0124806\n",
            "\n",
            "Global step: 2198,loss: 0.012560835\n",
            "\n",
            "Global step: 2199,loss: 0.029806167\n",
            "\n",
            "Global step: 2200,loss: 0.015296219\n",
            "\n",
            "Global step: 2201,loss: 0.018032862\n",
            "\n",
            "Global step: 2202,loss: 0.012023849\n",
            "\n",
            "Global step: 2203,loss: 0.014801493\n",
            "\n",
            "Global step: 2204,loss: 0.012585895\n",
            "\n",
            "Global step: 2205,loss: 0.013098657\n",
            "\n",
            "Global step: 2206,loss: 0.013540251\n",
            "\n",
            "Global step: 2207,loss: 0.013843033\n",
            "\n",
            "Global step: 2208,loss: 0.017354133\n",
            "\n",
            "Global step: 2209,loss: 0.012488317\n",
            "\n",
            "Global step: 2210,loss: 0.014400424\n",
            "\n",
            "Global step: 2211,loss: 0.01186056\n",
            "\n",
            "Global step: 2212,loss: 0.013483971\n",
            "\n",
            "Global step: 2213,loss: 0.022202522\n",
            "\n",
            "Global step: 2214,loss: 0.0124376975\n",
            "\n",
            "Global step: 2215,loss: 0.016521448\n",
            "\n",
            "Global step: 2216,loss: 0.011337359\n",
            "\n",
            "Global step: 2217,loss: 0.011490639\n",
            "\n",
            "Global step: 2218,loss: 0.01477956\n",
            "\n",
            "Global step: 2219,loss: 0.012404738\n",
            "\n",
            "Global step: 2220,loss: 0.01197624\n",
            "\n",
            "Global step: 2221,loss: 0.015033651\n",
            "\n",
            "Global step: 2222,loss: 0.013703185\n",
            "\n",
            "Global step: 2223,loss: 0.017773163\n",
            "\n",
            "Global step: 2224,loss: 0.023461677\n",
            "\n",
            "Global step: 2225,loss: 0.01407976\n",
            "\n",
            "Global step: 2226,loss: 0.014957259\n",
            "\n",
            "Global step: 2227,loss: 0.012530537\n",
            "\n",
            "Global step: 2228,loss: 0.02041026\n",
            "\n",
            "Global step: 2229,loss: 0.013846142\n",
            "\n",
            "Global step: 2230,loss: 0.011056294\n",
            "\n",
            "Global step: 2231,loss: 0.013300564\n",
            "\n",
            "Global step: 2232,loss: 0.013127076\n",
            "\n",
            "Global step: 2233,loss: 0.013019186\n",
            "\n",
            "Global step: 2234,loss: 0.017218782\n",
            "\n",
            "Global step: 2235,loss: 0.011702334\n",
            "\n",
            "Global step: 2236,loss: 0.015196063\n",
            "\n",
            "Global step: 2237,loss: 0.013902436\n",
            "\n",
            "Global step: 2238,loss: 0.0124932295\n",
            "\n",
            "Global step: 2239,loss: 0.017688926\n",
            "\n",
            "Global step: 2240,loss: 0.013470068\n",
            "\n",
            "Global step: 2241,loss: 0.013804499\n",
            "\n",
            "Global step: 2242,loss: 0.011677219\n",
            "\n",
            "Global step: 2243,loss: 0.015133161\n",
            "\n",
            "Global step: 2244,loss: 0.020291891\n",
            "\n",
            "Global step: 2245,loss: 0.013625624\n",
            "\n",
            "Global step: 2246,loss: 0.012899248\n",
            "\n",
            "Global step: 2247,loss: 0.013358683\n",
            "\n",
            "Global step: 2248,loss: 0.0136876665\n",
            "\n",
            "Global step: 2249,loss: 0.017356724\n",
            "\n",
            "Global step: 2250,loss: 0.013624757\n",
            "\n",
            "Global step: 2251,loss: 0.011317586\n",
            "\n",
            "Global step: 2252,loss: 0.012874956\n",
            "\n",
            "Global step: 2253,loss: 0.013533872\n",
            "\n",
            "Global step: 2254,loss: 0.01460118\n",
            "\n",
            "Global step: 2255,loss: 0.013741642\n",
            "\n",
            "Global step: 2256,loss: 0.016240818\n",
            "\n",
            "Global step: 2257,loss: 0.015572444\n",
            "\n",
            "Global step: 2258,loss: 0.012571984\n",
            "\n",
            "Global step: 2259,loss: 0.012889403\n",
            "\n",
            "Global step: 2260,loss: 0.012747464\n",
            "\n",
            "Global step: 2261,loss: 0.011819716\n",
            "\n",
            "Global step: 2262,loss: 0.01438012\n",
            "\n",
            "Global step: 2263,loss: 0.014618824\n",
            "\n",
            "Global step: 2264,loss: 0.0129619455\n",
            "\n",
            "Global step: 2265,loss: 0.015922498\n",
            "\n",
            "Global step: 2266,loss: 0.011803179\n",
            "\n",
            "Global step: 2267,loss: 0.011600978\n",
            "\n",
            "Global step: 2268,loss: 0.012196403\n",
            "\n",
            "Global step: 2269,loss: 0.015596006\n",
            "\n",
            "Global step: 2270,loss: 0.014787871\n",
            "\n",
            "Global step: 2271,loss: 0.01356631\n",
            "\n",
            "Global step: 2272,loss: 0.014163321\n",
            "\n",
            "Global step: 2273,loss: 0.013117285\n",
            "\n",
            "Global step: 2274,loss: 0.012279085\n",
            "\n",
            "Global step: 2275,loss: 0.010636876\n",
            "\n",
            "Global step: 2276,loss: 0.011170529\n",
            "\n",
            "Global step: 2277,loss: 0.020636126\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.58164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:48:35.253742 139837015512832 supervisor.py:1099] global_step/sec: 6.58164\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2278.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:48:35.256285 139837023905536 supervisor.py:1050] Recording summary at step 2278.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2278,loss: 0.013456512\n",
            "\n",
            "Global step: 2279,loss: 0.012005674\n",
            "\n",
            "Global step: 2280,loss: 0.012385323\n",
            "\n",
            "Global step: 2281,loss: 0.015169034\n",
            "\n",
            "Global step: 2282,loss: 0.012350677\n",
            "\n",
            "Global step: 2283,loss: 0.020296771\n",
            "\n",
            "Global step: 2284,loss: 0.02011593\n",
            "\n",
            "Global step: 2285,loss: 0.012100806\n",
            "\n",
            "Global step: 2286,loss: 0.011608508\n",
            "\n",
            "Global step: 2287,loss: 0.01601744\n",
            "\n",
            "Global step: 2288,loss: 0.016325153\n",
            "\n",
            "Global step: 2289,loss: 0.011891788\n",
            "\n",
            "Global step: 2290,loss: 0.016871888\n",
            "\n",
            "Global step: 2291,loss: 0.01291181\n",
            "\n",
            "Global step: 2292,loss: 0.015271349\n",
            "\n",
            "Global step: 2293,loss: 0.012487076\n",
            "\n",
            "Global step: 2294,loss: 0.012325478\n",
            "\n",
            "Global step: 2295,loss: 0.011959931\n",
            "\n",
            "Global step: 2296,loss: 0.017273419\n",
            "\n",
            "Global step: 2297,loss: 0.014600951\n",
            "\n",
            "Global step: 2298,loss: 0.0136982165\n",
            "\n",
            "Global step: 2299,loss: 0.010679289\n",
            "\n",
            "Global step: 2300,loss: 0.012714097\n",
            "\n",
            "Global step: 2301,loss: 0.013784296\n",
            "\n",
            "Global step: 2302,loss: 0.019993478\n",
            "\n",
            "Global step: 2303,loss: 0.014182275\n",
            "\n",
            "Global step: 2304,loss: 0.011758681\n",
            "\n",
            "Global step: 2305,loss: 0.013608564\n",
            "\n",
            "Global step: 2306,loss: 0.016489312\n",
            "\n",
            "Global step: 2307,loss: 0.01610946\n",
            "\n",
            "Global step: 2308,loss: 0.01158226\n",
            "\n",
            "Global step: 2309,loss: 0.015866969\n",
            "\n",
            "Global step: 2310,loss: 0.012246141\n",
            "\n",
            "Global step: 2311,loss: 0.011994487\n",
            "\n",
            "Global step: 2312,loss: 0.014127509\n",
            "\n",
            "Global step: 2313,loss: 0.014650309\n",
            "\n",
            "Global step: 2314,loss: 0.0115091065\n",
            "\n",
            "Global step: 2315,loss: 0.013523637\n",
            "\n",
            "Global step: 2316,loss: 0.011459377\n",
            "\n",
            "Global step: 2317,loss: 0.016697764\n",
            "\n",
            "Global step: 2318,loss: 0.011588968\n",
            "\n",
            "Global step: 2319,loss: 0.01348042\n",
            "\n",
            "Global step: 2320,loss: 0.011663259\n",
            "\n",
            "Global step: 2321,loss: 0.011329909\n",
            "\n",
            "Global step: 2322,loss: 0.012198382\n",
            "\n",
            "Global step: 2323,loss: 0.017577225\n",
            "\n",
            "Global step: 2324,loss: 0.011155255\n",
            "\n",
            "Global step: 2325,loss: 0.013587901\n",
            "\n",
            "Global step: 2326,loss: 0.012984581\n",
            "\n",
            "Global step: 2327,loss: 0.01134327\n",
            "\n",
            "Global step: 2328,loss: 0.01202816\n",
            "\n",
            "Global step: 2329,loss: 0.01259976\n",
            "\n",
            "Global step: 2330,loss: 0.019624703\n",
            "\n",
            "Global step: 2331,loss: 0.0129185105\n",
            "\n",
            "Global step: 2332,loss: 0.013825169\n",
            "\n",
            "Global step: 2333,loss: 0.022703117\n",
            "\n",
            "Global step: 2334,loss: 0.014867488\n",
            "\n",
            "Global step: 2335,loss: 0.015391327\n",
            "\n",
            "Global step: 2336,loss: 0.013074724\n",
            "\n",
            "Global step: 2337,loss: 0.011833696\n",
            "\n",
            "Global step: 2338,loss: 0.023570135\n",
            "\n",
            "Global step: 2339,loss: 0.012214366\n",
            "\n",
            "Global step: 2340,loss: 0.014891541\n",
            "\n",
            "Global step: 2341,loss: 0.010841178\n",
            "\n",
            "Global step: 2342,loss: 0.013145866\n",
            "\n",
            "Global step: 2343,loss: 0.014002277\n",
            "\n",
            "Global step: 2344,loss: 0.015944332\n",
            "\n",
            "Global step: 2345,loss: 0.0156072\n",
            "\n",
            "Global step: 2346,loss: 0.025581883\n",
            "\n",
            "Global step: 2347,loss: 0.011886502\n",
            "\n",
            "Global step: 2348,loss: 0.013059631\n",
            "\n",
            "Global step: 2349,loss: 0.018877909\n",
            "\n",
            "Global step: 2350,loss: 0.01462589\n",
            "\n",
            "Global step: 2351,loss: 0.015281135\n",
            "\n",
            "Global step: 2352,loss: 0.010394767\n",
            "\n",
            "Global step: 2353,loss: 0.012649154\n",
            "\n",
            "Global step: 2354,loss: 0.017456187\n",
            "\n",
            "Global step: 2355,loss: 0.013252236\n",
            "\n",
            "Global step: 2356,loss: 0.015354401\n",
            "\n",
            "Global step: 2357,loss: 0.012829309\n",
            "\n",
            "Global step: 2358,loss: 0.017642673\n",
            "\n",
            "Global step: 2359,loss: 0.015613994\n",
            "\n",
            "Global step: 2360,loss: 0.03243541\n",
            "\n",
            "Global step: 2361,loss: 0.011899479\n",
            "\n",
            "Global step: 2362,loss: 0.012104089\n",
            "\n",
            "Global step: 2363,loss: 0.021986714\n",
            "\n",
            "Global step: 2364,loss: 0.011781797\n",
            "\n",
            "Global step: 2365,loss: 0.017739188\n",
            "\n",
            "Global step: 2366,loss: 0.019615171\n",
            "\n",
            "Global step: 2367,loss: 0.013143234\n",
            "\n",
            "Global step: 2368,loss: 0.0141052\n",
            "\n",
            "Global step: 2369,loss: 0.015372332\n",
            "\n",
            "Global step: 2370,loss: 0.012425723\n",
            "\n",
            "Global step: 2371,loss: 0.011011548\n",
            "\n",
            "Global step: 2372,loss: 0.021847589\n",
            "\n",
            "Global step: 2373,loss: 0.016030736\n",
            "\n",
            "Global step: 2374,loss: 0.013745322\n",
            "\n",
            "Global step: 2375,loss: 0.017291846\n",
            "\n",
            "Global step: 2376,loss: 0.010680317\n",
            "\n",
            "Global step: 2377,loss: 0.016665278\n",
            "\n",
            "Global step: 2378,loss: 0.013247296\n",
            "\n",
            "Global step: 2379,loss: 0.025453953\n",
            "\n",
            "Global step: 2380,loss: 0.011607815\n",
            "\n",
            "Global step: 2381,loss: 0.015234426\n",
            "\n",
            "Global step: 2382,loss: 0.011963186\n",
            "\n",
            "Global step: 2383,loss: 0.01827178\n",
            "\n",
            "Global step: 2384,loss: 0.015617119\n",
            "\n",
            "Global step: 2385,loss: 0.018026933\n",
            "\n",
            "Global step: 2386,loss: 0.012667129\n",
            "\n",
            "Global step: 2387,loss: 0.011608362\n",
            "\n",
            "Global step: 2388,loss: 0.013278211\n",
            "\n",
            "Global step: 2389,loss: 0.012384113\n",
            "\n",
            "Global step: 2390,loss: 0.017088532\n",
            "\n",
            "Global step: 2391,loss: 0.017958596\n",
            "\n",
            "Global step: 2392,loss: 0.018282283\n",
            "\n",
            "Global step: 2393,loss: 0.013541174\n",
            "\n",
            "Global step: 2394,loss: 0.014395935\n",
            "\n",
            "Global step: 2395,loss: 0.011664551\n",
            "\n",
            "Global step: 2396,loss: 0.013320802\n",
            "\n",
            "Global step: 2397,loss: 0.016409384\n",
            "\n",
            "Global step: 2398,loss: 0.017417816\n",
            "\n",
            "Global step: 2399,loss: 0.01575292\n",
            "\n",
            "Global step: 2400,loss: 0.011021273\n",
            "\n",
            "Global step: 2401,loss: 0.03525515\n",
            "\n",
            "Global step: 2402,loss: 0.015182193\n",
            "\n",
            "Global step: 2403,loss: 0.015082381\n",
            "\n",
            "Global step: 2404,loss: 0.012294484\n",
            "\n",
            "Global step: 2405,loss: 0.015019943\n",
            "\n",
            "Global step: 2406,loss: 0.0117894765\n",
            "\n",
            "Global step: 2407,loss: 0.014433466\n",
            "\n",
            "Global step: 2408,loss: 0.015156895\n",
            "\n",
            "Global step: 2409,loss: 0.01379267\n",
            "\n",
            "Global step: 2410,loss: 0.013983525\n",
            "\n",
            "Global step: 2411,loss: 0.0131192645\n",
            "\n",
            "Global step: 2412,loss: 0.015358476\n",
            "\n",
            "Global step: 2413,loss: 0.013793046\n",
            "\n",
            "Global step: 2414,loss: 0.01711425\n",
            "\n",
            "Global step: 2415,loss: 0.012100818\n",
            "\n",
            "Global step: 2416,loss: 0.016633922\n",
            "\n",
            "Global step: 2417,loss: 0.013272214\n",
            "\n",
            "Global step: 2418,loss: 0.016640333\n",
            "\n",
            "Global step: 2419,loss: 0.01140171\n",
            "\n",
            "Global step: 2420,loss: 0.015834834\n",
            "\n",
            "Global step: 2421,loss: 0.011793647\n",
            "\n",
            "Global step: 2422,loss: 0.011621336\n",
            "\n",
            "Global step: 2423,loss: 0.01933245\n",
            "\n",
            "Global step: 2424,loss: 0.015380144\n",
            "\n",
            "Global step: 2425,loss: 0.010969825\n",
            "\n",
            "Global step: 2426,loss: 0.012135565\n",
            "\n",
            "Global step: 2427,loss: 0.012646284\n",
            "\n",
            "Global step: 2428,loss: 0.013426521\n",
            "\n",
            "Global step: 2429,loss: 0.011774981\n",
            "\n",
            "Global step: 2430,loss: 0.012456611\n",
            "\n",
            "Global step: 2431,loss: 0.013204408\n",
            "\n",
            "Global step: 2432,loss: 0.014635317\n",
            "\n",
            "Global step: 2433,loss: 0.017496994\n",
            "\n",
            "Global step: 2434,loss: 0.0116389375\n",
            "\n",
            "Global step: 2435,loss: 0.01341434\n",
            "\n",
            "Global step: 2436,loss: 0.012727319\n",
            "\n",
            "Global step: 2437,loss: 0.015067404\n",
            "\n",
            "Global step: 2438,loss: 0.012717618\n",
            "\n",
            "Global step: 2439,loss: 0.011303583\n",
            "\n",
            "Global step: 2440,loss: 0.012611942\n",
            "\n",
            "Global step: 2441,loss: 0.01929506\n",
            "\n",
            "Global step: 2442,loss: 0.0117659755\n",
            "\n",
            "Global step: 2443,loss: 0.02177172\n",
            "\n",
            "Global step: 2444,loss: 0.01420957\n",
            "\n",
            "Global step: 2445,loss: 0.01440858\n",
            "\n",
            "Global step: 2446,loss: 0.01582852\n",
            "\n",
            "Global step: 2447,loss: 0.0120403385\n",
            "\n",
            "Global step: 2448,loss: 0.012436081\n",
            "\n",
            "Global step: 2449,loss: 0.015186822\n",
            "\n",
            "Global step: 2450,loss: 0.011741768\n",
            "\n",
            "Global step: 2451,loss: 0.017383017\n",
            "\n",
            "Global step: 2452,loss: 0.025741566\n",
            "\n",
            "Global step: 2453,loss: 0.014087298\n",
            "\n",
            "Global step: 2454,loss: 0.013170526\n",
            "\n",
            "Global step: 2455,loss: 0.013313874\n",
            "\n",
            "Global step: 2456,loss: 0.014881745\n",
            "\n",
            "Global step: 2457,loss: 0.013567553\n",
            "\n",
            "Global step: 2458,loss: 0.018175472\n",
            "\n",
            "Global step: 2459,loss: 0.015595269\n",
            "\n",
            "Global step: 2460,loss: 0.0129568055\n",
            "\n",
            "Global step: 2461,loss: 0.012632214\n",
            "\n",
            "Global step: 2462,loss: 0.0140387155\n",
            "\n",
            "Global step: 2463,loss: 0.013484474\n",
            "\n",
            "Global step: 2464,loss: 0.01779153\n",
            "\n",
            "Global step: 2465,loss: 0.011529161\n",
            "\n",
            "Global step: 2466,loss: 0.012396045\n",
            "\n",
            "Global step: 2467,loss: 0.016420804\n",
            "\n",
            "Global step: 2468,loss: 0.0143810725\n",
            "\n",
            "Global step: 2469,loss: 0.023156125\n",
            "\n",
            "Global step: 2470,loss: 0.013094529\n",
            "\n",
            "Global step: 2471,loss: 0.011990206\n",
            "\n",
            "Global step: 2472,loss: 0.015729874\n",
            "\n",
            "Global step: 2473,loss: 0.013600215\n",
            "\n",
            "Global step: 2474,loss: 0.014430636\n",
            "\n",
            "Global step: 2475,loss: 0.012442412\n",
            "\n",
            "Global step: 2476,loss: 0.012075521\n",
            "\n",
            "Global step: 2477,loss: 0.018414557\n",
            "\n",
            "Global step: 2478,loss: 0.013380779\n",
            "\n",
            "Global step: 2479,loss: 0.012456352\n",
            "\n",
            "Global step: 2480,loss: 0.0122231655\n",
            "\n",
            "Global step: 2481,loss: 0.013030146\n",
            "\n",
            "Global step: 2482,loss: 0.01809685\n",
            "\n",
            "Global step: 2483,loss: 0.012689498\n",
            "\n",
            "Global step: 2484,loss: 0.018514026\n",
            "\n",
            "Global step: 2485,loss: 0.01487062\n",
            "\n",
            "Global step: 2486,loss: 0.01875307\n",
            "\n",
            "Global step: 2487,loss: 0.011520177\n",
            "\n",
            "Global step: 2488,loss: 0.018893067\n",
            "\n",
            "Global step: 2489,loss: 0.015356001\n",
            "\n",
            "Global step: 2490,loss: 0.012616335\n",
            "\n",
            "Global step: 2491,loss: 0.011921753\n",
            "\n",
            "Global step: 2492,loss: 0.017118376\n",
            "\n",
            "Global step: 2493,loss: 0.0116031505\n",
            "\n",
            "Global step: 2494,loss: 0.019149546\n",
            "\n",
            "Global step: 2495,loss: 0.012996292\n",
            "\n",
            "Global step: 2496,loss: 0.014120892\n",
            "\n",
            "Global step: 2497,loss: 0.011475394\n",
            "\n",
            "Global step: 2498,loss: 0.011513523\n",
            "\n",
            "Global step: 2499,loss: 0.024356939\n",
            "\n",
            "Global step: 2500,loss: 0.01360955\n",
            "\n",
            "Global step: 2501,loss: 0.014314525\n",
            "\n",
            "Global step: 2502,loss: 0.015124578\n",
            "\n",
            "Global step: 2503,loss: 0.011353812\n",
            "\n",
            "Global step: 2504,loss: 0.011934833\n",
            "\n",
            "Global step: 2505,loss: 0.016233029\n",
            "\n",
            "Global step: 2506,loss: 0.011391121\n",
            "\n",
            "Global step: 2507,loss: 0.011836647\n",
            "\n",
            "Global step: 2508,loss: 0.01080003\n",
            "\n",
            "Global step: 2509,loss: 0.014960207\n",
            "\n",
            "Global step: 2510,loss: 0.012614961\n",
            "\n",
            "Global step: 2511,loss: 0.01764322\n",
            "\n",
            "Global step: 2512,loss: 0.014259969\n",
            "\n",
            "Global step: 2513,loss: 0.012965985\n",
            "\n",
            "Global step: 2514,loss: 0.016744114\n",
            "\n",
            "Global step: 2515,loss: 0.014424837\n",
            "\n",
            "Global step: 2516,loss: 0.016329866\n",
            "\n",
            "Global step: 2517,loss: 0.015649479\n",
            "\n",
            "Global step: 2518,loss: 0.01700423\n",
            "\n",
            "Global step: 2519,loss: 0.017398369\n",
            "\n",
            "Global step: 2520,loss: 0.015786579\n",
            "\n",
            "Global step: 2521,loss: 0.021036267\n",
            "\n",
            "Global step: 2522,loss: 0.013447129\n",
            "\n",
            "Global step: 2523,loss: 0.013944229\n",
            "\n",
            "Global step: 2524,loss: 0.01324679\n",
            "\n",
            "Global step: 2525,loss: 0.0130844815\n",
            "\n",
            "Global step: 2526,loss: 0.013590283\n",
            "\n",
            "Global step: 2527,loss: 0.014121837\n",
            "\n",
            "Global step: 2528,loss: 0.017944457\n",
            "\n",
            "Global step: 2529,loss: 0.012993429\n",
            "\n",
            "Global step: 2530,loss: 0.012819834\n",
            "\n",
            "Global step: 2531,loss: 0.012538588\n",
            "\n",
            "Global step: 2532,loss: 0.01400983\n",
            "\n",
            "Global step: 2533,loss: 0.0134619195\n",
            "\n",
            "Global step: 2534,loss: 0.01322264\n",
            "\n",
            "Global step: 2535,loss: 0.014311332\n",
            "\n",
            "Global step: 2536,loss: 0.012543019\n",
            "\n",
            "Global step: 2537,loss: 0.011703215\n",
            "\n",
            "Global step: 2538,loss: 0.011783991\n",
            "\n",
            "Global step: 2539,loss: 0.0122526\n",
            "\n",
            "Global step: 2540,loss: 0.013045631\n",
            "\n",
            "Global step: 2541,loss: 0.015871942\n",
            "\n",
            "Global step: 2542,loss: 0.012119846\n",
            "\n",
            "Global step: 2543,loss: 0.0132596865\n",
            "\n",
            "Global step: 2544,loss: 0.024983922\n",
            "\n",
            "Global step: 2545,loss: 0.013063951\n",
            "\n",
            "Global step: 2546,loss: 0.011447336\n",
            "\n",
            "Global step: 2547,loss: 0.021122709\n",
            "\n",
            "Global step: 2548,loss: 0.01693137\n",
            "\n",
            "Global step: 2549,loss: 0.010944345\n",
            "\n",
            "Global step: 2550,loss: 0.015890315\n",
            "\n",
            "Global step: 2551,loss: 0.016024394\n",
            "\n",
            "Global step: 2552,loss: 0.013420769\n",
            "\n",
            "Global step: 2553,loss: 0.014455592\n",
            "\n",
            "Global step: 2554,loss: 0.012616613\n",
            "\n",
            "Global step: 2555,loss: 0.015480127\n",
            "\n",
            "Global step: 2556,loss: 0.012854695\n",
            "\n",
            "Global step: 2557,loss: 0.012518013\n",
            "\n",
            "Global step: 2558,loss: 0.017976841\n",
            "\n",
            "Global step: 2559,loss: 0.013496571\n",
            "\n",
            "Global step: 2560,loss: 0.013615056\n",
            "\n",
            "Global step: 2561,loss: 0.013138075\n",
            "\n",
            "Global step: 2562,loss: 0.014361819\n",
            "\n",
            "Global step: 2563,loss: 0.01671766\n",
            "\n",
            "Global step: 2564,loss: 0.029438598\n",
            "\n",
            "Global step: 2565,loss: 0.0128170755\n",
            "\n",
            "Global step: 2566,loss: 0.012797246\n",
            "\n",
            "Global step: 2567,loss: 0.02648716\n",
            "\n",
            "Global step: 2568,loss: 0.011358088\n",
            "\n",
            "Global step: 2569,loss: 0.010392303\n",
            "\n",
            "Global step: 2570,loss: 0.015667986\n",
            "\n",
            "Global step: 2571,loss: 0.013331575\n",
            "\n",
            "Global step: 2572,loss: 0.0122088\n",
            "\n",
            "Global step: 2573,loss: 0.020134412\n",
            "\n",
            "Global step: 2574,loss: 0.013022172\n",
            "\n",
            "Global step: 2575,loss: 0.02135596\n",
            "\n",
            "Global step: 2576,loss: 0.01498688\n",
            "\n",
            "Global step: 2577,loss: 0.013677934\n",
            "\n",
            "Global step: 2578,loss: 0.014549742\n",
            "\n",
            "Global step: 2579,loss: 0.015985584\n",
            "\n",
            "Global step: 2580,loss: 0.014206715\n",
            "\n",
            "Global step: 2581,loss: 0.013782352\n",
            "\n",
            "Global step: 2582,loss: 0.02633721\n",
            "\n",
            "Global step: 2583,loss: 0.013448186\n",
            "\n",
            "Global step: 2584,loss: 0.015803255\n",
            "\n",
            "Global step: 2585,loss: 0.023603974\n",
            "\n",
            "Global step: 2586,loss: 0.012142891\n",
            "\n",
            "Global step: 2587,loss: 0.01644255\n",
            "\n",
            "Global step: 2588,loss: 0.011896118\n",
            "\n",
            "Global step: 2589,loss: 0.019224\n",
            "\n",
            "Global step: 2590,loss: 0.011670288\n",
            "\n",
            "Global step: 2591,loss: 0.011033222\n",
            "\n",
            "Global step: 2592,loss: 0.013239385\n",
            "\n",
            "Global step: 2593,loss: 0.013850682\n",
            "\n",
            "Global step: 2594,loss: 0.012449196\n",
            "\n",
            "Global step: 2595,loss: 0.013834106\n",
            "\n",
            "Global step: 2596,loss: 0.014708381\n",
            "\n",
            "Global step: 2597,loss: 0.014567368\n",
            "\n",
            "Global step: 2598,loss: 0.011947528\n",
            "\n",
            "Global step: 2599,loss: 0.012295109\n",
            "\n",
            "Global step: 2600,loss: 0.014630809\n",
            "\n",
            "Global step: 2601,loss: 0.015314597\n",
            "\n",
            "Global step: 2602,loss: 0.014427644\n",
            "\n",
            "Global step: 2603,loss: 0.013457874\n",
            "\n",
            "Global step: 2604,loss: 0.010582389\n",
            "\n",
            "Global step: 2605,loss: 0.011456862\n",
            "\n",
            "Global step: 2606,loss: 0.01378926\n",
            "\n",
            "Global step: 2607,loss: 0.015272457\n",
            "\n",
            "Global step: 2608,loss: 0.0133067025\n",
            "\n",
            "Global step: 2609,loss: 0.011346242\n",
            "\n",
            "Global step: 2610,loss: 0.011809082\n",
            "\n",
            "Global step: 2611,loss: 0.011078298\n",
            "\n",
            "Global step: 2612,loss: 0.013756869\n",
            "\n",
            "Global step: 2613,loss: 0.023050562\n",
            "\n",
            "Global step: 2614,loss: 0.012447914\n",
            "\n",
            "Global step: 2615,loss: 0.018257905\n",
            "\n",
            "Global step: 2616,loss: 0.011946265\n",
            "\n",
            "Global step: 2617,loss: 0.012317913\n",
            "\n",
            "Global step: 2618,loss: 0.011685483\n",
            "\n",
            "Global step: 2619,loss: 0.012118584\n",
            "\n",
            "Global step: 2620,loss: 0.015765443\n",
            "\n",
            "Global step: 2621,loss: 0.012845769\n",
            "\n",
            "Global step: 2622,loss: 0.014105111\n",
            "\n",
            "Global step: 2623,loss: 0.016860789\n",
            "\n",
            "Global step: 2624,loss: 0.019899849\n",
            "\n",
            "Global step: 2625,loss: 0.0124921985\n",
            "\n",
            "Global step: 2626,loss: 0.017573725\n",
            "\n",
            "Global step: 2627,loss: 0.019587701\n",
            "\n",
            "Global step: 2628,loss: 0.014889047\n",
            "\n",
            "Global step: 2629,loss: 0.022731055\n",
            "\n",
            "Global step: 2630,loss: 0.012528389\n",
            "\n",
            "Global step: 2631,loss: 0.021004565\n",
            "\n",
            "Global step: 2632,loss: 0.0128870085\n",
            "\n",
            "Global step: 2633,loss: 0.01226664\n",
            "\n",
            "Global step: 2634,loss: 0.020810865\n",
            "\n",
            "Global step: 2635,loss: 0.012229744\n",
            "\n",
            "Global step: 2636,loss: 0.011268733\n",
            "\n",
            "Global step: 2637,loss: 0.011698564\n",
            "\n",
            "Global step: 2638,loss: 0.010495204\n",
            "\n",
            "Global step: 2639,loss: 0.010898557\n",
            "\n",
            "Global step: 2640,loss: 0.013197708\n",
            "\n",
            "Global step: 2641,loss: 0.014087027\n",
            "\n",
            "Global step: 2642,loss: 0.011631413\n",
            "\n",
            "Global step: 2643,loss: 0.011102273\n",
            "\n",
            "Global step: 2644,loss: 0.028682876\n",
            "\n",
            "Global step: 2645,loss: 0.015948191\n",
            "\n",
            "Global step: 2646,loss: 0.0120537905\n",
            "\n",
            "Global step: 2647,loss: 0.012636144\n",
            "\n",
            "Global step: 2648,loss: 0.018527891\n",
            "\n",
            "Global step: 2649,loss: 0.013227794\n",
            "\n",
            "Global step: 2650,loss: 0.012356551\n",
            "\n",
            "Global step: 2651,loss: 0.012083871\n",
            "\n",
            "Global step: 2652,loss: 0.017880492\n",
            "\n",
            "Global step: 2653,loss: 0.011506505\n",
            "\n",
            "Global step: 2654,loss: 0.019547893\n",
            "\n",
            "Global step: 2655,loss: 0.01335015\n",
            "\n",
            "Global step: 2656,loss: 0.012055794\n",
            "\n",
            "Global step: 2657,loss: 0.012773924\n",
            "\n",
            "Global step: 2658,loss: 0.014294919\n",
            "\n",
            "Global step: 2659,loss: 0.011622097\n",
            "\n",
            "Global step: 2660,loss: 0.013477102\n",
            "\n",
            "Global step: 2661,loss: 0.011706309\n",
            "\n",
            "Global step: 2662,loss: 0.01398704\n",
            "\n",
            "Global step: 2663,loss: 0.016847134\n",
            "\n",
            "Global step: 2664,loss: 0.014880087\n",
            "\n",
            "Global step: 2665,loss: 0.014156798\n",
            "\n",
            "Global step: 2666,loss: 0.013068948\n",
            "\n",
            "Global step: 2667,loss: 0.021350566\n",
            "\n",
            "Global step: 2668,loss: 0.014086949\n",
            "\n",
            "Global step: 2669,loss: 0.023372142\n",
            "\n",
            "Global step: 2670,loss: 0.011536369\n",
            "\n",
            "Global step: 2671,loss: 0.0123190805\n",
            "\n",
            "Global step: 2672,loss: 0.0122960135\n",
            "\n",
            "Global step: 2673,loss: 0.010688825\n",
            "\n",
            "Global step: 2674,loss: 0.015396686\n",
            "\n",
            "Global step: 2675,loss: 0.015521286\n",
            "\n",
            "Global step: 2676,loss: 0.014351681\n",
            "\n",
            "Global step: 2677,loss: 0.012376246\n",
            "\n",
            "Global step: 2678,loss: 0.016510323\n",
            "\n",
            "Global step: 2679,loss: 0.013526777\n",
            "\n",
            "Global step: 2680,loss: 0.014720048\n",
            "\n",
            "Global step: 2681,loss: 0.01182161\n",
            "\n",
            "Global step: 2682,loss: 0.01516581\n",
            "\n",
            "Global step: 2683,loss: 0.010967509\n",
            "\n",
            "Global step: 2684,loss: 0.015477959\n",
            "\n",
            "Global step: 2685,loss: 0.013596232\n",
            "\n",
            "Global step: 2686,loss: 0.012186194\n",
            "\n",
            "Global step: 2687,loss: 0.01139418\n",
            "\n",
            "Global step: 2688,loss: 0.019378373\n",
            "\n",
            "Global step: 2689,loss: 0.01371392\n",
            "\n",
            "Global step: 2690,loss: 0.0130665675\n",
            "\n",
            "Global step: 2691,loss: 0.012156913\n",
            "\n",
            "Global step: 2692,loss: 0.01738125\n",
            "\n",
            "Global step: 2693,loss: 0.01173177\n",
            "\n",
            "Global step: 2694,loss: 0.0155962305\n",
            "\n",
            "Global step: 2695,loss: 0.022148604\n",
            "\n",
            "Global step: 2696,loss: 0.0128842145\n",
            "\n",
            "Global step: 2697,loss: 0.011781947\n",
            "\n",
            "Global step: 2698,loss: 0.017056638\n",
            "\n",
            "Global step: 2699,loss: 0.0124387285\n",
            "\n",
            "Global step: 2700,loss: 0.012529578\n",
            "\n",
            "Global step: 2701,loss: 0.011482161\n",
            "\n",
            "Global step: 2702,loss: 0.011499833\n",
            "\n",
            "Global step: 2703,loss: 0.013376281\n",
            "\n",
            "Global step: 2704,loss: 0.012014614\n",
            "\n",
            "Global step: 2705,loss: 0.012383409\n",
            "\n",
            "Global step: 2706,loss: 0.0113831125\n",
            "\n",
            "Global step: 2707,loss: 0.016626187\n",
            "\n",
            "Global step: 2708,loss: 0.011772621\n",
            "\n",
            "Global step: 2709,loss: 0.016998937\n",
            "\n",
            "Global step: 2710,loss: 0.012349478\n",
            "\n",
            "Global step: 2711,loss: 0.010927208\n",
            "\n",
            "Global step: 2712,loss: 0.011888605\n",
            "\n",
            "Global step: 2713,loss: 0.0145129245\n",
            "\n",
            "Global step: 2714,loss: 0.019346807\n",
            "\n",
            "Global step: 2715,loss: 0.015581682\n",
            "\n",
            "Global step: 2716,loss: 0.0125462\n",
            "\n",
            "Global step: 2717,loss: 0.013166605\n",
            "\n",
            "Global step: 2718,loss: 0.020133309\n",
            "\n",
            "Global step: 2719,loss: 0.01300689\n",
            "\n",
            "Global step: 2720,loss: 0.0152675975\n",
            "\n",
            "Global step: 2721,loss: 0.016130358\n",
            "\n",
            "Global step: 2722,loss: 0.016522223\n",
            "\n",
            "Global step: 2723,loss: 0.015264561\n",
            "\n",
            "Global step: 2724,loss: 0.016186766\n",
            "\n",
            "Global step: 2725,loss: 0.014834864\n",
            "\n",
            "Global step: 2726,loss: 0.013733477\n",
            "\n",
            "Global step: 2727,loss: 0.010712931\n",
            "\n",
            "Global step: 2728,loss: 0.012763731\n",
            "\n",
            "Global step: 2729,loss: 0.012184542\n",
            "\n",
            "Global step: 2730,loss: 0.016670402\n",
            "\n",
            "Global step: 2731,loss: 0.022658225\n",
            "\n",
            "Global step: 2732,loss: 0.01636316\n",
            "\n",
            "Global step: 2733,loss: 0.011582168\n",
            "\n",
            "Global step: 2734,loss: 0.010735169\n",
            "\n",
            "Global step: 2735,loss: 0.011126682\n",
            "\n",
            "Global step: 2736,loss: 0.017944211\n",
            "\n",
            "Global step: 2737,loss: 0.022846792\n",
            "\n",
            "Global step: 2738,loss: 0.012165437\n",
            "\n",
            "Global step: 2739,loss: 0.017176254\n",
            "\n",
            "Global step: 2740,loss: 0.012202729\n",
            "\n",
            "Global step: 2741,loss: 0.019733205\n",
            "\n",
            "Global step: 2742,loss: 0.011865066\n",
            "\n",
            "Global step: 2743,loss: 0.012516809\n",
            "\n",
            "Global step: 2744,loss: 0.01759118\n",
            "\n",
            "Global step: 2745,loss: 0.011649486\n",
            "\n",
            "Global step: 2746,loss: 0.013699653\n",
            "\n",
            "Global step: 2747,loss: 0.013937549\n",
            "\n",
            "Global step: 2748,loss: 0.015554274\n",
            "\n",
            "Global step: 2749,loss: 0.01837541\n",
            "\n",
            "Global step: 2750,loss: 0.013892416\n",
            "\n",
            "Global step: 2751,loss: 0.019302882\n",
            "\n",
            "Global step: 2752,loss: 0.012365731\n",
            "\n",
            "Global step: 2753,loss: 0.012562811\n",
            "\n",
            "Global step: 2754,loss: 0.015816052\n",
            "\n",
            "Global step: 2755,loss: 0.01126855\n",
            "\n",
            "Global step: 2756,loss: 0.018149327\n",
            "\n",
            "Global step: 2757,loss: 0.017212758\n",
            "\n",
            "Global step: 2758,loss: 0.01241884\n",
            "\n",
            "Global step: 2759,loss: 0.01461851\n",
            "\n",
            "Global step: 2760,loss: 0.017669642\n",
            "\n",
            "Global step: 2761,loss: 0.011737547\n",
            "\n",
            "Global step: 2762,loss: 0.020230453\n",
            "\n",
            "Global step: 2763,loss: 0.019741515\n",
            "\n",
            "Global step: 2764,loss: 0.012612119\n",
            "\n",
            "Global step: 2765,loss: 0.013058808\n",
            "\n",
            "Global step: 2766,loss: 0.0130147645\n",
            "\n",
            "Global step: 2767,loss: 0.0118096685\n",
            "\n",
            "Global step: 2768,loss: 0.013059138\n",
            "\n",
            "Global step: 2769,loss: 0.015844673\n",
            "\n",
            "Global step: 2770,loss: 0.012871276\n",
            "\n",
            "Global step: 2771,loss: 0.014315168\n",
            "\n",
            "Global step: 2772,loss: 0.009946291\n",
            "\n",
            "Global step: 2773,loss: 0.013156407\n",
            "\n",
            "Global step: 2774,loss: 0.014652032\n",
            "\n",
            "Global step: 2775,loss: 0.015775092\n",
            "\n",
            "Global step: 2776,loss: 0.011593137\n",
            "\n",
            "Global step: 2777,loss: 0.013403773\n",
            "\n",
            "Global step: 2778,loss: 0.015576184\n",
            "\n",
            "Global step: 2779,loss: 0.011706059\n",
            "\n",
            "Global step: 2780,loss: 0.011965556\n",
            "\n",
            "Global step: 2781,loss: 0.012273092\n",
            "\n",
            "Global step: 2782,loss: 0.015993502\n",
            "\n",
            "Global step: 2783,loss: 0.013479665\n",
            "\n",
            "Global step: 2784,loss: 0.018391479\n",
            "\n",
            "Global step: 2785,loss: 0.013601458\n",
            "\n",
            "Global step: 2786,loss: 0.013528179\n",
            "\n",
            "Global step: 2787,loss: 0.014252026\n",
            "\n",
            "Global step: 2788,loss: 0.010982267\n",
            "\n",
            "Global step: 2789,loss: 0.013778717\n",
            "\n",
            "Global step: 2790,loss: 0.016472155\n",
            "\n",
            "Global step: 2791,loss: 0.011322709\n",
            "\n",
            "Global step: 2792,loss: 0.012822026\n",
            "\n",
            "Global step: 2793,loss: 0.013496822\n",
            "\n",
            "Global step: 2794,loss: 0.0127810715\n",
            "\n",
            "Global step: 2795,loss: 0.011397158\n",
            "\n",
            "Global step: 2796,loss: 0.010025856\n",
            "\n",
            "Global step: 2797,loss: 0.01470631\n",
            "\n",
            "Global step: 2798,loss: 0.016575348\n",
            "\n",
            "Global step: 2799,loss: 0.01367409\n",
            "\n",
            "Global step: 2800,loss: 0.010640524\n",
            "\n",
            "Global step: 2801,loss: 0.011068144\n",
            "\n",
            "Global step: 2802,loss: 0.014125996\n",
            "\n",
            "Global step: 2803,loss: 0.0116259875\n",
            "\n",
            "Global step: 2804,loss: 0.011182256\n",
            "\n",
            "Global step: 2805,loss: 0.023602448\n",
            "\n",
            "Global step: 2806,loss: 0.0123173455\n",
            "\n",
            "Global step: 2807,loss: 0.019876238\n",
            "\n",
            "Global step: 2808,loss: 0.018539764\n",
            "\n",
            "Global step: 2809,loss: 0.013452849\n",
            "\n",
            "Global step: 2810,loss: 0.012110756\n",
            "\n",
            "Global step: 2811,loss: 0.011535608\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2811,Val_Loss: 0.014346982686756512,  Val_acc: 0.9984642094017094 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:49:57.444446 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/15:\n",
            "Global step: 2812,loss: 0.011083709\n",
            "\n",
            "Global step: 2813,loss: 0.014066316\n",
            "\n",
            "Global step: 2814,loss: 0.011429066\n",
            "\n",
            "Global step: 2815,loss: 0.010646966\n",
            "\n",
            "Global step: 2816,loss: 0.011502062\n",
            "\n",
            "Global step: 2817,loss: 0.010689733\n",
            "\n",
            "Global step: 2818,loss: 0.011043834\n",
            "\n",
            "Global step: 2819,loss: 0.012874575\n",
            "\n",
            "Global step: 2820,loss: 0.011870673\n",
            "\n",
            "Global step: 2821,loss: 0.018028274\n",
            "\n",
            "Global step: 2822,loss: 0.011907585\n",
            "\n",
            "Global step: 2823,loss: 0.010854257\n",
            "\n",
            "Global step: 2824,loss: 0.014555726\n",
            "\n",
            "Global step: 2825,loss: 0.012576494\n",
            "\n",
            "Global step: 2826,loss: 0.014784866\n",
            "\n",
            "Global step: 2827,loss: 0.011141574\n",
            "\n",
            "Global step: 2828,loss: 0.011210538\n",
            "\n",
            "Global step: 2829,loss: 0.011036151\n",
            "\n",
            "Global step: 2830,loss: 0.010394938\n",
            "\n",
            "Global step: 2831,loss: 0.019394057\n",
            "\n",
            "Global step: 2832,loss: 0.013340073\n",
            "\n",
            "Global step: 2833,loss: 0.017958738\n",
            "\n",
            "Global step: 2834,loss: 0.016926946\n",
            "\n",
            "Global step: 2835,loss: 0.012388243\n",
            "\n",
            "Global step: 2836,loss: 0.01311858\n",
            "\n",
            "Global step: 2837,loss: 0.013288814\n",
            "\n",
            "Global step: 2838,loss: 0.012166211\n",
            "\n",
            "Global step: 2839,loss: 0.011225994\n",
            "\n",
            "Global step: 2840,loss: 0.0124459425\n",
            "\n",
            "Global step: 2841,loss: 0.010457169\n",
            "\n",
            "Global step: 2842,loss: 0.010578969\n",
            "\n",
            "Global step: 2843,loss: 0.010972315\n",
            "\n",
            "Global step: 2844,loss: 0.010897283\n",
            "\n",
            "Global step: 2845,loss: 0.010365315\n",
            "\n",
            "Global step: 2846,loss: 0.011933999\n",
            "\n",
            "Global step: 2847,loss: 0.020283647\n",
            "\n",
            "Global step: 2848,loss: 0.011878235\n",
            "\n",
            "Global step: 2849,loss: 0.011152095\n",
            "\n",
            "Global step: 2850,loss: 0.01201191\n",
            "\n",
            "Global step: 2851,loss: 0.011346627\n",
            "\n",
            "Global step: 2852,loss: 0.011304455\n",
            "\n",
            "Global step: 2853,loss: 0.012217716\n",
            "\n",
            "Global step: 2854,loss: 0.010733464\n",
            "\n",
            "Global step: 2855,loss: 0.011901593\n",
            "\n",
            "Global step: 2856,loss: 0.012359576\n",
            "\n",
            "Global step: 2857,loss: 0.011582608\n",
            "\n",
            "Global step: 2858,loss: 0.011926799\n",
            "\n",
            "Global step: 2859,loss: 0.015873067\n",
            "\n",
            "Global step: 2860,loss: 0.01497224\n",
            "\n",
            "Global step: 2861,loss: 0.018485885\n",
            "\n",
            "Global step: 2862,loss: 0.010138355\n",
            "\n",
            "Global step: 2863,loss: 0.0117553715\n",
            "\n",
            "Global step: 2864,loss: 0.010129337\n",
            "\n",
            "Global step: 2865,loss: 0.010901738\n",
            "\n",
            "Global step: 2866,loss: 0.01352584\n",
            "\n",
            "Global step: 2867,loss: 0.011585832\n",
            "\n",
            "Global step: 2868,loss: 0.011639601\n",
            "\n",
            "Global step: 2869,loss: 0.017432882\n",
            "\n",
            "Global step: 2870,loss: 0.010575357\n",
            "\n",
            "Global step: 2871,loss: 0.010541628\n",
            "\n",
            "Global step: 2872,loss: 0.010569901\n",
            "\n",
            "Global step: 2873,loss: 0.011295287\n",
            "\n",
            "Global step: 2874,loss: 0.011328594\n",
            "\n",
            "Global step: 2875,loss: 0.011456305\n",
            "\n",
            "Global step: 2876,loss: 0.01133753\n",
            "\n",
            "Global step: 2877,loss: 0.011500785\n",
            "\n",
            "Global step: 2878,loss: 0.014918564\n",
            "\n",
            "Global step: 2879,loss: 0.00991152\n",
            "\n",
            "Global step: 2880,loss: 0.011134243\n",
            "\n",
            "Global step: 2881,loss: 0.0105575705\n",
            "\n",
            "Global step: 2882,loss: 0.014948272\n",
            "\n",
            "Global step: 2883,loss: 0.0111053055\n",
            "\n",
            "Global step: 2884,loss: 0.010247437\n",
            "\n",
            "Global step: 2885,loss: 0.0126762865\n",
            "\n",
            "Global step: 2886,loss: 0.01655763\n",
            "\n",
            "Global step: 2887,loss: 0.016913783\n",
            "\n",
            "Global step: 2888,loss: 0.012000933\n",
            "\n",
            "Global step: 2889,loss: 0.011381699\n",
            "\n",
            "Global step: 2890,loss: 0.009597956\n",
            "\n",
            "Global step: 2891,loss: 0.010086845\n",
            "\n",
            "Global step: 2892,loss: 0.011964354\n",
            "\n",
            "Global step: 2893,loss: 0.011061366\n",
            "\n",
            "Global step: 2894,loss: 0.01057627\n",
            "\n",
            "Global step: 2895,loss: 0.010517078\n",
            "\n",
            "Global step: 2896,loss: 0.011416993\n",
            "\n",
            "Global step: 2897,loss: 0.015535024\n",
            "\n",
            "Global step: 2898,loss: 0.011367042\n",
            "\n",
            "Global step: 2899,loss: 0.011125534\n",
            "\n",
            "Global step: 2900,loss: 0.011769864\n",
            "\n",
            "Global step: 2901,loss: 0.010396579\n",
            "\n",
            "Global step: 2902,loss: 0.011039147\n",
            "\n",
            "Global step: 2903,loss: 0.012397994\n",
            "\n",
            "Global step: 2904,loss: 0.012725425\n",
            "\n",
            "Global step: 2905,loss: 0.010523219\n",
            "\n",
            "Global step: 2906,loss: 0.01248608\n",
            "\n",
            "Global step: 2907,loss: 0.009997393\n",
            "\n",
            "Global step: 2908,loss: 0.018782806\n",
            "\n",
            "Global step: 2909,loss: 0.010012869\n",
            "\n",
            "Global step: 2910,loss: 0.010986591\n",
            "\n",
            "Global step: 2911,loss: 0.01117065\n",
            "\n",
            "Global step: 2912,loss: 0.012319359\n",
            "\n",
            "Global step: 2913,loss: 0.012102371\n",
            "\n",
            "Global step: 2914,loss: 0.01107779\n",
            "\n",
            "Global step: 2915,loss: 0.012277939\n",
            "\n",
            "Global step: 2916,loss: 0.010548862\n",
            "\n",
            "Global step: 2917,loss: 0.012407521\n",
            "\n",
            "Global step: 2918,loss: 0.018265639\n",
            "\n",
            "Global step: 2919,loss: 0.010495803\n",
            "\n",
            "Global step: 2920,loss: 0.011547642\n",
            "\n",
            "Global step: 2921,loss: 0.010494034\n",
            "\n",
            "Global step: 2922,loss: 0.012096322\n",
            "\n",
            "Global step: 2923,loss: 0.010579811\n",
            "\n",
            "Global step: 2924,loss: 0.010779474\n",
            "\n",
            "Global step: 2925,loss: 0.011763419\n",
            "\n",
            "Global step: 2926,loss: 0.014025274\n",
            "\n",
            "Global step: 2927,loss: 0.010928719\n",
            "\n",
            "Global step: 2928,loss: 0.010954181\n",
            "\n",
            "Global step: 2929,loss: 0.011739632\n",
            "\n",
            "Global step: 2930,loss: 0.011269649\n",
            "\n",
            "Global step: 2931,loss: 0.011371474\n",
            "\n",
            "Global step: 2932,loss: 0.012359495\n",
            "\n",
            "Global step: 2933,loss: 0.010707068\n",
            "\n",
            "Global step: 2934,loss: 0.011122679\n",
            "\n",
            "Global step: 2935,loss: 0.010814305\n",
            "\n",
            "Global step: 2936,loss: 0.011408286\n",
            "\n",
            "Global step: 2937,loss: 0.011816766\n",
            "\n",
            "Global step: 2938,loss: 0.010834118\n",
            "\n",
            "Global step: 2939,loss: 0.010460748\n",
            "\n",
            "Global step: 2940,loss: 0.010824627\n",
            "\n",
            "Global step: 2941,loss: 0.010451168\n",
            "\n",
            "Global step: 2942,loss: 0.016377136\n",
            "\n",
            "Global step: 2943,loss: 0.010209887\n",
            "\n",
            "Global step: 2944,loss: 0.011648677\n",
            "\n",
            "Global step: 2945,loss: 0.010844263\n",
            "\n",
            "Global step: 2946,loss: 0.010797423\n",
            "\n",
            "Global step: 2947,loss: 0.014365628\n",
            "\n",
            "Global step: 2948,loss: 0.010290907\n",
            "\n",
            "Global step: 2949,loss: 0.01062806\n",
            "\n",
            "Global step: 2950,loss: 0.011915483\n",
            "\n",
            "Global step: 2951,loss: 0.010940601\n",
            "\n",
            "Global step: 2952,loss: 0.015820166\n",
            "\n",
            "Global step: 2953,loss: 0.013392363\n",
            "\n",
            "Global step: 2954,loss: 0.014488475\n",
            "\n",
            "Global step: 2955,loss: 0.01314778\n",
            "\n",
            "Global step: 2956,loss: 0.012260155\n",
            "\n",
            "Global step: 2957,loss: 0.021742191\n",
            "\n",
            "Global step: 2958,loss: 0.015897501\n",
            "\n",
            "Global step: 2959,loss: 0.011405318\n",
            "\n",
            "Global step: 2960,loss: 0.019592118\n",
            "\n",
            "Global step: 2961,loss: 0.016778344\n",
            "\n",
            "Global step: 2962,loss: 0.0110709\n",
            "\n",
            "Global step: 2963,loss: 0.015431511\n",
            "\n",
            "Global step: 2964,loss: 0.011181571\n",
            "\n",
            "Global step: 2965,loss: 0.015066573\n",
            "\n",
            "Global step: 2966,loss: 0.011192554\n",
            "\n",
            "Global step: 2967,loss: 0.010959817\n",
            "\n",
            "Global step: 2968,loss: 0.011653943\n",
            "\n",
            "Global step: 2969,loss: 0.009900288\n",
            "\n",
            "Global step: 2970,loss: 0.010147064\n",
            "\n",
            "Global step: 2971,loss: 0.014254414\n",
            "\n",
            "Global step: 2972,loss: 0.010516396\n",
            "\n",
            "Global step: 2973,loss: 0.0100042345\n",
            "\n",
            "Global step: 2974,loss: 0.010539634\n",
            "\n",
            "Global step: 2975,loss: 0.00999613\n",
            "\n",
            "Global step: 2976,loss: 0.013556834\n",
            "\n",
            "Global step: 2977,loss: 0.010381685\n",
            "\n",
            "Global step: 2978,loss: 0.011705594\n",
            "\n",
            "Global step: 2979,loss: 0.014536301\n",
            "\n",
            "Global step: 2980,loss: 0.010521179\n",
            "\n",
            "Global step: 2981,loss: 0.010090184\n",
            "\n",
            "Global step: 2982,loss: 0.010179024\n",
            "\n",
            "Global step: 2983,loss: 0.011453878\n",
            "\n",
            "Global step: 2984,loss: 0.010617145\n",
            "\n",
            "Global step: 2985,loss: 0.011007698\n",
            "\n",
            "Global step: 2986,loss: 0.009853547\n",
            "\n",
            "Global step: 2987,loss: 0.010967007\n",
            "\n",
            "Global step: 2988,loss: 0.01174669\n",
            "\n",
            "Global step: 2989,loss: 0.010132397\n",
            "\n",
            "Global step: 2990,loss: 0.0114291785\n",
            "\n",
            "Global step: 2991,loss: 0.010583626\n",
            "\n",
            "Global step: 2992,loss: 0.015425649\n",
            "\n",
            "Global step: 2993,loss: 0.0101757245\n",
            "\n",
            "Global step: 2994,loss: 0.011791312\n",
            "\n",
            "Global step: 2995,loss: 0.0105814915\n",
            "\n",
            "Global step: 2996,loss: 0.013294253\n",
            "\n",
            "Global step: 2997,loss: 0.010293896\n",
            "\n",
            "Global step: 2998,loss: 0.016327312\n",
            "\n",
            "Global step: 2999,loss: 0.01167018\n",
            "\n",
            "Global step: 3000,loss: 0.013025679\n",
            "\n",
            "Global step: 3001,loss: 0.010204175\n",
            "\n",
            "Global step: 3002,loss: 0.010839325\n",
            "\n",
            "Global step: 3003,loss: 0.010871775\n",
            "\n",
            "Global step: 3004,loss: 0.012868058\n",
            "\n",
            "Global step: 3005,loss: 0.010855572\n",
            "\n",
            "Global step: 3006,loss: 0.012806107\n",
            "\n",
            "Global step: 3007,loss: 0.011228117\n",
            "\n",
            "Global step: 3008,loss: 0.012618145\n",
            "\n",
            "Global step: 3009,loss: 0.011967947\n",
            "\n",
            "Global step: 3010,loss: 0.010416933\n",
            "\n",
            "Global step: 3011,loss: 0.00960358\n",
            "\n",
            "Global step: 3012,loss: 0.01039685\n",
            "\n",
            "Global step: 3013,loss: 0.010564209\n",
            "\n",
            "Global step: 3014,loss: 0.010842555\n",
            "\n",
            "Global step: 3015,loss: 0.01099428\n",
            "\n",
            "Global step: 3016,loss: 0.010517689\n",
            "\n",
            "Global step: 3017,loss: 0.012781401\n",
            "\n",
            "Global step: 3018,loss: 0.01404859\n",
            "\n",
            "Global step: 3019,loss: 0.0120691955\n",
            "\n",
            "Global step: 3020,loss: 0.010157413\n",
            "\n",
            "Global step: 3021,loss: 0.010011707\n",
            "\n",
            "Global step: 3022,loss: 0.010773623\n",
            "\n",
            "Global step: 3023,loss: 0.011124961\n",
            "\n",
            "Global step: 3024,loss: 0.010265514\n",
            "\n",
            "Global step: 3025,loss: 0.010765052\n",
            "\n",
            "Global step: 3026,loss: 0.011243962\n",
            "\n",
            "Global step: 3027,loss: 0.010428817\n",
            "\n",
            "Global step: 3028,loss: 0.012135392\n",
            "\n",
            "Global step: 3029,loss: 0.019730913\n",
            "\n",
            "Global step: 3030,loss: 0.010732038\n",
            "\n",
            "Global step: 3031,loss: 0.010913235\n",
            "\n",
            "Global step: 3032,loss: 0.0121166045\n",
            "\n",
            "Global step: 3033,loss: 0.012221747\n",
            "\n",
            "Global step: 3034,loss: 0.0108945\n",
            "\n",
            "Global step: 3035,loss: 0.012296782\n",
            "\n",
            "Global step: 3036,loss: 0.011938003\n",
            "\n",
            "Global step: 3037,loss: 0.011655059\n",
            "\n",
            "Global step: 3038,loss: 0.0101443175\n",
            "\n",
            "Global step: 3039,loss: 0.011150999\n",
            "\n",
            "Global step: 3040,loss: 0.010654328\n",
            "\n",
            "Global step: 3041,loss: 0.010437931\n",
            "\n",
            "Global step: 3042,loss: 0.011508405\n",
            "\n",
            "Global step: 3043,loss: 0.012154911\n",
            "\n",
            "Global step: 3044,loss: 0.011369996\n",
            "\n",
            "Global step: 3045,loss: 0.012540514\n",
            "\n",
            "Global step: 3046,loss: 0.010780225\n",
            "\n",
            "Global step: 3047,loss: 0.010177658\n",
            "\n",
            "Global step: 3048,loss: 0.0102196615\n",
            "\n",
            "Global step: 3049,loss: 0.014107548\n",
            "\n",
            "Global step: 3050,loss: 0.010142397\n",
            "\n",
            "Global step: 3051,loss: 0.011381034\n",
            "\n",
            "Global step: 3052,loss: 0.016960386\n",
            "\n",
            "Global step: 3053,loss: 0.010199898\n",
            "\n",
            "Global step: 3054,loss: 0.0103543205\n",
            "\n",
            "Global step: 3055,loss: 0.014991667\n",
            "\n",
            "Global step: 3056,loss: 0.010000592\n",
            "\n",
            "Global step: 3057,loss: 0.010983678\n",
            "\n",
            "Global step: 3058,loss: 0.019892137\n",
            "\n",
            "Global step: 3059,loss: 0.012024303\n",
            "\n",
            "Global step: 3060,loss: 0.012267901\n",
            "\n",
            "Global step: 3061,loss: 0.012192239\n",
            "\n",
            "Global step: 3062,loss: 0.00995964\n",
            "\n",
            "Global step: 3063,loss: 0.010251645\n",
            "\n",
            "Global step: 3064,loss: 0.010648808\n",
            "\n",
            "Global step: 3065,loss: 0.010418705\n",
            "\n",
            "Global step: 3066,loss: 0.010205436\n",
            "\n",
            "Global step: 3067,loss: 0.016493673\n",
            "\n",
            "Global step: 3068,loss: 0.010446257\n",
            "\n",
            "Global step: 3069,loss: 0.011346371\n",
            "\n",
            "Global step: 3070,loss: 0.011515212\n",
            "\n",
            "Global step: 3071,loss: 0.011112036\n",
            "\n",
            "Global step: 3072,loss: 0.011517987\n",
            "\n",
            "Global step: 3073,loss: 0.009874025\n",
            "\n",
            "Global step: 3074,loss: 0.01016019\n",
            "\n",
            "Global step: 3075,loss: 0.016937088\n",
            "\n",
            "Global step: 3076,loss: 0.010700201\n",
            "\n",
            "Global step: 3077,loss: 0.015780404\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.66819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:50:35.226263 139837015512832 supervisor.py:1099] global_step/sec: 6.66819\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 3078.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:50:35.226822 139837023905536 supervisor.py:1050] Recording summary at step 3078.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3078,loss: 0.010717929\n",
            "\n",
            "Global step: 3079,loss: 0.011342164\n",
            "\n",
            "Global step: 3080,loss: 0.009841578\n",
            "\n",
            "Global step: 3081,loss: 0.010043241\n",
            "\n",
            "Global step: 3082,loss: 0.013154271\n",
            "\n",
            "Global step: 3083,loss: 0.011096693\n",
            "\n",
            "Global step: 3084,loss: 0.011468069\n",
            "\n",
            "Global step: 3085,loss: 0.0113077285\n",
            "\n",
            "Global step: 3086,loss: 0.015068762\n",
            "\n",
            "Global step: 3087,loss: 0.013131289\n",
            "\n",
            "Global step: 3088,loss: 0.012966596\n",
            "\n",
            "Global step: 3089,loss: 0.010702086\n",
            "\n",
            "Global step: 3090,loss: 0.012716145\n",
            "\n",
            "Global step: 3091,loss: 0.010995527\n",
            "\n",
            "Global step: 3092,loss: 0.011114348\n",
            "\n",
            "Global step: 3093,loss: 0.01649221\n",
            "\n",
            "Global step: 3094,loss: 0.011216769\n",
            "\n",
            "Global step: 3095,loss: 0.01438818\n",
            "\n",
            "Global step: 3096,loss: 0.01139362\n",
            "\n",
            "Global step: 3097,loss: 0.015341191\n",
            "\n",
            "Global step: 3098,loss: 0.011656616\n",
            "\n",
            "Global step: 3099,loss: 0.010844326\n",
            "\n",
            "Global step: 3100,loss: 0.01149926\n",
            "\n",
            "Global step: 3101,loss: 0.0105848815\n",
            "\n",
            "Global step: 3102,loss: 0.011556601\n",
            "\n",
            "Global step: 3103,loss: 0.012126334\n",
            "\n",
            "Global step: 3104,loss: 0.020151924\n",
            "\n",
            "Global step: 3105,loss: 0.0131393215\n",
            "\n",
            "Global step: 3106,loss: 0.0140069295\n",
            "\n",
            "Global step: 3107,loss: 0.010779334\n",
            "\n",
            "Global step: 3108,loss: 0.014860612\n",
            "\n",
            "Global step: 3109,loss: 0.02087701\n",
            "\n",
            "Global step: 3110,loss: 0.015319057\n",
            "\n",
            "Global step: 3111,loss: 0.010352052\n",
            "\n",
            "Global step: 3112,loss: 0.011021951\n",
            "\n",
            "Global step: 3113,loss: 0.0105815325\n",
            "\n",
            "Global step: 3114,loss: 0.016525703\n",
            "\n",
            "Global step: 3115,loss: 0.015761765\n",
            "\n",
            "Global step: 3116,loss: 0.010440448\n",
            "\n",
            "Global step: 3117,loss: 0.012977578\n",
            "\n",
            "Global step: 3118,loss: 0.011294956\n",
            "\n",
            "Global step: 3119,loss: 0.018944863\n",
            "\n",
            "Global step: 3120,loss: 0.013472613\n",
            "\n",
            "Global step: 3121,loss: 0.012456596\n",
            "\n",
            "Global step: 3122,loss: 0.010817585\n",
            "\n",
            "Global step: 3123,loss: 0.011776099\n",
            "\n",
            "Global step: 3124,loss: 0.011742624\n",
            "\n",
            "Global step: 3125,loss: 0.012050589\n",
            "\n",
            "Global step: 3126,loss: 0.013751766\n",
            "\n",
            "Global step: 3127,loss: 0.012702112\n",
            "\n",
            "Global step: 3128,loss: 0.00955419\n",
            "\n",
            "Global step: 3129,loss: 0.01042787\n",
            "\n",
            "Global step: 3130,loss: 0.0128145795\n",
            "\n",
            "Global step: 3131,loss: 0.011383651\n",
            "\n",
            "Global step: 3132,loss: 0.010822881\n",
            "\n",
            "Global step: 3133,loss: 0.010079462\n",
            "\n",
            "Global step: 3134,loss: 0.010975128\n",
            "\n",
            "Global step: 3135,loss: 0.012883581\n",
            "\n",
            "Global step: 3136,loss: 0.011441724\n",
            "\n",
            "Global step: 3137,loss: 0.012130929\n",
            "\n",
            "Global step: 3138,loss: 0.011271507\n",
            "\n",
            "Global step: 3139,loss: 0.010384381\n",
            "\n",
            "Global step: 3140,loss: 0.021324893\n",
            "\n",
            "Global step: 3141,loss: 0.015026143\n",
            "\n",
            "Global step: 3142,loss: 0.010595422\n",
            "\n",
            "Global step: 3143,loss: 0.011019863\n",
            "\n",
            "Global step: 3144,loss: 0.011085322\n",
            "\n",
            "Global step: 3145,loss: 0.012732329\n",
            "\n",
            "Global step: 3146,loss: 0.010787364\n",
            "\n",
            "Global step: 3147,loss: 0.0096387975\n",
            "\n",
            "Global step: 3148,loss: 0.0108640045\n",
            "\n",
            "Global step: 3149,loss: 0.01446337\n",
            "\n",
            "Global step: 3150,loss: 0.010082913\n",
            "\n",
            "Global step: 3151,loss: 0.010126317\n",
            "\n",
            "Global step: 3152,loss: 0.021870647\n",
            "\n",
            "Global step: 3153,loss: 0.009744368\n",
            "\n",
            "Global step: 3154,loss: 0.011086155\n",
            "\n",
            "Global step: 3155,loss: 0.011213978\n",
            "\n",
            "Global step: 3156,loss: 0.010876432\n",
            "\n",
            "Global step: 3157,loss: 0.011340486\n",
            "\n",
            "Global step: 3158,loss: 0.013963578\n",
            "\n",
            "Global step: 3159,loss: 0.0133205205\n",
            "\n",
            "Global step: 3160,loss: 0.012222316\n",
            "\n",
            "Global step: 3161,loss: 0.01343724\n",
            "\n",
            "Global step: 3162,loss: 0.014697136\n",
            "\n",
            "Global step: 3163,loss: 0.010839359\n",
            "\n",
            "Global step: 3164,loss: 0.010646127\n",
            "\n",
            "Global step: 3165,loss: 0.012420485\n",
            "\n",
            "Global step: 3166,loss: 0.010398148\n",
            "\n",
            "Global step: 3167,loss: 0.012209015\n",
            "\n",
            "Global step: 3168,loss: 0.01154772\n",
            "\n",
            "Global step: 3169,loss: 0.012908561\n",
            "\n",
            "Global step: 3170,loss: 0.0106798485\n",
            "\n",
            "Global step: 3171,loss: 0.010184726\n",
            "\n",
            "Global step: 3172,loss: 0.013078116\n",
            "\n",
            "Global step: 3173,loss: 0.010608485\n",
            "\n",
            "Global step: 3174,loss: 0.010966076\n",
            "\n",
            "Global step: 3175,loss: 0.0110139605\n",
            "\n",
            "Global step: 3176,loss: 0.013008677\n",
            "\n",
            "Global step: 3177,loss: 0.011315064\n",
            "\n",
            "Global step: 3178,loss: 0.010978403\n",
            "\n",
            "Global step: 3179,loss: 0.015812706\n",
            "\n",
            "Global step: 3180,loss: 0.010825595\n",
            "\n",
            "Global step: 3181,loss: 0.015909618\n",
            "\n",
            "Global step: 3182,loss: 0.01900411\n",
            "\n",
            "Global step: 3183,loss: 0.013473625\n",
            "\n",
            "Global step: 3184,loss: 0.01490613\n",
            "\n",
            "Global step: 3185,loss: 0.02174668\n",
            "\n",
            "Global step: 3186,loss: 0.0105030555\n",
            "\n",
            "Global step: 3187,loss: 0.011302134\n",
            "\n",
            "Global step: 3188,loss: 0.01201813\n",
            "\n",
            "Global step: 3189,loss: 0.01011934\n",
            "\n",
            "Global step: 3190,loss: 0.014593276\n",
            "\n",
            "Global step: 3191,loss: 0.02021901\n",
            "\n",
            "Global step: 3192,loss: 0.012675185\n",
            "\n",
            "Global step: 3193,loss: 0.012482621\n",
            "\n",
            "Global step: 3194,loss: 0.011640519\n",
            "\n",
            "Global step: 3195,loss: 0.011485827\n",
            "\n",
            "Global step: 3196,loss: 0.0134911155\n",
            "\n",
            "Global step: 3197,loss: 0.010225213\n",
            "\n",
            "Global step: 3198,loss: 0.013204499\n",
            "\n",
            "Global step: 3199,loss: 0.013111761\n",
            "\n",
            "Global step: 3200,loss: 0.01326927\n",
            "\n",
            "Global step: 3201,loss: 0.010936819\n",
            "\n",
            "Global step: 3202,loss: 0.018237367\n",
            "\n",
            "Global step: 3203,loss: 0.010160198\n",
            "\n",
            "Global step: 3204,loss: 0.010168385\n",
            "\n",
            "Global step: 3205,loss: 0.0148429945\n",
            "\n",
            "Global step: 3206,loss: 0.010939993\n",
            "\n",
            "Global step: 3207,loss: 0.011098734\n",
            "\n",
            "Global step: 3208,loss: 0.015516099\n",
            "\n",
            "Global step: 3209,loss: 0.016037859\n",
            "\n",
            "Global step: 3210,loss: 0.01109235\n",
            "\n",
            "Global step: 3211,loss: 0.012014804\n",
            "\n",
            "Global step: 3212,loss: 0.010916815\n",
            "\n",
            "Global step: 3213,loss: 0.011401351\n",
            "\n",
            "Global step: 3214,loss: 0.01242535\n",
            "\n",
            "Global step: 3215,loss: 0.012040981\n",
            "\n",
            "Global step: 3216,loss: 0.010884144\n",
            "\n",
            "Global step: 3217,loss: 0.013993036\n",
            "\n",
            "Global step: 3218,loss: 0.009908891\n",
            "\n",
            "Global step: 3219,loss: 0.010636846\n",
            "\n",
            "Global step: 3220,loss: 0.015785351\n",
            "\n",
            "Global step: 3221,loss: 0.022041565\n",
            "\n",
            "Global step: 3222,loss: 0.012072534\n",
            "\n",
            "Global step: 3223,loss: 0.013364116\n",
            "\n",
            "Global step: 3224,loss: 0.014572622\n",
            "\n",
            "Global step: 3225,loss: 0.011032087\n",
            "\n",
            "Global step: 3226,loss: 0.011073202\n",
            "\n",
            "Global step: 3227,loss: 0.011683592\n",
            "\n",
            "Global step: 3228,loss: 0.014126933\n",
            "\n",
            "Global step: 3229,loss: 0.012284534\n",
            "\n",
            "Global step: 3230,loss: 0.0110162245\n",
            "\n",
            "Global step: 3231,loss: 0.010844511\n",
            "\n",
            "Global step: 3232,loss: 0.017781116\n",
            "\n",
            "Global step: 3233,loss: 0.009605457\n",
            "\n",
            "Global step: 3234,loss: 0.014045267\n",
            "\n",
            "Global step: 3235,loss: 0.013662105\n",
            "\n",
            "Global step: 3236,loss: 0.013695491\n",
            "\n",
            "Global step: 3237,loss: 0.013243677\n",
            "\n",
            "Global step: 3238,loss: 0.012900567\n",
            "\n",
            "Global step: 3239,loss: 0.01145217\n",
            "\n",
            "Global step: 3240,loss: 0.012345478\n",
            "\n",
            "Global step: 3241,loss: 0.010729972\n",
            "\n",
            "Global step: 3242,loss: 0.019876309\n",
            "\n",
            "Global step: 3243,loss: 0.012035848\n",
            "\n",
            "Global step: 3244,loss: 0.014094798\n",
            "\n",
            "Global step: 3245,loss: 0.013976565\n",
            "\n",
            "Global step: 3246,loss: 0.011181144\n",
            "\n",
            "Global step: 3247,loss: 0.011699709\n",
            "\n",
            "Global step: 3248,loss: 0.012055114\n",
            "\n",
            "Global step: 3249,loss: 0.012042308\n",
            "\n",
            "Global step: 3250,loss: 0.015475444\n",
            "\n",
            "Global step: 3251,loss: 0.010585371\n",
            "\n",
            "Global step: 3252,loss: 0.011022848\n",
            "\n",
            "Global step: 3253,loss: 0.009810399\n",
            "\n",
            "Global step: 3254,loss: 0.016111508\n",
            "\n",
            "Global step: 3255,loss: 0.011275887\n",
            "\n",
            "Global step: 3256,loss: 0.011132987\n",
            "\n",
            "Global step: 3257,loss: 0.010711096\n",
            "\n",
            "Global step: 3258,loss: 0.010023821\n",
            "\n",
            "Global step: 3259,loss: 0.010152558\n",
            "\n",
            "Global step: 3260,loss: 0.02074872\n",
            "\n",
            "Global step: 3261,loss: 0.011089755\n",
            "\n",
            "Global step: 3262,loss: 0.014591257\n",
            "\n",
            "Global step: 3263,loss: 0.012819345\n",
            "\n",
            "Global step: 3264,loss: 0.014495643\n",
            "\n",
            "Global step: 3265,loss: 0.010122543\n",
            "\n",
            "Global step: 3266,loss: 0.011016468\n",
            "\n",
            "Global step: 3267,loss: 0.010385495\n",
            "\n",
            "Global step: 3268,loss: 0.012271376\n",
            "\n",
            "Global step: 3269,loss: 0.013500294\n",
            "\n",
            "Global step: 3270,loss: 0.011418894\n",
            "\n",
            "Global step: 3271,loss: 0.01041884\n",
            "\n",
            "Global step: 3272,loss: 0.009763789\n",
            "\n",
            "Global step: 3273,loss: 0.010505793\n",
            "\n",
            "Global step: 3274,loss: 0.010684692\n",
            "\n",
            "Global step: 3275,loss: 0.012300789\n",
            "\n",
            "Global step: 3276,loss: 0.011478667\n",
            "\n",
            "Global step: 3277,loss: 0.014793898\n",
            "\n",
            "Global step: 3278,loss: 0.012452713\n",
            "\n",
            "Global step: 3279,loss: 0.010534197\n",
            "\n",
            "Global step: 3280,loss: 0.0107121365\n",
            "\n",
            "Global step: 3281,loss: 0.012678312\n",
            "\n",
            "Global step: 3282,loss: 0.011627695\n",
            "\n",
            "Global step: 3283,loss: 0.011984812\n",
            "\n",
            "Global step: 3284,loss: 0.0107552055\n",
            "\n",
            "Global step: 3285,loss: 0.010417678\n",
            "\n",
            "Global step: 3286,loss: 0.013774719\n",
            "\n",
            "Global step: 3287,loss: 0.01692299\n",
            "\n",
            "Global step: 3288,loss: 0.012498008\n",
            "\n",
            "Global step: 3289,loss: 0.014135405\n",
            "\n",
            "Global step: 3290,loss: 0.011789929\n",
            "\n",
            "Global step: 3291,loss: 0.012351226\n",
            "\n",
            "Global step: 3292,loss: 0.016869638\n",
            "\n",
            "Global step: 3293,loss: 0.01156643\n",
            "\n",
            "Global step: 3294,loss: 0.01449095\n",
            "\n",
            "Global step: 3295,loss: 0.010623674\n",
            "\n",
            "Global step: 3296,loss: 0.0106390165\n",
            "\n",
            "Global step: 3297,loss: 0.01030519\n",
            "\n",
            "Global step: 3298,loss: 0.011611742\n",
            "\n",
            "Global step: 3299,loss: 0.0114556085\n",
            "\n",
            "Global step: 3300,loss: 0.011526497\n",
            "\n",
            "Global step: 3301,loss: 0.01000333\n",
            "\n",
            "Global step: 3302,loss: 0.011584819\n",
            "\n",
            "Global step: 3303,loss: 0.0110176755\n",
            "\n",
            "Global step: 3304,loss: 0.024350911\n",
            "\n",
            "Global step: 3305,loss: 0.012550473\n",
            "\n",
            "Global step: 3306,loss: 0.010141192\n",
            "\n",
            "Global step: 3307,loss: 0.009576793\n",
            "\n",
            "Global step: 3308,loss: 0.010193015\n",
            "\n",
            "Global step: 3309,loss: 0.010576994\n",
            "\n",
            "Global step: 3310,loss: 0.010775362\n",
            "\n",
            "Global step: 3311,loss: 0.013249276\n",
            "\n",
            "Global step: 3312,loss: 0.011258999\n",
            "\n",
            "Global step: 3313,loss: 0.012337148\n",
            "\n",
            "Global step: 3314,loss: 0.0113951005\n",
            "\n",
            "Global step: 3315,loss: 0.014760112\n",
            "\n",
            "Global step: 3316,loss: 0.010725947\n",
            "\n",
            "Global step: 3317,loss: 0.011187352\n",
            "\n",
            "Global step: 3318,loss: 0.011404348\n",
            "\n",
            "Global step: 3319,loss: 0.0137113985\n",
            "\n",
            "Global step: 3320,loss: 0.010685939\n",
            "\n",
            "Global step: 3321,loss: 0.012587942\n",
            "\n",
            "Global step: 3322,loss: 0.01132178\n",
            "\n",
            "Global step: 3323,loss: 0.010004792\n",
            "\n",
            "Global step: 3324,loss: 0.019986995\n",
            "\n",
            "Global step: 3325,loss: 0.012989738\n",
            "\n",
            "Global step: 3326,loss: 0.010895804\n",
            "\n",
            "Global step: 3327,loss: 0.0116097415\n",
            "\n",
            "Global step: 3328,loss: 0.0127536645\n",
            "\n",
            "Global step: 3329,loss: 0.012707758\n",
            "\n",
            "Global step: 3330,loss: 0.018741284\n",
            "\n",
            "Global step: 3331,loss: 0.012837533\n",
            "\n",
            "Global step: 3332,loss: 0.010679867\n",
            "\n",
            "Global step: 3333,loss: 0.009502776\n",
            "\n",
            "Global step: 3334,loss: 0.012859656\n",
            "\n",
            "Global step: 3335,loss: 0.011560012\n",
            "\n",
            "Global step: 3336,loss: 0.009771042\n",
            "\n",
            "Global step: 3337,loss: 0.019402383\n",
            "\n",
            "Global step: 3338,loss: 0.011582576\n",
            "\n",
            "Global step: 3339,loss: 0.011822293\n",
            "\n",
            "Global step: 3340,loss: 0.016257107\n",
            "\n",
            "Global step: 3341,loss: 0.010220574\n",
            "\n",
            "Global step: 3342,loss: 0.012153372\n",
            "\n",
            "Global step: 3343,loss: 0.021910822\n",
            "\n",
            "Global step: 3344,loss: 0.011318514\n",
            "\n",
            "Global step: 3345,loss: 0.013488688\n",
            "\n",
            "Global step: 3346,loss: 0.011772031\n",
            "\n",
            "Global step: 3347,loss: 0.015363308\n",
            "\n",
            "Global step: 3348,loss: 0.016238302\n",
            "\n",
            "Global step: 3349,loss: 0.016131552\n",
            "\n",
            "Global step: 3350,loss: 0.012781373\n",
            "\n",
            "Global step: 3351,loss: 0.012484973\n",
            "\n",
            "Global step: 3352,loss: 0.011906857\n",
            "\n",
            "Global step: 3353,loss: 0.010820493\n",
            "\n",
            "Global step: 3354,loss: 0.01185802\n",
            "\n",
            "Global step: 3355,loss: 0.01076107\n",
            "\n",
            "Global step: 3356,loss: 0.019582234\n",
            "\n",
            "Global step: 3357,loss: 0.011471301\n",
            "\n",
            "Global step: 3358,loss: 0.01037265\n",
            "\n",
            "Global step: 3359,loss: 0.009936939\n",
            "\n",
            "Global step: 3360,loss: 0.010993404\n",
            "\n",
            "Global step: 3361,loss: 0.01111269\n",
            "\n",
            "Global step: 3362,loss: 0.012067425\n",
            "\n",
            "Global step: 3363,loss: 0.010742383\n",
            "\n",
            "Global step: 3364,loss: 0.014396752\n",
            "\n",
            "Global step: 3365,loss: 0.0120422635\n",
            "\n",
            "Global step: 3366,loss: 0.0103798285\n",
            "\n",
            "Global step: 3367,loss: 0.011831803\n",
            "\n",
            "Global step: 3368,loss: 0.011329358\n",
            "\n",
            "Global step: 3369,loss: 0.012425616\n",
            "\n",
            "Global step: 3370,loss: 0.017676173\n",
            "\n",
            "Global step: 3371,loss: 0.012319643\n",
            "\n",
            "Global step: 3372,loss: 0.012007034\n",
            "\n",
            "Global step: 3373,loss: 0.01049906\n",
            "\n",
            "Global step: 3374,loss: 0.012413065\n",
            "\n",
            "Global step: 3375,loss: 0.012551515\n",
            "\n",
            "Global step: 3376,loss: 0.012116576\n",
            "\n",
            "Global step: 3377,loss: 0.01504578\n",
            "\n",
            "Global step: 3378,loss: 0.010364566\n",
            "\n",
            "Global step: 3379,loss: 0.010572711\n",
            "\n",
            "Global step: 3380,loss: 0.027718041\n",
            "\n",
            "Global step: 3381,loss: 0.013299517\n",
            "\n",
            "Global step: 3382,loss: 0.010543241\n",
            "\n",
            "Global step: 3383,loss: 0.010349486\n",
            "\n",
            "Global step: 3384,loss: 0.017610224\n",
            "\n",
            "Global step: 3385,loss: 0.015081077\n",
            "\n",
            "Global step: 3386,loss: 0.011252287\n",
            "\n",
            "Global step: 3387,loss: 0.017707705\n",
            "\n",
            "Global step: 3388,loss: 0.010376038\n",
            "\n",
            "Global step: 3389,loss: 0.011715142\n",
            "\n",
            "Global step: 3390,loss: 0.01685612\n",
            "\n",
            "Global step: 3391,loss: 0.012122871\n",
            "\n",
            "Global step: 3392,loss: 0.012333244\n",
            "\n",
            "Global step: 3393,loss: 0.01074913\n",
            "\n",
            "Global step: 3394,loss: 0.013218703\n",
            "\n",
            "Global step: 3395,loss: 0.010895914\n",
            "\n",
            "Global step: 3396,loss: 0.011767544\n",
            "\n",
            "Global step: 3397,loss: 0.012349347\n",
            "\n",
            "Global step: 3398,loss: 0.016950555\n",
            "\n",
            "Global step: 3399,loss: 0.012788405\n",
            "\n",
            "Global step: 3400,loss: 0.010064943\n",
            "\n",
            "Global step: 3401,loss: 0.018786632\n",
            "\n",
            "Global step: 3402,loss: 0.010631811\n",
            "\n",
            "Global step: 3403,loss: 0.0135904495\n",
            "\n",
            "Global step: 3404,loss: 0.009684524\n",
            "\n",
            "Global step: 3405,loss: 0.010149711\n",
            "\n",
            "Global step: 3406,loss: 0.0121453125\n",
            "\n",
            "Global step: 3407,loss: 0.018797072\n",
            "\n",
            "Global step: 3408,loss: 0.012617456\n",
            "\n",
            "Global step: 3409,loss: 0.011267681\n",
            "\n",
            "Global step: 3410,loss: 0.012416879\n",
            "\n",
            "Global step: 3411,loss: 0.011952775\n",
            "\n",
            "Global step: 3412,loss: 0.010395758\n",
            "\n",
            "Global step: 3413,loss: 0.011643363\n",
            "\n",
            "Global step: 3414,loss: 0.010832673\n",
            "\n",
            "Global step: 3415,loss: 0.013075259\n",
            "\n",
            "Global step: 3416,loss: 0.014681928\n",
            "\n",
            "Global step: 3417,loss: 0.01236777\n",
            "\n",
            "Global step: 3418,loss: 0.0142833665\n",
            "\n",
            "Global step: 3419,loss: 0.011565487\n",
            "\n",
            "Global step: 3420,loss: 0.012366977\n",
            "\n",
            "Global step: 3421,loss: 0.010637557\n",
            "\n",
            "Global step: 3422,loss: 0.010844931\n",
            "\n",
            "Global step: 3423,loss: 0.012842028\n",
            "\n",
            "Global step: 3424,loss: 0.012516661\n",
            "\n",
            "Global step: 3425,loss: 0.011952358\n",
            "\n",
            "Global step: 3426,loss: 0.014151319\n",
            "\n",
            "Global step: 3427,loss: 0.010481581\n",
            "\n",
            "Global step: 3428,loss: 0.012691343\n",
            "\n",
            "Global step: 3429,loss: 0.011439063\n",
            "\n",
            "Global step: 3430,loss: 0.014598209\n",
            "\n",
            "Global step: 3431,loss: 0.009750453\n",
            "\n",
            "Global step: 3432,loss: 0.011229824\n",
            "\n",
            "Global step: 3433,loss: 0.013029827\n",
            "\n",
            "Global step: 3434,loss: 0.010285828\n",
            "\n",
            "Global step: 3435,loss: 0.010320298\n",
            "\n",
            "Global step: 3436,loss: 0.010656267\n",
            "\n",
            "Global step: 3437,loss: 0.011922047\n",
            "\n",
            "Global step: 3438,loss: 0.011412589\n",
            "\n",
            "Global step: 3439,loss: 0.01268315\n",
            "\n",
            "Global step: 3440,loss: 0.016652796\n",
            "\n",
            "Global step: 3441,loss: 0.014158306\n",
            "\n",
            "Global step: 3442,loss: 0.010663555\n",
            "\n",
            "Global step: 3443,loss: 0.015331462\n",
            "\n",
            "Global step: 3444,loss: 0.020307824\n",
            "\n",
            "Global step: 3445,loss: 0.0099514965\n",
            "\n",
            "Global step: 3446,loss: 0.013978116\n",
            "\n",
            "Global step: 3447,loss: 0.011930132\n",
            "\n",
            "Global step: 3448,loss: 0.012448708\n",
            "\n",
            "Global step: 3449,loss: 0.01112009\n",
            "\n",
            "Global step: 3450,loss: 0.011356911\n",
            "\n",
            "Global step: 3451,loss: 0.01300988\n",
            "\n",
            "Global step: 3452,loss: 0.017283317\n",
            "\n",
            "Global step: 3453,loss: 0.014045061\n",
            "\n",
            "Global step: 3454,loss: 0.012915659\n",
            "\n",
            "Global step: 3455,loss: 0.012053539\n",
            "\n",
            "Global step: 3456,loss: 0.013424939\n",
            "\n",
            "Global step: 3457,loss: 0.009982981\n",
            "\n",
            "Global step: 3458,loss: 0.010150207\n",
            "\n",
            "Global step: 3459,loss: 0.021780238\n",
            "\n",
            "Global step: 3460,loss: 0.010061901\n",
            "\n",
            "Global step: 3461,loss: 0.014777957\n",
            "\n",
            "Global step: 3462,loss: 0.0123134\n",
            "\n",
            "Global step: 3463,loss: 0.01471024\n",
            "\n",
            "Global step: 3464,loss: 0.010110138\n",
            "\n",
            "Global step: 3465,loss: 0.01066389\n",
            "\n",
            "Global step: 3466,loss: 0.010224793\n",
            "\n",
            "Global step: 3467,loss: 0.010519875\n",
            "\n",
            "Global step: 3468,loss: 0.011826255\n",
            "\n",
            "Global step: 3469,loss: 0.01464349\n",
            "\n",
            "Global step: 3470,loss: 0.012583168\n",
            "\n",
            "Global step: 3471,loss: 0.011122887\n",
            "\n",
            "Global step: 3472,loss: 0.010582054\n",
            "\n",
            "Global step: 3473,loss: 0.010281174\n",
            "\n",
            "Global step: 3474,loss: 0.012273818\n",
            "\n",
            "Global step: 3475,loss: 0.010839369\n",
            "\n",
            "Global step: 3476,loss: 0.011978108\n",
            "\n",
            "Global step: 3477,loss: 0.010332181\n",
            "\n",
            "Global step: 3478,loss: 0.011435179\n",
            "\n",
            "Global step: 3479,loss: 0.010086567\n",
            "\n",
            "Global step: 3480,loss: 0.012724677\n",
            "\n",
            "Global step: 3481,loss: 0.010720594\n",
            "\n",
            "Global step: 3482,loss: 0.010502248\n",
            "\n",
            "Global step: 3483,loss: 0.011340778\n",
            "\n",
            "Global step: 3484,loss: 0.012097053\n",
            "\n",
            "Global step: 3485,loss: 0.01068066\n",
            "\n",
            "Global step: 3486,loss: 0.015595799\n",
            "\n",
            "Global step: 3487,loss: 0.011431679\n",
            "\n",
            "Global step: 3488,loss: 0.009922069\n",
            "\n",
            "Global step: 3489,loss: 0.014122814\n",
            "\n",
            "Global step: 3490,loss: 0.009958112\n",
            "\n",
            "Global step: 3491,loss: 0.014528495\n",
            "\n",
            "Global step: 3492,loss: 0.011850383\n",
            "\n",
            "Global step: 3493,loss: 0.010752046\n",
            "\n",
            "Global step: 3494,loss: 0.01044322\n",
            "\n",
            "Global step: 3495,loss: 0.011845293\n",
            "\n",
            "Global step: 3496,loss: 0.011982275\n",
            "\n",
            "Global step: 3497,loss: 0.009629034\n",
            "\n",
            "Global step: 3498,loss: 0.010736454\n",
            "\n",
            "Global step: 3499,loss: 0.011485975\n",
            "\n",
            "Global step: 3500,loss: 0.014046067\n",
            "\n",
            "Global step: 3501,loss: 0.014680311\n",
            "\n",
            "Global step: 3502,loss: 0.010705064\n",
            "\n",
            "Global step: 3503,loss: 0.011423977\n",
            "\n",
            "Global step: 3504,loss: 0.01146776\n",
            "\n",
            "Global step: 3505,loss: 0.011818748\n",
            "\n",
            "Global step: 3506,loss: 0.010030949\n",
            "\n",
            "Global step: 3507,loss: 0.010102853\n",
            "\n",
            "Global step: 3508,loss: 0.010800154\n",
            "\n",
            "Global step: 3509,loss: 0.010506804\n",
            "\n",
            "Global step: 3510,loss: 0.009939455\n",
            "\n",
            "Global step: 3511,loss: 0.012508944\n",
            "\n",
            "Global step: 3512,loss: 0.009754499\n",
            "\n",
            "Global step: 3513,loss: 0.009520906\n",
            "\n",
            "Global step: 3514,loss: 0.0099797025\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3514,Val_Loss: 0.012696551334144723,  Val_acc: 0.9987980769230769 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:51:43.647126 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/15:\n",
            "Global step: 3515,loss: 0.010403297\n",
            "\n",
            "Global step: 3516,loss: 0.00960419\n",
            "\n",
            "Global step: 3517,loss: 0.009820748\n",
            "\n",
            "Global step: 3518,loss: 0.0097224815\n",
            "\n",
            "Global step: 3519,loss: 0.00951655\n",
            "\n",
            "Global step: 3520,loss: 0.009625895\n",
            "\n",
            "Global step: 3521,loss: 0.010826511\n",
            "\n",
            "Global step: 3522,loss: 0.010782666\n",
            "\n",
            "Global step: 3523,loss: 0.009394474\n",
            "\n",
            "Global step: 3524,loss: 0.012077833\n",
            "\n",
            "Global step: 3525,loss: 0.008623102\n",
            "\n",
            "Global step: 3526,loss: 0.009495872\n",
            "\n",
            "Global step: 3527,loss: 0.01116127\n",
            "\n",
            "Global step: 3528,loss: 0.009896016\n",
            "\n",
            "Global step: 3529,loss: 0.009338246\n",
            "\n",
            "Global step: 3530,loss: 0.009982559\n",
            "\n",
            "Global step: 3531,loss: 0.01830601\n",
            "\n",
            "Global step: 3532,loss: 0.01081452\n",
            "\n",
            "Global step: 3533,loss: 0.01139166\n",
            "\n",
            "Global step: 3534,loss: 0.01032547\n",
            "\n",
            "Global step: 3535,loss: 0.009337145\n",
            "\n",
            "Global step: 3536,loss: 0.01240645\n",
            "\n",
            "Global step: 3537,loss: 0.009843097\n",
            "\n",
            "Global step: 3538,loss: 0.01160201\n",
            "\n",
            "Global step: 3539,loss: 0.009481539\n",
            "\n",
            "Global step: 3540,loss: 0.010977992\n",
            "\n",
            "Global step: 3541,loss: 0.010506044\n",
            "\n",
            "Global step: 3542,loss: 0.010335093\n",
            "\n",
            "Global step: 3543,loss: 0.013695724\n",
            "\n",
            "Global step: 3544,loss: 0.009642336\n",
            "\n",
            "Global step: 3545,loss: 0.013062109\n",
            "\n",
            "Global step: 3546,loss: 0.0104207285\n",
            "\n",
            "Global step: 3547,loss: 0.008880536\n",
            "\n",
            "Global step: 3548,loss: 0.009115655\n",
            "\n",
            "Global step: 3549,loss: 0.011277373\n",
            "\n",
            "Global step: 3550,loss: 0.00977818\n",
            "\n",
            "Global step: 3551,loss: 0.009935512\n",
            "\n",
            "Global step: 3552,loss: 0.009835988\n",
            "\n",
            "Global step: 3553,loss: 0.009626624\n",
            "\n",
            "Global step: 3554,loss: 0.0123240715\n",
            "\n",
            "Global step: 3555,loss: 0.009548266\n",
            "\n",
            "Global step: 3556,loss: 0.010181969\n",
            "\n",
            "Global step: 3557,loss: 0.010564\n",
            "\n",
            "Global step: 3558,loss: 0.009797765\n",
            "\n",
            "Global step: 3559,loss: 0.0104290545\n",
            "\n",
            "Global step: 3560,loss: 0.009107115\n",
            "\n",
            "Global step: 3561,loss: 0.013203248\n",
            "\n",
            "Global step: 3562,loss: 0.009194587\n",
            "\n",
            "Global step: 3563,loss: 0.009571504\n",
            "\n",
            "Global step: 3564,loss: 0.0101220375\n",
            "\n",
            "Global step: 3565,loss: 0.009472597\n",
            "\n",
            "Global step: 3566,loss: 0.010943089\n",
            "\n",
            "Global step: 3567,loss: 0.010639261\n",
            "\n",
            "Global step: 3568,loss: 0.009981003\n",
            "\n",
            "Global step: 3569,loss: 0.009247549\n",
            "\n",
            "Global step: 3570,loss: 0.009375242\n",
            "\n",
            "Global step: 3571,loss: 0.009004966\n",
            "\n",
            "Global step: 3572,loss: 0.011251468\n",
            "\n",
            "Global step: 3573,loss: 0.013421779\n",
            "\n",
            "Global step: 3574,loss: 0.010167172\n",
            "\n",
            "Global step: 3575,loss: 0.011056386\n",
            "\n",
            "Global step: 3576,loss: 0.0109470105\n",
            "\n",
            "Global step: 3577,loss: 0.010045043\n",
            "\n",
            "Global step: 3578,loss: 0.009391069\n",
            "\n",
            "Global step: 3579,loss: 0.0097485725\n",
            "\n",
            "Global step: 3580,loss: 0.009936996\n",
            "\n",
            "Global step: 3581,loss: 0.013309754\n",
            "\n",
            "Global step: 3582,loss: 0.010388358\n",
            "\n",
            "Global step: 3583,loss: 0.009890929\n",
            "\n",
            "Global step: 3584,loss: 0.011562938\n",
            "\n",
            "Global step: 3585,loss: 0.0095605245\n",
            "\n",
            "Global step: 3586,loss: 0.010373753\n",
            "\n",
            "Global step: 3587,loss: 0.009956968\n",
            "\n",
            "Global step: 3588,loss: 0.011621135\n",
            "\n",
            "Global step: 3589,loss: 0.009274156\n",
            "\n",
            "Global step: 3590,loss: 0.011708962\n",
            "\n",
            "Global step: 3591,loss: 0.009853038\n",
            "\n",
            "Global step: 3592,loss: 0.009570325\n",
            "\n",
            "Global step: 3593,loss: 0.010944017\n",
            "\n",
            "Global step: 3594,loss: 0.009997012\n",
            "\n",
            "Global step: 3595,loss: 0.009896449\n",
            "\n",
            "Global step: 3596,loss: 0.010146714\n",
            "\n",
            "Global step: 3597,loss: 0.010707008\n",
            "\n",
            "Global step: 3598,loss: 0.008896599\n",
            "\n",
            "Global step: 3599,loss: 0.009643998\n",
            "\n",
            "Global step: 3600,loss: 0.010605527\n",
            "\n",
            "Global step: 3601,loss: 0.009162731\n",
            "\n",
            "Global step: 3602,loss: 0.010632079\n",
            "\n",
            "Global step: 3603,loss: 0.0092906775\n",
            "\n",
            "Global step: 3604,loss: 0.01098412\n",
            "\n",
            "Global step: 3605,loss: 0.009523114\n",
            "\n",
            "Global step: 3606,loss: 0.009736477\n",
            "\n",
            "Global step: 3607,loss: 0.0094886115\n",
            "\n",
            "Global step: 3608,loss: 0.010283022\n",
            "\n",
            "Global step: 3609,loss: 0.0137711\n",
            "\n",
            "Global step: 3610,loss: 0.010127512\n",
            "\n",
            "Global step: 3611,loss: 0.009925966\n",
            "\n",
            "Global step: 3612,loss: 0.010354351\n",
            "\n",
            "Global step: 3613,loss: 0.01032149\n",
            "\n",
            "Global step: 3614,loss: 0.009430594\n",
            "\n",
            "Global step: 3615,loss: 0.0100981705\n",
            "\n",
            "Global step: 3616,loss: 0.008771029\n",
            "\n",
            "Global step: 3617,loss: 0.0116068795\n",
            "\n",
            "Global step: 3618,loss: 0.008973651\n",
            "\n",
            "Global step: 3619,loss: 0.009449632\n",
            "\n",
            "Global step: 3620,loss: 0.009384091\n",
            "\n",
            "Global step: 3621,loss: 0.009265066\n",
            "\n",
            "Global step: 3622,loss: 0.010017963\n",
            "\n",
            "Global step: 3623,loss: 0.009949532\n",
            "\n",
            "Global step: 3624,loss: 0.0099104075\n",
            "\n",
            "Global step: 3625,loss: 0.010875963\n",
            "\n",
            "Global step: 3626,loss: 0.009993058\n",
            "\n",
            "Global step: 3627,loss: 0.010413541\n",
            "\n",
            "Global step: 3628,loss: 0.010265955\n",
            "\n",
            "Global step: 3629,loss: 0.009216467\n",
            "\n",
            "Global step: 3630,loss: 0.009574909\n",
            "\n",
            "Global step: 3631,loss: 0.009943576\n",
            "\n",
            "Global step: 3632,loss: 0.010122056\n",
            "\n",
            "Global step: 3633,loss: 0.009394126\n",
            "\n",
            "Global step: 3634,loss: 0.009049012\n",
            "\n",
            "Global step: 3635,loss: 0.011210657\n",
            "\n",
            "Global step: 3636,loss: 0.012727512\n",
            "\n",
            "Global step: 3637,loss: 0.009686837\n",
            "\n",
            "Global step: 3638,loss: 0.010995626\n",
            "\n",
            "Global step: 3639,loss: 0.009370342\n",
            "\n",
            "Global step: 3640,loss: 0.009872273\n",
            "\n",
            "Global step: 3641,loss: 0.009048974\n",
            "\n",
            "Global step: 3642,loss: 0.0108393505\n",
            "\n",
            "Global step: 3643,loss: 0.010470738\n",
            "\n",
            "Global step: 3644,loss: 0.011971584\n",
            "\n",
            "Global step: 3645,loss: 0.009819663\n",
            "\n",
            "Global step: 3646,loss: 0.010319545\n",
            "\n",
            "Global step: 3647,loss: 0.011355314\n",
            "\n",
            "Global step: 3648,loss: 0.010945986\n",
            "\n",
            "Global step: 3649,loss: 0.009140383\n",
            "\n",
            "Global step: 3650,loss: 0.010545885\n",
            "\n",
            "Global step: 3651,loss: 0.009885785\n",
            "\n",
            "Global step: 3652,loss: 0.010598988\n",
            "\n",
            "Global step: 3653,loss: 0.0109874485\n",
            "\n",
            "Global step: 3654,loss: 0.009065898\n",
            "\n",
            "Global step: 3655,loss: 0.009809948\n",
            "\n",
            "Global step: 3656,loss: 0.010675025\n",
            "\n",
            "Global step: 3657,loss: 0.008521021\n",
            "\n",
            "Global step: 3658,loss: 0.015253779\n",
            "\n",
            "Global step: 3659,loss: 0.011785337\n",
            "\n",
            "Global step: 3660,loss: 0.009577502\n",
            "\n",
            "Global step: 3661,loss: 0.010390434\n",
            "\n",
            "Global step: 3662,loss: 0.010439839\n",
            "\n",
            "Global step: 3663,loss: 0.01149899\n",
            "\n",
            "Global step: 3664,loss: 0.018164776\n",
            "\n",
            "Global step: 3665,loss: 0.009720508\n",
            "\n",
            "Global step: 3666,loss: 0.014738759\n",
            "\n",
            "Global step: 3667,loss: 0.010527537\n",
            "\n",
            "Global step: 3668,loss: 0.010069402\n",
            "\n",
            "Global step: 3669,loss: 0.015001233\n",
            "\n",
            "Global step: 3670,loss: 0.020048834\n",
            "\n",
            "Global step: 3671,loss: 0.025853518\n",
            "\n",
            "Global step: 3672,loss: 0.010396169\n",
            "\n",
            "Global step: 3673,loss: 0.015247844\n",
            "\n",
            "Global step: 3674,loss: 0.009834661\n",
            "\n",
            "Global step: 3675,loss: 0.010009596\n",
            "\n",
            "Global step: 3676,loss: 0.011017074\n",
            "\n",
            "Global step: 3677,loss: 0.011534086\n",
            "\n",
            "Global step: 3678,loss: 0.00989713\n",
            "\n",
            "Global step: 3679,loss: 0.011403812\n",
            "\n",
            "Global step: 3680,loss: 0.009911232\n",
            "\n",
            "Global step: 3681,loss: 0.010330245\n",
            "\n",
            "Global step: 3682,loss: 0.0098212315\n",
            "\n",
            "Global step: 3683,loss: 0.010939219\n",
            "\n",
            "Global step: 3684,loss: 0.014113508\n",
            "\n",
            "Global step: 3685,loss: 0.010691158\n",
            "\n",
            "Global step: 3686,loss: 0.018339954\n",
            "\n",
            "Global step: 3687,loss: 0.01397407\n",
            "\n",
            "Global step: 3688,loss: 0.009480668\n",
            "\n",
            "Global step: 3689,loss: 0.013831261\n",
            "\n",
            "Global step: 3690,loss: 0.010218342\n",
            "\n",
            "Global step: 3691,loss: 0.010836875\n",
            "\n",
            "Global step: 3692,loss: 0.012353208\n",
            "\n",
            "Global step: 3693,loss: 0.01060862\n",
            "\n",
            "Global step: 3694,loss: 0.009583613\n",
            "\n",
            "Global step: 3695,loss: 0.011058997\n",
            "\n",
            "Global step: 3696,loss: 0.009980164\n",
            "\n",
            "Global step: 3697,loss: 0.022568673\n",
            "\n",
            "Global step: 3698,loss: 0.01694843\n",
            "\n",
            "Global step: 3699,loss: 0.011803497\n",
            "\n",
            "Global step: 3700,loss: 0.01057811\n",
            "\n",
            "Global step: 3701,loss: 0.010212214\n",
            "\n",
            "Global step: 3702,loss: 0.011493123\n",
            "\n",
            "Global step: 3703,loss: 0.012191734\n",
            "\n",
            "Global step: 3704,loss: 0.010301183\n",
            "\n",
            "Global step: 3705,loss: 0.010772256\n",
            "\n",
            "Global step: 3706,loss: 0.010680184\n",
            "\n",
            "Global step: 3707,loss: 0.0102215\n",
            "\n",
            "Global step: 3708,loss: 0.010004817\n",
            "\n",
            "Global step: 3709,loss: 0.010799402\n",
            "\n",
            "Global step: 3710,loss: 0.009342585\n",
            "\n",
            "Global step: 3711,loss: 0.010740499\n",
            "\n",
            "Global step: 3712,loss: 0.01002383\n",
            "\n",
            "Global step: 3713,loss: 0.012178378\n",
            "\n",
            "Global step: 3714,loss: 0.009696247\n",
            "\n",
            "Global step: 3715,loss: 0.011907212\n",
            "\n",
            "Global step: 3716,loss: 0.010151254\n",
            "\n",
            "Global step: 3717,loss: 0.010441087\n",
            "\n",
            "Global step: 3718,loss: 0.010122108\n",
            "\n",
            "Global step: 3719,loss: 0.010671926\n",
            "\n",
            "Global step: 3720,loss: 0.011153784\n",
            "\n",
            "Global step: 3721,loss: 0.010415155\n",
            "\n",
            "Global step: 3722,loss: 0.010078575\n",
            "\n",
            "Global step: 3723,loss: 0.016916137\n",
            "\n",
            "Global step: 3724,loss: 0.010685552\n",
            "\n",
            "Global step: 3725,loss: 0.010438543\n",
            "\n",
            "Global step: 3726,loss: 0.015076493\n",
            "\n",
            "Global step: 3727,loss: 0.0104787685\n",
            "\n",
            "Global step: 3728,loss: 0.00901792\n",
            "\n",
            "Global step: 3729,loss: 0.009909507\n",
            "\n",
            "Global step: 3730,loss: 0.009624539\n",
            "\n",
            "Global step: 3731,loss: 0.010325851\n",
            "\n",
            "Global step: 3732,loss: 0.010684341\n",
            "\n",
            "Global step: 3733,loss: 0.010684049\n",
            "\n",
            "Global step: 3734,loss: 0.012364073\n",
            "\n",
            "Global step: 3735,loss: 0.012319882\n",
            "\n",
            "Global step: 3736,loss: 0.00962507\n",
            "\n",
            "Global step: 3737,loss: 0.009470217\n",
            "\n",
            "Global step: 3738,loss: 0.01151513\n",
            "\n",
            "Global step: 3739,loss: 0.010236022\n",
            "\n",
            "Global step: 3740,loss: 0.010104377\n",
            "\n",
            "Global step: 3741,loss: 0.01146448\n",
            "\n",
            "Global step: 3742,loss: 0.010085195\n",
            "\n",
            "Global step: 3743,loss: 0.014928721\n",
            "\n",
            "Global step: 3744,loss: 0.011169911\n",
            "\n",
            "Global step: 3745,loss: 0.010503755\n",
            "\n",
            "Global step: 3746,loss: 0.010657333\n",
            "\n",
            "Global step: 3747,loss: 0.010744972\n",
            "\n",
            "Global step: 3748,loss: 0.009619341\n",
            "\n",
            "Global step: 3749,loss: 0.01341068\n",
            "\n",
            "Global step: 3750,loss: 0.015063288\n",
            "\n",
            "Global step: 3751,loss: 0.009697385\n",
            "\n",
            "Global step: 3752,loss: 0.010757194\n",
            "\n",
            "Global step: 3753,loss: 0.009897816\n",
            "\n",
            "Global step: 3754,loss: 0.010126897\n",
            "\n",
            "Global step: 3755,loss: 0.01032793\n",
            "\n",
            "Global step: 3756,loss: 0.014898641\n",
            "\n",
            "Global step: 3757,loss: 0.0116769485\n",
            "\n",
            "Global step: 3758,loss: 0.01392624\n",
            "\n",
            "Global step: 3759,loss: 0.010235414\n",
            "\n",
            "Global step: 3760,loss: 0.0099404715\n",
            "\n",
            "Global step: 3761,loss: 0.010405051\n",
            "\n",
            "Global step: 3762,loss: 0.009521245\n",
            "\n",
            "Global step: 3763,loss: 0.009661701\n",
            "\n",
            "Global step: 3764,loss: 0.009686176\n",
            "\n",
            "Global step: 3765,loss: 0.009358563\n",
            "\n",
            "Global step: 3766,loss: 0.020471357\n",
            "\n",
            "Global step: 3767,loss: 0.009545411\n",
            "\n",
            "Global step: 3768,loss: 0.010106358\n",
            "\n",
            "Global step: 3769,loss: 0.009890921\n",
            "\n",
            "Global step: 3770,loss: 0.009934865\n",
            "\n",
            "Global step: 3771,loss: 0.010484874\n",
            "\n",
            "Global step: 3772,loss: 0.008890143\n",
            "\n",
            "Global step: 3773,loss: 0.009418386\n",
            "\n",
            "Global step: 3774,loss: 0.011941274\n",
            "\n",
            "Global step: 3775,loss: 0.009505586\n",
            "\n",
            "Global step: 3776,loss: 0.009628193\n",
            "\n",
            "Global step: 3777,loss: 0.01614691\n",
            "\n",
            "Global step: 3778,loss: 0.009502067\n",
            "\n",
            "Global step: 3779,loss: 0.009308322\n",
            "\n",
            "Global step: 3780,loss: 0.010779735\n",
            "\n",
            "Global step: 3781,loss: 0.0093897665\n",
            "\n",
            "Global step: 3782,loss: 0.0099308565\n",
            "\n",
            "Global step: 3783,loss: 0.009778719\n",
            "\n",
            "Global step: 3784,loss: 0.008294427\n",
            "\n",
            "Global step: 3785,loss: 0.009893112\n",
            "\n",
            "Global step: 3786,loss: 0.010634629\n",
            "\n",
            "Global step: 3787,loss: 0.0101366555\n",
            "\n",
            "Global step: 3788,loss: 0.008995778\n",
            "\n",
            "Global step: 3789,loss: 0.009115735\n",
            "\n",
            "Global step: 3790,loss: 0.011418966\n",
            "\n",
            "Global step: 3791,loss: 0.009133277\n",
            "\n",
            "Global step: 3792,loss: 0.010351565\n",
            "\n",
            "Global step: 3793,loss: 0.010530535\n",
            "\n",
            "Global step: 3794,loss: 0.011681593\n",
            "\n",
            "Global step: 3795,loss: 0.009266249\n",
            "\n",
            "Global step: 3796,loss: 0.009933427\n",
            "\n",
            "Global step: 3797,loss: 0.00940294\n",
            "\n",
            "Global step: 3798,loss: 0.011808962\n",
            "\n",
            "Global step: 3799,loss: 0.008148523\n",
            "\n",
            "Global step: 3800,loss: 0.010141568\n",
            "\n",
            "Global step: 3801,loss: 0.014038119\n",
            "\n",
            "Global step: 3802,loss: 0.010530541\n",
            "\n",
            "Global step: 3803,loss: 0.012596091\n",
            "\n",
            "Global step: 3804,loss: 0.009881436\n",
            "\n",
            "Global step: 3805,loss: 0.009545047\n",
            "\n",
            "Global step: 3806,loss: 0.0103798695\n",
            "\n",
            "Global step: 3807,loss: 0.009557081\n",
            "\n",
            "Global step: 3808,loss: 0.01014736\n",
            "\n",
            "Global step: 3809,loss: 0.012407316\n",
            "\n",
            "Global step: 3810,loss: 0.011630634\n",
            "\n",
            "Global step: 3811,loss: 0.009392514\n",
            "\n",
            "Global step: 3812,loss: 0.014265118\n",
            "\n",
            "Global step: 3813,loss: 0.009871726\n",
            "\n",
            "Global step: 3814,loss: 0.010547365\n",
            "\n",
            "Global step: 3815,loss: 0.010887645\n",
            "\n",
            "Global step: 3816,loss: 0.0094838645\n",
            "\n",
            "Global step: 3817,loss: 0.010155007\n",
            "\n",
            "Global step: 3818,loss: 0.009967633\n",
            "\n",
            "Global step: 3819,loss: 0.009744542\n",
            "\n",
            "Global step: 3820,loss: 0.010072244\n",
            "\n",
            "Global step: 3821,loss: 0.010672906\n",
            "\n",
            "Global step: 3822,loss: 0.009278819\n",
            "\n",
            "Global step: 3823,loss: 0.009019169\n",
            "\n",
            "Global step: 3824,loss: 0.008877991\n",
            "\n",
            "Global step: 3825,loss: 0.009434236\n",
            "\n",
            "Global step: 3826,loss: 0.012317676\n",
            "\n",
            "Global step: 3827,loss: 0.0117277475\n",
            "\n",
            "Global step: 3828,loss: 0.010210179\n",
            "\n",
            "Global step: 3829,loss: 0.009657456\n",
            "\n",
            "Global step: 3830,loss: 0.010407164\n",
            "\n",
            "Global step: 3831,loss: 0.011066696\n",
            "\n",
            "Global step: 3832,loss: 0.009731434\n",
            "\n",
            "Global step: 3833,loss: 0.009199321\n",
            "\n",
            "Global step: 3834,loss: 0.00920125\n",
            "\n",
            "Global step: 3835,loss: 0.010731591\n",
            "\n",
            "Global step: 3836,loss: 0.009226895\n",
            "\n",
            "Global step: 3837,loss: 0.009835283\n",
            "\n",
            "Global step: 3838,loss: 0.011087567\n",
            "\n",
            "Global step: 3839,loss: 0.00956228\n",
            "\n",
            "Global step: 3840,loss: 0.010927487\n",
            "\n",
            "Global step: 3841,loss: 0.009153063\n",
            "\n",
            "Global step: 3842,loss: 0.012595448\n",
            "\n",
            "Global step: 3843,loss: 0.010829398\n",
            "\n",
            "Global step: 3844,loss: 0.010489823\n",
            "\n",
            "Global step: 3845,loss: 0.0100398585\n",
            "\n",
            "Global step: 3846,loss: 0.009731936\n",
            "\n",
            "Global step: 3847,loss: 0.010342563\n",
            "\n",
            "Global step: 3848,loss: 0.008869\n",
            "\n",
            "Global step: 3849,loss: 0.009550421\n",
            "\n",
            "Global step: 3850,loss: 0.010126127\n",
            "\n",
            "Global step: 3851,loss: 0.010044091\n",
            "\n",
            "Global step: 3852,loss: 0.0115766805\n",
            "\n",
            "Global step: 3853,loss: 0.011644245\n",
            "\n",
            "Global step: 3854,loss: 0.0094627\n",
            "\n",
            "Global step: 3855,loss: 0.0090280445\n",
            "\n",
            "Global step: 3856,loss: 0.01065889\n",
            "\n",
            "Global step: 3857,loss: 0.01109536\n",
            "\n",
            "Global step: 3858,loss: 0.009607097\n",
            "\n",
            "Global step: 3859,loss: 0.0093922075\n",
            "\n",
            "Global step: 3860,loss: 0.009676756\n",
            "\n",
            "Global step: 3861,loss: 0.010625416\n",
            "\n",
            "Global step: 3862,loss: 0.010397076\n",
            "\n",
            "Global step: 3863,loss: 0.009655046\n",
            "\n",
            "Global step: 3864,loss: 0.008857564\n",
            "\n",
            "Global step: 3865,loss: 0.010078779\n",
            "\n",
            "Global step: 3866,loss: 0.013836464\n",
            "\n",
            "Global step: 3867,loss: 0.011140986\n",
            "\n",
            "Global step: 3868,loss: 0.011845456\n",
            "\n",
            "Global step: 3869,loss: 0.010422932\n",
            "\n",
            "Global step: 3870,loss: 0.010967703\n",
            "\n",
            "Global step: 3871,loss: 0.009560646\n",
            "\n",
            "Global step: 3872,loss: 0.011223304\n",
            "\n",
            "Global step: 3873,loss: 0.010564617\n",
            "\n",
            "Global step: 3874,loss: 0.016466247\n",
            "\n",
            "Global step: 3875,loss: 0.009211268\n",
            "\n",
            "Global step: 3876,loss: 0.00923942\n",
            "\n",
            "Global step: 3877,loss: 0.009992429\n",
            "\n",
            "Global step: 3878,loss: 0.009269592\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3879.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:52:35.187050 139837023905536 supervisor.py:1050] Recording summary at step 3879.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.67522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:52:35.222298 139837015512832 supervisor.py:1099] global_step/sec: 6.67522\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3879,loss: 0.009577934\n",
            "\n",
            "Global step: 3880,loss: 0.013787207\n",
            "\n",
            "Global step: 3881,loss: 0.00993685\n",
            "\n",
            "Global step: 3882,loss: 0.010401266\n",
            "\n",
            "Global step: 3883,loss: 0.009608945\n",
            "\n",
            "Global step: 3884,loss: 0.0096929995\n",
            "\n",
            "Global step: 3885,loss: 0.010058841\n",
            "\n",
            "Global step: 3886,loss: 0.010230348\n",
            "\n",
            "Global step: 3887,loss: 0.011265792\n",
            "\n",
            "Global step: 3888,loss: 0.013811275\n",
            "\n",
            "Global step: 3889,loss: 0.011890708\n",
            "\n",
            "Global step: 3890,loss: 0.010208451\n",
            "\n",
            "Global step: 3891,loss: 0.0090780575\n",
            "\n",
            "Global step: 3892,loss: 0.009431052\n",
            "\n",
            "Global step: 3893,loss: 0.010225749\n",
            "\n",
            "Global step: 3894,loss: 0.009127906\n",
            "\n",
            "Global step: 3895,loss: 0.009097789\n",
            "\n",
            "Global step: 3896,loss: 0.0109104905\n",
            "\n",
            "Global step: 3897,loss: 0.009424989\n",
            "\n",
            "Global step: 3898,loss: 0.009150589\n",
            "\n",
            "Global step: 3899,loss: 0.010167182\n",
            "\n",
            "Global step: 3900,loss: 0.012923117\n",
            "\n",
            "Global step: 3901,loss: 0.008681521\n",
            "\n",
            "Global step: 3902,loss: 0.0091279615\n",
            "\n",
            "Global step: 3903,loss: 0.009093371\n",
            "\n",
            "Global step: 3904,loss: 0.01320965\n",
            "\n",
            "Global step: 3905,loss: 0.009443756\n",
            "\n",
            "Global step: 3906,loss: 0.009979647\n",
            "\n",
            "Global step: 3907,loss: 0.013241665\n",
            "\n",
            "Global step: 3908,loss: 0.012146489\n",
            "\n",
            "Global step: 3909,loss: 0.010019797\n",
            "\n",
            "Global step: 3910,loss: 0.009235609\n",
            "\n",
            "Global step: 3911,loss: 0.010065674\n",
            "\n",
            "Global step: 3912,loss: 0.01005709\n",
            "\n",
            "Global step: 3913,loss: 0.009274845\n",
            "\n",
            "Global step: 3914,loss: 0.010333598\n",
            "\n",
            "Global step: 3915,loss: 0.009342272\n",
            "\n",
            "Global step: 3916,loss: 0.009258212\n",
            "\n",
            "Global step: 3917,loss: 0.00938462\n",
            "\n",
            "Global step: 3918,loss: 0.011205859\n",
            "\n",
            "Global step: 3919,loss: 0.009204611\n",
            "\n",
            "Global step: 3920,loss: 0.011432747\n",
            "\n",
            "Global step: 3921,loss: 0.009018076\n",
            "\n",
            "Global step: 3922,loss: 0.00999595\n",
            "\n",
            "Global step: 3923,loss: 0.011492486\n",
            "\n",
            "Global step: 3924,loss: 0.009744846\n",
            "\n",
            "Global step: 3925,loss: 0.009939598\n",
            "\n",
            "Global step: 3926,loss: 0.00949022\n",
            "\n",
            "Global step: 3927,loss: 0.013077827\n",
            "\n",
            "Global step: 3928,loss: 0.010148248\n",
            "\n",
            "Global step: 3929,loss: 0.010634661\n",
            "\n",
            "Global step: 3930,loss: 0.011666135\n",
            "\n",
            "Global step: 3931,loss: 0.009128987\n",
            "\n",
            "Global step: 3932,loss: 0.009996791\n",
            "\n",
            "Global step: 3933,loss: 0.010473767\n",
            "\n",
            "Global step: 3934,loss: 0.00927404\n",
            "\n",
            "Global step: 3935,loss: 0.010700529\n",
            "\n",
            "Global step: 3936,loss: 0.009112626\n",
            "\n",
            "Global step: 3937,loss: 0.010039297\n",
            "\n",
            "Global step: 3938,loss: 0.009971132\n",
            "\n",
            "Global step: 3939,loss: 0.015191063\n",
            "\n",
            "Global step: 3940,loss: 0.008818801\n",
            "\n",
            "Global step: 3941,loss: 0.009667548\n",
            "\n",
            "Global step: 3942,loss: 0.0089027565\n",
            "\n",
            "Global step: 3943,loss: 0.009320796\n",
            "\n",
            "Global step: 3944,loss: 0.009277739\n",
            "\n",
            "Global step: 3945,loss: 0.009926744\n",
            "\n",
            "Global step: 3946,loss: 0.010503337\n",
            "\n",
            "Global step: 3947,loss: 0.009631968\n",
            "\n",
            "Global step: 3948,loss: 0.009340872\n",
            "\n",
            "Global step: 3949,loss: 0.009751061\n",
            "\n",
            "Global step: 3950,loss: 0.010363181\n",
            "\n",
            "Global step: 3951,loss: 0.010907477\n",
            "\n",
            "Global step: 3952,loss: 0.009293501\n",
            "\n",
            "Global step: 3953,loss: 0.008788412\n",
            "\n",
            "Global step: 3954,loss: 0.013444335\n",
            "\n",
            "Global step: 3955,loss: 0.008817768\n",
            "\n",
            "Global step: 3956,loss: 0.015971037\n",
            "\n",
            "Global step: 3957,loss: 0.008917632\n",
            "\n",
            "Global step: 3958,loss: 0.008789771\n",
            "\n",
            "Global step: 3959,loss: 0.009852251\n",
            "\n",
            "Global step: 3960,loss: 0.0087785255\n",
            "\n",
            "Global step: 3961,loss: 0.009610388\n",
            "\n",
            "Global step: 3962,loss: 0.009922065\n",
            "\n",
            "Global step: 3963,loss: 0.013553914\n",
            "\n",
            "Global step: 3964,loss: 0.010075894\n",
            "\n",
            "Global step: 3965,loss: 0.009690812\n",
            "\n",
            "Global step: 3966,loss: 0.011876831\n",
            "\n",
            "Global step: 3967,loss: 0.009958313\n",
            "\n",
            "Global step: 3968,loss: 0.009865434\n",
            "\n",
            "Global step: 3969,loss: 0.009938102\n",
            "\n",
            "Global step: 3970,loss: 0.010638302\n",
            "\n",
            "Global step: 3971,loss: 0.010585328\n",
            "\n",
            "Global step: 3972,loss: 0.014576383\n",
            "\n",
            "Global step: 3973,loss: 0.01637036\n",
            "\n",
            "Global step: 3974,loss: 0.010721331\n",
            "\n",
            "Global step: 3975,loss: 0.011936232\n",
            "\n",
            "Global step: 3976,loss: 0.009956636\n",
            "\n",
            "Global step: 3977,loss: 0.018894844\n",
            "\n",
            "Global step: 3978,loss: 0.009001745\n",
            "\n",
            "Global step: 3979,loss: 0.009653207\n",
            "\n",
            "Global step: 3980,loss: 0.010958844\n",
            "\n",
            "Global step: 3981,loss: 0.018727913\n",
            "\n",
            "Global step: 3982,loss: 0.010095228\n",
            "\n",
            "Global step: 3983,loss: 0.00968979\n",
            "\n",
            "Global step: 3984,loss: 0.010684487\n",
            "\n",
            "Global step: 3985,loss: 0.013044804\n",
            "\n",
            "Global step: 3986,loss: 0.014873758\n",
            "\n",
            "Global step: 3987,loss: 0.011763935\n",
            "\n",
            "Global step: 3988,loss: 0.010474462\n",
            "\n",
            "Global step: 3989,loss: 0.012462318\n",
            "\n",
            "Global step: 3990,loss: 0.010795794\n",
            "\n",
            "Global step: 3991,loss: 0.011826026\n",
            "\n",
            "Global step: 3992,loss: 0.012464732\n",
            "\n",
            "Global step: 3993,loss: 0.009865669\n",
            "\n",
            "Global step: 3994,loss: 0.011668745\n",
            "\n",
            "Global step: 3995,loss: 0.009772092\n",
            "\n",
            "Global step: 3996,loss: 0.010113154\n",
            "\n",
            "Global step: 3997,loss: 0.012127158\n",
            "\n",
            "Global step: 3998,loss: 0.010388443\n",
            "\n",
            "Global step: 3999,loss: 0.017195499\n",
            "\n",
            "Global step: 4000,loss: 0.013934684\n",
            "\n",
            "Global step: 4001,loss: 0.0110220695\n",
            "\n",
            "Global step: 4002,loss: 0.014022273\n",
            "\n",
            "Global step: 4003,loss: 0.010907191\n",
            "\n",
            "Global step: 4004,loss: 0.011563365\n",
            "\n",
            "Global step: 4005,loss: 0.011050647\n",
            "\n",
            "Global step: 4006,loss: 0.010422914\n",
            "\n",
            "Global step: 4007,loss: 0.0107631385\n",
            "\n",
            "Global step: 4008,loss: 0.010582509\n",
            "\n",
            "Global step: 4009,loss: 0.01079024\n",
            "\n",
            "Global step: 4010,loss: 0.012134799\n",
            "\n",
            "Global step: 4011,loss: 0.008521327\n",
            "\n",
            "Global step: 4012,loss: 0.012583352\n",
            "\n",
            "Global step: 4013,loss: 0.010325876\n",
            "\n",
            "Global step: 4014,loss: 0.014044415\n",
            "\n",
            "Global step: 4015,loss: 0.009754397\n",
            "\n",
            "Global step: 4016,loss: 0.012792444\n",
            "\n",
            "Global step: 4017,loss: 0.009403364\n",
            "\n",
            "Global step: 4018,loss: 0.010342358\n",
            "\n",
            "Global step: 4019,loss: 0.009059855\n",
            "\n",
            "Global step: 4020,loss: 0.010502344\n",
            "\n",
            "Global step: 4021,loss: 0.015323049\n",
            "\n",
            "Global step: 4022,loss: 0.012355983\n",
            "\n",
            "Global step: 4023,loss: 0.009403564\n",
            "\n",
            "Global step: 4024,loss: 0.009213631\n",
            "\n",
            "Global step: 4025,loss: 0.009408291\n",
            "\n",
            "Global step: 4026,loss: 0.011379029\n",
            "\n",
            "Global step: 4027,loss: 0.009026763\n",
            "\n",
            "Global step: 4028,loss: 0.010910018\n",
            "\n",
            "Global step: 4029,loss: 0.010344543\n",
            "\n",
            "Global step: 4030,loss: 0.009021675\n",
            "\n",
            "Global step: 4031,loss: 0.009547154\n",
            "\n",
            "Global step: 4032,loss: 0.019727144\n",
            "\n",
            "Global step: 4033,loss: 0.011727728\n",
            "\n",
            "Global step: 4034,loss: 0.009494495\n",
            "\n",
            "Global step: 4035,loss: 0.010736306\n",
            "\n",
            "Global step: 4036,loss: 0.009730126\n",
            "\n",
            "Global step: 4037,loss: 0.012057135\n",
            "\n",
            "Global step: 4038,loss: 0.01356735\n",
            "\n",
            "Global step: 4039,loss: 0.009030035\n",
            "\n",
            "Global step: 4040,loss: 0.009095872\n",
            "\n",
            "Global step: 4041,loss: 0.014716517\n",
            "\n",
            "Global step: 4042,loss: 0.010207889\n",
            "\n",
            "Global step: 4043,loss: 0.00932958\n",
            "\n",
            "Global step: 4044,loss: 0.011845041\n",
            "\n",
            "Global step: 4045,loss: 0.014090472\n",
            "\n",
            "Global step: 4046,loss: 0.009882365\n",
            "\n",
            "Global step: 4047,loss: 0.009610412\n",
            "\n",
            "Global step: 4048,loss: 0.010027775\n",
            "\n",
            "Global step: 4049,loss: 0.012216641\n",
            "\n",
            "Global step: 4050,loss: 0.010558487\n",
            "\n",
            "Global step: 4051,loss: 0.009303825\n",
            "\n",
            "Global step: 4052,loss: 0.009223968\n",
            "\n",
            "Global step: 4053,loss: 0.0122241285\n",
            "\n",
            "Global step: 4054,loss: 0.011563811\n",
            "\n",
            "Global step: 4055,loss: 0.00967938\n",
            "\n",
            "Global step: 4056,loss: 0.009338241\n",
            "\n",
            "Global step: 4057,loss: 0.009816747\n",
            "\n",
            "Global step: 4058,loss: 0.009765027\n",
            "\n",
            "Global step: 4059,loss: 0.010281311\n",
            "\n",
            "Global step: 4060,loss: 0.016439635\n",
            "\n",
            "Global step: 4061,loss: 0.00900524\n",
            "\n",
            "Global step: 4062,loss: 0.00960788\n",
            "\n",
            "Global step: 4063,loss: 0.010273998\n",
            "\n",
            "Global step: 4064,loss: 0.0099039525\n",
            "\n",
            "Global step: 4065,loss: 0.008627979\n",
            "\n",
            "Global step: 4066,loss: 0.00974196\n",
            "\n",
            "Global step: 4067,loss: 0.009527376\n",
            "\n",
            "Global step: 4068,loss: 0.009863757\n",
            "\n",
            "Global step: 4069,loss: 0.008908392\n",
            "\n",
            "Global step: 4070,loss: 0.010340042\n",
            "\n",
            "Global step: 4071,loss: 0.009163585\n",
            "\n",
            "Global step: 4072,loss: 0.008938521\n",
            "\n",
            "Global step: 4073,loss: 0.015745286\n",
            "\n",
            "Global step: 4074,loss: 0.010026558\n",
            "\n",
            "Global step: 4075,loss: 0.009668745\n",
            "\n",
            "Global step: 4076,loss: 0.014057846\n",
            "\n",
            "Global step: 4077,loss: 0.009622614\n",
            "\n",
            "Global step: 4078,loss: 0.012247879\n",
            "\n",
            "Global step: 4079,loss: 0.008716433\n",
            "\n",
            "Global step: 4080,loss: 0.011205591\n",
            "\n",
            "Global step: 4081,loss: 0.009041902\n",
            "\n",
            "Global step: 4082,loss: 0.008934225\n",
            "\n",
            "Global step: 4083,loss: 0.010860646\n",
            "\n",
            "Global step: 4084,loss: 0.008372232\n",
            "\n",
            "Global step: 4085,loss: 0.011619812\n",
            "\n",
            "Global step: 4086,loss: 0.011755188\n",
            "\n",
            "Global step: 4087,loss: 0.010288253\n",
            "\n",
            "Global step: 4088,loss: 0.010061026\n",
            "\n",
            "Global step: 4089,loss: 0.008657345\n",
            "\n",
            "Global step: 4090,loss: 0.010808863\n",
            "\n",
            "Global step: 4091,loss: 0.0092053255\n",
            "\n",
            "Global step: 4092,loss: 0.012582034\n",
            "\n",
            "Global step: 4093,loss: 0.009987201\n",
            "\n",
            "Global step: 4094,loss: 0.009199449\n",
            "\n",
            "Global step: 4095,loss: 0.010028506\n",
            "\n",
            "Global step: 4096,loss: 0.009263013\n",
            "\n",
            "Global step: 4097,loss: 0.011673178\n",
            "\n",
            "Global step: 4098,loss: 0.010711357\n",
            "\n",
            "Global step: 4099,loss: 0.011154003\n",
            "\n",
            "Global step: 4100,loss: 0.0093487855\n",
            "\n",
            "Global step: 4101,loss: 0.009574084\n",
            "\n",
            "Global step: 4102,loss: 0.011305176\n",
            "\n",
            "Global step: 4103,loss: 0.0110827135\n",
            "\n",
            "Global step: 4104,loss: 0.0090807\n",
            "\n",
            "Global step: 4105,loss: 0.009739739\n",
            "\n",
            "Global step: 4106,loss: 0.009281612\n",
            "\n",
            "Global step: 4107,loss: 0.009253504\n",
            "\n",
            "Global step: 4108,loss: 0.010113598\n",
            "\n",
            "Global step: 4109,loss: 0.00928846\n",
            "\n",
            "Global step: 4110,loss: 0.012446918\n",
            "\n",
            "Global step: 4111,loss: 0.009432458\n",
            "\n",
            "Global step: 4112,loss: 0.009460219\n",
            "\n",
            "Global step: 4113,loss: 0.010766185\n",
            "\n",
            "Global step: 4114,loss: 0.009522819\n",
            "\n",
            "Global step: 4115,loss: 0.008614099\n",
            "\n",
            "Global step: 4116,loss: 0.013837135\n",
            "\n",
            "Global step: 4117,loss: 0.012959135\n",
            "\n",
            "Global step: 4118,loss: 0.009530683\n",
            "\n",
            "Global step: 4119,loss: 0.0100366\n",
            "\n",
            "Global step: 4120,loss: 0.009449731\n",
            "\n",
            "Global step: 4121,loss: 0.011949103\n",
            "\n",
            "Global step: 4122,loss: 0.009920018\n",
            "\n",
            "Global step: 4123,loss: 0.011690381\n",
            "\n",
            "Global step: 4124,loss: 0.011959387\n",
            "\n",
            "Global step: 4125,loss: 0.009831614\n",
            "\n",
            "Global step: 4126,loss: 0.009212957\n",
            "\n",
            "Global step: 4127,loss: 0.009961601\n",
            "\n",
            "Global step: 4128,loss: 0.008862851\n",
            "\n",
            "Global step: 4129,loss: 0.009342274\n",
            "\n",
            "Global step: 4130,loss: 0.009365362\n",
            "\n",
            "Global step: 4131,loss: 0.011998366\n",
            "\n",
            "Global step: 4132,loss: 0.008913608\n",
            "\n",
            "Global step: 4133,loss: 0.009245222\n",
            "\n",
            "Global step: 4134,loss: 0.010024513\n",
            "\n",
            "Global step: 4135,loss: 0.010602938\n",
            "\n",
            "Global step: 4136,loss: 0.0108769275\n",
            "\n",
            "Global step: 4137,loss: 0.008772157\n",
            "\n",
            "Global step: 4138,loss: 0.010479169\n",
            "\n",
            "Global step: 4139,loss: 0.02088049\n",
            "\n",
            "Global step: 4140,loss: 0.009311898\n",
            "\n",
            "Global step: 4141,loss: 0.008687094\n",
            "\n",
            "Global step: 4142,loss: 0.0099222325\n",
            "\n",
            "Global step: 4143,loss: 0.011207995\n",
            "\n",
            "Global step: 4144,loss: 0.014201889\n",
            "\n",
            "Global step: 4145,loss: 0.009958407\n",
            "\n",
            "Global step: 4146,loss: 0.008413741\n",
            "\n",
            "Global step: 4147,loss: 0.01309668\n",
            "\n",
            "Global step: 4148,loss: 0.01041672\n",
            "\n",
            "Global step: 4149,loss: 0.009564966\n",
            "\n",
            "Global step: 4150,loss: 0.016240317\n",
            "\n",
            "Global step: 4151,loss: 0.012383174\n",
            "\n",
            "Global step: 4152,loss: 0.009067725\n",
            "\n",
            "Global step: 4153,loss: 0.010019579\n",
            "\n",
            "Global step: 4154,loss: 0.010651238\n",
            "\n",
            "Global step: 4155,loss: 0.010400703\n",
            "\n",
            "Global step: 4156,loss: 0.011740394\n",
            "\n",
            "Global step: 4157,loss: 0.01027099\n",
            "\n",
            "Global step: 4158,loss: 0.009718876\n",
            "\n",
            "Global step: 4159,loss: 0.009525392\n",
            "\n",
            "Global step: 4160,loss: 0.010763763\n",
            "\n",
            "Global step: 4161,loss: 0.008865671\n",
            "\n",
            "Global step: 4162,loss: 0.0102499975\n",
            "\n",
            "Global step: 4163,loss: 0.010018581\n",
            "\n",
            "Global step: 4164,loss: 0.010243496\n",
            "\n",
            "Global step: 4165,loss: 0.009766009\n",
            "\n",
            "Global step: 4166,loss: 0.009441902\n",
            "\n",
            "Global step: 4167,loss: 0.018796237\n",
            "\n",
            "Global step: 4168,loss: 0.009096066\n",
            "\n",
            "Global step: 4169,loss: 0.016461179\n",
            "\n",
            "Global step: 4170,loss: 0.01012405\n",
            "\n",
            "Global step: 4171,loss: 0.010399375\n",
            "\n",
            "Global step: 4172,loss: 0.010290945\n",
            "\n",
            "Global step: 4173,loss: 0.011346879\n",
            "\n",
            "Global step: 4174,loss: 0.009403399\n",
            "\n",
            "Global step: 4175,loss: 0.010919953\n",
            "\n",
            "Global step: 4176,loss: 0.010010395\n",
            "\n",
            "Global step: 4177,loss: 0.009107333\n",
            "\n",
            "Global step: 4178,loss: 0.010925514\n",
            "\n",
            "Global step: 4179,loss: 0.018854853\n",
            "\n",
            "Global step: 4180,loss: 0.02129044\n",
            "\n",
            "Global step: 4181,loss: 0.009752506\n",
            "\n",
            "Global step: 4182,loss: 0.011873855\n",
            "\n",
            "Global step: 4183,loss: 0.009730132\n",
            "\n",
            "Global step: 4184,loss: 0.010795132\n",
            "\n",
            "Global step: 4185,loss: 0.012588687\n",
            "\n",
            "Global step: 4186,loss: 0.010191889\n",
            "\n",
            "Global step: 4187,loss: 0.015253309\n",
            "\n",
            "Global step: 4188,loss: 0.011721481\n",
            "\n",
            "Global step: 4189,loss: 0.008992426\n",
            "\n",
            "Global step: 4190,loss: 0.011125818\n",
            "\n",
            "Global step: 4191,loss: 0.012605063\n",
            "\n",
            "Global step: 4192,loss: 0.010230094\n",
            "\n",
            "Global step: 4193,loss: 0.009931486\n",
            "\n",
            "Global step: 4194,loss: 0.01240488\n",
            "\n",
            "Global step: 4195,loss: 0.009335173\n",
            "\n",
            "Global step: 4196,loss: 0.011353542\n",
            "\n",
            "Global step: 4197,loss: 0.009354105\n",
            "\n",
            "Global step: 4198,loss: 0.009249734\n",
            "\n",
            "Global step: 4199,loss: 0.0093314955\n",
            "\n",
            "Global step: 4200,loss: 0.011330299\n",
            "\n",
            "Global step: 4201,loss: 0.008890033\n",
            "\n",
            "Global step: 4202,loss: 0.009643652\n",
            "\n",
            "Global step: 4203,loss: 0.010424533\n",
            "\n",
            "Global step: 4204,loss: 0.009194573\n",
            "\n",
            "Global step: 4205,loss: 0.009640395\n",
            "\n",
            "Global step: 4206,loss: 0.010795251\n",
            "\n",
            "Global step: 4207,loss: 0.010145685\n",
            "\n",
            "Global step: 4208,loss: 0.009763807\n",
            "\n",
            "Global step: 4209,loss: 0.011614976\n",
            "\n",
            "Global step: 4210,loss: 0.009335223\n",
            "\n",
            "Global step: 4211,loss: 0.0094281165\n",
            "\n",
            "Global step: 4212,loss: 0.0107014915\n",
            "\n",
            "Global step: 4213,loss: 0.009369014\n",
            "\n",
            "Global step: 4214,loss: 0.009105169\n",
            "\n",
            "Global step: 4215,loss: 0.009077147\n",
            "\n",
            "Global step: 4216,loss: 0.015403859\n",
            "\n",
            "Global step: 4217,loss: 0.010252278\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4217,Val_Loss: 0.011585392869817905,  Val_acc: 0.9991319444444444 Improved\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:53:29.413246 139840769816448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:53:30.042393 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 7/15:\n",
            "Global step: 4218,loss: 0.009334827\n",
            "\n",
            "Global step: 4219,loss: 0.009505315\n",
            "\n",
            "Global step: 4220,loss: 0.010714422\n",
            "\n",
            "Global step: 4221,loss: 0.009036472\n",
            "\n",
            "Global step: 4222,loss: 0.010823429\n",
            "\n",
            "Global step: 4223,loss: 0.010718804\n",
            "\n",
            "Global step: 4224,loss: 0.019915655\n",
            "\n",
            "Global step: 4225,loss: 0.009448002\n",
            "\n",
            "Global step: 4226,loss: 0.010282707\n",
            "\n",
            "Global step: 4227,loss: 0.009504661\n",
            "\n",
            "Global step: 4228,loss: 0.008890718\n",
            "\n",
            "Global step: 4229,loss: 0.009151084\n",
            "\n",
            "Global step: 4230,loss: 0.008662913\n",
            "\n",
            "Global step: 4231,loss: 0.0091897305\n",
            "\n",
            "Global step: 4232,loss: 0.008751283\n",
            "\n",
            "Global step: 4233,loss: 0.009709008\n",
            "\n",
            "Global step: 4234,loss: 0.01043143\n",
            "\n",
            "Global step: 4235,loss: 0.010472196\n",
            "\n",
            "Global step: 4236,loss: 0.0088886\n",
            "\n",
            "Global step: 4237,loss: 0.009783477\n",
            "\n",
            "Global step: 4238,loss: 0.008724493\n",
            "\n",
            "Global step: 4239,loss: 0.0089662\n",
            "\n",
            "Global step: 4240,loss: 0.008917883\n",
            "\n",
            "Global step: 4241,loss: 0.010585064\n",
            "\n",
            "Global step: 4242,loss: 0.008608465\n",
            "\n",
            "Global step: 4243,loss: 0.0091461195\n",
            "\n",
            "Global step: 4244,loss: 0.008688394\n",
            "\n",
            "Global step: 4245,loss: 0.009516172\n",
            "\n",
            "Global step: 4246,loss: 0.009141125\n",
            "\n",
            "Global step: 4247,loss: 0.009099406\n",
            "\n",
            "Global step: 4248,loss: 0.009383482\n",
            "\n",
            "Global step: 4249,loss: 0.010793716\n",
            "\n",
            "Global step: 4250,loss: 0.009026326\n",
            "\n",
            "Global step: 4251,loss: 0.014207669\n",
            "\n",
            "Global step: 4252,loss: 0.009204149\n",
            "\n",
            "Global step: 4253,loss: 0.00878015\n",
            "\n",
            "Global step: 4254,loss: 0.008717647\n",
            "\n",
            "Global step: 4255,loss: 0.011031073\n",
            "\n",
            "Global step: 4256,loss: 0.009042469\n",
            "\n",
            "Global step: 4257,loss: 0.011554584\n",
            "\n",
            "Global step: 4258,loss: 0.008581111\n",
            "\n",
            "Global step: 4259,loss: 0.009548545\n",
            "\n",
            "Global step: 4260,loss: 0.008651295\n",
            "\n",
            "Global step: 4261,loss: 0.0118736755\n",
            "\n",
            "Global step: 4262,loss: 0.009990933\n",
            "\n",
            "Global step: 4263,loss: 0.009330283\n",
            "\n",
            "Global step: 4264,loss: 0.009720413\n",
            "\n",
            "Global step: 4265,loss: 0.010111185\n",
            "\n",
            "Global step: 4266,loss: 0.009383148\n",
            "\n",
            "Global step: 4267,loss: 0.00960854\n",
            "\n",
            "Global step: 4268,loss: 0.009020406\n",
            "\n",
            "Global step: 4269,loss: 0.009347197\n",
            "\n",
            "Global step: 4270,loss: 0.009153727\n",
            "\n",
            "Global step: 4271,loss: 0.008849487\n",
            "\n",
            "Global step: 4272,loss: 0.009925965\n",
            "\n",
            "Global step: 4273,loss: 0.0092779\n",
            "\n",
            "Global step: 4274,loss: 0.010050453\n",
            "\n",
            "Global step: 4275,loss: 0.008831024\n",
            "\n",
            "Global step: 4276,loss: 0.010096463\n",
            "\n",
            "Global step: 4277,loss: 0.010301462\n",
            "\n",
            "Global step: 4278,loss: 0.009162008\n",
            "\n",
            "Global step: 4279,loss: 0.008622037\n",
            "\n",
            "Global step: 4280,loss: 0.009752423\n",
            "\n",
            "Global step: 4281,loss: 0.0085562\n",
            "\n",
            "Global step: 4282,loss: 0.009267133\n",
            "\n",
            "Global step: 4283,loss: 0.009969867\n",
            "\n",
            "Global step: 4284,loss: 0.008761738\n",
            "\n",
            "Global step: 4285,loss: 0.008943856\n",
            "\n",
            "Global step: 4286,loss: 0.008733613\n",
            "\n",
            "Global step: 4287,loss: 0.008987375\n",
            "\n",
            "Global step: 4288,loss: 0.008722478\n",
            "\n",
            "Global step: 4289,loss: 0.008991952\n",
            "\n",
            "Global step: 4290,loss: 0.016616605\n",
            "\n",
            "Global step: 4291,loss: 0.009188388\n",
            "\n",
            "Global step: 4292,loss: 0.009556656\n",
            "\n",
            "Global step: 4293,loss: 0.015125158\n",
            "\n",
            "Global step: 4294,loss: 0.009900683\n",
            "\n",
            "Global step: 4295,loss: 0.009537573\n",
            "\n",
            "Global step: 4296,loss: 0.010208444\n",
            "\n",
            "Global step: 4297,loss: 0.008535046\n",
            "\n",
            "Global step: 4298,loss: 0.009150826\n",
            "\n",
            "Global step: 4299,loss: 0.008847041\n",
            "\n",
            "Global step: 4300,loss: 0.008161096\n",
            "\n",
            "Global step: 4301,loss: 0.012065545\n",
            "\n",
            "Global step: 4302,loss: 0.011002165\n",
            "\n",
            "Global step: 4303,loss: 0.009659735\n",
            "\n",
            "Global step: 4304,loss: 0.008493231\n",
            "\n",
            "Global step: 4305,loss: 0.0090724975\n",
            "\n",
            "Global step: 4306,loss: 0.009118348\n",
            "\n",
            "Global step: 4307,loss: 0.010657766\n",
            "\n",
            "Global step: 4308,loss: 0.00887616\n",
            "\n",
            "Global step: 4309,loss: 0.009284011\n",
            "\n",
            "Global step: 4310,loss: 0.009972576\n",
            "\n",
            "Global step: 4311,loss: 0.00928502\n",
            "\n",
            "Global step: 4312,loss: 0.009788376\n",
            "\n",
            "Global step: 4313,loss: 0.010800379\n",
            "\n",
            "Global step: 4314,loss: 0.008629438\n",
            "\n",
            "Global step: 4315,loss: 0.010080869\n",
            "\n",
            "Global step: 4316,loss: 0.008578131\n",
            "\n",
            "Global step: 4317,loss: 0.009437346\n",
            "\n",
            "Global step: 4318,loss: 0.010017988\n",
            "\n",
            "Global step: 4319,loss: 0.008833097\n",
            "\n",
            "Global step: 4320,loss: 0.0098525435\n",
            "\n",
            "Global step: 4321,loss: 0.009320054\n",
            "\n",
            "Global step: 4322,loss: 0.008365665\n",
            "\n",
            "Global step: 4323,loss: 0.0113422945\n",
            "\n",
            "Global step: 4324,loss: 0.008822183\n",
            "\n",
            "Global step: 4325,loss: 0.008862831\n",
            "\n",
            "Global step: 4326,loss: 0.008989051\n",
            "\n",
            "Global step: 4327,loss: 0.009298171\n",
            "\n",
            "Global step: 4328,loss: 0.009521139\n",
            "\n",
            "Global step: 4329,loss: 0.008738331\n",
            "\n",
            "Global step: 4330,loss: 0.008938283\n",
            "\n",
            "Global step: 4331,loss: 0.0084556835\n",
            "\n",
            "Global step: 4332,loss: 0.011522047\n",
            "\n",
            "Global step: 4333,loss: 0.008681714\n",
            "\n",
            "Global step: 4334,loss: 0.008598145\n",
            "\n",
            "Global step: 4335,loss: 0.009724988\n",
            "\n",
            "Global step: 4336,loss: 0.008415829\n",
            "\n",
            "Global step: 4337,loss: 0.009481149\n",
            "\n",
            "Global step: 4338,loss: 0.009318071\n",
            "\n",
            "Global step: 4339,loss: 0.008338656\n",
            "\n",
            "Global step: 4340,loss: 0.009892813\n",
            "\n",
            "Global step: 4341,loss: 0.009192572\n",
            "\n",
            "Global step: 4342,loss: 0.009255327\n",
            "\n",
            "Global step: 4343,loss: 0.008600622\n",
            "\n",
            "Global step: 4344,loss: 0.009618037\n",
            "\n",
            "Global step: 4345,loss: 0.009016617\n",
            "\n",
            "Global step: 4346,loss: 0.009890256\n",
            "\n",
            "Global step: 4347,loss: 0.008694365\n",
            "\n",
            "Global step: 4348,loss: 0.0126937125\n",
            "\n",
            "Global step: 4349,loss: 0.008159304\n",
            "\n",
            "Global step: 4350,loss: 0.0121207945\n",
            "\n",
            "Global step: 4351,loss: 0.00869453\n",
            "\n",
            "Global step: 4352,loss: 0.012965746\n",
            "\n",
            "Global step: 4353,loss: 0.01688068\n",
            "\n",
            "Global step: 4354,loss: 0.009724228\n",
            "\n",
            "Global step: 4355,loss: 0.01127039\n",
            "\n",
            "Global step: 4356,loss: 0.009174896\n",
            "\n",
            "Global step: 4357,loss: 0.008740571\n",
            "\n",
            "Global step: 4358,loss: 0.009815697\n",
            "\n",
            "Global step: 4359,loss: 0.009799716\n",
            "\n",
            "Global step: 4360,loss: 0.009031927\n",
            "\n",
            "Global step: 4361,loss: 0.011235923\n",
            "\n",
            "Global step: 4362,loss: 0.010587719\n",
            "\n",
            "Global step: 4363,loss: 0.012472994\n",
            "\n",
            "Global step: 4364,loss: 0.0135210045\n",
            "\n",
            "Global step: 4365,loss: 0.009352352\n",
            "\n",
            "Global step: 4366,loss: 0.0097000515\n",
            "\n",
            "Global step: 4367,loss: 0.009226163\n",
            "\n",
            "Global step: 4368,loss: 0.009818464\n",
            "\n",
            "Global step: 4369,loss: 0.009451993\n",
            "\n",
            "Global step: 4370,loss: 0.008894931\n",
            "\n",
            "Global step: 4371,loss: 0.009150296\n",
            "\n",
            "Global step: 4372,loss: 0.010564499\n",
            "\n",
            "Global step: 4373,loss: 0.015963014\n",
            "\n",
            "Global step: 4374,loss: 0.008278839\n",
            "\n",
            "Global step: 4375,loss: 0.009061947\n",
            "\n",
            "Global step: 4376,loss: 0.008872483\n",
            "\n",
            "Global step: 4377,loss: 0.0084025\n",
            "\n",
            "Global step: 4378,loss: 0.009126145\n",
            "\n",
            "Global step: 4379,loss: 0.009320689\n",
            "\n",
            "Global step: 4380,loss: 0.0092313625\n",
            "\n",
            "Global step: 4381,loss: 0.009034437\n",
            "\n",
            "Global step: 4382,loss: 0.0089027025\n",
            "\n",
            "Global step: 4383,loss: 0.009071128\n",
            "\n",
            "Global step: 4384,loss: 0.009894953\n",
            "\n",
            "Global step: 4385,loss: 0.009634197\n",
            "\n",
            "Global step: 4386,loss: 0.011757515\n",
            "\n",
            "Global step: 4387,loss: 0.008780824\n",
            "\n",
            "Global step: 4388,loss: 0.009512464\n",
            "\n",
            "Global step: 4389,loss: 0.00883555\n",
            "\n",
            "Global step: 4390,loss: 0.011129562\n",
            "\n",
            "Global step: 4391,loss: 0.009300837\n",
            "\n",
            "Global step: 4392,loss: 0.009188192\n",
            "\n",
            "Global step: 4393,loss: 0.008593608\n",
            "\n",
            "Global step: 4394,loss: 0.009144438\n",
            "\n",
            "Global step: 4395,loss: 0.008807968\n",
            "\n",
            "Global step: 4396,loss: 0.008764561\n",
            "\n",
            "Global step: 4397,loss: 0.008061086\n",
            "\n",
            "Global step: 4398,loss: 0.008814574\n",
            "\n",
            "Global step: 4399,loss: 0.0091135325\n",
            "\n",
            "Global step: 4400,loss: 0.009012849\n",
            "\n",
            "Global step: 4401,loss: 0.008530455\n",
            "\n",
            "Global step: 4402,loss: 0.009459572\n",
            "\n",
            "Global step: 4403,loss: 0.010463897\n",
            "\n",
            "Global step: 4404,loss: 0.009237034\n",
            "\n",
            "Global step: 4405,loss: 0.015336011\n",
            "\n",
            "Global step: 4406,loss: 0.008166911\n",
            "\n",
            "Global step: 4407,loss: 0.008904832\n",
            "\n",
            "Global step: 4408,loss: 0.011374862\n",
            "\n",
            "Global step: 4409,loss: 0.008370122\n",
            "\n",
            "Global step: 4410,loss: 0.0106247645\n",
            "\n",
            "Global step: 4411,loss: 0.009937017\n",
            "\n",
            "Global step: 4412,loss: 0.009055787\n",
            "\n",
            "Global step: 4413,loss: 0.009161697\n",
            "\n",
            "Global step: 4414,loss: 0.009062601\n",
            "\n",
            "Global step: 4415,loss: 0.008489421\n",
            "\n",
            "Global step: 4416,loss: 0.00940684\n",
            "\n",
            "Global step: 4417,loss: 0.010466043\n",
            "\n",
            "Global step: 4418,loss: 0.008165433\n",
            "\n",
            "Global step: 4419,loss: 0.011195108\n",
            "\n",
            "Global step: 4420,loss: 0.009407688\n",
            "\n",
            "Global step: 4421,loss: 0.008582486\n",
            "\n",
            "Global step: 4422,loss: 0.009915709\n",
            "\n",
            "Global step: 4423,loss: 0.008579987\n",
            "\n",
            "Global step: 4424,loss: 0.008407448\n",
            "\n",
            "Global step: 4425,loss: 0.009051139\n",
            "\n",
            "Global step: 4426,loss: 0.00889704\n",
            "\n",
            "Global step: 4427,loss: 0.008702601\n",
            "\n",
            "Global step: 4428,loss: 0.009299505\n",
            "\n",
            "Global step: 4429,loss: 0.009828849\n",
            "\n",
            "Global step: 4430,loss: 0.008595444\n",
            "\n",
            "Global step: 4431,loss: 0.008873198\n",
            "\n",
            "Global step: 4432,loss: 0.0093290135\n",
            "\n",
            "Global step: 4433,loss: 0.010079166\n",
            "\n",
            "Global step: 4434,loss: 0.008279167\n",
            "\n",
            "Global step: 4435,loss: 0.009733876\n",
            "\n",
            "Global step: 4436,loss: 0.008628301\n",
            "\n",
            "Global step: 4437,loss: 0.013030825\n",
            "\n",
            "Global step: 4438,loss: 0.0097571965\n",
            "\n",
            "Global step: 4439,loss: 0.00827551\n",
            "\n",
            "Global step: 4440,loss: 0.0115764\n",
            "\n",
            "Global step: 4441,loss: 0.008946549\n",
            "\n",
            "Global step: 4442,loss: 0.011523001\n",
            "\n",
            "Global step: 4443,loss: 0.009402637\n",
            "\n",
            "Global step: 4444,loss: 0.009624851\n",
            "\n",
            "Global step: 4445,loss: 0.009314764\n",
            "\n",
            "Global step: 4446,loss: 0.0098872455\n",
            "\n",
            "Global step: 4447,loss: 0.010447173\n",
            "\n",
            "Global step: 4448,loss: 0.008346617\n",
            "\n",
            "Global step: 4449,loss: 0.014018234\n",
            "\n",
            "Global step: 4450,loss: 0.009242503\n",
            "\n",
            "Global step: 4451,loss: 0.008836142\n",
            "\n",
            "Global step: 4452,loss: 0.009835923\n",
            "\n",
            "Global step: 4453,loss: 0.008535956\n",
            "\n",
            "Global step: 4454,loss: 0.011534885\n",
            "\n",
            "Global step: 4455,loss: 0.009779579\n",
            "\n",
            "Global step: 4456,loss: 0.010225907\n",
            "\n",
            "Global step: 4457,loss: 0.008512272\n",
            "\n",
            "Global step: 4458,loss: 0.009395953\n",
            "\n",
            "Global step: 4459,loss: 0.009226998\n",
            "\n",
            "Global step: 4460,loss: 0.0095011275\n",
            "\n",
            "Global step: 4461,loss: 0.00896605\n",
            "\n",
            "Global step: 4462,loss: 0.008888065\n",
            "\n",
            "Global step: 4463,loss: 0.009544615\n",
            "\n",
            "Global step: 4464,loss: 0.008914723\n",
            "\n",
            "Global step: 4465,loss: 0.008607021\n",
            "\n",
            "Global step: 4466,loss: 0.011403676\n",
            "\n",
            "Global step: 4467,loss: 0.008874949\n",
            "\n",
            "Global step: 4468,loss: 0.014418954\n",
            "\n",
            "Global step: 4469,loss: 0.008576877\n",
            "\n",
            "Global step: 4470,loss: 0.009028018\n",
            "\n",
            "Global step: 4471,loss: 0.008331156\n",
            "\n",
            "Global step: 4472,loss: 0.00958962\n",
            "\n",
            "Global step: 4473,loss: 0.009607069\n",
            "\n",
            "Global step: 4474,loss: 0.009191076\n",
            "\n",
            "Global step: 4475,loss: 0.009899912\n",
            "\n",
            "Global step: 4476,loss: 0.012424187\n",
            "\n",
            "Global step: 4477,loss: 0.008909178\n",
            "\n",
            "Global step: 4478,loss: 0.0091232825\n",
            "\n",
            "Global step: 4479,loss: 0.0089092385\n",
            "\n",
            "Global step: 4480,loss: 0.010093435\n",
            "\n",
            "Global step: 4481,loss: 0.008090116\n",
            "\n",
            "Global step: 4482,loss: 0.009661584\n",
            "\n",
            "Global step: 4483,loss: 0.00919428\n",
            "\n",
            "Global step: 4484,loss: 0.008515291\n",
            "\n",
            "Global step: 4485,loss: 0.009065706\n",
            "\n",
            "Global step: 4486,loss: 0.009204817\n",
            "\n",
            "Global step: 4487,loss: 0.009149665\n",
            "\n",
            "Global step: 4488,loss: 0.009052046\n",
            "\n",
            "Global step: 4489,loss: 0.011501549\n",
            "\n",
            "Global step: 4490,loss: 0.009278679\n",
            "\n",
            "Global step: 4491,loss: 0.010553873\n",
            "\n",
            "Global step: 4492,loss: 0.008764735\n",
            "\n",
            "Global step: 4493,loss: 0.010693125\n",
            "\n",
            "Global step: 4494,loss: 0.008732121\n",
            "\n",
            "Global step: 4495,loss: 0.009244898\n",
            "\n",
            "Global step: 4496,loss: 0.008800735\n",
            "\n",
            "Global step: 4497,loss: 0.009560174\n",
            "\n",
            "Global step: 4498,loss: 0.0085149\n",
            "\n",
            "Global step: 4499,loss: 0.010103933\n",
            "\n",
            "Global step: 4500,loss: 0.010462871\n",
            "\n",
            "Global step: 4501,loss: 0.008966236\n",
            "\n",
            "Global step: 4502,loss: 0.0086306585\n",
            "\n",
            "Global step: 4503,loss: 0.008147447\n",
            "\n",
            "Global step: 4504,loss: 0.008523112\n",
            "\n",
            "Global step: 4505,loss: 0.009070691\n",
            "\n",
            "Global step: 4506,loss: 0.013272048\n",
            "\n",
            "Global step: 4507,loss: 0.016213197\n",
            "\n",
            "Global step: 4508,loss: 0.008484539\n",
            "\n",
            "Global step: 4509,loss: 0.008870936\n",
            "\n",
            "Global step: 4510,loss: 0.00852252\n",
            "\n",
            "Global step: 4511,loss: 0.008496674\n",
            "\n",
            "Global step: 4512,loss: 0.011488536\n",
            "\n",
            "Global step: 4513,loss: 0.010981549\n",
            "\n",
            "Global step: 4514,loss: 0.008225538\n",
            "\n",
            "Global step: 4515,loss: 0.008958323\n",
            "\n",
            "Global step: 4516,loss: 0.00948707\n",
            "\n",
            "Global step: 4517,loss: 0.009064498\n",
            "\n",
            "Global step: 4518,loss: 0.008127136\n",
            "\n",
            "Global step: 4519,loss: 0.007964076\n",
            "\n",
            "Global step: 4520,loss: 0.015852064\n",
            "\n",
            "Global step: 4521,loss: 0.0101309\n",
            "\n",
            "Global step: 4522,loss: 0.009470441\n",
            "\n",
            "Global step: 4523,loss: 0.011124114\n",
            "\n",
            "Global step: 4524,loss: 0.009044459\n",
            "\n",
            "Global step: 4525,loss: 0.008453731\n",
            "\n",
            "Global step: 4526,loss: 0.009072937\n",
            "\n",
            "Global step: 4527,loss: 0.008657483\n",
            "\n",
            "Global step: 4528,loss: 0.008741153\n",
            "\n",
            "Global step: 4529,loss: 0.009126305\n",
            "\n",
            "Global step: 4530,loss: 0.008781209\n",
            "\n",
            "Global step: 4531,loss: 0.011952125\n",
            "\n",
            "Global step: 4532,loss: 0.00895275\n",
            "\n",
            "Global step: 4533,loss: 0.010535008\n",
            "\n",
            "Global step: 4534,loss: 0.00939406\n",
            "\n",
            "Global step: 4535,loss: 0.008785967\n",
            "\n",
            "Global step: 4536,loss: 0.009100008\n",
            "\n",
            "Global step: 4537,loss: 0.0090057105\n",
            "\n",
            "Global step: 4538,loss: 0.009017114\n",
            "\n",
            "Global step: 4539,loss: 0.012231057\n",
            "\n",
            "Global step: 4540,loss: 0.014592023\n",
            "\n",
            "Global step: 4541,loss: 0.00871447\n",
            "\n",
            "Global step: 4542,loss: 0.009631481\n",
            "\n",
            "Global step: 4543,loss: 0.008423522\n",
            "\n",
            "Global step: 4544,loss: 0.011217532\n",
            "\n",
            "Global step: 4545,loss: 0.009149517\n",
            "\n",
            "Global step: 4546,loss: 0.009345509\n",
            "\n",
            "Global step: 4547,loss: 0.009206131\n",
            "\n",
            "Global step: 4548,loss: 0.009078714\n",
            "\n",
            "Global step: 4549,loss: 0.009188882\n",
            "\n",
            "Global step: 4550,loss: 0.011341879\n",
            "\n",
            "Global step: 4551,loss: 0.011948157\n",
            "\n",
            "Global step: 4552,loss: 0.009438772\n",
            "\n",
            "Global step: 4553,loss: 0.0105914045\n",
            "\n",
            "Global step: 4554,loss: 0.00817278\n",
            "\n",
            "Global step: 4555,loss: 0.009894713\n",
            "\n",
            "Global step: 4556,loss: 0.009177348\n",
            "\n",
            "Global step: 4557,loss: 0.010435142\n",
            "\n",
            "Global step: 4558,loss: 0.008910596\n",
            "\n",
            "Global step: 4559,loss: 0.008333506\n",
            "\n",
            "Global step: 4560,loss: 0.013876917\n",
            "\n",
            "Global step: 4561,loss: 0.009928317\n",
            "\n",
            "Global step: 4562,loss: 0.008982445\n",
            "\n",
            "Global step: 4563,loss: 0.008758396\n",
            "\n",
            "Global step: 4564,loss: 0.009060591\n",
            "\n",
            "Global step: 4565,loss: 0.010575784\n",
            "\n",
            "Global step: 4566,loss: 0.0102882255\n",
            "\n",
            "Global step: 4567,loss: 0.00970723\n",
            "\n",
            "Global step: 4568,loss: 0.008576727\n",
            "\n",
            "Global step: 4569,loss: 0.007700294\n",
            "\n",
            "Global step: 4570,loss: 0.008427203\n",
            "\n",
            "Global step: 4571,loss: 0.008786033\n",
            "\n",
            "Global step: 4572,loss: 0.012518938\n",
            "\n",
            "Global step: 4573,loss: 0.008678581\n",
            "\n",
            "Global step: 4574,loss: 0.008441939\n",
            "\n",
            "Global step: 4575,loss: 0.008713758\n",
            "\n",
            "Global step: 4576,loss: 0.009910869\n",
            "\n",
            "Global step: 4577,loss: 0.011390724\n",
            "\n",
            "Global step: 4578,loss: 0.008721795\n",
            "\n",
            "Global step: 4579,loss: 0.008600384\n",
            "\n",
            "Global step: 4580,loss: 0.009826625\n",
            "\n",
            "Global step: 4581,loss: 0.00935703\n",
            "\n",
            "Global step: 4582,loss: 0.010656703\n",
            "\n",
            "Global step: 4583,loss: 0.010479109\n",
            "\n",
            "Global step: 4584,loss: 0.009554912\n",
            "\n",
            "Global step: 4585,loss: 0.00861215\n",
            "\n",
            "Global step: 4586,loss: 0.00769113\n",
            "\n",
            "Global step: 4587,loss: 0.009042783\n",
            "\n",
            "Global step: 4588,loss: 0.009130954\n",
            "\n",
            "Global step: 4589,loss: 0.009583935\n",
            "\n",
            "Global step: 4590,loss: 0.013188955\n",
            "\n",
            "Global step: 4591,loss: 0.008988788\n",
            "\n",
            "Global step: 4592,loss: 0.0076700784\n",
            "\n",
            "Global step: 4593,loss: 0.008385963\n",
            "\n",
            "Global step: 4594,loss: 0.0081427405\n",
            "\n",
            "Global step: 4595,loss: 0.009343659\n",
            "\n",
            "Global step: 4596,loss: 0.014420837\n",
            "\n",
            "Global step: 4597,loss: 0.009555803\n",
            "\n",
            "Global step: 4598,loss: 0.008719322\n",
            "\n",
            "Global step: 4599,loss: 0.009452822\n",
            "\n",
            "Global step: 4600,loss: 0.010516783\n",
            "\n",
            "Global step: 4601,loss: 0.009446495\n",
            "\n",
            "Global step: 4602,loss: 0.0093690045\n",
            "\n",
            "Global step: 4603,loss: 0.008863179\n",
            "\n",
            "Global step: 4604,loss: 0.009021911\n",
            "\n",
            "Global step: 4605,loss: 0.009080281\n",
            "\n",
            "Global step: 4606,loss: 0.010561383\n",
            "\n",
            "Global step: 4607,loss: 0.008112149\n",
            "\n",
            "Global step: 4608,loss: 0.008939442\n",
            "\n",
            "Global step: 4609,loss: 0.008618133\n",
            "\n",
            "Global step: 4610,loss: 0.009471942\n",
            "\n",
            "Global step: 4611,loss: 0.009996902\n",
            "\n",
            "Global step: 4612,loss: 0.008778214\n",
            "\n",
            "Global step: 4613,loss: 0.008369511\n",
            "\n",
            "Global step: 4614,loss: 0.008483785\n",
            "\n",
            "Global step: 4615,loss: 0.009092821\n",
            "\n",
            "Global step: 4616,loss: 0.008730873\n",
            "\n",
            "Global step: 4617,loss: 0.008982485\n",
            "\n",
            "Global step: 4618,loss: 0.008078107\n",
            "\n",
            "Global step: 4619,loss: 0.008520187\n",
            "\n",
            "Global step: 4620,loss: 0.008952295\n",
            "\n",
            "Global step: 4621,loss: 0.008697875\n",
            "\n",
            "Global step: 4622,loss: 0.009197478\n",
            "\n",
            "Global step: 4623,loss: 0.008330125\n",
            "\n",
            "Global step: 4624,loss: 0.008938214\n",
            "\n",
            "Global step: 4625,loss: 0.008422334\n",
            "\n",
            "Global step: 4626,loss: 0.009885451\n",
            "\n",
            "Global step: 4627,loss: 0.009978872\n",
            "\n",
            "Global step: 4628,loss: 0.009428694\n",
            "\n",
            "Global step: 4629,loss: 0.011186667\n",
            "\n",
            "Global step: 4630,loss: 0.009198703\n",
            "\n",
            "Global step: 4631,loss: 0.010152791\n",
            "\n",
            "Global step: 4632,loss: 0.008924233\n",
            "\n",
            "Global step: 4633,loss: 0.013386274\n",
            "\n",
            "Global step: 4634,loss: 0.009228676\n",
            "\n",
            "Global step: 4635,loss: 0.011453413\n",
            "\n",
            "Global step: 4636,loss: 0.010465947\n",
            "\n",
            "Global step: 4637,loss: 0.008216755\n",
            "\n",
            "Global step: 4638,loss: 0.0110068405\n",
            "\n",
            "Global step: 4639,loss: 0.0089693675\n",
            "\n",
            "Global step: 4640,loss: 0.008736628\n",
            "\n",
            "Global step: 4641,loss: 0.009106778\n",
            "\n",
            "Global step: 4642,loss: 0.009325812\n",
            "\n",
            "Global step: 4643,loss: 0.009639233\n",
            "\n",
            "Global step: 4644,loss: 0.009667777\n",
            "\n",
            "Global step: 4645,loss: 0.008966152\n",
            "\n",
            "Global step: 4646,loss: 0.009737213\n",
            "\n",
            "Global step: 4647,loss: 0.00818215\n",
            "\n",
            "Global step: 4648,loss: 0.00924221\n",
            "\n",
            "Global step: 4649,loss: 0.008903218\n",
            "\n",
            "Global step: 4650,loss: 0.008466237\n",
            "\n",
            "Global step: 4651,loss: 0.009026317\n",
            "\n",
            "Global step: 4652,loss: 0.008596286\n",
            "\n",
            "Global step: 4653,loss: 0.008125868\n",
            "\n",
            "Global step: 4654,loss: 0.009449289\n",
            "\n",
            "Global step: 4655,loss: 0.009413438\n",
            "\n",
            "Global step: 4656,loss: 0.008322536\n",
            "\n",
            "Global step: 4657,loss: 0.008998394\n",
            "\n",
            "Global step: 4658,loss: 0.008152519\n",
            "\n",
            "Global step: 4659,loss: 0.009276414\n",
            "\n",
            "Global step: 4660,loss: 0.01056348\n",
            "\n",
            "Global step: 4661,loss: 0.008213884\n",
            "\n",
            "Global step: 4662,loss: 0.008889818\n",
            "\n",
            "Global step: 4663,loss: 0.008564848\n",
            "\n",
            "Global step: 4664,loss: 0.008601144\n",
            "\n",
            "Global step: 4665,loss: 0.008550575\n",
            "\n",
            "Global step: 4666,loss: 0.008700363\n",
            "\n",
            "Global step: 4667,loss: 0.008133428\n",
            "\n",
            "Global step: 4668,loss: 0.007867737\n",
            "\n",
            "Global step: 4669,loss: 0.008548474\n",
            "\n",
            "Global step: 4670,loss: 0.010676579\n",
            "\n",
            "Global step: 4671,loss: 0.008703111\n",
            "\n",
            "Global step: 4672,loss: 0.008739685\n",
            "\n",
            "Global step: 4673,loss: 0.009129717\n",
            "\n",
            "Global step: 4674,loss: 0.011496857\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.63333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:54:35.222314 139837015512832 supervisor.py:1099] global_step/sec: 6.63333\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 4675.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:54:35.228127 139837023905536 supervisor.py:1050] Recording summary at step 4675.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4675,loss: 0.010423655\n",
            "\n",
            "Global step: 4676,loss: 0.009481869\n",
            "\n",
            "Global step: 4677,loss: 0.011439301\n",
            "\n",
            "Global step: 4678,loss: 0.008134692\n",
            "\n",
            "Global step: 4679,loss: 0.008753935\n",
            "\n",
            "Global step: 4680,loss: 0.009381246\n",
            "\n",
            "Global step: 4681,loss: 0.009541278\n",
            "\n",
            "Global step: 4682,loss: 0.008709349\n",
            "\n",
            "Global step: 4683,loss: 0.0083563505\n",
            "\n",
            "Global step: 4684,loss: 0.009081639\n",
            "\n",
            "Global step: 4685,loss: 0.009090424\n",
            "\n",
            "Global step: 4686,loss: 0.009565393\n",
            "\n",
            "Global step: 4687,loss: 0.008400091\n",
            "\n",
            "Global step: 4688,loss: 0.008409149\n",
            "\n",
            "Global step: 4689,loss: 0.009135823\n",
            "\n",
            "Global step: 4690,loss: 0.008024042\n",
            "\n",
            "Global step: 4691,loss: 0.008754263\n",
            "\n",
            "Global step: 4692,loss: 0.009090546\n",
            "\n",
            "Global step: 4693,loss: 0.008690086\n",
            "\n",
            "Global step: 4694,loss: 0.008349441\n",
            "\n",
            "Global step: 4695,loss: 0.008487686\n",
            "\n",
            "Global step: 4696,loss: 0.009119669\n",
            "\n",
            "Global step: 4697,loss: 0.009287932\n",
            "\n",
            "Global step: 4698,loss: 0.01180237\n",
            "\n",
            "Global step: 4699,loss: 0.008741215\n",
            "\n",
            "Global step: 4700,loss: 0.008965306\n",
            "\n",
            "Global step: 4701,loss: 0.009801794\n",
            "\n",
            "Global step: 4702,loss: 0.008927987\n",
            "\n",
            "Global step: 4703,loss: 0.009044108\n",
            "\n",
            "Global step: 4704,loss: 0.013824371\n",
            "\n",
            "Global step: 4705,loss: 0.009235061\n",
            "\n",
            "Global step: 4706,loss: 0.009451973\n",
            "\n",
            "Global step: 4707,loss: 0.0096118655\n",
            "\n",
            "Global step: 4708,loss: 0.010615363\n",
            "\n",
            "Global step: 4709,loss: 0.0088127935\n",
            "\n",
            "Global step: 4710,loss: 0.00833738\n",
            "\n",
            "Global step: 4711,loss: 0.008937449\n",
            "\n",
            "Global step: 4712,loss: 0.009109099\n",
            "\n",
            "Global step: 4713,loss: 0.008793288\n",
            "\n",
            "Global step: 4714,loss: 0.011048287\n",
            "\n",
            "Global step: 4715,loss: 0.008374471\n",
            "\n",
            "Global step: 4716,loss: 0.008726958\n",
            "\n",
            "Global step: 4717,loss: 0.008098365\n",
            "\n",
            "Global step: 4718,loss: 0.009350539\n",
            "\n",
            "Global step: 4719,loss: 0.009533552\n",
            "\n",
            "Global step: 4720,loss: 0.010455306\n",
            "\n",
            "Global step: 4721,loss: 0.009536638\n",
            "\n",
            "Global step: 4722,loss: 0.007765603\n",
            "\n",
            "Global step: 4723,loss: 0.009078573\n",
            "\n",
            "Global step: 4724,loss: 0.008814402\n",
            "\n",
            "Global step: 4725,loss: 0.008909997\n",
            "\n",
            "Global step: 4726,loss: 0.0148543855\n",
            "\n",
            "Global step: 4727,loss: 0.008539594\n",
            "\n",
            "Global step: 4728,loss: 0.01119484\n",
            "\n",
            "Global step: 4729,loss: 0.008726931\n",
            "\n",
            "Global step: 4730,loss: 0.009064025\n",
            "\n",
            "Global step: 4731,loss: 0.008750135\n",
            "\n",
            "Global step: 4732,loss: 0.00790443\n",
            "\n",
            "Global step: 4733,loss: 0.008669152\n",
            "\n",
            "Global step: 4734,loss: 0.010564739\n",
            "\n",
            "Global step: 4735,loss: 0.009820279\n",
            "\n",
            "Global step: 4736,loss: 0.009433907\n",
            "\n",
            "Global step: 4737,loss: 0.008875528\n",
            "\n",
            "Global step: 4738,loss: 0.008227898\n",
            "\n",
            "Global step: 4739,loss: 0.009356455\n",
            "\n",
            "Global step: 4740,loss: 0.008245705\n",
            "\n",
            "Global step: 4741,loss: 0.008247183\n",
            "\n",
            "Global step: 4742,loss: 0.020463452\n",
            "\n",
            "Global step: 4743,loss: 0.00905663\n",
            "\n",
            "Global step: 4744,loss: 0.008875783\n",
            "\n",
            "Global step: 4745,loss: 0.008409122\n",
            "\n",
            "Global step: 4746,loss: 0.0094107315\n",
            "\n",
            "Global step: 4747,loss: 0.009384423\n",
            "\n",
            "Global step: 4748,loss: 0.00880271\n",
            "\n",
            "Global step: 4749,loss: 0.008146476\n",
            "\n",
            "Global step: 4750,loss: 0.009055598\n",
            "\n",
            "Global step: 4751,loss: 0.008194173\n",
            "\n",
            "Global step: 4752,loss: 0.008626464\n",
            "\n",
            "Global step: 4753,loss: 0.008557192\n",
            "\n",
            "Global step: 4754,loss: 0.009618565\n",
            "\n",
            "Global step: 4755,loss: 0.008416372\n",
            "\n",
            "Global step: 4756,loss: 0.007694804\n",
            "\n",
            "Global step: 4757,loss: 0.008651724\n",
            "\n",
            "Global step: 4758,loss: 0.008599833\n",
            "\n",
            "Global step: 4759,loss: 0.010977742\n",
            "\n",
            "Global step: 4760,loss: 0.008799538\n",
            "\n",
            "Global step: 4761,loss: 0.008421996\n",
            "\n",
            "Global step: 4762,loss: 0.016560532\n",
            "\n",
            "Global step: 4763,loss: 0.008990664\n",
            "\n",
            "Global step: 4764,loss: 0.008291595\n",
            "\n",
            "Global step: 4765,loss: 0.00924328\n",
            "\n",
            "Global step: 4766,loss: 0.008931427\n",
            "\n",
            "Global step: 4767,loss: 0.008120783\n",
            "\n",
            "Global step: 4768,loss: 0.008233044\n",
            "\n",
            "Global step: 4769,loss: 0.00898605\n",
            "\n",
            "Global step: 4770,loss: 0.007921649\n",
            "\n",
            "Global step: 4771,loss: 0.009176567\n",
            "\n",
            "Global step: 4772,loss: 0.00851904\n",
            "\n",
            "Global step: 4773,loss: 0.011334905\n",
            "\n",
            "Global step: 4774,loss: 0.0089187585\n",
            "\n",
            "Global step: 4775,loss: 0.00878794\n",
            "\n",
            "Global step: 4776,loss: 0.00846638\n",
            "\n",
            "Global step: 4777,loss: 0.008844973\n",
            "\n",
            "Global step: 4778,loss: 0.012656409\n",
            "\n",
            "Global step: 4779,loss: 0.00869284\n",
            "\n",
            "Global step: 4780,loss: 0.010667931\n",
            "\n",
            "Global step: 4781,loss: 0.008439643\n",
            "\n",
            "Global step: 4782,loss: 0.008220053\n",
            "\n",
            "Global step: 4783,loss: 0.010215245\n",
            "\n",
            "Global step: 4784,loss: 0.008789598\n",
            "\n",
            "Global step: 4785,loss: 0.008355468\n",
            "\n",
            "Global step: 4786,loss: 0.008807322\n",
            "\n",
            "Global step: 4787,loss: 0.009907859\n",
            "\n",
            "Global step: 4788,loss: 0.008397828\n",
            "\n",
            "Global step: 4789,loss: 0.008559732\n",
            "\n",
            "Global step: 4790,loss: 0.008635472\n",
            "\n",
            "Global step: 4791,loss: 0.009137836\n",
            "\n",
            "Global step: 4792,loss: 0.008260515\n",
            "\n",
            "Global step: 4793,loss: 0.011484377\n",
            "\n",
            "Global step: 4794,loss: 0.008473291\n",
            "\n",
            "Global step: 4795,loss: 0.008295512\n",
            "\n",
            "Global step: 4796,loss: 0.008467948\n",
            "\n",
            "Global step: 4797,loss: 0.008777812\n",
            "\n",
            "Global step: 4798,loss: 0.008103531\n",
            "\n",
            "Global step: 4799,loss: 0.02032988\n",
            "\n",
            "Global step: 4800,loss: 0.008406243\n",
            "\n",
            "Global step: 4801,loss: 0.008573683\n",
            "\n",
            "Global step: 4802,loss: 0.008452852\n",
            "\n",
            "Global step: 4803,loss: 0.010028567\n",
            "\n",
            "Global step: 4804,loss: 0.008234477\n",
            "\n",
            "Global step: 4805,loss: 0.009583045\n",
            "\n",
            "Global step: 4806,loss: 0.009370199\n",
            "\n",
            "Global step: 4807,loss: 0.008949142\n",
            "\n",
            "Global step: 4808,loss: 0.01386267\n",
            "\n",
            "Global step: 4809,loss: 0.00890464\n",
            "\n",
            "Global step: 4810,loss: 0.008747862\n",
            "\n",
            "Global step: 4811,loss: 0.008189722\n",
            "\n",
            "Global step: 4812,loss: 0.0092030745\n",
            "\n",
            "Global step: 4813,loss: 0.009642869\n",
            "\n",
            "Global step: 4814,loss: 0.008264838\n",
            "\n",
            "Global step: 4815,loss: 0.008530054\n",
            "\n",
            "Global step: 4816,loss: 0.008860764\n",
            "\n",
            "Global step: 4817,loss: 0.009312892\n",
            "\n",
            "Global step: 4818,loss: 0.008873299\n",
            "\n",
            "Global step: 4819,loss: 0.0094136875\n",
            "\n",
            "Global step: 4820,loss: 0.008546054\n",
            "\n",
            "Global step: 4821,loss: 0.008661096\n",
            "\n",
            "Global step: 4822,loss: 0.0124198\n",
            "\n",
            "Global step: 4823,loss: 0.008583758\n",
            "\n",
            "Global step: 4824,loss: 0.008278498\n",
            "\n",
            "Global step: 4825,loss: 0.00914616\n",
            "\n",
            "Global step: 4826,loss: 0.008535225\n",
            "\n",
            "Global step: 4827,loss: 0.008244578\n",
            "\n",
            "Global step: 4828,loss: 0.008514226\n",
            "\n",
            "Global step: 4829,loss: 0.012065299\n",
            "\n",
            "Global step: 4830,loss: 0.008468605\n",
            "\n",
            "Global step: 4831,loss: 0.008116276\n",
            "\n",
            "Global step: 4832,loss: 0.018872265\n",
            "\n",
            "Global step: 4833,loss: 0.008307933\n",
            "\n",
            "Global step: 4834,loss: 0.007948281\n",
            "\n",
            "Global step: 4835,loss: 0.0084527\n",
            "\n",
            "Global step: 4836,loss: 0.008633581\n",
            "\n",
            "Global step: 4837,loss: 0.0076920805\n",
            "\n",
            "Global step: 4838,loss: 0.0086209895\n",
            "\n",
            "Global step: 4839,loss: 0.010452614\n",
            "\n",
            "Global step: 4840,loss: 0.009082867\n",
            "\n",
            "Global step: 4841,loss: 0.008988299\n",
            "\n",
            "Global step: 4842,loss: 0.008396501\n",
            "\n",
            "Global step: 4843,loss: 0.010442788\n",
            "\n",
            "Global step: 4844,loss: 0.007944192\n",
            "\n",
            "Global step: 4845,loss: 0.008984808\n",
            "\n",
            "Global step: 4846,loss: 0.00900529\n",
            "\n",
            "Global step: 4847,loss: 0.008453205\n",
            "\n",
            "Global step: 4848,loss: 0.0105249565\n",
            "\n",
            "Global step: 4849,loss: 0.00844013\n",
            "\n",
            "Global step: 4850,loss: 0.008253842\n",
            "\n",
            "Global step: 4851,loss: 0.009269439\n",
            "\n",
            "Global step: 4852,loss: 0.008344213\n",
            "\n",
            "Global step: 4853,loss: 0.008909497\n",
            "\n",
            "Global step: 4854,loss: 0.012520831\n",
            "\n",
            "Global step: 4855,loss: 0.008325725\n",
            "\n",
            "Global step: 4856,loss: 0.008423526\n",
            "\n",
            "Global step: 4857,loss: 0.008652502\n",
            "\n",
            "Global step: 4858,loss: 0.0094950255\n",
            "\n",
            "Global step: 4859,loss: 0.009823751\n",
            "\n",
            "Global step: 4860,loss: 0.00886529\n",
            "\n",
            "Global step: 4861,loss: 0.010106843\n",
            "\n",
            "Global step: 4862,loss: 0.007975534\n",
            "\n",
            "Global step: 4863,loss: 0.008087905\n",
            "\n",
            "Global step: 4864,loss: 0.008666694\n",
            "\n",
            "Global step: 4865,loss: 0.008280844\n",
            "\n",
            "Global step: 4866,loss: 0.010369981\n",
            "\n",
            "Global step: 4867,loss: 0.008668621\n",
            "\n",
            "Global step: 4868,loss: 0.008587268\n",
            "\n",
            "Global step: 4869,loss: 0.008106868\n",
            "\n",
            "Global step: 4870,loss: 0.008157028\n",
            "\n",
            "Global step: 4871,loss: 0.009739615\n",
            "\n",
            "Global step: 4872,loss: 0.009514948\n",
            "\n",
            "Global step: 4873,loss: 0.009073608\n",
            "\n",
            "Global step: 4874,loss: 0.008285915\n",
            "\n",
            "Global step: 4875,loss: 0.010414161\n",
            "\n",
            "Global step: 4876,loss: 0.008384667\n",
            "\n",
            "Global step: 4877,loss: 0.011924788\n",
            "\n",
            "Global step: 4878,loss: 0.009157594\n",
            "\n",
            "Global step: 4879,loss: 0.016168175\n",
            "\n",
            "Global step: 4880,loss: 0.00893925\n",
            "\n",
            "Global step: 4881,loss: 0.008805903\n",
            "\n",
            "Global step: 4882,loss: 0.012863506\n",
            "\n",
            "Global step: 4883,loss: 0.008689332\n",
            "\n",
            "Global step: 4884,loss: 0.009428848\n",
            "\n",
            "Global step: 4885,loss: 0.00814189\n",
            "\n",
            "Global step: 4886,loss: 0.0084315445\n",
            "\n",
            "Global step: 4887,loss: 0.008389797\n",
            "\n",
            "Global step: 4888,loss: 0.007901933\n",
            "\n",
            "Global step: 4889,loss: 0.008111468\n",
            "\n",
            "Global step: 4890,loss: 0.008678725\n",
            "\n",
            "Global step: 4891,loss: 0.008382084\n",
            "\n",
            "Global step: 4892,loss: 0.010468551\n",
            "\n",
            "Global step: 4893,loss: 0.008596487\n",
            "\n",
            "Global step: 4894,loss: 0.008205766\n",
            "\n",
            "Global step: 4895,loss: 0.008812603\n",
            "\n",
            "Global step: 4896,loss: 0.007836544\n",
            "\n",
            "Global step: 4897,loss: 0.013049836\n",
            "\n",
            "Global step: 4898,loss: 0.009032262\n",
            "\n",
            "Global step: 4899,loss: 0.008078741\n",
            "\n",
            "Global step: 4900,loss: 0.008947528\n",
            "\n",
            "Global step: 4901,loss: 0.00874121\n",
            "\n",
            "Global step: 4902,loss: 0.008131823\n",
            "\n",
            "Global step: 4903,loss: 0.008848958\n",
            "\n",
            "Global step: 4904,loss: 0.008458545\n",
            "\n",
            "Global step: 4905,loss: 0.008504944\n",
            "\n",
            "Global step: 4906,loss: 0.008259771\n",
            "\n",
            "Global step: 4907,loss: 0.008181035\n",
            "\n",
            "Global step: 4908,loss: 0.008614315\n",
            "\n",
            "Global step: 4909,loss: 0.009309418\n",
            "\n",
            "Global step: 4910,loss: 0.007998413\n",
            "\n",
            "Global step: 4911,loss: 0.0077955895\n",
            "\n",
            "Global step: 4912,loss: 0.008330523\n",
            "\n",
            "Global step: 4913,loss: 0.008337248\n",
            "\n",
            "Global step: 4914,loss: 0.007933287\n",
            "\n",
            "Global step: 4915,loss: 0.007932389\n",
            "\n",
            "Global step: 4916,loss: 0.009104495\n",
            "\n",
            "Global step: 4917,loss: 0.008990687\n",
            "\n",
            "Global step: 4918,loss: 0.008343421\n",
            "\n",
            "Global step: 4919,loss: 0.011776822\n",
            "\n",
            "Global step: 4920,loss: 0.0083672935\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 4920,val_loss: 0.010468987119185109\n",
            "\n",
            "Training for epoch 8/15:\n",
            "Global step: 4921,loss: 0.008064761\n",
            "\n",
            "Global step: 4922,loss: 0.008674025\n",
            "\n",
            "Global step: 4923,loss: 0.008288885\n",
            "\n",
            "Global step: 4924,loss: 0.008336868\n",
            "\n",
            "Global step: 4925,loss: 0.008315321\n",
            "\n",
            "Global step: 4926,loss: 0.007948577\n",
            "\n",
            "Global step: 4927,loss: 0.008520497\n",
            "\n",
            "Global step: 4928,loss: 0.0092390925\n",
            "\n",
            "Global step: 4929,loss: 0.009432202\n",
            "\n",
            "Global step: 4930,loss: 0.008047788\n",
            "\n",
            "Global step: 4931,loss: 0.00805022\n",
            "\n",
            "Global step: 4932,loss: 0.00790962\n",
            "\n",
            "Global step: 4933,loss: 0.008324396\n",
            "\n",
            "Global step: 4934,loss: 0.007436472\n",
            "\n",
            "Global step: 4935,loss: 0.008642381\n",
            "\n",
            "Global step: 4936,loss: 0.008161947\n",
            "\n",
            "Global step: 4937,loss: 0.008180159\n",
            "\n",
            "Global step: 4938,loss: 0.0078819245\n",
            "\n",
            "Global step: 4939,loss: 0.007988058\n",
            "\n",
            "Global step: 4940,loss: 0.007903011\n",
            "\n",
            "Global step: 4941,loss: 0.007953426\n",
            "\n",
            "Global step: 4942,loss: 0.008065017\n",
            "\n",
            "Global step: 4943,loss: 0.0077608135\n",
            "\n",
            "Global step: 4944,loss: 0.007918977\n",
            "\n",
            "Global step: 4945,loss: 0.008516721\n",
            "\n",
            "Global step: 4946,loss: 0.0072956914\n",
            "\n",
            "Global step: 4947,loss: 0.00911976\n",
            "\n",
            "Global step: 4948,loss: 0.013274413\n",
            "\n",
            "Global step: 4949,loss: 0.008053989\n",
            "\n",
            "Global step: 4950,loss: 0.0077003003\n",
            "\n",
            "Global step: 4951,loss: 0.007997552\n",
            "\n",
            "Global step: 4952,loss: 0.007507264\n",
            "\n",
            "Global step: 4953,loss: 0.008561728\n",
            "\n",
            "Global step: 4954,loss: 0.0075703245\n",
            "\n",
            "Global step: 4955,loss: 0.0085343905\n",
            "\n",
            "Global step: 4956,loss: 0.008595988\n",
            "\n",
            "Global step: 4957,loss: 0.007837625\n",
            "\n",
            "Global step: 4958,loss: 0.00787374\n",
            "\n",
            "Global step: 4959,loss: 0.008160454\n",
            "\n",
            "Global step: 4960,loss: 0.008204332\n",
            "\n",
            "Global step: 4961,loss: 0.008076676\n",
            "\n",
            "Global step: 4962,loss: 0.007969621\n",
            "\n",
            "Global step: 4963,loss: 0.008374449\n",
            "\n",
            "Global step: 4964,loss: 0.008130593\n",
            "\n",
            "Global step: 4965,loss: 0.008254762\n",
            "\n",
            "Global step: 4966,loss: 0.008285793\n",
            "\n",
            "Global step: 4967,loss: 0.0076173153\n",
            "\n",
            "Global step: 4968,loss: 0.00786703\n",
            "\n",
            "Global step: 4969,loss: 0.007397029\n",
            "\n",
            "Global step: 4970,loss: 0.0078044003\n",
            "\n",
            "Global step: 4971,loss: 0.007919368\n",
            "\n",
            "Global step: 4972,loss: 0.008029292\n",
            "\n",
            "Global step: 4973,loss: 0.0077065793\n",
            "\n",
            "Global step: 4974,loss: 0.008217931\n",
            "\n",
            "Global step: 4975,loss: 0.007970255\n",
            "\n",
            "Global step: 4976,loss: 0.008263012\n",
            "\n",
            "Global step: 4977,loss: 0.0072334823\n",
            "\n",
            "Global step: 4978,loss: 0.008441592\n",
            "\n",
            "Global step: 4979,loss: 0.008554588\n",
            "\n",
            "Global step: 4980,loss: 0.008240829\n",
            "\n",
            "Global step: 4981,loss: 0.007844095\n",
            "\n",
            "Global step: 4982,loss: 0.008042693\n",
            "\n",
            "Global step: 4983,loss: 0.010092473\n",
            "\n",
            "Global step: 4984,loss: 0.009804418\n",
            "\n",
            "Global step: 4985,loss: 0.0074931975\n",
            "\n",
            "Global step: 4986,loss: 0.0073974454\n",
            "\n",
            "Global step: 4987,loss: 0.008727688\n",
            "\n",
            "Global step: 4988,loss: 0.009705858\n",
            "\n",
            "Global step: 4989,loss: 0.007580616\n",
            "\n",
            "Global step: 4990,loss: 0.007930927\n",
            "\n",
            "Global step: 4991,loss: 0.00814429\n",
            "\n",
            "Global step: 4992,loss: 0.007642053\n",
            "\n",
            "Global step: 4993,loss: 0.008126017\n",
            "\n",
            "Global step: 4994,loss: 0.008127577\n",
            "\n",
            "Global step: 4995,loss: 0.00800198\n",
            "\n",
            "Global step: 4996,loss: 0.008261754\n",
            "\n",
            "Global step: 4997,loss: 0.00876622\n",
            "\n",
            "Global step: 4998,loss: 0.009073582\n",
            "\n",
            "Global step: 4999,loss: 0.00836232\n",
            "\n",
            "Global step: 5000,loss: 0.007937816\n",
            "\n",
            "Global step: 5001,loss: 0.008496007\n",
            "\n",
            "Global step: 5002,loss: 0.008406967\n",
            "\n",
            "Global step: 5003,loss: 0.008743452\n",
            "\n",
            "Global step: 5004,loss: 0.008231983\n",
            "\n",
            "Global step: 5005,loss: 0.008416929\n",
            "\n",
            "Global step: 5006,loss: 0.00820381\n",
            "\n",
            "Global step: 5007,loss: 0.008758625\n",
            "\n",
            "Global step: 5008,loss: 0.008461758\n",
            "\n",
            "Global step: 5009,loss: 0.008011552\n",
            "\n",
            "Global step: 5010,loss: 0.008578042\n",
            "\n",
            "Global step: 5011,loss: 0.0088130515\n",
            "\n",
            "Global step: 5012,loss: 0.00865299\n",
            "\n",
            "Global step: 5013,loss: 0.008556836\n",
            "\n",
            "Global step: 5014,loss: 0.007395167\n",
            "\n",
            "Global step: 5015,loss: 0.008248796\n",
            "\n",
            "Global step: 5016,loss: 0.008690654\n",
            "\n",
            "Global step: 5017,loss: 0.008813469\n",
            "\n",
            "Global step: 5018,loss: 0.0075220065\n",
            "\n",
            "Global step: 5019,loss: 0.008737465\n",
            "\n",
            "Global step: 5020,loss: 0.008128175\n",
            "\n",
            "Global step: 5021,loss: 0.0078767575\n",
            "\n",
            "Global step: 5022,loss: 0.0075007305\n",
            "\n",
            "Global step: 5023,loss: 0.0077480148\n",
            "\n",
            "Global step: 5024,loss: 0.008048936\n",
            "\n",
            "Global step: 5025,loss: 0.010044552\n",
            "\n",
            "Global step: 5026,loss: 0.007921811\n",
            "\n",
            "Global step: 5027,loss: 0.007964127\n",
            "\n",
            "Global step: 5028,loss: 0.0072299717\n",
            "\n",
            "Global step: 5029,loss: 0.0088158585\n",
            "\n",
            "Global step: 5030,loss: 0.007860694\n",
            "\n",
            "Global step: 5031,loss: 0.008207019\n",
            "\n",
            "Global step: 5032,loss: 0.008049547\n",
            "\n",
            "Global step: 5033,loss: 0.007349504\n",
            "\n",
            "Global step: 5034,loss: 0.0081478795\n",
            "\n",
            "Global step: 5035,loss: 0.0075438702\n",
            "\n",
            "Global step: 5036,loss: 0.0076465253\n",
            "\n",
            "Global step: 5037,loss: 0.0107308775\n",
            "\n",
            "Global step: 5038,loss: 0.007484489\n",
            "\n",
            "Global step: 5039,loss: 0.008204527\n",
            "\n",
            "Global step: 5040,loss: 0.009346878\n",
            "\n",
            "Global step: 5041,loss: 0.008440845\n",
            "\n",
            "Global step: 5042,loss: 0.008295314\n",
            "\n",
            "Global step: 5043,loss: 0.008125786\n",
            "\n",
            "Global step: 5044,loss: 0.007946534\n",
            "\n",
            "Global step: 5045,loss: 0.010113528\n",
            "\n",
            "Global step: 5046,loss: 0.008589579\n",
            "\n",
            "Global step: 5047,loss: 0.007825078\n",
            "\n",
            "Global step: 5048,loss: 0.012115861\n",
            "\n",
            "Global step: 5049,loss: 0.009147594\n",
            "\n",
            "Global step: 5050,loss: 0.0081628505\n",
            "\n",
            "Global step: 5051,loss: 0.007934199\n",
            "\n",
            "Global step: 5052,loss: 0.008469284\n",
            "\n",
            "Global step: 5053,loss: 0.008306751\n",
            "\n",
            "Global step: 5054,loss: 0.012626066\n",
            "\n",
            "Global step: 5055,loss: 0.0086439885\n",
            "\n",
            "Global step: 5056,loss: 0.008553574\n",
            "\n",
            "Global step: 5057,loss: 0.0077127153\n",
            "\n",
            "Global step: 5058,loss: 0.009965135\n",
            "\n",
            "Global step: 5059,loss: 0.007932112\n",
            "\n",
            "Global step: 5060,loss: 0.008391585\n",
            "\n",
            "Global step: 5061,loss: 0.008074362\n",
            "\n",
            "Global step: 5062,loss: 0.007884558\n",
            "\n",
            "Global step: 5063,loss: 0.00854729\n",
            "\n",
            "Global step: 5064,loss: 0.007971566\n",
            "\n",
            "Global step: 5065,loss: 0.007889148\n",
            "\n",
            "Global step: 5066,loss: 0.008264512\n",
            "\n",
            "Global step: 5067,loss: 0.008975384\n",
            "\n",
            "Global step: 5068,loss: 0.007574169\n",
            "\n",
            "Global step: 5069,loss: 0.008074098\n",
            "\n",
            "Global step: 5070,loss: 0.008102494\n",
            "\n",
            "Global step: 5071,loss: 0.010604242\n",
            "\n",
            "Global step: 5072,loss: 0.007904014\n",
            "\n",
            "Global step: 5073,loss: 0.00798826\n",
            "\n",
            "Global step: 5074,loss: 0.007545909\n",
            "\n",
            "Global step: 5075,loss: 0.007524742\n",
            "\n",
            "Global step: 5076,loss: 0.0079108495\n",
            "\n",
            "Global step: 5077,loss: 0.008384329\n",
            "\n",
            "Global step: 5078,loss: 0.007300234\n",
            "\n",
            "Global step: 5079,loss: 0.008433601\n",
            "\n",
            "Global step: 5080,loss: 0.0073086843\n",
            "\n",
            "Global step: 5081,loss: 0.008194619\n",
            "\n",
            "Global step: 5082,loss: 0.00887741\n",
            "\n",
            "Global step: 5083,loss: 0.0083923\n",
            "\n",
            "Global step: 5084,loss: 0.00884342\n",
            "\n",
            "Global step: 5085,loss: 0.00838516\n",
            "\n",
            "Global step: 5086,loss: 0.009055776\n",
            "\n",
            "Global step: 5087,loss: 0.008069845\n",
            "\n",
            "Global step: 5088,loss: 0.00842876\n",
            "\n",
            "Global step: 5089,loss: 0.007842119\n",
            "\n",
            "Global step: 5090,loss: 0.007957759\n",
            "\n",
            "Global step: 5091,loss: 0.010573365\n",
            "\n",
            "Global step: 5092,loss: 0.00782732\n",
            "\n",
            "Global step: 5093,loss: 0.007725494\n",
            "\n",
            "Global step: 5094,loss: 0.007868188\n",
            "\n",
            "Global step: 5095,loss: 0.007865242\n",
            "\n",
            "Global step: 5096,loss: 0.008020065\n",
            "\n",
            "Global step: 5097,loss: 0.007944664\n",
            "\n",
            "Global step: 5098,loss: 0.007590214\n",
            "\n",
            "Global step: 5099,loss: 0.011591778\n",
            "\n",
            "Global step: 5100,loss: 0.014128554\n",
            "\n",
            "Global step: 5101,loss: 0.008188087\n",
            "\n",
            "Global step: 5102,loss: 0.009120126\n",
            "\n",
            "Global step: 5103,loss: 0.0078689465\n",
            "\n",
            "Global step: 5104,loss: 0.007999285\n",
            "\n",
            "Global step: 5105,loss: 0.008502681\n",
            "\n",
            "Global step: 5106,loss: 0.00818735\n",
            "\n",
            "Global step: 5107,loss: 0.008260293\n",
            "\n",
            "Global step: 5108,loss: 0.008277607\n",
            "\n",
            "Global step: 5109,loss: 0.007894229\n",
            "\n",
            "Global step: 5110,loss: 0.008407222\n",
            "\n",
            "Global step: 5111,loss: 0.009044374\n",
            "\n",
            "Global step: 5112,loss: 0.008074126\n",
            "\n",
            "Global step: 5113,loss: 0.008390138\n",
            "\n",
            "Global step: 5114,loss: 0.007382489\n",
            "\n",
            "Global step: 5115,loss: 0.0089626\n",
            "\n",
            "Global step: 5116,loss: 0.007872656\n",
            "\n",
            "Global step: 5117,loss: 0.008736673\n",
            "\n",
            "Global step: 5118,loss: 0.00834236\n",
            "\n",
            "Global step: 5119,loss: 0.007828712\n",
            "\n",
            "Global step: 5120,loss: 0.0077545005\n",
            "\n",
            "Global step: 5121,loss: 0.008009752\n",
            "\n",
            "Global step: 5122,loss: 0.008977059\n",
            "\n",
            "Global step: 5123,loss: 0.008521485\n",
            "\n",
            "Global step: 5124,loss: 0.007832724\n",
            "\n",
            "Global step: 5125,loss: 0.008105025\n",
            "\n",
            "Global step: 5126,loss: 0.007853089\n",
            "\n",
            "Global step: 5127,loss: 0.008737181\n",
            "\n",
            "Global step: 5128,loss: 0.008034166\n",
            "\n",
            "Global step: 5129,loss: 0.0072837113\n",
            "\n",
            "Global step: 5130,loss: 0.0082332445\n",
            "\n",
            "Global step: 5131,loss: 0.00791252\n",
            "\n",
            "Global step: 5132,loss: 0.008974337\n",
            "\n",
            "Global step: 5133,loss: 0.007347776\n",
            "\n",
            "Global step: 5134,loss: 0.009328654\n",
            "\n",
            "Global step: 5135,loss: 0.008139528\n",
            "\n",
            "Global step: 5136,loss: 0.008161168\n",
            "\n",
            "Global step: 5137,loss: 0.008014854\n",
            "\n",
            "Global step: 5138,loss: 0.0084841885\n",
            "\n",
            "Global step: 5139,loss: 0.008489401\n",
            "\n",
            "Global step: 5140,loss: 0.007977619\n",
            "\n",
            "Global step: 5141,loss: 0.0074092103\n",
            "\n",
            "Global step: 5142,loss: 0.0076568783\n",
            "\n",
            "Global step: 5143,loss: 0.0077280975\n",
            "\n",
            "Global step: 5144,loss: 0.007780564\n",
            "\n",
            "Global step: 5145,loss: 0.0076445057\n",
            "\n",
            "Global step: 5146,loss: 0.007932069\n",
            "\n",
            "Global step: 5147,loss: 0.00820413\n",
            "\n",
            "Global step: 5148,loss: 0.00792925\n",
            "\n",
            "Global step: 5149,loss: 0.007472329\n",
            "\n",
            "Global step: 5150,loss: 0.007926347\n",
            "\n",
            "Global step: 5151,loss: 0.007477677\n",
            "\n",
            "Global step: 5152,loss: 0.008438816\n",
            "\n",
            "Global step: 5153,loss: 0.008258541\n",
            "\n",
            "Global step: 5154,loss: 0.009390003\n",
            "\n",
            "Global step: 5155,loss: 0.007120306\n",
            "\n",
            "Global step: 5156,loss: 0.008121479\n",
            "\n",
            "Global step: 5157,loss: 0.007699269\n",
            "\n",
            "Global step: 5158,loss: 0.007392673\n",
            "\n",
            "Global step: 5159,loss: 0.008030768\n",
            "\n",
            "Global step: 5160,loss: 0.007594551\n",
            "\n",
            "Global step: 5161,loss: 0.007637995\n",
            "\n",
            "Global step: 5162,loss: 0.008075606\n",
            "\n",
            "Global step: 5163,loss: 0.0074601774\n",
            "\n",
            "Global step: 5164,loss: 0.008313615\n",
            "\n",
            "Global step: 5165,loss: 0.008841114\n",
            "\n",
            "Global step: 5166,loss: 0.008423067\n",
            "\n",
            "Global step: 5167,loss: 0.008011324\n",
            "\n",
            "Global step: 5168,loss: 0.0077892523\n",
            "\n",
            "Global step: 5169,loss: 0.0081720445\n",
            "\n",
            "Global step: 5170,loss: 0.007776209\n",
            "\n",
            "Global step: 5171,loss: 0.008414581\n",
            "\n",
            "Global step: 5172,loss: 0.008539121\n",
            "\n",
            "Global step: 5173,loss: 0.008909768\n",
            "\n",
            "Global step: 5174,loss: 0.0075792572\n",
            "\n",
            "Global step: 5175,loss: 0.0073586795\n",
            "\n",
            "Global step: 5176,loss: 0.007305922\n",
            "\n",
            "Global step: 5177,loss: 0.007830973\n",
            "\n",
            "Global step: 5178,loss: 0.012116785\n",
            "\n",
            "Global step: 5179,loss: 0.007631067\n",
            "\n",
            "Global step: 5180,loss: 0.0075659286\n",
            "\n",
            "Global step: 5181,loss: 0.0076708645\n",
            "\n",
            "Global step: 5182,loss: 0.007668382\n",
            "\n",
            "Global step: 5183,loss: 0.009917179\n",
            "\n",
            "Global step: 5184,loss: 0.008213257\n",
            "\n",
            "Global step: 5185,loss: 0.007996358\n",
            "\n",
            "Global step: 5186,loss: 0.008349283\n",
            "\n",
            "Global step: 5187,loss: 0.008106511\n",
            "\n",
            "Global step: 5188,loss: 0.008132349\n",
            "\n",
            "Global step: 5189,loss: 0.00853957\n",
            "\n",
            "Global step: 5190,loss: 0.0077768057\n",
            "\n",
            "Global step: 5191,loss: 0.008294536\n",
            "\n",
            "Global step: 5192,loss: 0.007714087\n",
            "\n",
            "Global step: 5193,loss: 0.008121204\n",
            "\n",
            "Global step: 5194,loss: 0.008634025\n",
            "\n",
            "Global step: 5195,loss: 0.007490436\n",
            "\n",
            "Global step: 5196,loss: 0.007692446\n",
            "\n",
            "Global step: 5197,loss: 0.009678793\n",
            "\n",
            "Global step: 5198,loss: 0.0072814627\n",
            "\n",
            "Global step: 5199,loss: 0.007906973\n",
            "\n",
            "Global step: 5200,loss: 0.007727069\n",
            "\n",
            "Global step: 5201,loss: 0.00746449\n",
            "\n",
            "Global step: 5202,loss: 0.007558072\n",
            "\n",
            "Global step: 5203,loss: 0.008798622\n",
            "\n",
            "Global step: 5204,loss: 0.008022501\n",
            "\n",
            "Global step: 5205,loss: 0.0077805817\n",
            "\n",
            "Global step: 5206,loss: 0.008699832\n",
            "\n",
            "Global step: 5207,loss: 0.008137947\n",
            "\n",
            "Global step: 5208,loss: 0.008396561\n",
            "\n",
            "Global step: 5209,loss: 0.007471511\n",
            "\n",
            "Global step: 5210,loss: 0.008397808\n",
            "\n",
            "Global step: 5211,loss: 0.009764025\n",
            "\n",
            "Global step: 5212,loss: 0.007554514\n",
            "\n",
            "Global step: 5213,loss: 0.008236658\n",
            "\n",
            "Global step: 5214,loss: 0.007816009\n",
            "\n",
            "Global step: 5215,loss: 0.0076837908\n",
            "\n",
            "Global step: 5216,loss: 0.011203638\n",
            "\n",
            "Global step: 5217,loss: 0.008461691\n",
            "\n",
            "Global step: 5218,loss: 0.008255936\n",
            "\n",
            "Global step: 5219,loss: 0.008432552\n",
            "\n",
            "Global step: 5220,loss: 0.007828737\n",
            "\n",
            "Global step: 5221,loss: 0.008046909\n",
            "\n",
            "Global step: 5222,loss: 0.0076077287\n",
            "\n",
            "Global step: 5223,loss: 0.007770355\n",
            "\n",
            "Global step: 5224,loss: 0.008215317\n",
            "\n",
            "Global step: 5225,loss: 0.0076305545\n",
            "\n",
            "Global step: 5226,loss: 0.007924256\n",
            "\n",
            "Global step: 5227,loss: 0.007857526\n",
            "\n",
            "Global step: 5228,loss: 0.009206096\n",
            "\n",
            "Global step: 5229,loss: 0.0077691674\n",
            "\n",
            "Global step: 5230,loss: 0.008123291\n",
            "\n",
            "Global step: 5231,loss: 0.009073747\n",
            "\n",
            "Global step: 5232,loss: 0.00783515\n",
            "\n",
            "Global step: 5233,loss: 0.007788693\n",
            "\n",
            "Global step: 5234,loss: 0.00783124\n",
            "\n",
            "Global step: 5235,loss: 0.007751498\n",
            "\n",
            "Global step: 5236,loss: 0.008687606\n",
            "\n",
            "Global step: 5237,loss: 0.008732677\n",
            "\n",
            "Global step: 5238,loss: 0.011744078\n",
            "\n",
            "Global step: 5239,loss: 0.007820278\n",
            "\n",
            "Global step: 5240,loss: 0.007820468\n",
            "\n",
            "Global step: 5241,loss: 0.009722698\n",
            "\n",
            "Global step: 5242,loss: 0.0075549753\n",
            "\n",
            "Global step: 5243,loss: 0.007853822\n",
            "\n",
            "Global step: 5244,loss: 0.008410804\n",
            "\n",
            "Global step: 5245,loss: 0.008216774\n",
            "\n",
            "Global step: 5246,loss: 0.011445291\n",
            "\n",
            "Global step: 5247,loss: 0.008125958\n",
            "\n",
            "Global step: 5248,loss: 0.00783629\n",
            "\n",
            "Global step: 5249,loss: 0.0076148873\n",
            "\n",
            "Global step: 5250,loss: 0.009528221\n",
            "\n",
            "Global step: 5251,loss: 0.008250117\n",
            "\n",
            "Global step: 5252,loss: 0.007956611\n",
            "\n",
            "Global step: 5253,loss: 0.008232521\n",
            "\n",
            "Global step: 5254,loss: 0.00906332\n",
            "\n",
            "Global step: 5255,loss: 0.008325286\n",
            "\n",
            "Global step: 5256,loss: 0.0084470445\n",
            "\n",
            "Global step: 5257,loss: 0.007959957\n",
            "\n",
            "Global step: 5258,loss: 0.008497031\n",
            "\n",
            "Global step: 5259,loss: 0.008357614\n",
            "\n",
            "Global step: 5260,loss: 0.0072351266\n",
            "\n",
            "Global step: 5261,loss: 0.008137188\n",
            "\n",
            "Global step: 5262,loss: 0.0077088187\n",
            "\n",
            "Global step: 5263,loss: 0.007975029\n",
            "\n",
            "Global step: 5264,loss: 0.007737636\n",
            "\n",
            "Global step: 5265,loss: 0.0069144163\n",
            "\n",
            "Global step: 5266,loss: 0.0076384693\n",
            "\n",
            "Global step: 5267,loss: 0.0078886505\n",
            "\n",
            "Global step: 5268,loss: 0.007819472\n",
            "\n",
            "Global step: 5269,loss: 0.0076638446\n",
            "\n",
            "Global step: 5270,loss: 0.0074840346\n",
            "\n",
            "Global step: 5271,loss: 0.009283804\n",
            "\n",
            "Global step: 5272,loss: 0.0081752045\n",
            "\n",
            "Global step: 5273,loss: 0.0074741743\n",
            "\n",
            "Global step: 5274,loss: 0.008088433\n",
            "\n",
            "Global step: 5275,loss: 0.0080801975\n",
            "\n",
            "Global step: 5276,loss: 0.008387797\n",
            "\n",
            "Global step: 5277,loss: 0.008210365\n",
            "\n",
            "Global step: 5278,loss: 0.007714992\n",
            "\n",
            "Global step: 5279,loss: 0.008164252\n",
            "\n",
            "Global step: 5280,loss: 0.0076784915\n",
            "\n",
            "Global step: 5281,loss: 0.0069898027\n",
            "\n",
            "Global step: 5282,loss: 0.008298721\n",
            "\n",
            "Global step: 5283,loss: 0.007683009\n",
            "\n",
            "Global step: 5284,loss: 0.008657855\n",
            "\n",
            "Global step: 5285,loss: 0.0075229825\n",
            "\n",
            "Global step: 5286,loss: 0.0077753593\n",
            "\n",
            "Global step: 5287,loss: 0.010023776\n",
            "\n",
            "Global step: 5288,loss: 0.007873222\n",
            "\n",
            "Global step: 5289,loss: 0.008411026\n",
            "\n",
            "Global step: 5290,loss: 0.0077402033\n",
            "\n",
            "Global step: 5291,loss: 0.007550875\n",
            "\n",
            "Global step: 5292,loss: 0.008458865\n",
            "\n",
            "Global step: 5293,loss: 0.008326581\n",
            "\n",
            "Global step: 5294,loss: 0.007968131\n",
            "\n",
            "Global step: 5295,loss: 0.0076057455\n",
            "\n",
            "Global step: 5296,loss: 0.0167442\n",
            "\n",
            "Global step: 5297,loss: 0.008053567\n",
            "\n",
            "Global step: 5298,loss: 0.007791294\n",
            "\n",
            "Global step: 5299,loss: 0.009038017\n",
            "\n",
            "Global step: 5300,loss: 0.008702053\n",
            "\n",
            "Global step: 5301,loss: 0.00823547\n",
            "\n",
            "Global step: 5302,loss: 0.0074442085\n",
            "\n",
            "Global step: 5303,loss: 0.008528048\n",
            "\n",
            "Global step: 5304,loss: 0.0080132745\n",
            "\n",
            "Global step: 5305,loss: 0.008561221\n",
            "\n",
            "Global step: 5306,loss: 0.007325288\n",
            "\n",
            "Global step: 5307,loss: 0.007924039\n",
            "\n",
            "Global step: 5308,loss: 0.0081039835\n",
            "\n",
            "Global step: 5309,loss: 0.008635046\n",
            "\n",
            "Global step: 5310,loss: 0.008013159\n",
            "\n",
            "Global step: 5311,loss: 0.0070090657\n",
            "\n",
            "Global step: 5312,loss: 0.008035137\n",
            "\n",
            "Global step: 5313,loss: 0.008323306\n",
            "\n",
            "Global step: 5314,loss: 0.008234282\n",
            "\n",
            "Global step: 5315,loss: 0.008866567\n",
            "\n",
            "Global step: 5316,loss: 0.007982669\n",
            "\n",
            "Global step: 5317,loss: 0.0076305848\n",
            "\n",
            "Global step: 5318,loss: 0.007733818\n",
            "\n",
            "Global step: 5319,loss: 0.007899987\n",
            "\n",
            "Global step: 5320,loss: 0.0077239936\n",
            "\n",
            "Global step: 5321,loss: 0.012880061\n",
            "\n",
            "Global step: 5322,loss: 0.007915229\n",
            "\n",
            "Global step: 5323,loss: 0.007998381\n",
            "\n",
            "Global step: 5324,loss: 0.0076696076\n",
            "\n",
            "Global step: 5325,loss: 0.007680329\n",
            "\n",
            "Global step: 5326,loss: 0.008181552\n",
            "\n",
            "Global step: 5327,loss: 0.007740096\n",
            "\n",
            "Global step: 5328,loss: 0.00774877\n",
            "\n",
            "Global step: 5329,loss: 0.008572429\n",
            "\n",
            "Global step: 5330,loss: 0.007493534\n",
            "\n",
            "Global step: 5331,loss: 0.008087032\n",
            "\n",
            "Global step: 5332,loss: 0.007727653\n",
            "\n",
            "Global step: 5333,loss: 0.012705378\n",
            "\n",
            "Global step: 5334,loss: 0.007693949\n",
            "\n",
            "Global step: 5335,loss: 0.007532913\n",
            "\n",
            "Global step: 5336,loss: 0.007887842\n",
            "\n",
            "Global step: 5337,loss: 0.0073645404\n",
            "\n",
            "Global step: 5338,loss: 0.007987657\n",
            "\n",
            "Global step: 5339,loss: 0.0085069835\n",
            "\n",
            "Global step: 5340,loss: 0.0073714918\n",
            "\n",
            "Global step: 5341,loss: 0.0076696957\n",
            "\n",
            "Global step: 5342,loss: 0.0076780664\n",
            "\n",
            "Global step: 5343,loss: 0.008444534\n",
            "\n",
            "Global step: 5344,loss: 0.0083413925\n",
            "\n",
            "Global step: 5345,loss: 0.0075499173\n",
            "\n",
            "Global step: 5346,loss: 0.008460233\n",
            "\n",
            "Global step: 5347,loss: 0.0075073037\n",
            "\n",
            "Global step: 5348,loss: 0.007822618\n",
            "\n",
            "Global step: 5349,loss: 0.008263505\n",
            "\n",
            "Global step: 5350,loss: 0.0074337423\n",
            "\n",
            "Global step: 5351,loss: 0.008673711\n",
            "\n",
            "Global step: 5352,loss: 0.00930304\n",
            "\n",
            "Global step: 5353,loss: 0.008685738\n",
            "\n",
            "Global step: 5354,loss: 0.007634119\n",
            "\n",
            "Global step: 5355,loss: 0.0071995463\n",
            "\n",
            "Global step: 5356,loss: 0.007715398\n",
            "\n",
            "Global step: 5357,loss: 0.0081972815\n",
            "\n",
            "Global step: 5358,loss: 0.007820564\n",
            "\n",
            "Global step: 5359,loss: 0.009008107\n",
            "\n",
            "Global step: 5360,loss: 0.008166256\n",
            "\n",
            "Global step: 5361,loss: 0.0079000145\n",
            "\n",
            "Global step: 5362,loss: 0.0075745434\n",
            "\n",
            "Global step: 5363,loss: 0.008693794\n",
            "\n",
            "Global step: 5364,loss: 0.008406668\n",
            "\n",
            "Global step: 5365,loss: 0.008396145\n",
            "\n",
            "Global step: 5366,loss: 0.008593263\n",
            "\n",
            "Global step: 5367,loss: 0.007841147\n",
            "\n",
            "Global step: 5368,loss: 0.008106192\n",
            "\n",
            "Global step: 5369,loss: 0.0074289804\n",
            "\n",
            "Global step: 5370,loss: 0.00792751\n",
            "\n",
            "Global step: 5371,loss: 0.007729386\n",
            "\n",
            "Global step: 5372,loss: 0.007875146\n",
            "\n",
            "Global step: 5373,loss: 0.0075874235\n",
            "\n",
            "Global step: 5374,loss: 0.007753848\n",
            "\n",
            "Global step: 5375,loss: 0.0077776928\n",
            "\n",
            "Global step: 5376,loss: 0.0083210915\n",
            "\n",
            "Global step: 5377,loss: 0.007975669\n",
            "\n",
            "Global step: 5378,loss: 0.008105925\n",
            "\n",
            "Global step: 5379,loss: 0.007531952\n",
            "\n",
            "Global step: 5380,loss: 0.008061456\n",
            "\n",
            "Global step: 5381,loss: 0.0074687405\n",
            "\n",
            "Global step: 5382,loss: 0.00775996\n",
            "\n",
            "Global step: 5383,loss: 0.008285722\n",
            "\n",
            "Global step: 5384,loss: 0.008417471\n",
            "\n",
            "Global step: 5385,loss: 0.008203504\n",
            "\n",
            "Global step: 5386,loss: 0.0072448235\n",
            "\n",
            "Global step: 5387,loss: 0.007117432\n",
            "\n",
            "Global step: 5388,loss: 0.008336917\n",
            "\n",
            "Global step: 5389,loss: 0.007289953\n",
            "\n",
            "Global step: 5390,loss: 0.0072844727\n",
            "\n",
            "Global step: 5391,loss: 0.008073336\n",
            "\n",
            "Global step: 5392,loss: 0.007807157\n",
            "\n",
            "Global step: 5393,loss: 0.007015695\n",
            "\n",
            "Global step: 5394,loss: 0.00791801\n",
            "\n",
            "Global step: 5395,loss: 0.008003373\n",
            "\n",
            "Global step: 5396,loss: 0.0075404523\n",
            "\n",
            "Global step: 5397,loss: 0.008181668\n",
            "\n",
            "Global step: 5398,loss: 0.008083508\n",
            "\n",
            "Global step: 5399,loss: 0.008394943\n",
            "\n",
            "Global step: 5400,loss: 0.008583596\n",
            "\n",
            "Global step: 5401,loss: 0.0075706923\n",
            "\n",
            "Global step: 5402,loss: 0.007842346\n",
            "\n",
            "Global step: 5403,loss: 0.00840713\n",
            "\n",
            "Global step: 5404,loss: 0.00696438\n",
            "\n",
            "Global step: 5405,loss: 0.0072868066\n",
            "\n",
            "Global step: 5406,loss: 0.008107509\n",
            "\n",
            "Global step: 5407,loss: 0.008207727\n",
            "\n",
            "Global step: 5408,loss: 0.008064125\n",
            "\n",
            "Global step: 5409,loss: 0.00767592\n",
            "\n",
            "Global step: 5410,loss: 0.008077231\n",
            "\n",
            "Global step: 5411,loss: 0.008237271\n",
            "\n",
            "Global step: 5412,loss: 0.008336991\n",
            "\n",
            "Global step: 5413,loss: 0.008279771\n",
            "\n",
            "Global step: 5414,loss: 0.008030418\n",
            "\n",
            "Global step: 5415,loss: 0.007008757\n",
            "\n",
            "Global step: 5416,loss: 0.014377921\n",
            "\n",
            "Global step: 5417,loss: 0.007484323\n",
            "\n",
            "Global step: 5418,loss: 0.007592008\n",
            "\n",
            "Global step: 5419,loss: 0.0075083883\n",
            "\n",
            "Global step: 5420,loss: 0.008237774\n",
            "\n",
            "Global step: 5421,loss: 0.007936842\n",
            "\n",
            "Global step: 5422,loss: 0.008054429\n",
            "\n",
            "Global step: 5423,loss: 0.007619555\n",
            "\n",
            "Global step: 5424,loss: 0.008263455\n",
            "\n",
            "Global step: 5425,loss: 0.007956067\n",
            "\n",
            "Global step: 5426,loss: 0.0074251117\n",
            "\n",
            "Global step: 5427,loss: 0.007872416\n",
            "\n",
            "Global step: 5428,loss: 0.008300626\n",
            "\n",
            "Global step: 5429,loss: 0.0076254574\n",
            "\n",
            "Global step: 5430,loss: 0.0078682965\n",
            "\n",
            "Global step: 5431,loss: 0.007408085\n",
            "\n",
            "Global step: 5432,loss: 0.0080623375\n",
            "\n",
            "Global step: 5433,loss: 0.007694855\n",
            "\n",
            "Global step: 5434,loss: 0.008337245\n",
            "\n",
            "Global step: 5435,loss: 0.00789498\n",
            "\n",
            "Global step: 5436,loss: 0.007928869\n",
            "\n",
            "Global step: 5437,loss: 0.007915065\n",
            "\n",
            "Global step: 5438,loss: 0.007489025\n",
            "\n",
            "Global step: 5439,loss: 0.008472515\n",
            "\n",
            "Global step: 5440,loss: 0.0074382806\n",
            "\n",
            "Global step: 5441,loss: 0.00766103\n",
            "\n",
            "Global step: 5442,loss: 0.007417648\n",
            "\n",
            "Global step: 5443,loss: 0.00758504\n",
            "\n",
            "Global step: 5444,loss: 0.007833954\n",
            "\n",
            "Global step: 5445,loss: 0.008029955\n",
            "\n",
            "Global step: 5446,loss: 0.007439476\n",
            "\n",
            "Global step: 5447,loss: 0.0076747676\n",
            "\n",
            "Global step: 5448,loss: 0.0073862993\n",
            "\n",
            "Global step: 5449,loss: 0.007432729\n",
            "\n",
            "Global step: 5450,loss: 0.007927161\n",
            "\n",
            "Global step: 5451,loss: 0.008817443\n",
            "\n",
            "Global step: 5452,loss: 0.008612675\n",
            "\n",
            "Global step: 5453,loss: 0.008483791\n",
            "\n",
            "Global step: 5454,loss: 0.0073121274\n",
            "\n",
            "Global step: 5455,loss: 0.007058279\n",
            "\n",
            "Global step: 5456,loss: 0.0076205926\n",
            "\n",
            "Global step: 5457,loss: 0.007666106\n",
            "\n",
            "Global step: 5458,loss: 0.008285788\n",
            "\n",
            "Global step: 5459,loss: 0.008792398\n",
            "\n",
            "Global step: 5460,loss: 0.00798587\n",
            "\n",
            "Global step: 5461,loss: 0.008457837\n",
            "\n",
            "Global step: 5462,loss: 0.007489752\n",
            "\n",
            "Global step: 5463,loss: 0.009469786\n",
            "\n",
            "Global step: 5464,loss: 0.007836796\n",
            "\n",
            "Global step: 5465,loss: 0.007912093\n",
            "\n",
            "Global step: 5466,loss: 0.0073569985\n",
            "\n",
            "Global step: 5467,loss: 0.007611213\n",
            "\n",
            "Global step: 5468,loss: 0.008212784\n",
            "\n",
            "Global step: 5469,loss: 0.007848689\n",
            "\n",
            "Global step: 5470,loss: 0.0077867997\n",
            "\n",
            "Global step: 5471,loss: 0.008744249\n",
            "\n",
            "Global step: 5472,loss: 0.008887678\n",
            "\n",
            "Global step: 5473,loss: 0.007583781\n",
            "\n",
            "Global step: 5474,loss: 0.008530398\n",
            "\n",
            "Global step: 5475,loss: 0.0075047165\n",
            "\n",
            "Global step: 5476,loss: 0.007451786\n",
            "\n",
            "Global step: 5477,loss: 0.0074665323\n",
            "\n",
            "Global step: 5478,loss: 0.00808968\n",
            "\n",
            "Global step: 5479,loss: 0.009303283\n",
            "\n",
            "Global step: 5480,loss: 0.0077973143\n",
            "\n",
            "Global step: 5481,loss: 0.007412376\n",
            "\n",
            "Global step: 5482,loss: 0.008541178\n",
            "\n",
            "Global step: 5483,loss: 0.008717239\n",
            "\n",
            "Global step: 5484,loss: 0.007819661\n",
            "\n",
            "Global step: 5485,loss: 0.007343241\n",
            "\n",
            "Global step: 5486,loss: 0.007395947\n",
            "\n",
            "Global step: 5487,loss: 0.02363877\n",
            "\n",
            "Global step: 5488,loss: 0.008449678\n",
            "\n",
            "Global step: 5489,loss: 0.0089049665\n",
            "\n",
            "Global step: 5490,loss: 0.008129851\n",
            "\n",
            "Global step: 5491,loss: 0.008554325\n",
            "\n",
            "Global step: 5492,loss: 0.0076513067\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.81552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:56:35.242688 139837015512832 supervisor.py:1099] global_step/sec: 6.81552\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 5493.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:56:35.253901 139837023905536 supervisor.py:1050] Recording summary at step 5493.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5493,loss: 0.007536005\n",
            "\n",
            "Global step: 5494,loss: 0.007744264\n",
            "\n",
            "Global step: 5495,loss: 0.008912482\n",
            "\n",
            "Global step: 5496,loss: 0.009081935\n",
            "\n",
            "Global step: 5497,loss: 0.008124819\n",
            "\n",
            "Global step: 5498,loss: 0.0076278285\n",
            "\n",
            "Global step: 5499,loss: 0.0073358677\n",
            "\n",
            "Global step: 5500,loss: 0.007303181\n",
            "\n",
            "Global step: 5501,loss: 0.007905739\n",
            "\n",
            "Global step: 5502,loss: 0.007739683\n",
            "\n",
            "Global step: 5503,loss: 0.008359406\n",
            "\n",
            "Global step: 5504,loss: 0.009122092\n",
            "\n",
            "Global step: 5505,loss: 0.008573988\n",
            "\n",
            "Global step: 5506,loss: 0.00863094\n",
            "\n",
            "Global step: 5507,loss: 0.008043583\n",
            "\n",
            "Global step: 5508,loss: 0.008665459\n",
            "\n",
            "Global step: 5509,loss: 0.00783326\n",
            "\n",
            "Global step: 5510,loss: 0.008410408\n",
            "\n",
            "Global step: 5511,loss: 0.008354014\n",
            "\n",
            "Global step: 5512,loss: 0.009009885\n",
            "\n",
            "Global step: 5513,loss: 0.0074739656\n",
            "\n",
            "Global step: 5514,loss: 0.008041605\n",
            "\n",
            "Global step: 5515,loss: 0.0075172954\n",
            "\n",
            "Global step: 5516,loss: 0.008043339\n",
            "\n",
            "Global step: 5517,loss: 0.01705246\n",
            "\n",
            "Global step: 5518,loss: 0.008411419\n",
            "\n",
            "Global step: 5519,loss: 0.010390943\n",
            "\n",
            "Global step: 5520,loss: 0.008639202\n",
            "\n",
            "Global step: 5521,loss: 0.008281437\n",
            "\n",
            "Global step: 5522,loss: 0.008396739\n",
            "\n",
            "Global step: 5523,loss: 0.008971632\n",
            "\n",
            "Global step: 5524,loss: 0.009515422\n",
            "\n",
            "Global step: 5525,loss: 0.008684468\n",
            "\n",
            "Global step: 5526,loss: 0.007870734\n",
            "\n",
            "Global step: 5527,loss: 0.008860014\n",
            "\n",
            "Global step: 5528,loss: 0.008354504\n",
            "\n",
            "Global step: 5529,loss: 0.009164012\n",
            "\n",
            "Global step: 5530,loss: 0.008783769\n",
            "\n",
            "Global step: 5531,loss: 0.011732176\n",
            "\n",
            "Global step: 5532,loss: 0.0073444108\n",
            "\n",
            "Global step: 5533,loss: 0.0072277687\n",
            "\n",
            "Global step: 5534,loss: 0.007921988\n",
            "\n",
            "Global step: 5535,loss: 0.007668866\n",
            "\n",
            "Global step: 5536,loss: 0.006878889\n",
            "\n",
            "Global step: 5537,loss: 0.007874931\n",
            "\n",
            "Global step: 5538,loss: 0.008281399\n",
            "\n",
            "Global step: 5539,loss: 0.008279849\n",
            "\n",
            "Global step: 5540,loss: 0.008657395\n",
            "\n",
            "Global step: 5541,loss: 0.016466519\n",
            "\n",
            "Global step: 5542,loss: 0.008585156\n",
            "\n",
            "Global step: 5543,loss: 0.0073429164\n",
            "\n",
            "Global step: 5544,loss: 0.011341706\n",
            "\n",
            "Global step: 5545,loss: 0.008331537\n",
            "\n",
            "Global step: 5546,loss: 0.0141031705\n",
            "\n",
            "Global step: 5547,loss: 0.011630152\n",
            "\n",
            "Global step: 5548,loss: 0.0075667873\n",
            "\n",
            "Global step: 5549,loss: 0.008241158\n",
            "\n",
            "Global step: 5550,loss: 0.008219875\n",
            "\n",
            "Global step: 5551,loss: 0.009419017\n",
            "\n",
            "Global step: 5552,loss: 0.009645947\n",
            "\n",
            "Global step: 5553,loss: 0.008804111\n",
            "\n",
            "Global step: 5554,loss: 0.009077345\n",
            "\n",
            "Global step: 5555,loss: 0.007592642\n",
            "\n",
            "Global step: 5556,loss: 0.008271225\n",
            "\n",
            "Global step: 5557,loss: 0.013683493\n",
            "\n",
            "Global step: 5558,loss: 0.008605769\n",
            "\n",
            "Global step: 5559,loss: 0.008653039\n",
            "\n",
            "Global step: 5560,loss: 0.008716348\n",
            "\n",
            "Global step: 5561,loss: 0.008848511\n",
            "\n",
            "Global step: 5562,loss: 0.008375437\n",
            "\n",
            "Global step: 5563,loss: 0.010465927\n",
            "\n",
            "Global step: 5564,loss: 0.01046752\n",
            "\n",
            "Global step: 5565,loss: 0.009684271\n",
            "\n",
            "Global step: 5566,loss: 0.008496622\n",
            "\n",
            "Global step: 5567,loss: 0.008843822\n",
            "\n",
            "Global step: 5568,loss: 0.008735114\n",
            "\n",
            "Global step: 5569,loss: 0.0077664135\n",
            "\n",
            "Global step: 5570,loss: 0.0075616003\n",
            "\n",
            "Global step: 5571,loss: 0.009254269\n",
            "\n",
            "Global step: 5572,loss: 0.009104011\n",
            "\n",
            "Global step: 5573,loss: 0.008277076\n",
            "\n",
            "Global step: 5574,loss: 0.007965086\n",
            "\n",
            "Global step: 5575,loss: 0.0080816075\n",
            "\n",
            "Global step: 5576,loss: 0.016673762\n",
            "\n",
            "Global step: 5577,loss: 0.008412675\n",
            "\n",
            "Global step: 5578,loss: 0.008300947\n",
            "\n",
            "Global step: 5579,loss: 0.008471857\n",
            "\n",
            "Global step: 5580,loss: 0.018669985\n",
            "\n",
            "Global step: 5581,loss: 0.00796996\n",
            "\n",
            "Global step: 5582,loss: 0.007536626\n",
            "\n",
            "Global step: 5583,loss: 0.010109808\n",
            "\n",
            "Global step: 5584,loss: 0.008240084\n",
            "\n",
            "Global step: 5585,loss: 0.008180852\n",
            "\n",
            "Global step: 5586,loss: 0.0085498225\n",
            "\n",
            "Global step: 5587,loss: 0.009210218\n",
            "\n",
            "Global step: 5588,loss: 0.008149017\n",
            "\n",
            "Global step: 5589,loss: 0.0072358805\n",
            "\n",
            "Global step: 5590,loss: 0.012840306\n",
            "\n",
            "Global step: 5591,loss: 0.008955625\n",
            "\n",
            "Global step: 5592,loss: 0.008495603\n",
            "\n",
            "Global step: 5593,loss: 0.008159822\n",
            "\n",
            "Global step: 5594,loss: 0.008344933\n",
            "\n",
            "Global step: 5595,loss: 0.007939001\n",
            "\n",
            "Global step: 5596,loss: 0.008250212\n",
            "\n",
            "Global step: 5597,loss: 0.007991036\n",
            "\n",
            "Global step: 5598,loss: 0.010732364\n",
            "\n",
            "Global step: 5599,loss: 0.008505169\n",
            "\n",
            "Global step: 5600,loss: 0.0072714966\n",
            "\n",
            "Global step: 5601,loss: 0.008280009\n",
            "\n",
            "Global step: 5602,loss: 0.008246015\n",
            "\n",
            "Global step: 5603,loss: 0.0074165207\n",
            "\n",
            "Global step: 5604,loss: 0.0087511195\n",
            "\n",
            "Global step: 5605,loss: 0.007726165\n",
            "\n",
            "Global step: 5606,loss: 0.007796487\n",
            "\n",
            "Global step: 5607,loss: 0.008428863\n",
            "\n",
            "Global step: 5608,loss: 0.008139753\n",
            "\n",
            "Global step: 5609,loss: 0.008408036\n",
            "\n",
            "Global step: 5610,loss: 0.007945793\n",
            "\n",
            "Global step: 5611,loss: 0.007590904\n",
            "\n",
            "Global step: 5612,loss: 0.009095367\n",
            "\n",
            "Global step: 5613,loss: 0.00812933\n",
            "\n",
            "Global step: 5614,loss: 0.007076521\n",
            "\n",
            "Global step: 5615,loss: 0.008568903\n",
            "\n",
            "Global step: 5616,loss: 0.00782192\n",
            "\n",
            "Global step: 5617,loss: 0.008580439\n",
            "\n",
            "Global step: 5618,loss: 0.0074475496\n",
            "\n",
            "Global step: 5619,loss: 0.009990351\n",
            "\n",
            "Global step: 5620,loss: 0.0074821957\n",
            "\n",
            "Global step: 5621,loss: 0.0075336257\n",
            "\n",
            "Global step: 5622,loss: 0.013623973\n",
            "\n",
            "Global step: 5623,loss: 0.008244302\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 5623,val_loss: 0.010243656591345102\n",
            "\n",
            "Training for epoch 9/15:\n",
            "Global step: 5624,loss: 0.00823084\n",
            "\n",
            "Global step: 5625,loss: 0.008184197\n",
            "\n",
            "Global step: 5626,loss: 0.007660744\n",
            "\n",
            "Global step: 5627,loss: 0.007227306\n",
            "\n",
            "Global step: 5628,loss: 0.0074124243\n",
            "\n",
            "Global step: 5629,loss: 0.00794009\n",
            "\n",
            "Global step: 5630,loss: 0.008390008\n",
            "\n",
            "Global step: 5631,loss: 0.008558225\n",
            "\n",
            "Global step: 5632,loss: 0.0074073994\n",
            "\n",
            "Global step: 5633,loss: 0.0073419698\n",
            "\n",
            "Global step: 5634,loss: 0.0077512427\n",
            "\n",
            "Global step: 5635,loss: 0.008585391\n",
            "\n",
            "Global step: 5636,loss: 0.008050438\n",
            "\n",
            "Global step: 5637,loss: 0.007875985\n",
            "\n",
            "Global step: 5638,loss: 0.0082062\n",
            "\n",
            "Global step: 5639,loss: 0.007669963\n",
            "\n",
            "Global step: 5640,loss: 0.007987967\n",
            "\n",
            "Global step: 5641,loss: 0.008037999\n",
            "\n",
            "Global step: 5642,loss: 0.008490168\n",
            "\n",
            "Global step: 5643,loss: 0.007574733\n",
            "\n",
            "Global step: 5644,loss: 0.008440192\n",
            "\n",
            "Global step: 5645,loss: 0.0073093907\n",
            "\n",
            "Global step: 5646,loss: 0.007929908\n",
            "\n",
            "Global step: 5647,loss: 0.011088587\n",
            "\n",
            "Global step: 5648,loss: 0.008199155\n",
            "\n",
            "Global step: 5649,loss: 0.008037218\n",
            "\n",
            "Global step: 5650,loss: 0.0080553265\n",
            "\n",
            "Global step: 5651,loss: 0.007815743\n",
            "\n",
            "Global step: 5652,loss: 0.0075006317\n",
            "\n",
            "Global step: 5653,loss: 0.008290952\n",
            "\n",
            "Global step: 5654,loss: 0.007536434\n",
            "\n",
            "Global step: 5655,loss: 0.007374543\n",
            "\n",
            "Global step: 5656,loss: 0.0074060564\n",
            "\n",
            "Global step: 5657,loss: 0.007748998\n",
            "\n",
            "Global step: 5658,loss: 0.007849723\n",
            "\n",
            "Global step: 5659,loss: 0.0075908056\n",
            "\n",
            "Global step: 5660,loss: 0.008254308\n",
            "\n",
            "Global step: 5661,loss: 0.017602168\n",
            "\n",
            "Global step: 5662,loss: 0.007958311\n",
            "\n",
            "Global step: 5663,loss: 0.007387519\n",
            "\n",
            "Global step: 5664,loss: 0.007599078\n",
            "\n",
            "Global step: 5665,loss: 0.007768892\n",
            "\n",
            "Global step: 5666,loss: 0.0081376955\n",
            "\n",
            "Global step: 5667,loss: 0.0075790505\n",
            "\n",
            "Global step: 5668,loss: 0.006969759\n",
            "\n",
            "Global step: 5669,loss: 0.0068805744\n",
            "\n",
            "Global step: 5670,loss: 0.0072905053\n",
            "\n",
            "Global step: 5671,loss: 0.008033533\n",
            "\n",
            "Global step: 5672,loss: 0.0078079007\n",
            "\n",
            "Global step: 5673,loss: 0.00819353\n",
            "\n",
            "Global step: 5674,loss: 0.0071913684\n",
            "\n",
            "Global step: 5675,loss: 0.006798216\n",
            "\n",
            "Global step: 5676,loss: 0.007235917\n",
            "\n",
            "Global step: 5677,loss: 0.006821265\n",
            "\n",
            "Global step: 5678,loss: 0.008603997\n",
            "\n",
            "Global step: 5679,loss: 0.007483248\n",
            "\n",
            "Global step: 5680,loss: 0.007987814\n",
            "\n",
            "Global step: 5681,loss: 0.0075973812\n",
            "\n",
            "Global step: 5682,loss: 0.0074540987\n",
            "\n",
            "Global step: 5683,loss: 0.007347851\n",
            "\n",
            "Global step: 5684,loss: 0.008753915\n",
            "\n",
            "Global step: 5685,loss: 0.0077465074\n",
            "\n",
            "Global step: 5686,loss: 0.0071238824\n",
            "\n",
            "Global step: 5687,loss: 0.0072387317\n",
            "\n",
            "Global step: 5688,loss: 0.008134172\n",
            "\n",
            "Global step: 5689,loss: 0.007516552\n",
            "\n",
            "Global step: 5690,loss: 0.008142021\n",
            "\n",
            "Global step: 5691,loss: 0.0071866997\n",
            "\n",
            "Global step: 5692,loss: 0.0075032935\n",
            "\n",
            "Global step: 5693,loss: 0.0076002446\n",
            "\n",
            "Global step: 5694,loss: 0.0083083315\n",
            "\n",
            "Global step: 5695,loss: 0.007525143\n",
            "\n",
            "Global step: 5696,loss: 0.0070755356\n",
            "\n",
            "Global step: 5697,loss: 0.0077713127\n",
            "\n",
            "Global step: 5698,loss: 0.0077443533\n",
            "\n",
            "Global step: 5699,loss: 0.0075339777\n",
            "\n",
            "Global step: 5700,loss: 0.008949389\n",
            "\n",
            "Global step: 5701,loss: 0.008590206\n",
            "\n",
            "Global step: 5702,loss: 0.00741813\n",
            "\n",
            "Global step: 5703,loss: 0.0070797987\n",
            "\n",
            "Global step: 5704,loss: 0.007848094\n",
            "\n",
            "Global step: 5705,loss: 0.008117454\n",
            "\n",
            "Global step: 5706,loss: 0.007634441\n",
            "\n",
            "Global step: 5707,loss: 0.007981914\n",
            "\n",
            "Global step: 5708,loss: 0.006813463\n",
            "\n",
            "Global step: 5709,loss: 0.007626345\n",
            "\n",
            "Global step: 5710,loss: 0.0070644654\n",
            "\n",
            "Global step: 5711,loss: 0.007999485\n",
            "\n",
            "Global step: 5712,loss: 0.007580259\n",
            "\n",
            "Global step: 5713,loss: 0.0082058655\n",
            "\n",
            "Global step: 5714,loss: 0.008034717\n",
            "\n",
            "Global step: 5715,loss: 0.008263044\n",
            "\n",
            "Global step: 5716,loss: 0.007525113\n",
            "\n",
            "Global step: 5717,loss: 0.010066697\n",
            "\n",
            "Global step: 5718,loss: 0.008347799\n",
            "\n",
            "Global step: 5719,loss: 0.0077538043\n",
            "\n",
            "Global step: 5720,loss: 0.008746045\n",
            "\n",
            "Global step: 5721,loss: 0.008610544\n",
            "\n",
            "Global step: 5722,loss: 0.008262716\n",
            "\n",
            "Global step: 5723,loss: 0.007698949\n",
            "\n",
            "Global step: 5724,loss: 0.008581917\n",
            "\n",
            "Global step: 5725,loss: 0.0072266357\n",
            "\n",
            "Global step: 5726,loss: 0.0074171955\n",
            "\n",
            "Global step: 5727,loss: 0.007953286\n",
            "\n",
            "Global step: 5728,loss: 0.007838361\n",
            "\n",
            "Global step: 5729,loss: 0.007730446\n",
            "\n",
            "Global step: 5730,loss: 0.0080087315\n",
            "\n",
            "Global step: 5731,loss: 0.008191382\n",
            "\n",
            "Global step: 5732,loss: 0.009050228\n",
            "\n",
            "Global step: 5733,loss: 0.0077565485\n",
            "\n",
            "Global step: 5734,loss: 0.007611879\n",
            "\n",
            "Global step: 5735,loss: 0.009467374\n",
            "\n",
            "Global step: 5736,loss: 0.0072390162\n",
            "\n",
            "Global step: 5737,loss: 0.0074824174\n",
            "\n",
            "Global step: 5738,loss: 0.007799276\n",
            "\n",
            "Global step: 5739,loss: 0.00897156\n",
            "\n",
            "Global step: 5740,loss: 0.007866614\n",
            "\n",
            "Global step: 5741,loss: 0.0070709074\n",
            "\n",
            "Global step: 5742,loss: 0.007490451\n",
            "\n",
            "Global step: 5743,loss: 0.009155219\n",
            "\n",
            "Global step: 5744,loss: 0.008139168\n",
            "\n",
            "Global step: 5745,loss: 0.0077618794\n",
            "\n",
            "Global step: 5746,loss: 0.013326901\n",
            "\n",
            "Global step: 5747,loss: 0.008291105\n",
            "\n",
            "Global step: 5748,loss: 0.0077239\n",
            "\n",
            "Global step: 5749,loss: 0.0077017606\n",
            "\n",
            "Global step: 5750,loss: 0.0075550117\n",
            "\n",
            "Global step: 5751,loss: 0.008239313\n",
            "\n",
            "Global step: 5752,loss: 0.00860607\n",
            "\n",
            "Global step: 5753,loss: 0.007572519\n",
            "\n",
            "Global step: 5754,loss: 0.007667091\n",
            "\n",
            "Global step: 5755,loss: 0.0072758547\n",
            "\n",
            "Global step: 5756,loss: 0.006807608\n",
            "\n",
            "Global step: 5757,loss: 0.007493965\n",
            "\n",
            "Global step: 5758,loss: 0.0072139837\n",
            "\n",
            "Global step: 5759,loss: 0.0068973666\n",
            "\n",
            "Global step: 5760,loss: 0.008592695\n",
            "\n",
            "Global step: 5761,loss: 0.007879763\n",
            "\n",
            "Global step: 5762,loss: 0.008278274\n",
            "\n",
            "Global step: 5763,loss: 0.007140742\n",
            "\n",
            "Global step: 5764,loss: 0.007562662\n",
            "\n",
            "Global step: 5765,loss: 0.0070263995\n",
            "\n",
            "Global step: 5766,loss: 0.008431053\n",
            "\n",
            "Global step: 5767,loss: 0.007700705\n",
            "\n",
            "Global step: 5768,loss: 0.007353225\n",
            "\n",
            "Global step: 5769,loss: 0.008161294\n",
            "\n",
            "Global step: 5770,loss: 0.008042871\n",
            "\n",
            "Global step: 5771,loss: 0.007874314\n",
            "\n",
            "Global step: 5772,loss: 0.007306304\n",
            "\n",
            "Global step: 5773,loss: 0.00795211\n",
            "\n",
            "Global step: 5774,loss: 0.007832739\n",
            "\n",
            "Global step: 5775,loss: 0.008357369\n",
            "\n",
            "Global step: 5776,loss: 0.007015268\n",
            "\n",
            "Global step: 5777,loss: 0.0080853775\n",
            "\n",
            "Global step: 5778,loss: 0.007248619\n",
            "\n",
            "Global step: 5779,loss: 0.007270249\n",
            "\n",
            "Global step: 5780,loss: 0.0073425993\n",
            "\n",
            "Global step: 5781,loss: 0.0074303057\n",
            "\n",
            "Global step: 5782,loss: 0.0074390145\n",
            "\n",
            "Global step: 5783,loss: 0.007989011\n",
            "\n",
            "Global step: 5784,loss: 0.0078072306\n",
            "\n",
            "Global step: 5785,loss: 0.007447678\n",
            "\n",
            "Global step: 5786,loss: 0.0078822365\n",
            "\n",
            "Global step: 5787,loss: 0.007737253\n",
            "\n",
            "Global step: 5788,loss: 0.007854975\n",
            "\n",
            "Global step: 5789,loss: 0.0071727633\n",
            "\n",
            "Global step: 5790,loss: 0.007746552\n",
            "\n",
            "Global step: 5791,loss: 0.007372037\n",
            "\n",
            "Global step: 5792,loss: 0.007989218\n",
            "\n",
            "Global step: 5793,loss: 0.0075965496\n",
            "\n",
            "Global step: 5794,loss: 0.010124801\n",
            "\n",
            "Global step: 5795,loss: 0.007312513\n",
            "\n",
            "Global step: 5796,loss: 0.0077498504\n",
            "\n",
            "Global step: 5797,loss: 0.011220912\n",
            "\n",
            "Global step: 5798,loss: 0.011828147\n",
            "\n",
            "Global step: 5799,loss: 0.0069432673\n",
            "\n",
            "Global step: 5800,loss: 0.0078073833\n",
            "\n",
            "Global step: 5801,loss: 0.009062078\n",
            "\n",
            "Global step: 5802,loss: 0.0071850927\n",
            "\n",
            "Global step: 5803,loss: 0.0072354306\n",
            "\n",
            "Global step: 5804,loss: 0.008640755\n",
            "\n",
            "Global step: 5805,loss: 0.0070562223\n",
            "\n",
            "Global step: 5806,loss: 0.008224807\n",
            "\n",
            "Global step: 5807,loss: 0.0068593915\n",
            "\n",
            "Global step: 5808,loss: 0.007600107\n",
            "\n",
            "Global step: 5809,loss: 0.007899853\n",
            "\n",
            "Global step: 5810,loss: 0.0073422976\n",
            "\n",
            "Global step: 5811,loss: 0.007395718\n",
            "\n",
            "Global step: 5812,loss: 0.0072406526\n",
            "\n",
            "Global step: 5813,loss: 0.0072954325\n",
            "\n",
            "Global step: 5814,loss: 0.00895025\n",
            "\n",
            "Global step: 5815,loss: 0.007853071\n",
            "\n",
            "Global step: 5816,loss: 0.007867159\n",
            "\n",
            "Global step: 5817,loss: 0.0074412622\n",
            "\n",
            "Global step: 5818,loss: 0.006989545\n",
            "\n",
            "Global step: 5819,loss: 0.0074450253\n",
            "\n",
            "Global step: 5820,loss: 0.0071094995\n",
            "\n",
            "Global step: 5821,loss: 0.0074602826\n",
            "\n",
            "Global step: 5822,loss: 0.0074257497\n",
            "\n",
            "Global step: 5823,loss: 0.007541013\n",
            "\n",
            "Global step: 5824,loss: 0.0076161637\n",
            "\n",
            "Global step: 5825,loss: 0.008729026\n",
            "\n",
            "Global step: 5826,loss: 0.007872923\n",
            "\n",
            "Global step: 5827,loss: 0.0077054836\n",
            "\n",
            "Global step: 5828,loss: 0.007601331\n",
            "\n",
            "Global step: 5829,loss: 0.007204741\n",
            "\n",
            "Global step: 5830,loss: 0.0074231373\n",
            "\n",
            "Global step: 5831,loss: 0.006840096\n",
            "\n",
            "Global step: 5832,loss: 0.006950539\n",
            "\n",
            "Global step: 5833,loss: 0.008040499\n",
            "\n",
            "Global step: 5834,loss: 0.007308516\n",
            "\n",
            "Global step: 5835,loss: 0.007265387\n",
            "\n",
            "Global step: 5836,loss: 0.0072393036\n",
            "\n",
            "Global step: 5837,loss: 0.0071162703\n",
            "\n",
            "Global step: 5838,loss: 0.007401396\n",
            "\n",
            "Global step: 5839,loss: 0.0073825046\n",
            "\n",
            "Global step: 5840,loss: 0.008004133\n",
            "\n",
            "Global step: 5841,loss: 0.007398869\n",
            "\n",
            "Global step: 5842,loss: 0.0074508684\n",
            "\n",
            "Global step: 5843,loss: 0.008498281\n",
            "\n",
            "Global step: 5844,loss: 0.008505307\n",
            "\n",
            "Global step: 5845,loss: 0.008173272\n",
            "\n",
            "Global step: 5846,loss: 0.0071007973\n",
            "\n",
            "Global step: 5847,loss: 0.007702353\n",
            "\n",
            "Global step: 5848,loss: 0.007323318\n",
            "\n",
            "Global step: 5849,loss: 0.0075533986\n",
            "\n",
            "Global step: 5850,loss: 0.007673034\n",
            "\n",
            "Global step: 5851,loss: 0.007070872\n",
            "\n",
            "Global step: 5852,loss: 0.0072072702\n",
            "\n",
            "Global step: 5853,loss: 0.0076215854\n",
            "\n",
            "Global step: 5854,loss: 0.007678789\n",
            "\n",
            "Global step: 5855,loss: 0.0067423754\n",
            "\n",
            "Global step: 5856,loss: 0.007170067\n",
            "\n",
            "Global step: 5857,loss: 0.0073687145\n",
            "\n",
            "Global step: 5858,loss: 0.0077628302\n",
            "\n",
            "Global step: 5859,loss: 0.00775009\n",
            "\n",
            "Global step: 5860,loss: 0.008119632\n",
            "\n",
            "Global step: 5861,loss: 0.008437887\n",
            "\n",
            "Global step: 5862,loss: 0.0075663305\n",
            "\n",
            "Global step: 5863,loss: 0.007664993\n",
            "\n",
            "Global step: 5864,loss: 0.007094369\n",
            "\n",
            "Global step: 5865,loss: 0.0076259687\n",
            "\n",
            "Global step: 5866,loss: 0.0074160825\n",
            "\n",
            "Global step: 5867,loss: 0.006606426\n",
            "\n",
            "Global step: 5868,loss: 0.007580947\n",
            "\n",
            "Global step: 5869,loss: 0.008186036\n",
            "\n",
            "Global step: 5870,loss: 0.0069540995\n",
            "\n",
            "Global step: 5871,loss: 0.0074938457\n",
            "\n",
            "Global step: 5872,loss: 0.006994862\n",
            "\n",
            "Global step: 5873,loss: 0.007412872\n",
            "\n",
            "Global step: 5874,loss: 0.0074711703\n",
            "\n",
            "Global step: 5875,loss: 0.007830746\n",
            "\n",
            "Global step: 5876,loss: 0.0072753117\n",
            "\n",
            "Global step: 5877,loss: 0.007534866\n",
            "\n",
            "Global step: 5878,loss: 0.007528909\n",
            "\n",
            "Global step: 5879,loss: 0.0074154204\n",
            "\n",
            "Global step: 5880,loss: 0.0075944983\n",
            "\n",
            "Global step: 5881,loss: 0.0066878106\n",
            "\n",
            "Global step: 5882,loss: 0.007273709\n",
            "\n",
            "Global step: 5883,loss: 0.0073191747\n",
            "\n",
            "Global step: 5884,loss: 0.007841418\n",
            "\n",
            "Global step: 5885,loss: 0.010309668\n",
            "\n",
            "Global step: 5886,loss: 0.0077724745\n",
            "\n",
            "Global step: 5887,loss: 0.0072561107\n",
            "\n",
            "Global step: 5888,loss: 0.007101343\n",
            "\n",
            "Global step: 5889,loss: 0.007004952\n",
            "\n",
            "Global step: 5890,loss: 0.007427008\n",
            "\n",
            "Global step: 5891,loss: 0.008049808\n",
            "\n",
            "Global step: 5892,loss: 0.0070122206\n",
            "\n",
            "Global step: 5893,loss: 0.0072529167\n",
            "\n",
            "Global step: 5894,loss: 0.007153171\n",
            "\n",
            "Global step: 5895,loss: 0.007422767\n",
            "\n",
            "Global step: 5896,loss: 0.007191218\n",
            "\n",
            "Global step: 5897,loss: 0.007340705\n",
            "\n",
            "Global step: 5898,loss: 0.007449169\n",
            "\n",
            "Global step: 5899,loss: 0.0071084937\n",
            "\n",
            "Global step: 5900,loss: 0.007372525\n",
            "\n",
            "Global step: 5901,loss: 0.006919115\n",
            "\n",
            "Global step: 5902,loss: 0.007792479\n",
            "\n",
            "Global step: 5903,loss: 0.008332393\n",
            "\n",
            "Global step: 5904,loss: 0.0070997365\n",
            "\n",
            "Global step: 5905,loss: 0.0075589884\n",
            "\n",
            "Global step: 5906,loss: 0.007736325\n",
            "\n",
            "Global step: 5907,loss: 0.007886799\n",
            "\n",
            "Global step: 5908,loss: 0.0072407243\n",
            "\n",
            "Global step: 5909,loss: 0.008068908\n",
            "\n",
            "Global step: 5910,loss: 0.0070340596\n",
            "\n",
            "Global step: 5911,loss: 0.007618209\n",
            "\n",
            "Global step: 5912,loss: 0.0073965625\n",
            "\n",
            "Global step: 5913,loss: 0.007187974\n",
            "\n",
            "Global step: 5914,loss: 0.00766035\n",
            "\n",
            "Global step: 5915,loss: 0.0075672464\n",
            "\n",
            "Global step: 5916,loss: 0.007345973\n",
            "\n",
            "Global step: 5917,loss: 0.007911867\n",
            "\n",
            "Global step: 5918,loss: 0.0073096575\n",
            "\n",
            "Global step: 5919,loss: 0.0070417924\n",
            "\n",
            "Global step: 5920,loss: 0.00750089\n",
            "\n",
            "Global step: 5921,loss: 0.007049885\n",
            "\n",
            "Global step: 5922,loss: 0.0071725207\n",
            "\n",
            "Global step: 5923,loss: 0.0068541784\n",
            "\n",
            "Global step: 5924,loss: 0.007772048\n",
            "\n",
            "Global step: 5925,loss: 0.0081807505\n",
            "\n",
            "Global step: 5926,loss: 0.0069087576\n",
            "\n",
            "Global step: 5927,loss: 0.007108327\n",
            "\n",
            "Global step: 5928,loss: 0.007307971\n",
            "\n",
            "Global step: 5929,loss: 0.0069650086\n",
            "\n",
            "Global step: 5930,loss: 0.008502981\n",
            "\n",
            "Global step: 5931,loss: 0.007595226\n",
            "\n",
            "Global step: 5932,loss: 0.006573278\n",
            "\n",
            "Global step: 5933,loss: 0.0070380485\n",
            "\n",
            "Global step: 5934,loss: 0.0074072303\n",
            "\n",
            "Global step: 5935,loss: 0.0073175025\n",
            "\n",
            "Global step: 5936,loss: 0.0078356955\n",
            "\n",
            "Global step: 5937,loss: 0.0066352985\n",
            "\n",
            "Global step: 5938,loss: 0.007053143\n",
            "\n",
            "Global step: 5939,loss: 0.0066327313\n",
            "\n",
            "Global step: 5940,loss: 0.0073256465\n",
            "\n",
            "Global step: 5941,loss: 0.0075619784\n",
            "\n",
            "Global step: 5942,loss: 0.008035723\n",
            "\n",
            "Global step: 5943,loss: 0.009151333\n",
            "\n",
            "Global step: 5944,loss: 0.0071742306\n",
            "\n",
            "Global step: 5945,loss: 0.007831156\n",
            "\n",
            "Global step: 5946,loss: 0.006844612\n",
            "\n",
            "Global step: 5947,loss: 0.0075756004\n",
            "\n",
            "Global step: 5948,loss: 0.0073665134\n",
            "\n",
            "Global step: 5949,loss: 0.00672611\n",
            "\n",
            "Global step: 5950,loss: 0.008096987\n",
            "\n",
            "Global step: 5951,loss: 0.0073492816\n",
            "\n",
            "Global step: 5952,loss: 0.007804959\n",
            "\n",
            "Global step: 5953,loss: 0.0074388837\n",
            "\n",
            "Global step: 5954,loss: 0.008952169\n",
            "\n",
            "Global step: 5955,loss: 0.007698268\n",
            "\n",
            "Global step: 5956,loss: 0.007423233\n",
            "\n",
            "Global step: 5957,loss: 0.0071036443\n",
            "\n",
            "Global step: 5958,loss: 0.007798158\n",
            "\n",
            "Global step: 5959,loss: 0.007464726\n",
            "\n",
            "Global step: 5960,loss: 0.0114135835\n",
            "\n",
            "Global step: 5961,loss: 0.0070773656\n",
            "\n",
            "Global step: 5962,loss: 0.0082648955\n",
            "\n",
            "Global step: 5963,loss: 0.009052039\n",
            "\n",
            "Global step: 5964,loss: 0.007084057\n",
            "\n",
            "Global step: 5965,loss: 0.007696828\n",
            "\n",
            "Global step: 5966,loss: 0.007857962\n",
            "\n",
            "Global step: 5967,loss: 0.0070759724\n",
            "\n",
            "Global step: 5968,loss: 0.007001096\n",
            "\n",
            "Global step: 5969,loss: 0.007019039\n",
            "\n",
            "Global step: 5970,loss: 0.009270571\n",
            "\n",
            "Global step: 5971,loss: 0.007647214\n",
            "\n",
            "Global step: 5972,loss: 0.0077004796\n",
            "\n",
            "Global step: 5973,loss: 0.0076271044\n",
            "\n",
            "Global step: 5974,loss: 0.0075784004\n",
            "\n",
            "Global step: 5975,loss: 0.0066700415\n",
            "\n",
            "Global step: 5976,loss: 0.0068250033\n",
            "\n",
            "Global step: 5977,loss: 0.0074764476\n",
            "\n",
            "Global step: 5978,loss: 0.008137365\n",
            "\n",
            "Global step: 5979,loss: 0.007010125\n",
            "\n",
            "Global step: 5980,loss: 0.00789876\n",
            "\n",
            "Global step: 5981,loss: 0.0067543075\n",
            "\n",
            "Global step: 5982,loss: 0.010146585\n",
            "\n",
            "Global step: 5983,loss: 0.008170386\n",
            "\n",
            "Global step: 5984,loss: 0.007947638\n",
            "\n",
            "Global step: 5985,loss: 0.007245965\n",
            "\n",
            "Global step: 5986,loss: 0.010620492\n",
            "\n",
            "Global step: 5987,loss: 0.006998362\n",
            "\n",
            "Global step: 5988,loss: 0.007429973\n",
            "\n",
            "Global step: 5989,loss: 0.007850498\n",
            "\n",
            "Global step: 5990,loss: 0.00740465\n",
            "\n",
            "Global step: 5991,loss: 0.0073439586\n",
            "\n",
            "Global step: 5992,loss: 0.0069314046\n",
            "\n",
            "Global step: 5993,loss: 0.0077210097\n",
            "\n",
            "Global step: 5994,loss: 0.007213014\n",
            "\n",
            "Global step: 5995,loss: 0.007215463\n",
            "\n",
            "Global step: 5996,loss: 0.0077783503\n",
            "\n",
            "Global step: 5997,loss: 0.007850987\n",
            "\n",
            "Global step: 5998,loss: 0.00735718\n",
            "\n",
            "Global step: 5999,loss: 0.012260691\n",
            "\n",
            "Global step: 6000,loss: 0.012452151\n",
            "\n",
            "Global step: 6001,loss: 0.007419556\n",
            "\n",
            "Global step: 6002,loss: 0.007292746\n",
            "\n",
            "Global step: 6003,loss: 0.0071375472\n",
            "\n",
            "Global step: 6004,loss: 0.007592476\n",
            "\n",
            "Global step: 6005,loss: 0.007597725\n",
            "\n",
            "Global step: 6006,loss: 0.009731658\n",
            "\n",
            "Global step: 6007,loss: 0.0072886404\n",
            "\n",
            "Global step: 6008,loss: 0.008763339\n",
            "\n",
            "Global step: 6009,loss: 0.008385236\n",
            "\n",
            "Global step: 6010,loss: 0.007004752\n",
            "\n",
            "Global step: 6011,loss: 0.0075222715\n",
            "\n",
            "Global step: 6012,loss: 0.007420161\n",
            "\n",
            "Global step: 6013,loss: 0.008066301\n",
            "\n",
            "Global step: 6014,loss: 0.007379046\n",
            "\n",
            "Global step: 6015,loss: 0.00729548\n",
            "\n",
            "Global step: 6016,loss: 0.0070030736\n",
            "\n",
            "Global step: 6017,loss: 0.0075917933\n",
            "\n",
            "Global step: 6018,loss: 0.0078024874\n",
            "\n",
            "Global step: 6019,loss: 0.00775507\n",
            "\n",
            "Global step: 6020,loss: 0.008261938\n",
            "\n",
            "Global step: 6021,loss: 0.007858451\n",
            "\n",
            "Global step: 6022,loss: 0.008113461\n",
            "\n",
            "Global step: 6023,loss: 0.0074462458\n",
            "\n",
            "Global step: 6024,loss: 0.007200158\n",
            "\n",
            "Global step: 6025,loss: 0.007479322\n",
            "\n",
            "Global step: 6026,loss: 0.0077682175\n",
            "\n",
            "Global step: 6027,loss: 0.008047278\n",
            "\n",
            "Global step: 6028,loss: 0.0074848724\n",
            "\n",
            "Global step: 6029,loss: 0.006501954\n",
            "\n",
            "Global step: 6030,loss: 0.0076244385\n",
            "\n",
            "Global step: 6031,loss: 0.008424012\n",
            "\n",
            "Global step: 6032,loss: 0.0070153424\n",
            "\n",
            "Global step: 6033,loss: 0.0076431874\n",
            "\n",
            "Global step: 6034,loss: 0.00812051\n",
            "\n",
            "Global step: 6035,loss: 0.0070717284\n",
            "\n",
            "Global step: 6036,loss: 0.007642986\n",
            "\n",
            "Global step: 6037,loss: 0.0075202533\n",
            "\n",
            "Global step: 6038,loss: 0.007619492\n",
            "\n",
            "Global step: 6039,loss: 0.0074751195\n",
            "\n",
            "Global step: 6040,loss: 0.0071692155\n",
            "\n",
            "Global step: 6041,loss: 0.007430291\n",
            "\n",
            "Global step: 6042,loss: 0.007019719\n",
            "\n",
            "Global step: 6043,loss: 0.0077551706\n",
            "\n",
            "Global step: 6044,loss: 0.007689667\n",
            "\n",
            "Global step: 6045,loss: 0.007964892\n",
            "\n",
            "Global step: 6046,loss: 0.008479606\n",
            "\n",
            "Global step: 6047,loss: 0.0066101886\n",
            "\n",
            "Global step: 6048,loss: 0.007096541\n",
            "\n",
            "Global step: 6049,loss: 0.00751417\n",
            "\n",
            "Global step: 6050,loss: 0.011634578\n",
            "\n",
            "Global step: 6051,loss: 0.0075930995\n",
            "\n",
            "Global step: 6052,loss: 0.0076915817\n",
            "\n",
            "Global step: 6053,loss: 0.010673631\n",
            "\n",
            "Global step: 6054,loss: 0.007852098\n",
            "\n",
            "Global step: 6055,loss: 0.0074346466\n",
            "\n",
            "Global step: 6056,loss: 0.0077083306\n",
            "\n",
            "Global step: 6057,loss: 0.007272011\n",
            "\n",
            "Global step: 6058,loss: 0.008413521\n",
            "\n",
            "Global step: 6059,loss: 0.007405944\n",
            "\n",
            "Global step: 6060,loss: 0.007673182\n",
            "\n",
            "Global step: 6061,loss: 0.007049547\n",
            "\n",
            "Global step: 6062,loss: 0.007505102\n",
            "\n",
            "Global step: 6063,loss: 0.007542505\n",
            "\n",
            "Global step: 6064,loss: 0.0073477337\n",
            "\n",
            "Global step: 6065,loss: 0.007327083\n",
            "\n",
            "Global step: 6066,loss: 0.006951811\n",
            "\n",
            "Global step: 6067,loss: 0.00746889\n",
            "\n",
            "Global step: 6068,loss: 0.0069873948\n",
            "\n",
            "Global step: 6069,loss: 0.007095884\n",
            "\n",
            "Global step: 6070,loss: 0.0073506352\n",
            "\n",
            "Global step: 6071,loss: 0.00798155\n",
            "\n",
            "Global step: 6072,loss: 0.0092775365\n",
            "\n",
            "Global step: 6073,loss: 0.007375309\n",
            "\n",
            "Global step: 6074,loss: 0.00872308\n",
            "\n",
            "Global step: 6075,loss: 0.009256304\n",
            "\n",
            "Global step: 6076,loss: 0.00688969\n",
            "\n",
            "Global step: 6077,loss: 0.008627659\n",
            "\n",
            "Global step: 6078,loss: 0.007882255\n",
            "\n",
            "Global step: 6079,loss: 0.007068621\n",
            "\n",
            "Global step: 6080,loss: 0.007232301\n",
            "\n",
            "Global step: 6081,loss: 0.007425769\n",
            "\n",
            "Global step: 6082,loss: 0.007567006\n",
            "\n",
            "Global step: 6083,loss: 0.007861542\n",
            "\n",
            "Global step: 6084,loss: 0.007668083\n",
            "\n",
            "Global step: 6085,loss: 0.0071679726\n",
            "\n",
            "Global step: 6086,loss: 0.0080376575\n",
            "\n",
            "Global step: 6087,loss: 0.00729135\n",
            "\n",
            "Global step: 6088,loss: 0.007853677\n",
            "\n",
            "Global step: 6089,loss: 0.0072446223\n",
            "\n",
            "Global step: 6090,loss: 0.007860851\n",
            "\n",
            "Global step: 6091,loss: 0.009024471\n",
            "\n",
            "Global step: 6092,loss: 0.00778524\n",
            "\n",
            "Global step: 6093,loss: 0.007646118\n",
            "\n",
            "Global step: 6094,loss: 0.007842124\n",
            "\n",
            "Global step: 6095,loss: 0.0076510254\n",
            "\n",
            "Global step: 6096,loss: 0.008191185\n",
            "\n",
            "Global step: 6097,loss: 0.0075399894\n",
            "\n",
            "Global step: 6098,loss: 0.010751063\n",
            "\n",
            "Global step: 6099,loss: 0.0071028005\n",
            "\n",
            "Global step: 6100,loss: 0.0073925997\n",
            "\n",
            "Global step: 6101,loss: 0.0077851806\n",
            "\n",
            "Global step: 6102,loss: 0.007083083\n",
            "\n",
            "Global step: 6103,loss: 0.0070450935\n",
            "\n",
            "Global step: 6104,loss: 0.007926535\n",
            "\n",
            "Global step: 6105,loss: 0.0071041025\n",
            "\n",
            "Global step: 6106,loss: 0.0072037997\n",
            "\n",
            "Global step: 6107,loss: 0.0076271305\n",
            "\n",
            "Global step: 6108,loss: 0.012279842\n",
            "\n",
            "Global step: 6109,loss: 0.007130251\n",
            "\n",
            "Global step: 6110,loss: 0.0074183824\n",
            "\n",
            "Global step: 6111,loss: 0.0077298535\n",
            "\n",
            "Global step: 6112,loss: 0.0073651616\n",
            "\n",
            "Global step: 6113,loss: 0.007487501\n",
            "\n",
            "Global step: 6114,loss: 0.0073733525\n",
            "\n",
            "Global step: 6115,loss: 0.0073820464\n",
            "\n",
            "Global step: 6116,loss: 0.007741251\n",
            "\n",
            "Global step: 6117,loss: 0.007611573\n",
            "\n",
            "Global step: 6118,loss: 0.0073761074\n",
            "\n",
            "Global step: 6119,loss: 0.0069346414\n",
            "\n",
            "Global step: 6120,loss: 0.0069994386\n",
            "\n",
            "Global step: 6121,loss: 0.007515141\n",
            "\n",
            "Global step: 6122,loss: 0.006888099\n",
            "\n",
            "Global step: 6123,loss: 0.0075126193\n",
            "\n",
            "Global step: 6124,loss: 0.008470851\n",
            "\n",
            "Global step: 6125,loss: 0.0077592777\n",
            "\n",
            "Global step: 6126,loss: 0.0090706935\n",
            "\n",
            "Global step: 6127,loss: 0.008190076\n",
            "\n",
            "Global step: 6128,loss: 0.008217334\n",
            "\n",
            "Global step: 6129,loss: 0.006903229\n",
            "\n",
            "Global step: 6130,loss: 0.0071232845\n",
            "\n",
            "Global step: 6131,loss: 0.0068768435\n",
            "\n",
            "Global step: 6132,loss: 0.0070908056\n",
            "\n",
            "Global step: 6133,loss: 0.0075136153\n",
            "\n",
            "Global step: 6134,loss: 0.006983008\n",
            "\n",
            "Global step: 6135,loss: 0.0069223205\n",
            "\n",
            "Global step: 6136,loss: 0.007206014\n",
            "\n",
            "Global step: 6137,loss: 0.0073612635\n",
            "\n",
            "Global step: 6138,loss: 0.0077176215\n",
            "\n",
            "Global step: 6139,loss: 0.0071773897\n",
            "\n",
            "Global step: 6140,loss: 0.0070015965\n",
            "\n",
            "Global step: 6141,loss: 0.007025975\n",
            "\n",
            "Global step: 6142,loss: 0.008243108\n",
            "\n",
            "Global step: 6143,loss: 0.007353972\n",
            "\n",
            "Global step: 6144,loss: 0.006784409\n",
            "\n",
            "Global step: 6145,loss: 0.0091510005\n",
            "\n",
            "Global step: 6146,loss: 0.007817757\n",
            "\n",
            "Global step: 6147,loss: 0.010134001\n",
            "\n",
            "Global step: 6148,loss: 0.0073052268\n",
            "\n",
            "Global step: 6149,loss: 0.008226674\n",
            "\n",
            "Global step: 6150,loss: 0.0075880038\n",
            "\n",
            "Global step: 6151,loss: 0.007228809\n",
            "\n",
            "Global step: 6152,loss: 0.008087508\n",
            "\n",
            "Global step: 6153,loss: 0.0071558226\n",
            "\n",
            "Global step: 6154,loss: 0.007108897\n",
            "\n",
            "Global step: 6155,loss: 0.007080315\n",
            "\n",
            "Global step: 6156,loss: 0.007593452\n",
            "\n",
            "Global step: 6157,loss: 0.007031642\n",
            "\n",
            "Global step: 6158,loss: 0.007364956\n",
            "\n",
            "Global step: 6159,loss: 0.0075466633\n",
            "\n",
            "Global step: 6160,loss: 0.006965877\n",
            "\n",
            "Global step: 6161,loss: 0.0073526925\n",
            "\n",
            "Global step: 6162,loss: 0.0075454284\n",
            "\n",
            "Global step: 6163,loss: 0.007023195\n",
            "\n",
            "Global step: 6164,loss: 0.0073195915\n",
            "\n",
            "Global step: 6165,loss: 0.007477906\n",
            "\n",
            "Global step: 6166,loss: 0.0067880913\n",
            "\n",
            "Global step: 6167,loss: 0.0077274046\n",
            "\n",
            "Global step: 6168,loss: 0.0070607653\n",
            "\n",
            "Global step: 6169,loss: 0.0073667355\n",
            "\n",
            "Global step: 6170,loss: 0.007431518\n",
            "\n",
            "Global step: 6171,loss: 0.008416705\n",
            "\n",
            "Global step: 6172,loss: 0.0076688873\n",
            "\n",
            "Global step: 6173,loss: 0.007350442\n",
            "\n",
            "Global step: 6174,loss: 0.00765404\n",
            "\n",
            "Global step: 6175,loss: 0.0077552893\n",
            "\n",
            "Global step: 6176,loss: 0.0073368424\n",
            "\n",
            "Global step: 6177,loss: 0.007087772\n",
            "\n",
            "Global step: 6178,loss: 0.0067531513\n",
            "\n",
            "Global step: 6179,loss: 0.007519128\n",
            "\n",
            "Global step: 6180,loss: 0.008067602\n",
            "\n",
            "Global step: 6181,loss: 0.0077137956\n",
            "\n",
            "Global step: 6182,loss: 0.007056986\n",
            "\n",
            "Global step: 6183,loss: 0.0068339235\n",
            "\n",
            "Global step: 6184,loss: 0.007814671\n",
            "\n",
            "Global step: 6185,loss: 0.007056224\n",
            "\n",
            "Global step: 6186,loss: 0.00658616\n",
            "\n",
            "Global step: 6187,loss: 0.007279766\n",
            "\n",
            "Global step: 6188,loss: 0.0072712167\n",
            "\n",
            "Global step: 6189,loss: 0.007446843\n",
            "\n",
            "Global step: 6190,loss: 0.007761552\n",
            "\n",
            "Global step: 6191,loss: 0.0064824475\n",
            "\n",
            "Global step: 6192,loss: 0.008514334\n",
            "\n",
            "Global step: 6193,loss: 0.0072381548\n",
            "\n",
            "Global step: 6194,loss: 0.0072017196\n",
            "\n",
            "Global step: 6195,loss: 0.0076716514\n",
            "\n",
            "Global step: 6196,loss: 0.008313436\n",
            "\n",
            "Global step: 6197,loss: 0.006924147\n",
            "\n",
            "Global step: 6198,loss: 0.008311267\n",
            "\n",
            "Global step: 6199,loss: 0.0068947203\n",
            "\n",
            "Global step: 6200,loss: 0.007113548\n",
            "\n",
            "Global step: 6201,loss: 0.007219291\n",
            "\n",
            "Global step: 6202,loss: 0.007389101\n",
            "\n",
            "Global step: 6203,loss: 0.0076419692\n",
            "\n",
            "Global step: 6204,loss: 0.007445146\n",
            "\n",
            "Global step: 6205,loss: 0.0073861904\n",
            "\n",
            "Global step: 6206,loss: 0.007592488\n",
            "\n",
            "Global step: 6207,loss: 0.007715207\n",
            "\n",
            "Global step: 6208,loss: 0.0073450753\n",
            "\n",
            "Global step: 6209,loss: 0.0069204494\n",
            "\n",
            "Global step: 6210,loss: 0.007774627\n",
            "\n",
            "Global step: 6211,loss: 0.0067238426\n",
            "\n",
            "Global step: 6212,loss: 0.0088271145\n",
            "\n",
            "Global step: 6213,loss: 0.007762287\n",
            "\n",
            "Global step: 6214,loss: 0.0070856344\n",
            "\n",
            "Global step: 6215,loss: 0.0074684485\n",
            "\n",
            "Global step: 6216,loss: 0.009929508\n",
            "\n",
            "Global step: 6217,loss: 0.0077094836\n",
            "\n",
            "Global step: 6218,loss: 0.00862131\n",
            "\n",
            "Global step: 6219,loss: 0.008027497\n",
            "\n",
            "Global step: 6220,loss: 0.00696806\n",
            "\n",
            "Global step: 6221,loss: 0.006973729\n",
            "\n",
            "Global step: 6222,loss: 0.007441188\n",
            "\n",
            "Global step: 6223,loss: 0.008684909\n",
            "\n",
            "Global step: 6224,loss: 0.007907942\n",
            "\n",
            "Global step: 6225,loss: 0.0075386916\n",
            "\n",
            "Global step: 6226,loss: 0.01195466\n",
            "\n",
            "Global step: 6227,loss: 0.007923518\n",
            "\n",
            "Global step: 6228,loss: 0.01261239\n",
            "\n",
            "Global step: 6229,loss: 0.007300736\n",
            "\n",
            "Global step: 6230,loss: 0.007588823\n",
            "\n",
            "Global step: 6231,loss: 0.008386611\n",
            "\n",
            "Global step: 6232,loss: 0.007981673\n",
            "\n",
            "Global step: 6233,loss: 0.0077446355\n",
            "\n",
            "Global step: 6234,loss: 0.008192852\n",
            "\n",
            "Global step: 6235,loss: 0.00738013\n",
            "\n",
            "Global step: 6236,loss: 0.0082255\n",
            "\n",
            "Global step: 6237,loss: 0.007949518\n",
            "\n",
            "Global step: 6238,loss: 0.008042492\n",
            "\n",
            "Global step: 6239,loss: 0.0076430896\n",
            "\n",
            "Global step: 6240,loss: 0.008840446\n",
            "\n",
            "Global step: 6241,loss: 0.007894258\n",
            "\n",
            "Global step: 6242,loss: 0.0076969606\n",
            "\n",
            "Global step: 6243,loss: 0.00803559\n",
            "\n",
            "Global step: 6244,loss: 0.007285372\n",
            "\n",
            "Global step: 6245,loss: 0.007562732\n",
            "\n",
            "Global step: 6246,loss: 0.0082684755\n",
            "\n",
            "Global step: 6247,loss: 0.008813309\n",
            "\n",
            "Global step: 6248,loss: 0.0071370616\n",
            "\n",
            "Global step: 6249,loss: 0.0074195657\n",
            "\n",
            "Global step: 6250,loss: 0.007894908\n",
            "\n",
            "Global step: 6251,loss: 0.008130422\n",
            "\n",
            "Global step: 6252,loss: 0.009530646\n",
            "\n",
            "Global step: 6253,loss: 0.007975056\n",
            "\n",
            "Global step: 6254,loss: 0.007895896\n",
            "\n",
            "Global step: 6255,loss: 0.008849098\n",
            "\n",
            "Global step: 6256,loss: 0.008862343\n",
            "\n",
            "Global step: 6257,loss: 0.0086795585\n",
            "\n",
            "Global step: 6258,loss: 0.008709616\n",
            "\n",
            "Global step: 6259,loss: 0.0073184036\n",
            "\n",
            "Global step: 6260,loss: 0.0073055658\n",
            "\n",
            "Global step: 6261,loss: 0.008395695\n",
            "\n",
            "Global step: 6262,loss: 0.0076906392\n",
            "\n",
            "Global step: 6263,loss: 0.008087053\n",
            "\n",
            "Global step: 6264,loss: 0.00774259\n",
            "\n",
            "Global step: 6265,loss: 0.008885997\n",
            "\n",
            "Global step: 6266,loss: 0.0075917924\n",
            "\n",
            "Global step: 6267,loss: 0.007544427\n",
            "\n",
            "Global step: 6268,loss: 0.0077598663\n",
            "\n",
            "Global step: 6269,loss: 0.0072635673\n",
            "\n",
            "Global step: 6270,loss: 0.0075715138\n",
            "\n",
            "Global step: 6271,loss: 0.007236547\n",
            "\n",
            "Global step: 6272,loss: 0.008482442\n",
            "\n",
            "Global step: 6273,loss: 0.0077468175\n",
            "\n",
            "Global step: 6274,loss: 0.0077895527\n",
            "\n",
            "Global step: 6275,loss: 0.007924428\n",
            "\n",
            "Global step: 6276,loss: 0.007895754\n",
            "\n",
            "Global step: 6277,loss: 0.009649986\n",
            "\n",
            "Global step: 6278,loss: 0.007661786\n",
            "\n",
            "Global step: 6279,loss: 0.007346474\n",
            "\n",
            "Global step: 6280,loss: 0.0073946915\n",
            "\n",
            "Global step: 6281,loss: 0.0074368976\n",
            "\n",
            "Global step: 6282,loss: 0.00702674\n",
            "\n",
            "Global step: 6283,loss: 0.007873133\n",
            "\n",
            "Global step: 6284,loss: 0.007499538\n",
            "\n",
            "Global step: 6285,loss: 0.0075866496\n",
            "\n",
            "Global step: 6286,loss: 0.008507984\n",
            "\n",
            "Global step: 6287,loss: 0.007993562\n",
            "\n",
            "Global step: 6288,loss: 0.007555138\n",
            "\n",
            "Global step: 6289,loss: 0.00745311\n",
            "\n",
            "Global step: 6290,loss: 0.007367136\n",
            "\n",
            "Global step: 6291,loss: 0.0073968717\n",
            "\n",
            "Global step: 6292,loss: 0.008648017\n",
            "\n",
            "Global step: 6293,loss: 0.008363347\n",
            "\n",
            "Global step: 6294,loss: 0.0071068835\n",
            "\n",
            "Global step: 6295,loss: 0.008022981\n",
            "\n",
            "Global step: 6296,loss: 0.009316338\n",
            "\n",
            "Global step: 6297,loss: 0.007758071\n",
            "\n",
            "Global step: 6298,loss: 0.00763263\n",
            "\n",
            "Global step: 6299,loss: 0.0079202615\n",
            "\n",
            "Global step: 6300,loss: 0.0071156467\n",
            "\n",
            "Global step: 6301,loss: 0.0076926374\n",
            "\n",
            "Global step: 6302,loss: 0.00696173\n",
            "\n",
            "Global step: 6303,loss: 0.007713654\n",
            "\n",
            "Global step: 6304,loss: 0.007455258\n",
            "\n",
            "Global step: 6305,loss: 0.0069654197\n",
            "\n",
            "Global step: 6306,loss: 0.010067437\n",
            "\n",
            "Global step: 6307,loss: 0.007924215\n",
            "\n",
            "Global step: 6308,loss: 0.0076978216\n",
            "\n",
            "Global step: 6309,loss: 0.0074361153\n",
            "\n",
            "Global step: 6310,loss: 0.008975478\n",
            "\n",
            "Global step: 6311,loss: 0.0077267005\n",
            "\n",
            "Global step: 6312,loss: 0.007314378\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.83248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:58:35.257400 139837015512832 supervisor.py:1099] global_step/sec: 6.83248\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 6313.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 12:58:35.268630 139837023905536 supervisor.py:1050] Recording summary at step 6313.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 6313,loss: 0.007707019\n",
            "\n",
            "Global step: 6314,loss: 0.007385941\n",
            "\n",
            "Global step: 6315,loss: 0.006909988\n",
            "\n",
            "Global step: 6316,loss: 0.0072935293\n",
            "\n",
            "Global step: 6317,loss: 0.0068940436\n",
            "\n",
            "Global step: 6318,loss: 0.0075619295\n",
            "\n",
            "Global step: 6319,loss: 0.007989639\n",
            "\n",
            "Global step: 6320,loss: 0.0068019284\n",
            "\n",
            "Global step: 6321,loss: 0.0074470565\n",
            "\n",
            "Global step: 6322,loss: 0.0071310154\n",
            "\n",
            "Global step: 6323,loss: 0.007828583\n",
            "\n",
            "Global step: 6324,loss: 0.00764714\n",
            "\n",
            "Global step: 6325,loss: 0.0067919246\n",
            "\n",
            "Global step: 6326,loss: 0.007858414\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 6326,Val_Loss: 0.009481955275862899,  Val_acc: 0.999198717948718 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 12:58:44.580683 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 10/15:\n",
            "Global step: 6327,loss: 0.007329938\n",
            "\n",
            "Global step: 6328,loss: 0.0072838534\n",
            "\n",
            "Global step: 6329,loss: 0.0075519597\n",
            "\n",
            "Global step: 6330,loss: 0.014765892\n",
            "\n",
            "Global step: 6331,loss: 0.0071533127\n",
            "\n",
            "Global step: 6332,loss: 0.008748976\n",
            "\n",
            "Global step: 6333,loss: 0.010160377\n",
            "\n",
            "Global step: 6334,loss: 0.010726217\n",
            "\n",
            "Global step: 6335,loss: 0.0070127356\n",
            "\n",
            "Global step: 6336,loss: 0.009109987\n",
            "\n",
            "Global step: 6337,loss: 0.008137764\n",
            "\n",
            "Global step: 6338,loss: 0.0067317537\n",
            "\n",
            "Global step: 6339,loss: 0.0067569846\n",
            "\n",
            "Global step: 6340,loss: 0.007065597\n",
            "\n",
            "Global step: 6341,loss: 0.007187391\n",
            "\n",
            "Global step: 6342,loss: 0.0074469093\n",
            "\n",
            "Global step: 6343,loss: 0.008059552\n",
            "\n",
            "Global step: 6344,loss: 0.007163102\n",
            "\n",
            "Global step: 6345,loss: 0.0071964962\n",
            "\n",
            "Global step: 6346,loss: 0.008558107\n",
            "\n",
            "Global step: 6347,loss: 0.0066728247\n",
            "\n",
            "Global step: 6348,loss: 0.0071989307\n",
            "\n",
            "Global step: 6349,loss: 0.0074465103\n",
            "\n",
            "Global step: 6350,loss: 0.0074911998\n",
            "\n",
            "Global step: 6351,loss: 0.007570137\n",
            "\n",
            "Global step: 6352,loss: 0.007371694\n",
            "\n",
            "Global step: 6353,loss: 0.0076876683\n",
            "\n",
            "Global step: 6354,loss: 0.00719338\n",
            "\n",
            "Global step: 6355,loss: 0.0074514807\n",
            "\n",
            "Global step: 6356,loss: 0.007595637\n",
            "\n",
            "Global step: 6357,loss: 0.0077973\n",
            "\n",
            "Global step: 6358,loss: 0.007260034\n",
            "\n",
            "Global step: 6359,loss: 0.0073169474\n",
            "\n",
            "Global step: 6360,loss: 0.007124798\n",
            "\n",
            "Global step: 6361,loss: 0.0076099555\n",
            "\n",
            "Global step: 6362,loss: 0.0071737957\n",
            "\n",
            "Global step: 6363,loss: 0.0071767555\n",
            "\n",
            "Global step: 6364,loss: 0.006684693\n",
            "\n",
            "Global step: 6365,loss: 0.0075497865\n",
            "\n",
            "Global step: 6366,loss: 0.007350427\n",
            "\n",
            "Global step: 6367,loss: 0.007424716\n",
            "\n",
            "Global step: 6368,loss: 0.0073081423\n",
            "\n",
            "Global step: 6369,loss: 0.00749004\n",
            "\n",
            "Global step: 6370,loss: 0.0075964257\n",
            "\n",
            "Global step: 6371,loss: 0.007695468\n",
            "\n",
            "Global step: 6372,loss: 0.006757811\n",
            "\n",
            "Global step: 6373,loss: 0.007236328\n",
            "\n",
            "Global step: 6374,loss: 0.010988834\n",
            "\n",
            "Global step: 6375,loss: 0.007372744\n",
            "\n",
            "Global step: 6376,loss: 0.0072405064\n",
            "\n",
            "Global step: 6377,loss: 0.0073655546\n",
            "\n",
            "Global step: 6378,loss: 0.0072060134\n",
            "\n",
            "Global step: 6379,loss: 0.0073372424\n",
            "\n",
            "Global step: 6380,loss: 0.0072743855\n",
            "\n",
            "Global step: 6381,loss: 0.0068271416\n",
            "\n",
            "Global step: 6382,loss: 0.0074572433\n",
            "\n",
            "Global step: 6383,loss: 0.008072412\n",
            "\n",
            "Global step: 6384,loss: 0.0068345144\n",
            "\n",
            "Global step: 6385,loss: 0.0073836828\n",
            "\n",
            "Global step: 6386,loss: 0.0066721817\n",
            "\n",
            "Global step: 6387,loss: 0.006313265\n",
            "\n",
            "Global step: 6388,loss: 0.0071776034\n",
            "\n",
            "Global step: 6389,loss: 0.006777726\n",
            "\n",
            "Global step: 6390,loss: 0.007190623\n",
            "\n",
            "Global step: 6391,loss: 0.0071045994\n",
            "\n",
            "Global step: 6392,loss: 0.006954191\n",
            "\n",
            "Global step: 6393,loss: 0.007141557\n",
            "\n",
            "Global step: 6394,loss: 0.006745924\n",
            "\n",
            "Global step: 6395,loss: 0.007280554\n",
            "\n",
            "Global step: 6396,loss: 0.008252654\n",
            "\n",
            "Global step: 6397,loss: 0.007897798\n",
            "\n",
            "Global step: 6398,loss: 0.007135515\n",
            "\n",
            "Global step: 6399,loss: 0.0069033564\n",
            "\n",
            "Global step: 6400,loss: 0.0069989604\n",
            "\n",
            "Global step: 6401,loss: 0.0067233657\n",
            "\n",
            "Global step: 6402,loss: 0.00699139\n",
            "\n",
            "Global step: 6403,loss: 0.007434464\n",
            "\n",
            "Global step: 6404,loss: 0.007016863\n",
            "\n",
            "Global step: 6405,loss: 0.0076180454\n",
            "\n",
            "Global step: 6406,loss: 0.0067539373\n",
            "\n",
            "Global step: 6407,loss: 0.0066120373\n",
            "\n",
            "Global step: 6408,loss: 0.007307049\n",
            "\n",
            "Global step: 6409,loss: 0.007079099\n",
            "\n",
            "Global step: 6410,loss: 0.007897835\n",
            "\n",
            "Global step: 6411,loss: 0.0071154637\n",
            "\n",
            "Global step: 6412,loss: 0.0074505527\n",
            "\n",
            "Global step: 6413,loss: 0.007855352\n",
            "\n",
            "Global step: 6414,loss: 0.0067773927\n",
            "\n",
            "Global step: 6415,loss: 0.0069701616\n",
            "\n",
            "Global step: 6416,loss: 0.007304068\n",
            "\n",
            "Global step: 6417,loss: 0.007577249\n",
            "\n",
            "Global step: 6418,loss: 0.0072451825\n",
            "\n",
            "Global step: 6419,loss: 0.007107112\n",
            "\n",
            "Global step: 6420,loss: 0.007278187\n",
            "\n",
            "Global step: 6421,loss: 0.0071308254\n",
            "\n",
            "Global step: 6422,loss: 0.007218746\n",
            "\n",
            "Global step: 6423,loss: 0.0069082207\n",
            "\n",
            "Global step: 6424,loss: 0.007206229\n",
            "\n",
            "Global step: 6425,loss: 0.0071116504\n",
            "\n",
            "Global step: 6426,loss: 0.0070578028\n",
            "\n",
            "Global step: 6427,loss: 0.007042588\n",
            "\n",
            "Global step: 6428,loss: 0.0069778045\n",
            "\n",
            "Global step: 6429,loss: 0.0067605684\n",
            "\n",
            "Global step: 6430,loss: 0.0071247565\n",
            "\n",
            "Global step: 6431,loss: 0.0074696643\n",
            "\n",
            "Global step: 6432,loss: 0.006907052\n",
            "\n",
            "Global step: 6433,loss: 0.00688867\n",
            "\n",
            "Global step: 6434,loss: 0.0073271086\n",
            "\n",
            "Global step: 6435,loss: 0.007198354\n",
            "\n",
            "Global step: 6436,loss: 0.0064976453\n",
            "\n",
            "Global step: 6437,loss: 0.007133852\n",
            "\n",
            "Global step: 6438,loss: 0.0069406\n",
            "\n",
            "Global step: 6439,loss: 0.007127152\n",
            "\n",
            "Global step: 6440,loss: 0.006889103\n",
            "\n",
            "Global step: 6441,loss: 0.0066777025\n",
            "\n",
            "Global step: 6442,loss: 0.006448585\n",
            "\n",
            "Global step: 6443,loss: 0.006696903\n",
            "\n",
            "Global step: 6444,loss: 0.007494114\n",
            "\n",
            "Global step: 6445,loss: 0.0071171066\n",
            "\n",
            "Global step: 6446,loss: 0.007228752\n",
            "\n",
            "Global step: 6447,loss: 0.006952275\n",
            "\n",
            "Global step: 6448,loss: 0.0061372514\n",
            "\n",
            "Global step: 6449,loss: 0.007464411\n",
            "\n",
            "Global step: 6450,loss: 0.006657895\n",
            "\n",
            "Global step: 6451,loss: 0.006812475\n",
            "\n",
            "Global step: 6452,loss: 0.00695849\n",
            "\n",
            "Global step: 6453,loss: 0.0069835614\n",
            "\n",
            "Global step: 6454,loss: 0.0072695203\n",
            "\n",
            "Global step: 6455,loss: 0.007101816\n",
            "\n",
            "Global step: 6456,loss: 0.0069205277\n",
            "\n",
            "Global step: 6457,loss: 0.0067215217\n",
            "\n",
            "Global step: 6458,loss: 0.00691572\n",
            "\n",
            "Global step: 6459,loss: 0.007482693\n",
            "\n",
            "Global step: 6460,loss: 0.007536175\n",
            "\n",
            "Global step: 6461,loss: 0.0068428945\n",
            "\n",
            "Global step: 6462,loss: 0.006561829\n",
            "\n",
            "Global step: 6463,loss: 0.0068593025\n",
            "\n",
            "Global step: 6464,loss: 0.00735288\n",
            "\n",
            "Global step: 6465,loss: 0.006619979\n",
            "\n",
            "Global step: 6466,loss: 0.006630263\n",
            "\n",
            "Global step: 6467,loss: 0.007191464\n",
            "\n",
            "Global step: 6468,loss: 0.0067450423\n",
            "\n",
            "Global step: 6469,loss: 0.0068273963\n",
            "\n",
            "Global step: 6470,loss: 0.0071576997\n",
            "\n",
            "Global step: 6471,loss: 0.0059638545\n",
            "\n",
            "Global step: 6472,loss: 0.0067341444\n",
            "\n",
            "Global step: 6473,loss: 0.007734627\n",
            "\n",
            "Global step: 6474,loss: 0.006992733\n",
            "\n",
            "Global step: 6475,loss: 0.0069772704\n",
            "\n",
            "Global step: 6476,loss: 0.0070367744\n",
            "\n",
            "Global step: 6477,loss: 0.007113282\n",
            "\n",
            "Global step: 6478,loss: 0.0070386454\n",
            "\n",
            "Global step: 6479,loss: 0.0064306273\n",
            "\n",
            "Global step: 6480,loss: 0.0068282527\n",
            "\n",
            "Global step: 6481,loss: 0.007101119\n",
            "\n",
            "Global step: 6482,loss: 0.0069151567\n",
            "\n",
            "Global step: 6483,loss: 0.0070008016\n",
            "\n",
            "Global step: 6484,loss: 0.006804369\n",
            "\n",
            "Global step: 6485,loss: 0.00689157\n",
            "\n",
            "Global step: 6486,loss: 0.008613885\n",
            "\n",
            "Global step: 6487,loss: 0.007600956\n",
            "\n",
            "Global step: 6488,loss: 0.0070594265\n",
            "\n",
            "Global step: 6489,loss: 0.0065238695\n",
            "\n",
            "Global step: 6490,loss: 0.0063387044\n",
            "\n",
            "Global step: 6491,loss: 0.0071940245\n",
            "\n",
            "Global step: 6492,loss: 0.006386136\n",
            "\n",
            "Global step: 6493,loss: 0.007567075\n",
            "\n",
            "Global step: 6494,loss: 0.006215193\n",
            "\n",
            "Global step: 6495,loss: 0.007137363\n",
            "\n",
            "Global step: 6496,loss: 0.007931676\n",
            "\n",
            "Global step: 6497,loss: 0.007064431\n",
            "\n",
            "Global step: 6498,loss: 0.0069958754\n",
            "\n",
            "Global step: 6499,loss: 0.0069419714\n",
            "\n",
            "Global step: 6500,loss: 0.006281486\n",
            "\n",
            "Global step: 6501,loss: 0.0070029516\n",
            "\n",
            "Global step: 6502,loss: 0.007216132\n",
            "\n",
            "Global step: 6503,loss: 0.006790154\n",
            "\n",
            "Global step: 6504,loss: 0.0060586897\n",
            "\n",
            "Global step: 6505,loss: 0.008277023\n",
            "\n",
            "Global step: 6506,loss: 0.006870224\n",
            "\n",
            "Global step: 6507,loss: 0.0067513357\n",
            "\n",
            "Global step: 6508,loss: 0.007318904\n",
            "\n",
            "Global step: 6509,loss: 0.0069491393\n",
            "\n",
            "Global step: 6510,loss: 0.0070304186\n",
            "\n",
            "Global step: 6511,loss: 0.0072393967\n",
            "\n",
            "Global step: 6512,loss: 0.0070826183\n",
            "\n",
            "Global step: 6513,loss: 0.006644081\n",
            "\n",
            "Global step: 6514,loss: 0.00661146\n",
            "\n",
            "Global step: 6515,loss: 0.006899276\n",
            "\n",
            "Global step: 6516,loss: 0.007067964\n",
            "\n",
            "Global step: 6517,loss: 0.0069077136\n",
            "\n",
            "Global step: 6518,loss: 0.006805772\n",
            "\n",
            "Global step: 6519,loss: 0.007460262\n",
            "\n",
            "Global step: 6520,loss: 0.007043255\n",
            "\n",
            "Global step: 6521,loss: 0.0070153577\n",
            "\n",
            "Global step: 6522,loss: 0.007101182\n",
            "\n",
            "Global step: 6523,loss: 0.0072890734\n",
            "\n",
            "Global step: 6524,loss: 0.007321059\n",
            "\n",
            "Global step: 6525,loss: 0.0067989277\n",
            "\n",
            "Global step: 6526,loss: 0.0073302602\n",
            "\n",
            "Global step: 6527,loss: 0.0070161866\n",
            "\n",
            "Global step: 6528,loss: 0.0064291437\n",
            "\n",
            "Global step: 6529,loss: 0.007534059\n",
            "\n",
            "Global step: 6530,loss: 0.0066472623\n",
            "\n",
            "Global step: 6531,loss: 0.008048925\n",
            "\n",
            "Global step: 6532,loss: 0.0069897585\n",
            "\n",
            "Global step: 6533,loss: 0.006825093\n",
            "\n",
            "Global step: 6534,loss: 0.007664059\n",
            "\n",
            "Global step: 6535,loss: 0.006976914\n",
            "\n",
            "Global step: 6536,loss: 0.0067000445\n",
            "\n",
            "Global step: 6537,loss: 0.007042705\n",
            "\n",
            "Global step: 6538,loss: 0.006815408\n",
            "\n",
            "Global step: 6539,loss: 0.0079729585\n",
            "\n",
            "Global step: 6540,loss: 0.006690132\n",
            "\n",
            "Global step: 6541,loss: 0.0070900973\n",
            "\n",
            "Global step: 6542,loss: 0.012598759\n",
            "\n",
            "Global step: 6543,loss: 0.0073420214\n",
            "\n",
            "Global step: 6544,loss: 0.008959887\n",
            "\n",
            "Global step: 6545,loss: 0.0072135604\n",
            "\n",
            "Global step: 6546,loss: 0.0069593084\n",
            "\n",
            "Global step: 6547,loss: 0.0070785047\n",
            "\n",
            "Global step: 6548,loss: 0.007472107\n",
            "\n",
            "Global step: 6549,loss: 0.0062179742\n",
            "\n",
            "Global step: 6550,loss: 0.0069907587\n",
            "\n",
            "Global step: 6551,loss: 0.007029713\n",
            "\n",
            "Global step: 6552,loss: 0.0072508883\n",
            "\n",
            "Global step: 6553,loss: 0.0071384814\n",
            "\n",
            "Global step: 6554,loss: 0.0071206153\n",
            "\n",
            "Global step: 6555,loss: 0.0067381165\n",
            "\n",
            "Global step: 6556,loss: 0.007126125\n",
            "\n",
            "Global step: 6557,loss: 0.007073818\n",
            "\n",
            "Global step: 6558,loss: 0.007125565\n",
            "\n",
            "Global step: 6559,loss: 0.0076720063\n",
            "\n",
            "Global step: 6560,loss: 0.0066506173\n",
            "\n",
            "Global step: 6561,loss: 0.007894803\n",
            "\n",
            "Global step: 6562,loss: 0.0075740516\n",
            "\n",
            "Global step: 6563,loss: 0.00624651\n",
            "\n",
            "Global step: 6564,loss: 0.006801825\n",
            "\n",
            "Global step: 6565,loss: 0.006824975\n",
            "\n",
            "Global step: 6566,loss: 0.0076850113\n",
            "\n",
            "Global step: 6567,loss: 0.006846746\n",
            "\n",
            "Global step: 6568,loss: 0.0075555826\n",
            "\n",
            "Global step: 6569,loss: 0.007875613\n",
            "\n",
            "Global step: 6570,loss: 0.0068333996\n",
            "\n",
            "Global step: 6571,loss: 0.006669153\n",
            "\n",
            "Global step: 6572,loss: 0.007829574\n",
            "\n",
            "Global step: 6573,loss: 0.007416902\n",
            "\n",
            "Global step: 6574,loss: 0.0068745413\n",
            "\n",
            "Global step: 6575,loss: 0.008453738\n",
            "\n",
            "Global step: 6576,loss: 0.0064653885\n",
            "\n",
            "Global step: 6577,loss: 0.006913418\n",
            "\n",
            "Global step: 6578,loss: 0.0065989606\n",
            "\n",
            "Global step: 6579,loss: 0.0069265757\n",
            "\n",
            "Global step: 6580,loss: 0.0065412377\n",
            "\n",
            "Global step: 6581,loss: 0.0073232143\n",
            "\n",
            "Global step: 6582,loss: 0.0066401726\n",
            "\n",
            "Global step: 6583,loss: 0.006936311\n",
            "\n",
            "Global step: 6584,loss: 0.0073918756\n",
            "\n",
            "Global step: 6585,loss: 0.006784732\n",
            "\n",
            "Global step: 6586,loss: 0.0066150865\n",
            "\n",
            "Global step: 6587,loss: 0.006976399\n",
            "\n",
            "Global step: 6588,loss: 0.0066540204\n",
            "\n",
            "Global step: 6589,loss: 0.0074072685\n",
            "\n",
            "Global step: 6590,loss: 0.006512254\n",
            "\n",
            "Global step: 6591,loss: 0.007541172\n",
            "\n",
            "Global step: 6592,loss: 0.0070174425\n",
            "\n",
            "Global step: 6593,loss: 0.0062608873\n",
            "\n",
            "Global step: 6594,loss: 0.013164071\n",
            "\n",
            "Global step: 6595,loss: 0.0063570766\n",
            "\n",
            "Global step: 6596,loss: 0.0069598723\n",
            "\n",
            "Global step: 6597,loss: 0.0069182147\n",
            "\n",
            "Global step: 6598,loss: 0.008712974\n",
            "\n",
            "Global step: 6599,loss: 0.006518609\n",
            "\n",
            "Global step: 6600,loss: 0.007779629\n",
            "\n",
            "Global step: 6601,loss: 0.006463561\n",
            "\n",
            "Global step: 6602,loss: 0.007798239\n",
            "\n",
            "Global step: 6603,loss: 0.006149238\n",
            "\n",
            "Global step: 6604,loss: 0.0068009584\n",
            "\n",
            "Global step: 6605,loss: 0.0073269513\n",
            "\n",
            "Global step: 6606,loss: 0.006391297\n",
            "\n",
            "Global step: 6607,loss: 0.0063797208\n",
            "\n",
            "Global step: 6608,loss: 0.0072059026\n",
            "\n",
            "Global step: 6609,loss: 0.0070211077\n",
            "\n",
            "Global step: 6610,loss: 0.007001535\n",
            "\n",
            "Global step: 6611,loss: 0.007273166\n",
            "\n",
            "Global step: 6612,loss: 0.0066962885\n",
            "\n",
            "Global step: 6613,loss: 0.00784504\n",
            "\n",
            "Global step: 6614,loss: 0.0072838636\n",
            "\n",
            "Global step: 6615,loss: 0.0071829315\n",
            "\n",
            "Global step: 6616,loss: 0.007166157\n",
            "\n",
            "Global step: 6617,loss: 0.007212182\n",
            "\n",
            "Global step: 6618,loss: 0.0071731796\n",
            "\n",
            "Global step: 6619,loss: 0.007220139\n",
            "\n",
            "Global step: 6620,loss: 0.011621095\n",
            "\n",
            "Global step: 6621,loss: 0.007052135\n",
            "\n",
            "Global step: 6622,loss: 0.0069765314\n",
            "\n",
            "Global step: 6623,loss: 0.007321707\n",
            "\n",
            "Global step: 6624,loss: 0.0065141926\n",
            "\n",
            "Global step: 6625,loss: 0.006893556\n",
            "\n",
            "Global step: 6626,loss: 0.0076672835\n",
            "\n",
            "Global step: 6627,loss: 0.0065759844\n",
            "\n",
            "Global step: 6628,loss: 0.0070878863\n",
            "\n",
            "Global step: 6629,loss: 0.006997555\n",
            "\n",
            "Global step: 6630,loss: 0.007668159\n",
            "\n",
            "Global step: 6631,loss: 0.007095975\n",
            "\n",
            "Global step: 6632,loss: 0.0063304165\n",
            "\n",
            "Global step: 6633,loss: 0.006790605\n",
            "\n",
            "Global step: 6634,loss: 0.0067412984\n",
            "\n",
            "Global step: 6635,loss: 0.0077017047\n",
            "\n",
            "Global step: 6636,loss: 0.007015266\n",
            "\n",
            "Global step: 6637,loss: 0.007027019\n",
            "\n",
            "Global step: 6638,loss: 0.006941518\n",
            "\n",
            "Global step: 6639,loss: 0.006303562\n",
            "\n",
            "Global step: 6640,loss: 0.00615393\n",
            "\n",
            "Global step: 6641,loss: 0.0069635347\n",
            "\n",
            "Global step: 6642,loss: 0.0065191067\n",
            "\n",
            "Global step: 6643,loss: 0.00656016\n",
            "\n",
            "Global step: 6644,loss: 0.0071574226\n",
            "\n",
            "Global step: 6645,loss: 0.0065205023\n",
            "\n",
            "Global step: 6646,loss: 0.006418579\n",
            "\n",
            "Global step: 6647,loss: 0.007172876\n",
            "\n",
            "Global step: 6648,loss: 0.0069278614\n",
            "\n",
            "Global step: 6649,loss: 0.0067424467\n",
            "\n",
            "Global step: 6650,loss: 0.0064507676\n",
            "\n",
            "Global step: 6651,loss: 0.006911844\n",
            "\n",
            "Global step: 6652,loss: 0.0069038575\n",
            "\n",
            "Global step: 6653,loss: 0.0064575737\n",
            "\n",
            "Global step: 6654,loss: 0.007363422\n",
            "\n",
            "Global step: 6655,loss: 0.00681008\n",
            "\n",
            "Global step: 6656,loss: 0.006209011\n",
            "\n",
            "Global step: 6657,loss: 0.0070621446\n",
            "\n",
            "Global step: 6658,loss: 0.006748671\n",
            "\n",
            "Global step: 6659,loss: 0.007079166\n",
            "\n",
            "Global step: 6660,loss: 0.0073497402\n",
            "\n",
            "Global step: 6661,loss: 0.006668312\n",
            "\n",
            "Global step: 6662,loss: 0.0063710804\n",
            "\n",
            "Global step: 6663,loss: 0.006438767\n",
            "\n",
            "Global step: 6664,loss: 0.0067895735\n",
            "\n",
            "Global step: 6665,loss: 0.0064920294\n",
            "\n",
            "Global step: 6666,loss: 0.0068097194\n",
            "\n",
            "Global step: 6667,loss: 0.0072416943\n",
            "\n",
            "Global step: 6668,loss: 0.007097235\n",
            "\n",
            "Global step: 6669,loss: 0.0064830207\n",
            "\n",
            "Global step: 6670,loss: 0.0066899\n",
            "\n",
            "Global step: 6671,loss: 0.0070038484\n",
            "\n",
            "Global step: 6672,loss: 0.007055574\n",
            "\n",
            "Global step: 6673,loss: 0.0073395385\n",
            "\n",
            "Global step: 6674,loss: 0.007123528\n",
            "\n",
            "Global step: 6675,loss: 0.006436061\n",
            "\n",
            "Global step: 6676,loss: 0.0066072633\n",
            "\n",
            "Global step: 6677,loss: 0.0063434886\n",
            "\n",
            "Global step: 6678,loss: 0.006613963\n",
            "\n",
            "Global step: 6679,loss: 0.006312422\n",
            "\n",
            "Global step: 6680,loss: 0.0070814714\n",
            "\n",
            "Global step: 6681,loss: 0.00767437\n",
            "\n",
            "Global step: 6682,loss: 0.006981894\n",
            "\n",
            "Global step: 6683,loss: 0.0068335133\n",
            "\n",
            "Global step: 6684,loss: 0.006665886\n",
            "\n",
            "Global step: 6685,loss: 0.007340077\n",
            "\n",
            "Global step: 6686,loss: 0.006643386\n",
            "\n",
            "Global step: 6687,loss: 0.0067675165\n",
            "\n",
            "Global step: 6688,loss: 0.007138664\n",
            "\n",
            "Global step: 6689,loss: 0.007083812\n",
            "\n",
            "Global step: 6690,loss: 0.0065917885\n",
            "\n",
            "Global step: 6691,loss: 0.006475298\n",
            "\n",
            "Global step: 6692,loss: 0.0069814064\n",
            "\n",
            "Global step: 6693,loss: 0.0064806896\n",
            "\n",
            "Global step: 6694,loss: 0.0064854557\n",
            "\n",
            "Global step: 6695,loss: 0.008349119\n",
            "\n",
            "Global step: 6696,loss: 0.007157513\n",
            "\n",
            "Global step: 6697,loss: 0.006140823\n",
            "\n",
            "Global step: 6698,loss: 0.0067667593\n",
            "\n",
            "Global step: 6699,loss: 0.007601378\n",
            "\n",
            "Global step: 6700,loss: 0.0064657815\n",
            "\n",
            "Global step: 6701,loss: 0.007015903\n",
            "\n",
            "Global step: 6702,loss: 0.0074974447\n",
            "\n",
            "Global step: 6703,loss: 0.007302956\n",
            "\n",
            "Global step: 6704,loss: 0.0069480897\n",
            "\n",
            "Global step: 6705,loss: 0.0070467056\n",
            "\n",
            "Global step: 6706,loss: 0.0069182543\n",
            "\n",
            "Global step: 6707,loss: 0.00687444\n",
            "\n",
            "Global step: 6708,loss: 0.0064513013\n",
            "\n",
            "Global step: 6709,loss: 0.007030342\n",
            "\n",
            "Global step: 6710,loss: 0.0071089\n",
            "\n",
            "Global step: 6711,loss: 0.0065695248\n",
            "\n",
            "Global step: 6712,loss: 0.0061660125\n",
            "\n",
            "Global step: 6713,loss: 0.007517631\n",
            "\n",
            "Global step: 6714,loss: 0.0070345225\n",
            "\n",
            "Global step: 6715,loss: 0.006437878\n",
            "\n",
            "Global step: 6716,loss: 0.0068558836\n",
            "\n",
            "Global step: 6717,loss: 0.0070369663\n",
            "\n",
            "Global step: 6718,loss: 0.0071005626\n",
            "\n",
            "Global step: 6719,loss: 0.0065219016\n",
            "\n",
            "Global step: 6720,loss: 0.0069267377\n",
            "\n",
            "Global step: 6721,loss: 0.0067077037\n",
            "\n",
            "Global step: 6722,loss: 0.007094929\n",
            "\n",
            "Global step: 6723,loss: 0.007018008\n",
            "\n",
            "Global step: 6724,loss: 0.007482545\n",
            "\n",
            "Global step: 6725,loss: 0.013826983\n",
            "\n",
            "Global step: 6726,loss: 0.006594153\n",
            "\n",
            "Global step: 6727,loss: 0.010229977\n",
            "\n",
            "Global step: 6728,loss: 0.0068952856\n",
            "\n",
            "Global step: 6729,loss: 0.007330385\n",
            "\n",
            "Global step: 6730,loss: 0.0076388917\n",
            "\n",
            "Global step: 6731,loss: 0.007178903\n",
            "\n",
            "Global step: 6732,loss: 0.0086349975\n",
            "\n",
            "Global step: 6733,loss: 0.012572815\n",
            "\n",
            "Global step: 6734,loss: 0.0070916684\n",
            "\n",
            "Global step: 6735,loss: 0.0073150573\n",
            "\n",
            "Global step: 6736,loss: 0.0074558454\n",
            "\n",
            "Global step: 6737,loss: 0.007009763\n",
            "\n",
            "Global step: 6738,loss: 0.0076671755\n",
            "\n",
            "Global step: 6739,loss: 0.0069630793\n",
            "\n",
            "Global step: 6740,loss: 0.007839364\n",
            "\n",
            "Global step: 6741,loss: 0.0070657595\n",
            "\n",
            "Global step: 6742,loss: 0.007100708\n",
            "\n",
            "Global step: 6743,loss: 0.0076666926\n",
            "\n",
            "Global step: 6744,loss: 0.0068680784\n",
            "\n",
            "Global step: 6745,loss: 0.007259429\n",
            "\n",
            "Global step: 6746,loss: 0.0081085265\n",
            "\n",
            "Global step: 6747,loss: 0.006621678\n",
            "\n",
            "Global step: 6748,loss: 0.0074953903\n",
            "\n",
            "Global step: 6749,loss: 0.0074171415\n",
            "\n",
            "Global step: 6750,loss: 0.006679716\n",
            "\n",
            "Global step: 6751,loss: 0.0067931754\n",
            "\n",
            "Global step: 6752,loss: 0.008335829\n",
            "\n",
            "Global step: 6753,loss: 0.0074843597\n",
            "\n",
            "Global step: 6754,loss: 0.007468055\n",
            "\n",
            "Global step: 6755,loss: 0.009530904\n",
            "\n",
            "Global step: 6756,loss: 0.0072439355\n",
            "\n",
            "Global step: 6757,loss: 0.0072436784\n",
            "\n",
            "Global step: 6758,loss: 0.007673458\n",
            "\n",
            "Global step: 6759,loss: 0.00712116\n",
            "\n",
            "Global step: 6760,loss: 0.0068402817\n",
            "\n",
            "Global step: 6761,loss: 0.0070113745\n",
            "\n",
            "Global step: 6762,loss: 0.008138125\n",
            "\n",
            "Global step: 6763,loss: 0.0070179035\n",
            "\n",
            "Global step: 6764,loss: 0.0070883227\n",
            "\n",
            "Global step: 6765,loss: 0.0069962433\n",
            "\n",
            "Global step: 6766,loss: 0.017939847\n",
            "\n",
            "Global step: 6767,loss: 0.007115794\n",
            "\n",
            "Global step: 6768,loss: 0.00681211\n",
            "\n",
            "Global step: 6769,loss: 0.0081781475\n",
            "\n",
            "Global step: 6770,loss: 0.0072999815\n",
            "\n",
            "Global step: 6771,loss: 0.0074333833\n",
            "\n",
            "Global step: 6772,loss: 0.0074422765\n",
            "\n",
            "Global step: 6773,loss: 0.0075786635\n",
            "\n",
            "Global step: 6774,loss: 0.007141144\n",
            "\n",
            "Global step: 6775,loss: 0.0068834135\n",
            "\n",
            "Global step: 6776,loss: 0.006924648\n",
            "\n",
            "Global step: 6777,loss: 0.006697206\n",
            "\n",
            "Global step: 6778,loss: 0.007804568\n",
            "\n",
            "Global step: 6779,loss: 0.006828382\n",
            "\n",
            "Global step: 6780,loss: 0.00688945\n",
            "\n",
            "Global step: 6781,loss: 0.0070027513\n",
            "\n",
            "Global step: 6782,loss: 0.0073208883\n",
            "\n",
            "Global step: 6783,loss: 0.007970501\n",
            "\n",
            "Global step: 6784,loss: 0.0070886165\n",
            "\n",
            "Global step: 6785,loss: 0.0075523006\n",
            "\n",
            "Global step: 6786,loss: 0.0071850196\n",
            "\n",
            "Global step: 6787,loss: 0.006471008\n",
            "\n",
            "Global step: 6788,loss: 0.007484733\n",
            "\n",
            "Global step: 6789,loss: 0.0066385684\n",
            "\n",
            "Global step: 6790,loss: 0.0072264276\n",
            "\n",
            "Global step: 6791,loss: 0.0068576736\n",
            "\n",
            "Global step: 6792,loss: 0.006687369\n",
            "\n",
            "Global step: 6793,loss: 0.006880965\n",
            "\n",
            "Global step: 6794,loss: 0.007736478\n",
            "\n",
            "Global step: 6795,loss: 0.0068322322\n",
            "\n",
            "Global step: 6796,loss: 0.0069424566\n",
            "\n",
            "Global step: 6797,loss: 0.006537702\n",
            "\n",
            "Global step: 6798,loss: 0.0064907894\n",
            "\n",
            "Global step: 6799,loss: 0.0072539775\n",
            "\n",
            "Global step: 6800,loss: 0.0074943234\n",
            "\n",
            "Global step: 6801,loss: 0.007508706\n",
            "\n",
            "Global step: 6802,loss: 0.0069944924\n",
            "\n",
            "Global step: 6803,loss: 0.0069631827\n",
            "\n",
            "Global step: 6804,loss: 0.007333994\n",
            "\n",
            "Global step: 6805,loss: 0.0066496255\n",
            "\n",
            "Global step: 6806,loss: 0.006718297\n",
            "\n",
            "Global step: 6807,loss: 0.007561945\n",
            "\n",
            "Global step: 6808,loss: 0.0067519713\n",
            "\n",
            "Global step: 6809,loss: 0.0077687027\n",
            "\n",
            "Global step: 6810,loss: 0.006245893\n",
            "\n",
            "Global step: 6811,loss: 0.0069491793\n",
            "\n",
            "Global step: 6812,loss: 0.0067982487\n",
            "\n",
            "Global step: 6813,loss: 0.0069862106\n",
            "\n",
            "Global step: 6814,loss: 0.007653066\n",
            "\n",
            "Global step: 6815,loss: 0.0067710276\n",
            "\n",
            "Global step: 6816,loss: 0.007534283\n",
            "\n",
            "Global step: 6817,loss: 0.0066781407\n",
            "\n",
            "Global step: 6818,loss: 0.006990633\n",
            "\n",
            "Global step: 6819,loss: 0.00653451\n",
            "\n",
            "Global step: 6820,loss: 0.0070151435\n",
            "\n",
            "Global step: 6821,loss: 0.006751063\n",
            "\n",
            "Global step: 6822,loss: 0.008594633\n",
            "\n",
            "Global step: 6823,loss: 0.006988083\n",
            "\n",
            "Global step: 6824,loss: 0.007169465\n",
            "\n",
            "Global step: 6825,loss: 0.0071706637\n",
            "\n",
            "Global step: 6826,loss: 0.007091319\n",
            "\n",
            "Global step: 6827,loss: 0.0070141065\n",
            "\n",
            "Global step: 6828,loss: 0.008183525\n",
            "\n",
            "Global step: 6829,loss: 0.0068446007\n",
            "\n",
            "Global step: 6830,loss: 0.0070576495\n",
            "\n",
            "Global step: 6831,loss: 0.0075483774\n",
            "\n",
            "Global step: 6832,loss: 0.0071457354\n",
            "\n",
            "Global step: 6833,loss: 0.0067708134\n",
            "\n",
            "Global step: 6834,loss: 0.0060915407\n",
            "\n",
            "Global step: 6835,loss: 0.0064261197\n",
            "\n",
            "Global step: 6836,loss: 0.0067326077\n",
            "\n",
            "Global step: 6837,loss: 0.0071775294\n",
            "\n",
            "Global step: 6838,loss: 0.0072817616\n",
            "\n",
            "Global step: 6839,loss: 0.0071222913\n",
            "\n",
            "Global step: 6840,loss: 0.0075206356\n",
            "\n",
            "Global step: 6841,loss: 0.006809626\n",
            "\n",
            "Global step: 6842,loss: 0.0071587902\n",
            "\n",
            "Global step: 6843,loss: 0.006771691\n",
            "\n",
            "Global step: 6844,loss: 0.0064267367\n",
            "\n",
            "Global step: 6845,loss: 0.00688435\n",
            "\n",
            "Global step: 6846,loss: 0.0071109165\n",
            "\n",
            "Global step: 6847,loss: 0.007156912\n",
            "\n",
            "Global step: 6848,loss: 0.0077745314\n",
            "\n",
            "Global step: 6849,loss: 0.0065783304\n",
            "\n",
            "Global step: 6850,loss: 0.0062238243\n",
            "\n",
            "Global step: 6851,loss: 0.006452306\n",
            "\n",
            "Global step: 6852,loss: 0.006980044\n",
            "\n",
            "Global step: 6853,loss: 0.0075136274\n",
            "\n",
            "Global step: 6854,loss: 0.00664941\n",
            "\n",
            "Global step: 6855,loss: 0.0066987835\n",
            "\n",
            "Global step: 6856,loss: 0.0090204105\n",
            "\n",
            "Global step: 6857,loss: 0.006890898\n",
            "\n",
            "Global step: 6858,loss: 0.0067319097\n",
            "\n",
            "Global step: 6859,loss: 0.0071284827\n",
            "\n",
            "Global step: 6860,loss: 0.006945967\n",
            "\n",
            "Global step: 6861,loss: 0.0068043866\n",
            "\n",
            "Global step: 6862,loss: 0.0070574135\n",
            "\n",
            "Global step: 6863,loss: 0.0069797924\n",
            "\n",
            "Global step: 6864,loss: 0.007140309\n",
            "\n",
            "Global step: 6865,loss: 0.0068965014\n",
            "\n",
            "Global step: 6866,loss: 0.006727241\n",
            "\n",
            "Global step: 6867,loss: 0.0070807976\n",
            "\n",
            "Global step: 6868,loss: 0.007141306\n",
            "\n",
            "Global step: 6869,loss: 0.0068760337\n",
            "\n",
            "Global step: 6870,loss: 0.006480576\n",
            "\n",
            "Global step: 6871,loss: 0.006605436\n",
            "\n",
            "Global step: 6872,loss: 0.0067549995\n",
            "\n",
            "Global step: 6873,loss: 0.007732601\n",
            "\n",
            "Global step: 6874,loss: 0.007686206\n",
            "\n",
            "Global step: 6875,loss: 0.0069831787\n",
            "\n",
            "Global step: 6876,loss: 0.0069780694\n",
            "\n",
            "Global step: 6877,loss: 0.006611683\n",
            "\n",
            "Global step: 6878,loss: 0.006957592\n",
            "\n",
            "Global step: 6879,loss: 0.0069919666\n",
            "\n",
            "Global step: 6880,loss: 0.0073013934\n",
            "\n",
            "Global step: 6881,loss: 0.0069168797\n",
            "\n",
            "Global step: 6882,loss: 0.0069081\n",
            "\n",
            "Global step: 6883,loss: 0.0070091905\n",
            "\n",
            "Global step: 6884,loss: 0.006841411\n",
            "\n",
            "Global step: 6885,loss: 0.007272725\n",
            "\n",
            "Global step: 6886,loss: 0.007111149\n",
            "\n",
            "Global step: 6887,loss: 0.0072702207\n",
            "\n",
            "Global step: 6888,loss: 0.0068007754\n",
            "\n",
            "Global step: 6889,loss: 0.007180905\n",
            "\n",
            "Global step: 6890,loss: 0.0062988126\n",
            "\n",
            "Global step: 6891,loss: 0.0067474768\n",
            "\n",
            "Global step: 6892,loss: 0.0076177027\n",
            "\n",
            "Global step: 6893,loss: 0.006887271\n",
            "\n",
            "Global step: 6894,loss: 0.007061721\n",
            "\n",
            "Global step: 6895,loss: 0.0071436795\n",
            "\n",
            "Global step: 6896,loss: 0.006773367\n",
            "\n",
            "Global step: 6897,loss: 0.0069305347\n",
            "\n",
            "Global step: 6898,loss: 0.007427621\n",
            "\n",
            "Global step: 6899,loss: 0.00970288\n",
            "\n",
            "Global step: 6900,loss: 0.0068713566\n",
            "\n",
            "Global step: 6901,loss: 0.007088335\n",
            "\n",
            "Global step: 6902,loss: 0.0072784624\n",
            "\n",
            "Global step: 6903,loss: 0.008282846\n",
            "\n",
            "Global step: 6904,loss: 0.007216492\n",
            "\n",
            "Global step: 6905,loss: 0.0076773935\n",
            "\n",
            "Global step: 6906,loss: 0.00693134\n",
            "\n",
            "Global step: 6907,loss: 0.0069445726\n",
            "\n",
            "Global step: 6908,loss: 0.0070664645\n",
            "\n",
            "Global step: 6909,loss: 0.010269865\n",
            "\n",
            "Global step: 6910,loss: 0.0077077756\n",
            "\n",
            "Global step: 6911,loss: 0.007425537\n",
            "\n",
            "Global step: 6912,loss: 0.008313179\n",
            "\n",
            "Global step: 6913,loss: 0.0064546876\n",
            "\n",
            "Global step: 6914,loss: 0.006549629\n",
            "\n",
            "Global step: 6915,loss: 0.00731406\n",
            "\n",
            "Global step: 6916,loss: 0.008218473\n",
            "\n",
            "Global step: 6917,loss: 0.0069268076\n",
            "\n",
            "Global step: 6918,loss: 0.006973093\n",
            "\n",
            "Global step: 6919,loss: 0.007058255\n",
            "\n",
            "Global step: 6920,loss: 0.0073498334\n",
            "\n",
            "Global step: 6921,loss: 0.007066941\n",
            "\n",
            "Global step: 6922,loss: 0.0068800543\n",
            "\n",
            "Global step: 6923,loss: 0.0069393422\n",
            "\n",
            "Global step: 6924,loss: 0.0072127073\n",
            "\n",
            "Global step: 6925,loss: 0.0067914426\n",
            "\n",
            "Global step: 6926,loss: 0.008867379\n",
            "\n",
            "Global step: 6927,loss: 0.006762322\n",
            "\n",
            "Global step: 6928,loss: 0.012039149\n",
            "\n",
            "Global step: 6929,loss: 0.0064597246\n",
            "\n",
            "Global step: 6930,loss: 0.006912633\n",
            "\n",
            "Global step: 6931,loss: 0.007169795\n",
            "\n",
            "Global step: 6932,loss: 0.0077905254\n",
            "\n",
            "Global step: 6933,loss: 0.0068905437\n",
            "\n",
            "Global step: 6934,loss: 0.0083927475\n",
            "\n",
            "Global step: 6935,loss: 0.008030622\n",
            "\n",
            "Global step: 6936,loss: 0.0067116674\n",
            "\n",
            "Global step: 6937,loss: 0.007913922\n",
            "\n",
            "Global step: 6938,loss: 0.009380345\n",
            "\n",
            "Global step: 6939,loss: 0.006887821\n",
            "\n",
            "Global step: 6940,loss: 0.007908726\n",
            "\n",
            "Global step: 6941,loss: 0.0066526583\n",
            "\n",
            "Global step: 6942,loss: 0.007242421\n",
            "\n",
            "Global step: 6943,loss: 0.007858855\n",
            "\n",
            "Global step: 6944,loss: 0.0067934017\n",
            "\n",
            "Global step: 6945,loss: 0.008241451\n",
            "\n",
            "Global step: 6946,loss: 0.007345904\n",
            "\n",
            "Global step: 6947,loss: 0.009171618\n",
            "\n",
            "Global step: 6948,loss: 0.007430842\n",
            "\n",
            "Global step: 6949,loss: 0.008926567\n",
            "\n",
            "Global step: 6950,loss: 0.008417089\n",
            "\n",
            "Global step: 6951,loss: 0.0083451765\n",
            "\n",
            "Global step: 6952,loss: 0.0071452446\n",
            "\n",
            "Global step: 6953,loss: 0.007741619\n",
            "\n",
            "Global step: 6954,loss: 0.0072394023\n",
            "\n",
            "Global step: 6955,loss: 0.0077002738\n",
            "\n",
            "Global step: 6956,loss: 0.007387018\n",
            "\n",
            "Global step: 6957,loss: 0.0071247187\n",
            "\n",
            "Global step: 6958,loss: 0.008066199\n",
            "\n",
            "Global step: 6959,loss: 0.007247971\n",
            "\n",
            "Global step: 6960,loss: 0.008024273\n",
            "\n",
            "Global step: 6961,loss: 0.008320002\n",
            "\n",
            "Global step: 6962,loss: 0.007113398\n",
            "\n",
            "Global step: 6963,loss: 0.012777945\n",
            "\n",
            "Global step: 6964,loss: 0.007314077\n",
            "\n",
            "Global step: 6965,loss: 0.00902749\n",
            "\n",
            "Global step: 6966,loss: 0.0074714525\n",
            "\n",
            "Global step: 6967,loss: 0.008367764\n",
            "\n",
            "Global step: 6968,loss: 0.007470373\n",
            "\n",
            "Global step: 6969,loss: 0.008932397\n",
            "\n",
            "Global step: 6970,loss: 0.008528282\n",
            "\n",
            "Global step: 6971,loss: 0.007637108\n",
            "\n",
            "Global step: 6972,loss: 0.008770512\n",
            "\n",
            "Global step: 6973,loss: 0.008065402\n",
            "\n",
            "Global step: 6974,loss: 0.007166346\n",
            "\n",
            "Global step: 6975,loss: 0.0075347903\n",
            "\n",
            "Global step: 6976,loss: 0.007149138\n",
            "\n",
            "Global step: 6977,loss: 0.009018501\n",
            "\n",
            "Global step: 6978,loss: 0.007108159\n",
            "\n",
            "Global step: 6979,loss: 0.009890475\n",
            "\n",
            "Global step: 6980,loss: 0.009100489\n",
            "\n",
            "Global step: 6981,loss: 0.009059254\n",
            "\n",
            "Global step: 6982,loss: 0.008144331\n",
            "\n",
            "Global step: 6983,loss: 0.0070855618\n",
            "\n",
            "Global step: 6984,loss: 0.010562602\n",
            "\n",
            "Global step: 6985,loss: 0.006941777\n",
            "\n",
            "Global step: 6986,loss: 0.0076984703\n",
            "\n",
            "Global step: 6987,loss: 0.0075658555\n",
            "\n",
            "Global step: 6988,loss: 0.007967704\n",
            "\n",
            "Global step: 6989,loss: 0.007150135\n",
            "\n",
            "Global step: 6990,loss: 0.0077117523\n",
            "\n",
            "Global step: 6991,loss: 0.008578351\n",
            "\n",
            "Global step: 6992,loss: 0.008483328\n",
            "\n",
            "Global step: 6993,loss: 0.008677021\n",
            "\n",
            "Global step: 6994,loss: 0.007224953\n",
            "\n",
            "Global step: 6995,loss: 0.0076978747\n",
            "\n",
            "Global step: 6996,loss: 0.007569856\n",
            "\n",
            "Global step: 6997,loss: 0.009506797\n",
            "\n",
            "Global step: 6998,loss: 0.0075759483\n",
            "\n",
            "Global step: 6999,loss: 0.0072590355\n",
            "\n",
            "Global step: 7000,loss: 0.008842222\n",
            "\n",
            "Global step: 7001,loss: 0.0071293283\n",
            "\n",
            "Global step: 7002,loss: 0.008914217\n",
            "\n",
            "Global step: 7003,loss: 0.0077588256\n",
            "\n",
            "Global step: 7004,loss: 0.008289751\n",
            "\n",
            "Global step: 7005,loss: 0.0077055125\n",
            "\n",
            "Global step: 7006,loss: 0.009131683\n",
            "\n",
            "Global step: 7007,loss: 0.007608394\n",
            "\n",
            "Global step: 7008,loss: 0.007860968\n",
            "\n",
            "Global step: 7009,loss: 0.0073834304\n",
            "\n",
            "Global step: 7010,loss: 0.008687897\n",
            "\n",
            "Global step: 7011,loss: 0.0073432033\n",
            "\n",
            "Global step: 7012,loss: 0.0077815047\n",
            "\n",
            "Global step: 7013,loss: 0.0070926845\n",
            "\n",
            "Global step: 7014,loss: 0.008027901\n",
            "\n",
            "Global step: 7015,loss: 0.011545083\n",
            "\n",
            "Global step: 7016,loss: 0.008733466\n",
            "\n",
            "Global step: 7017,loss: 0.007146935\n",
            "\n",
            "Global step: 7018,loss: 0.008823583\n",
            "\n",
            "Global step: 7019,loss: 0.009063882\n",
            "\n",
            "Global step: 7020,loss: 0.008105083\n",
            "\n",
            "Global step: 7021,loss: 0.0071643787\n",
            "\n",
            "Global step: 7022,loss: 0.009194732\n",
            "\n",
            "Global step: 7023,loss: 0.009923756\n",
            "\n",
            "Global step: 7024,loss: 0.00794631\n",
            "\n",
            "Global step: 7025,loss: 0.009482114\n",
            "\n",
            "Global step: 7026,loss: 0.008212442\n",
            "\n",
            "Global step: 7027,loss: 0.007519195\n",
            "\n",
            "Global step: 7028,loss: 0.0075584017\n",
            "\n",
            "Global step: 7029,loss: 0.0076799914\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 7029,val_loss: 0.009968722867978435\n",
            "\n",
            "Training for epoch 11/15:\n",
            "Global step: 7030,loss: 0.0073125097\n",
            "\n",
            "Global step: 7031,loss: 0.0077785035\n",
            "\n",
            "Global step: 7032,loss: 0.008806275\n",
            "\n",
            "Global step: 7033,loss: 0.0074354373\n",
            "\n",
            "Global step: 7034,loss: 0.00907362\n",
            "\n",
            "Global step: 7035,loss: 0.009024543\n",
            "\n",
            "Global step: 7036,loss: 0.007057119\n",
            "\n",
            "Global step: 7037,loss: 0.007862183\n",
            "\n",
            "Global step: 7038,loss: 0.007557422\n",
            "\n",
            "Global step: 7039,loss: 0.007137399\n",
            "\n",
            "Global step: 7040,loss: 0.007707935\n",
            "\n",
            "Global step: 7041,loss: 0.0073628686\n",
            "\n",
            "Global step: 7042,loss: 0.0069978256\n",
            "\n",
            "Global step: 7043,loss: 0.008193593\n",
            "\n",
            "Global step: 7044,loss: 0.0072365203\n",
            "\n",
            "Global step: 7045,loss: 0.008090818\n",
            "\n",
            "Global step: 7046,loss: 0.008370507\n",
            "\n",
            "Global step: 7047,loss: 0.00664512\n",
            "\n",
            "Global step: 7048,loss: 0.0096871015\n",
            "\n",
            "Global step: 7049,loss: 0.008006536\n",
            "\n",
            "Global step: 7050,loss: 0.0074546942\n",
            "\n",
            "Global step: 7051,loss: 0.007105004\n",
            "\n",
            "Global step: 7052,loss: 0.006939579\n",
            "\n",
            "Global step: 7053,loss: 0.0073895603\n",
            "\n",
            "Global step: 7054,loss: 0.007687968\n",
            "\n",
            "Global step: 7055,loss: 0.0065548876\n",
            "\n",
            "Global step: 7056,loss: 0.006579391\n",
            "\n",
            "Global step: 7057,loss: 0.0067660217\n",
            "\n",
            "Global step: 7058,loss: 0.007473251\n",
            "\n",
            "Global step: 7059,loss: 0.010008317\n",
            "\n",
            "Global step: 7060,loss: 0.009168724\n",
            "\n",
            "Global step: 7061,loss: 0.007618002\n",
            "\n",
            "Global step: 7062,loss: 0.0065727187\n",
            "\n",
            "Global step: 7063,loss: 0.007037679\n",
            "\n",
            "Global step: 7064,loss: 0.00838357\n",
            "\n",
            "Global step: 7065,loss: 0.009114032\n",
            "\n",
            "Global step: 7066,loss: 0.007547294\n",
            "\n",
            "Global step: 7067,loss: 0.0073092356\n",
            "\n",
            "Global step: 7068,loss: 0.007046504\n",
            "\n",
            "Global step: 7069,loss: 0.007470898\n",
            "\n",
            "Global step: 7070,loss: 0.010184948\n",
            "\n",
            "Global step: 7071,loss: 0.006658417\n",
            "\n",
            "Global step: 7072,loss: 0.0070763\n",
            "\n",
            "Global step: 7073,loss: 0.007724623\n",
            "\n",
            "Global step: 7074,loss: 0.0069188694\n",
            "\n",
            "Global step: 7075,loss: 0.007535454\n",
            "\n",
            "Global step: 7076,loss: 0.0070543117\n",
            "\n",
            "Global step: 7077,loss: 0.009248469\n",
            "\n",
            "Global step: 7078,loss: 0.007465578\n",
            "\n",
            "Global step: 7079,loss: 0.007126469\n",
            "\n",
            "Global step: 7080,loss: 0.0067867707\n",
            "\n",
            "Global step: 7081,loss: 0.0069016395\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.40887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:00:35.247331 139837015512832 supervisor.py:1099] global_step/sec: 6.40887\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 7082.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:00:35.257214 139837023905536 supervisor.py:1050] Recording summary at step 7082.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 7082,loss: 0.0066627255\n",
            "\n",
            "Global step: 7083,loss: 0.00850421\n",
            "\n",
            "Global step: 7084,loss: 0.012884987\n",
            "\n",
            "Global step: 7085,loss: 0.006977697\n",
            "\n",
            "Global step: 7086,loss: 0.007936447\n",
            "\n",
            "Global step: 7087,loss: 0.006634986\n",
            "\n",
            "Global step: 7088,loss: 0.007640141\n",
            "\n",
            "Global step: 7089,loss: 0.00715552\n",
            "\n",
            "Global step: 7090,loss: 0.00806897\n",
            "\n",
            "Global step: 7091,loss: 0.009413633\n",
            "\n",
            "Global step: 7092,loss: 0.0070866887\n",
            "\n",
            "Global step: 7093,loss: 0.0067644147\n",
            "\n",
            "Global step: 7094,loss: 0.0074515305\n",
            "\n",
            "Global step: 7095,loss: 0.007696023\n",
            "\n",
            "Global step: 7096,loss: 0.007104613\n",
            "\n",
            "Global step: 7097,loss: 0.007935032\n",
            "\n",
            "Global step: 7098,loss: 0.008803736\n",
            "\n",
            "Global step: 7099,loss: 0.007070124\n",
            "\n",
            "Global step: 7100,loss: 0.006925839\n",
            "\n",
            "Global step: 7101,loss: 0.007585464\n",
            "\n",
            "Global step: 7102,loss: 0.0072093224\n",
            "\n",
            "Global step: 7103,loss: 0.0068536643\n",
            "\n",
            "Global step: 7104,loss: 0.007318038\n",
            "\n",
            "Global step: 7105,loss: 0.0075340793\n",
            "\n",
            "Global step: 7106,loss: 0.006817043\n",
            "\n",
            "Global step: 7107,loss: 0.007091567\n",
            "\n",
            "Global step: 7108,loss: 0.0073036067\n",
            "\n",
            "Global step: 7109,loss: 0.0069098123\n",
            "\n",
            "Global step: 7110,loss: 0.007836454\n",
            "\n",
            "Global step: 7111,loss: 0.0077221915\n",
            "\n",
            "Global step: 7112,loss: 0.0074110134\n",
            "\n",
            "Global step: 7113,loss: 0.006868338\n",
            "\n",
            "Global step: 7114,loss: 0.006975904\n",
            "\n",
            "Global step: 7115,loss: 0.007228169\n",
            "\n",
            "Global step: 7116,loss: 0.0068900427\n",
            "\n",
            "Global step: 7117,loss: 0.011707418\n",
            "\n",
            "Global step: 7118,loss: 0.007631366\n",
            "\n",
            "Global step: 7119,loss: 0.006661602\n",
            "\n",
            "Global step: 7120,loss: 0.009935405\n",
            "\n",
            "Global step: 7121,loss: 0.008244846\n",
            "\n",
            "Global step: 7122,loss: 0.0101904925\n",
            "\n",
            "Global step: 7123,loss: 0.007526975\n",
            "\n",
            "Global step: 7124,loss: 0.009475299\n",
            "\n",
            "Global step: 7125,loss: 0.0070033567\n",
            "\n",
            "Global step: 7126,loss: 0.008170657\n",
            "\n",
            "Global step: 7127,loss: 0.0076208673\n",
            "\n",
            "Global step: 7128,loss: 0.009719156\n",
            "\n",
            "Global step: 7129,loss: 0.007883081\n",
            "\n",
            "Global step: 7130,loss: 0.0071925004\n",
            "\n",
            "Global step: 7131,loss: 0.0070396964\n",
            "\n",
            "Global step: 7132,loss: 0.00739793\n",
            "\n",
            "Global step: 7133,loss: 0.008399639\n",
            "\n",
            "Global step: 7134,loss: 0.008999089\n",
            "\n",
            "Global step: 7135,loss: 0.007826101\n",
            "\n",
            "Global step: 7136,loss: 0.007954413\n",
            "\n",
            "Global step: 7137,loss: 0.0076396097\n",
            "\n",
            "Global step: 7138,loss: 0.0075686546\n",
            "\n",
            "Global step: 7139,loss: 0.0071922434\n",
            "\n",
            "Global step: 7140,loss: 0.00756037\n",
            "\n",
            "Global step: 7141,loss: 0.0071401987\n",
            "\n",
            "Global step: 7142,loss: 0.0069900663\n",
            "\n",
            "Global step: 7143,loss: 0.007316835\n",
            "\n",
            "Global step: 7144,loss: 0.006957765\n",
            "\n",
            "Global step: 7145,loss: 0.00695921\n",
            "\n",
            "Global step: 7146,loss: 0.007780495\n",
            "\n",
            "Global step: 7147,loss: 0.007078453\n",
            "\n",
            "Global step: 7148,loss: 0.0085777715\n",
            "\n",
            "Global step: 7149,loss: 0.007898668\n",
            "\n",
            "Global step: 7150,loss: 0.0073794834\n",
            "\n",
            "Global step: 7151,loss: 0.0073653157\n",
            "\n",
            "Global step: 7152,loss: 0.007781893\n",
            "\n",
            "Global step: 7153,loss: 0.0064442665\n",
            "\n",
            "Global step: 7154,loss: 0.008588186\n",
            "\n",
            "Global step: 7155,loss: 0.0077131325\n",
            "\n",
            "Global step: 7156,loss: 0.00730266\n",
            "\n",
            "Global step: 7157,loss: 0.006902377\n",
            "\n",
            "Global step: 7158,loss: 0.006927522\n",
            "\n",
            "Global step: 7159,loss: 0.008948131\n",
            "\n",
            "Global step: 7160,loss: 0.0070085023\n",
            "\n",
            "Global step: 7161,loss: 0.0072472086\n",
            "\n",
            "Global step: 7162,loss: 0.0076859063\n",
            "\n",
            "Global step: 7163,loss: 0.0071191983\n",
            "\n",
            "Global step: 7164,loss: 0.0069256015\n",
            "\n",
            "Global step: 7165,loss: 0.006860819\n",
            "\n",
            "Global step: 7166,loss: 0.007387759\n",
            "\n",
            "Global step: 7167,loss: 0.006910167\n",
            "\n",
            "Global step: 7168,loss: 0.007442356\n",
            "\n",
            "Global step: 7169,loss: 0.006646774\n",
            "\n",
            "Global step: 7170,loss: 0.0076101124\n",
            "\n",
            "Global step: 7171,loss: 0.006338571\n",
            "\n",
            "Global step: 7172,loss: 0.006892708\n",
            "\n",
            "Global step: 7173,loss: 0.0066365874\n",
            "\n",
            "Global step: 7174,loss: 0.0067962226\n",
            "\n",
            "Global step: 7175,loss: 0.0070764082\n",
            "\n",
            "Global step: 7176,loss: 0.008272562\n",
            "\n",
            "Global step: 7177,loss: 0.00700736\n",
            "\n",
            "Global step: 7178,loss: 0.0070003225\n",
            "\n",
            "Global step: 7179,loss: 0.007309226\n",
            "\n",
            "Global step: 7180,loss: 0.00995805\n",
            "\n",
            "Global step: 7181,loss: 0.007215026\n",
            "\n",
            "Global step: 7182,loss: 0.0074619623\n",
            "\n",
            "Global step: 7183,loss: 0.00670606\n",
            "\n",
            "Global step: 7184,loss: 0.008334174\n",
            "\n",
            "Global step: 7185,loss: 0.007692165\n",
            "\n",
            "Global step: 7186,loss: 0.007824763\n",
            "\n",
            "Global step: 7187,loss: 0.007306176\n",
            "\n",
            "Global step: 7188,loss: 0.0070546265\n",
            "\n",
            "Global step: 7189,loss: 0.007579081\n",
            "\n",
            "Global step: 7190,loss: 0.0075347256\n",
            "\n",
            "Global step: 7191,loss: 0.0070616966\n",
            "\n",
            "Global step: 7192,loss: 0.007707057\n",
            "\n",
            "Global step: 7193,loss: 0.007099701\n",
            "\n",
            "Global step: 7194,loss: 0.0067072622\n",
            "\n",
            "Global step: 7195,loss: 0.007125452\n",
            "\n",
            "Global step: 7196,loss: 0.007814566\n",
            "\n",
            "Global step: 7197,loss: 0.008152768\n",
            "\n",
            "Global step: 7198,loss: 0.009106917\n",
            "\n",
            "Global step: 7199,loss: 0.006978662\n",
            "\n",
            "Global step: 7200,loss: 0.008127755\n",
            "\n",
            "Global step: 7201,loss: 0.0074308943\n",
            "\n",
            "Global step: 7202,loss: 0.0070754294\n",
            "\n",
            "Global step: 7203,loss: 0.006474799\n",
            "\n",
            "Global step: 7204,loss: 0.006360954\n",
            "\n",
            "Global step: 7205,loss: 0.0071903826\n",
            "\n",
            "Global step: 7206,loss: 0.008059876\n",
            "\n",
            "Global step: 7207,loss: 0.006842803\n",
            "\n",
            "Global step: 7208,loss: 0.00736579\n",
            "\n",
            "Global step: 7209,loss: 0.006494072\n",
            "\n",
            "Global step: 7210,loss: 0.013009965\n",
            "\n",
            "Global step: 7211,loss: 0.006990228\n",
            "\n",
            "Global step: 7212,loss: 0.007515528\n",
            "\n",
            "Global step: 7213,loss: 0.006902981\n",
            "\n",
            "Global step: 7214,loss: 0.0070996797\n",
            "\n",
            "Global step: 7215,loss: 0.0073447246\n",
            "\n",
            "Global step: 7216,loss: 0.007647615\n",
            "\n",
            "Global step: 7217,loss: 0.008214923\n",
            "\n",
            "Global step: 7218,loss: 0.0070636175\n",
            "\n",
            "Global step: 7219,loss: 0.007041125\n",
            "\n",
            "Global step: 7220,loss: 0.0072393795\n",
            "\n",
            "Global step: 7221,loss: 0.006453666\n",
            "\n",
            "Global step: 7222,loss: 0.007451206\n",
            "\n",
            "Global step: 7223,loss: 0.0071260473\n",
            "\n",
            "Global step: 7224,loss: 0.009310355\n",
            "\n",
            "Global step: 7225,loss: 0.007400361\n",
            "\n",
            "Global step: 7226,loss: 0.00726406\n",
            "\n",
            "Global step: 7227,loss: 0.0075930557\n",
            "\n",
            "Global step: 7228,loss: 0.007052158\n",
            "\n",
            "Global step: 7229,loss: 0.0076233726\n",
            "\n",
            "Global step: 7230,loss: 0.017217299\n",
            "\n",
            "Global step: 7231,loss: 0.008534024\n",
            "\n",
            "Global step: 7232,loss: 0.007763066\n",
            "\n",
            "Global step: 7233,loss: 0.0074624647\n",
            "\n",
            "Global step: 7234,loss: 0.0064516664\n",
            "\n",
            "Global step: 7235,loss: 0.006912913\n",
            "\n",
            "Global step: 7236,loss: 0.0074664317\n",
            "\n",
            "Global step: 7237,loss: 0.007233535\n",
            "\n",
            "Global step: 7238,loss: 0.008440885\n",
            "\n",
            "Global step: 7239,loss: 0.0068195127\n",
            "\n",
            "Global step: 7240,loss: 0.007345994\n",
            "\n",
            "Global step: 7241,loss: 0.0064381603\n",
            "\n",
            "Global step: 7242,loss: 0.008320066\n",
            "\n",
            "Global step: 7243,loss: 0.0063953446\n",
            "\n",
            "Global step: 7244,loss: 0.0077708494\n",
            "\n",
            "Global step: 7245,loss: 0.0076396763\n",
            "\n",
            "Global step: 7246,loss: 0.008187945\n",
            "\n",
            "Global step: 7247,loss: 0.0066392133\n",
            "\n",
            "Global step: 7248,loss: 0.0074814195\n",
            "\n",
            "Global step: 7249,loss: 0.007764443\n",
            "\n",
            "Global step: 7250,loss: 0.0073659956\n",
            "\n",
            "Global step: 7251,loss: 0.0070583634\n",
            "\n",
            "Global step: 7252,loss: 0.009561244\n",
            "\n",
            "Global step: 7253,loss: 0.0069858497\n",
            "\n",
            "Global step: 7254,loss: 0.0069359667\n",
            "\n",
            "Global step: 7255,loss: 0.0067489236\n",
            "\n",
            "Global step: 7256,loss: 0.006664428\n",
            "\n",
            "Global step: 7257,loss: 0.0077375704\n",
            "\n",
            "Global step: 7258,loss: 0.007248035\n",
            "\n",
            "Global step: 7259,loss: 0.006879719\n",
            "\n",
            "Global step: 7260,loss: 0.011488665\n",
            "\n",
            "Global step: 7261,loss: 0.007876465\n",
            "\n",
            "Global step: 7262,loss: 0.007146554\n",
            "\n",
            "Global step: 7263,loss: 0.007358583\n",
            "\n",
            "Global step: 7264,loss: 0.007290878\n",
            "\n",
            "Global step: 7265,loss: 0.010900447\n",
            "\n",
            "Global step: 7266,loss: 0.00677519\n",
            "\n",
            "Global step: 7267,loss: 0.007881198\n",
            "\n",
            "Global step: 7268,loss: 0.009055333\n",
            "\n",
            "Global step: 7269,loss: 0.007657887\n",
            "\n",
            "Global step: 7270,loss: 0.007913928\n",
            "\n",
            "Global step: 7271,loss: 0.007338931\n",
            "\n",
            "Global step: 7272,loss: 0.006633164\n",
            "\n",
            "Global step: 7273,loss: 0.008039052\n",
            "\n",
            "Global step: 7274,loss: 0.0069240103\n",
            "\n",
            "Global step: 7275,loss: 0.0077447616\n",
            "\n",
            "Global step: 7276,loss: 0.0075700795\n",
            "\n",
            "Global step: 7277,loss: 0.008959277\n",
            "\n",
            "Global step: 7278,loss: 0.0076913894\n",
            "\n",
            "Global step: 7279,loss: 0.007720114\n",
            "\n",
            "Global step: 7280,loss: 0.0074296193\n",
            "\n",
            "Global step: 7281,loss: 0.007829603\n",
            "\n",
            "Global step: 7282,loss: 0.0071996646\n",
            "\n",
            "Global step: 7283,loss: 0.008210077\n",
            "\n",
            "Global step: 7284,loss: 0.00677857\n",
            "\n",
            "Global step: 7285,loss: 0.0064015887\n",
            "\n",
            "Global step: 7286,loss: 0.0072591817\n",
            "\n",
            "Global step: 7287,loss: 0.007106711\n",
            "\n",
            "Global step: 7288,loss: 0.00793303\n",
            "\n",
            "Global step: 7289,loss: 0.007261735\n",
            "\n",
            "Global step: 7290,loss: 0.0073676007\n",
            "\n",
            "Global step: 7291,loss: 0.0065240613\n",
            "\n",
            "Global step: 7292,loss: 0.006505804\n",
            "\n",
            "Global step: 7293,loss: 0.007861676\n",
            "\n",
            "Global step: 7294,loss: 0.007534345\n",
            "\n",
            "Global step: 7295,loss: 0.007076716\n",
            "\n",
            "Global step: 7296,loss: 0.010814028\n",
            "\n",
            "Global step: 7297,loss: 0.0072263535\n",
            "\n",
            "Global step: 7298,loss: 0.007509729\n",
            "\n",
            "Global step: 7299,loss: 0.0068765627\n",
            "\n",
            "Global step: 7300,loss: 0.007022363\n",
            "\n",
            "Global step: 7301,loss: 0.012280658\n",
            "\n",
            "Global step: 7302,loss: 0.0071629086\n",
            "\n",
            "Global step: 7303,loss: 0.007993811\n",
            "\n",
            "Global step: 7304,loss: 0.0071810633\n",
            "\n",
            "Global step: 7305,loss: 0.007096691\n",
            "\n",
            "Global step: 7306,loss: 0.0068332674\n",
            "\n",
            "Global step: 7307,loss: 0.006814346\n",
            "\n",
            "Global step: 7308,loss: 0.007666916\n",
            "\n",
            "Global step: 7309,loss: 0.007497614\n",
            "\n",
            "Global step: 7310,loss: 0.006679822\n",
            "\n",
            "Global step: 7311,loss: 0.007984661\n",
            "\n",
            "Global step: 7312,loss: 0.0081645055\n",
            "\n",
            "Global step: 7313,loss: 0.0069917003\n",
            "\n",
            "Global step: 7314,loss: 0.0072080703\n",
            "\n",
            "Global step: 7315,loss: 0.0067058178\n",
            "\n",
            "Global step: 7316,loss: 0.008576551\n",
            "\n",
            "Global step: 7317,loss: 0.0072853332\n",
            "\n",
            "Global step: 7318,loss: 0.007102193\n",
            "\n",
            "Global step: 7319,loss: 0.007302869\n",
            "\n",
            "Global step: 7320,loss: 0.007288877\n",
            "\n",
            "Global step: 7321,loss: 0.00883198\n",
            "\n",
            "Global step: 7322,loss: 0.0073480196\n",
            "\n",
            "Global step: 7323,loss: 0.012846881\n",
            "\n",
            "Global step: 7324,loss: 0.007167304\n",
            "\n",
            "Global step: 7325,loss: 0.0072510713\n",
            "\n",
            "Global step: 7326,loss: 0.0069121523\n",
            "\n",
            "Global step: 7327,loss: 0.0073557934\n",
            "\n",
            "Global step: 7328,loss: 0.007510487\n",
            "\n",
            "Global step: 7329,loss: 0.006733309\n",
            "\n",
            "Global step: 7330,loss: 0.0077700033\n",
            "\n",
            "Global step: 7331,loss: 0.0068750037\n",
            "\n",
            "Global step: 7332,loss: 0.0075131464\n",
            "\n",
            "Global step: 7333,loss: 0.0067773727\n",
            "\n",
            "Global step: 7334,loss: 0.0067572203\n",
            "\n",
            "Global step: 7335,loss: 0.006731718\n",
            "\n",
            "Global step: 7336,loss: 0.006795453\n",
            "\n",
            "Global step: 7337,loss: 0.0075124246\n",
            "\n",
            "Global step: 7338,loss: 0.0069682207\n",
            "\n",
            "Global step: 7339,loss: 0.0069917585\n",
            "\n",
            "Global step: 7340,loss: 0.007450918\n",
            "\n",
            "Global step: 7341,loss: 0.006285386\n",
            "\n",
            "Global step: 7342,loss: 0.006993124\n",
            "\n",
            "Global step: 7343,loss: 0.006830749\n",
            "\n",
            "Global step: 7344,loss: 0.0072552967\n",
            "\n",
            "Global step: 7345,loss: 0.0071162926\n",
            "\n",
            "Global step: 7346,loss: 0.007790563\n",
            "\n",
            "Global step: 7347,loss: 0.0071652494\n",
            "\n",
            "Global step: 7348,loss: 0.0067191515\n",
            "\n",
            "Global step: 7349,loss: 0.0066617564\n",
            "\n",
            "Global step: 7350,loss: 0.0073628793\n",
            "\n",
            "Global step: 7351,loss: 0.006573196\n",
            "\n",
            "Global step: 7352,loss: 0.0073925513\n",
            "\n",
            "Global step: 7353,loss: 0.0075301677\n",
            "\n",
            "Global step: 7354,loss: 0.006968078\n",
            "\n",
            "Global step: 7355,loss: 0.006497665\n",
            "\n",
            "Global step: 7356,loss: 0.0069133835\n",
            "\n",
            "Global step: 7357,loss: 0.007173579\n",
            "\n",
            "Global step: 7358,loss: 0.006765531\n",
            "\n",
            "Global step: 7359,loss: 0.0063059432\n",
            "\n",
            "Global step: 7360,loss: 0.006129751\n",
            "\n",
            "Global step: 7361,loss: 0.0070537664\n",
            "\n",
            "Global step: 7362,loss: 0.0076326616\n",
            "\n",
            "Global step: 7363,loss: 0.0066324174\n",
            "\n",
            "Global step: 7364,loss: 0.006958024\n",
            "\n",
            "Global step: 7365,loss: 0.0067559546\n",
            "\n",
            "Global step: 7366,loss: 0.0071661193\n",
            "\n",
            "Global step: 7367,loss: 0.0068703555\n",
            "\n",
            "Global step: 7368,loss: 0.007478036\n",
            "\n",
            "Global step: 7369,loss: 0.0066880635\n",
            "\n",
            "Global step: 7370,loss: 0.0068347445\n",
            "\n",
            "Global step: 7371,loss: 0.006641861\n",
            "\n",
            "Global step: 7372,loss: 0.0064181373\n",
            "\n",
            "Global step: 7373,loss: 0.008100431\n",
            "\n",
            "Global step: 7374,loss: 0.008139641\n",
            "\n",
            "Global step: 7375,loss: 0.0074078897\n",
            "\n",
            "Global step: 7376,loss: 0.007124542\n",
            "\n",
            "Global step: 7377,loss: 0.006777711\n",
            "\n",
            "Global step: 7378,loss: 0.0065195966\n",
            "\n",
            "Global step: 7379,loss: 0.006697778\n",
            "\n",
            "Global step: 7380,loss: 0.0066067656\n",
            "\n",
            "Global step: 7381,loss: 0.0069714813\n",
            "\n",
            "Global step: 7382,loss: 0.0070826206\n",
            "\n",
            "Global step: 7383,loss: 0.0075434884\n",
            "\n",
            "Global step: 7384,loss: 0.0062456317\n",
            "\n",
            "Global step: 7385,loss: 0.0065389615\n",
            "\n",
            "Global step: 7386,loss: 0.007779904\n",
            "\n",
            "Global step: 7387,loss: 0.007431413\n",
            "\n",
            "Global step: 7388,loss: 0.0065467656\n",
            "\n",
            "Global step: 7389,loss: 0.008728983\n",
            "\n",
            "Global step: 7390,loss: 0.0067752693\n",
            "\n",
            "Global step: 7391,loss: 0.0065881717\n",
            "\n",
            "Global step: 7392,loss: 0.0069288523\n",
            "\n",
            "Global step: 7393,loss: 0.0066922363\n",
            "\n",
            "Global step: 7394,loss: 0.008656606\n",
            "\n",
            "Global step: 7395,loss: 0.0072033624\n",
            "\n",
            "Global step: 7396,loss: 0.0065429094\n",
            "\n",
            "Global step: 7397,loss: 0.0065044537\n",
            "\n",
            "Global step: 7398,loss: 0.0068467413\n",
            "\n",
            "Global step: 7399,loss: 0.0072219353\n",
            "\n",
            "Global step: 7400,loss: 0.0075145164\n",
            "\n",
            "Global step: 7401,loss: 0.010281544\n",
            "\n",
            "Global step: 7402,loss: 0.007202821\n",
            "\n",
            "Global step: 7403,loss: 0.008698635\n",
            "\n",
            "Global step: 7404,loss: 0.0071733147\n",
            "\n",
            "Global step: 7405,loss: 0.006668907\n",
            "\n",
            "Global step: 7406,loss: 0.0077135377\n",
            "\n",
            "Global step: 7407,loss: 0.0068559116\n",
            "\n",
            "Global step: 7408,loss: 0.010280348\n",
            "\n",
            "Global step: 7409,loss: 0.007188345\n",
            "\n",
            "Global step: 7410,loss: 0.0065942826\n",
            "\n",
            "Global step: 7411,loss: 0.00781982\n",
            "\n",
            "Global step: 7412,loss: 0.008692457\n",
            "\n",
            "Global step: 7413,loss: 0.0069048596\n",
            "\n",
            "Global step: 7414,loss: 0.0065034917\n",
            "\n",
            "Global step: 7415,loss: 0.0068821954\n",
            "\n",
            "Global step: 7416,loss: 0.008155853\n",
            "\n",
            "Global step: 7417,loss: 0.006968834\n",
            "\n",
            "Global step: 7418,loss: 0.008485929\n",
            "\n",
            "Global step: 7419,loss: 0.0067753936\n",
            "\n",
            "Global step: 7420,loss: 0.008107081\n",
            "\n",
            "Global step: 7421,loss: 0.006667099\n",
            "\n",
            "Global step: 7422,loss: 0.007498311\n",
            "\n",
            "Global step: 7423,loss: 0.011406401\n",
            "\n",
            "Global step: 7424,loss: 0.010226171\n",
            "\n",
            "Global step: 7425,loss: 0.0068200734\n",
            "\n",
            "Global step: 7426,loss: 0.0077596446\n",
            "\n",
            "Global step: 7427,loss: 0.0070860414\n",
            "\n",
            "Global step: 7428,loss: 0.0067761964\n",
            "\n",
            "Global step: 7429,loss: 0.0121739665\n",
            "\n",
            "Global step: 7430,loss: 0.0072576567\n",
            "\n",
            "Global step: 7431,loss: 0.007890606\n",
            "\n",
            "Global step: 7432,loss: 0.011058555\n",
            "\n",
            "Global step: 7433,loss: 0.0068758065\n",
            "\n",
            "Global step: 7434,loss: 0.01134414\n",
            "\n",
            "Global step: 7435,loss: 0.010763066\n",
            "\n",
            "Global step: 7436,loss: 0.009085165\n",
            "\n",
            "Global step: 7437,loss: 0.007076347\n",
            "\n",
            "Global step: 7438,loss: 0.008507302\n",
            "\n",
            "Global step: 7439,loss: 0.010624988\n",
            "\n",
            "Global step: 7440,loss: 0.007869284\n",
            "\n",
            "Global step: 7441,loss: 0.007373121\n",
            "\n",
            "Global step: 7442,loss: 0.008471516\n",
            "\n",
            "Global step: 7443,loss: 0.007561397\n",
            "\n",
            "Global step: 7444,loss: 0.008094134\n",
            "\n",
            "Global step: 7445,loss: 0.009543827\n",
            "\n",
            "Global step: 7446,loss: 0.008172212\n",
            "\n",
            "Global step: 7447,loss: 0.009215257\n",
            "\n",
            "Global step: 7448,loss: 0.007729063\n",
            "\n",
            "Global step: 7449,loss: 0.007119211\n",
            "\n",
            "Global step: 7450,loss: 0.0074561383\n",
            "\n",
            "Global step: 7451,loss: 0.009161953\n",
            "\n",
            "Global step: 7452,loss: 0.0077457563\n",
            "\n",
            "Global step: 7453,loss: 0.007405161\n",
            "\n",
            "Global step: 7454,loss: 0.007419778\n",
            "\n",
            "Global step: 7455,loss: 0.0069980714\n",
            "\n",
            "Global step: 7456,loss: 0.008435315\n",
            "\n",
            "Global step: 7457,loss: 0.008000391\n",
            "\n",
            "Global step: 7458,loss: 0.008124694\n",
            "\n",
            "Global step: 7459,loss: 0.008404605\n",
            "\n",
            "Global step: 7460,loss: 0.00782628\n",
            "\n",
            "Global step: 7461,loss: 0.007908436\n",
            "\n",
            "Global step: 7462,loss: 0.007870012\n",
            "\n",
            "Global step: 7463,loss: 0.0069305706\n",
            "\n",
            "Global step: 7464,loss: 0.008203781\n",
            "\n",
            "Global step: 7465,loss: 0.0077637387\n",
            "\n",
            "Global step: 7466,loss: 0.007252194\n",
            "\n",
            "Global step: 7467,loss: 0.007384966\n",
            "\n",
            "Global step: 7468,loss: 0.006746953\n",
            "\n",
            "Global step: 7469,loss: 0.009316855\n",
            "\n",
            "Global step: 7470,loss: 0.0072008073\n",
            "\n",
            "Global step: 7471,loss: 0.006711393\n",
            "\n",
            "Global step: 7472,loss: 0.0079062255\n",
            "\n",
            "Global step: 7473,loss: 0.0069704503\n",
            "\n",
            "Global step: 7474,loss: 0.007780904\n",
            "\n",
            "Global step: 7475,loss: 0.0068430584\n",
            "\n",
            "Global step: 7476,loss: 0.009403706\n",
            "\n",
            "Global step: 7477,loss: 0.01232666\n",
            "\n",
            "Global step: 7478,loss: 0.007473342\n",
            "\n",
            "Global step: 7479,loss: 0.0077618104\n",
            "\n",
            "Global step: 7480,loss: 0.0073950593\n",
            "\n",
            "Global step: 7481,loss: 0.0073881694\n",
            "\n",
            "Global step: 7482,loss: 0.0077157263\n",
            "\n",
            "Global step: 7483,loss: 0.0064636706\n",
            "\n",
            "Global step: 7484,loss: 0.0105157085\n",
            "\n",
            "Global step: 7485,loss: 0.007114884\n",
            "\n",
            "Global step: 7486,loss: 0.008731968\n",
            "\n",
            "Global step: 7487,loss: 0.016781729\n",
            "\n",
            "Global step: 7488,loss: 0.0086760055\n",
            "\n",
            "Global step: 7489,loss: 0.007235616\n",
            "\n",
            "Global step: 7490,loss: 0.0082104765\n",
            "\n",
            "Global step: 7491,loss: 0.007240038\n",
            "\n",
            "Global step: 7492,loss: 0.00702025\n",
            "\n",
            "Global step: 7493,loss: 0.009259612\n",
            "\n",
            "Global step: 7494,loss: 0.008289194\n",
            "\n",
            "Global step: 7495,loss: 0.0080065\n",
            "\n",
            "Global step: 7496,loss: 0.0074554523\n",
            "\n",
            "Global step: 7497,loss: 0.006963628\n",
            "\n",
            "Global step: 7498,loss: 0.007070192\n",
            "\n",
            "Global step: 7499,loss: 0.007374836\n",
            "\n",
            "Global step: 7500,loss: 0.007773574\n",
            "\n",
            "Global step: 7501,loss: 0.007924587\n",
            "\n",
            "Global step: 7502,loss: 0.0069136256\n",
            "\n",
            "Global step: 7503,loss: 0.008134556\n",
            "\n",
            "Global step: 7504,loss: 0.008308943\n",
            "\n",
            "Global step: 7505,loss: 0.008001182\n",
            "\n",
            "Global step: 7506,loss: 0.0074992697\n",
            "\n",
            "Global step: 7507,loss: 0.0068599004\n",
            "\n",
            "Global step: 7508,loss: 0.006532726\n",
            "\n",
            "Global step: 7509,loss: 0.0068936385\n",
            "\n",
            "Global step: 7510,loss: 0.0071832514\n",
            "\n",
            "Global step: 7511,loss: 0.0071891784\n",
            "\n",
            "Global step: 7512,loss: 0.008019629\n",
            "\n",
            "Global step: 7513,loss: 0.0068104495\n",
            "\n",
            "Global step: 7514,loss: 0.008843866\n",
            "\n",
            "Global step: 7515,loss: 0.008373706\n",
            "\n",
            "Global step: 7516,loss: 0.008948233\n",
            "\n",
            "Global step: 7517,loss: 0.00722207\n",
            "\n",
            "Global step: 7518,loss: 0.009770347\n",
            "\n",
            "Global step: 7519,loss: 0.0074201217\n",
            "\n",
            "Global step: 7520,loss: 0.0075928494\n",
            "\n",
            "Global step: 7521,loss: 0.0074896514\n",
            "\n",
            "Global step: 7522,loss: 0.0071928767\n",
            "\n",
            "Global step: 7523,loss: 0.007619948\n",
            "\n",
            "Global step: 7524,loss: 0.009118881\n",
            "\n",
            "Global step: 7525,loss: 0.008941855\n",
            "\n",
            "Global step: 7526,loss: 0.0089732995\n",
            "\n",
            "Global step: 7527,loss: 0.007287519\n",
            "\n",
            "Global step: 7528,loss: 0.007617021\n",
            "\n",
            "Global step: 7529,loss: 0.008123044\n",
            "\n",
            "Global step: 7530,loss: 0.008352761\n",
            "\n",
            "Global step: 7531,loss: 0.0065587536\n",
            "\n",
            "Global step: 7532,loss: 0.0069660926\n",
            "\n",
            "Global step: 7533,loss: 0.0069082584\n",
            "\n",
            "Global step: 7534,loss: 0.008229568\n",
            "\n",
            "Global step: 7535,loss: 0.007103646\n",
            "\n",
            "Global step: 7536,loss: 0.00741424\n",
            "\n",
            "Global step: 7537,loss: 0.00676684\n",
            "\n",
            "Global step: 7538,loss: 0.007767318\n",
            "\n",
            "Global step: 7539,loss: 0.008691972\n",
            "\n",
            "Global step: 7540,loss: 0.006611377\n",
            "\n",
            "Global step: 7541,loss: 0.0070924507\n",
            "\n",
            "Global step: 7542,loss: 0.0080334265\n",
            "\n",
            "Global step: 7543,loss: 0.007097032\n",
            "\n",
            "Global step: 7544,loss: 0.0067228344\n",
            "\n",
            "Global step: 7545,loss: 0.008370766\n",
            "\n",
            "Global step: 7546,loss: 0.009184855\n",
            "\n",
            "Global step: 7547,loss: 0.0107426755\n",
            "\n",
            "Global step: 7548,loss: 0.0071344734\n",
            "\n",
            "Global step: 7549,loss: 0.0073111504\n",
            "\n",
            "Global step: 7550,loss: 0.008834004\n",
            "\n",
            "Global step: 7551,loss: 0.006949804\n",
            "\n",
            "Global step: 7552,loss: 0.007666705\n",
            "\n",
            "Global step: 7553,loss: 0.0071440074\n",
            "\n",
            "Global step: 7554,loss: 0.007915238\n",
            "\n",
            "Global step: 7555,loss: 0.006933807\n",
            "\n",
            "Global step: 7556,loss: 0.006696658\n",
            "\n",
            "Global step: 7557,loss: 0.009131468\n",
            "\n",
            "Global step: 7558,loss: 0.006727082\n",
            "\n",
            "Global step: 7559,loss: 0.0099235\n",
            "\n",
            "Global step: 7560,loss: 0.0137427\n",
            "\n",
            "Global step: 7561,loss: 0.0073526967\n",
            "\n",
            "Global step: 7562,loss: 0.006838911\n",
            "\n",
            "Global step: 7563,loss: 0.008242506\n",
            "\n",
            "Global step: 7564,loss: 0.007364148\n",
            "\n",
            "Global step: 7565,loss: 0.008510603\n",
            "\n",
            "Global step: 7566,loss: 0.007999216\n",
            "\n",
            "Global step: 7567,loss: 0.0063199783\n",
            "\n",
            "Global step: 7568,loss: 0.0081726955\n",
            "\n",
            "Global step: 7569,loss: 0.008011423\n",
            "\n",
            "Global step: 7570,loss: 0.007953339\n",
            "\n",
            "Global step: 7571,loss: 0.008415803\n",
            "\n",
            "Global step: 7572,loss: 0.008870159\n",
            "\n",
            "Global step: 7573,loss: 0.007540719\n",
            "\n",
            "Global step: 7574,loss: 0.0078221625\n",
            "\n",
            "Global step: 7575,loss: 0.007207234\n",
            "\n",
            "Global step: 7576,loss: 0.0070925555\n",
            "\n",
            "Global step: 7577,loss: 0.008706432\n",
            "\n",
            "Global step: 7578,loss: 0.007996177\n",
            "\n",
            "Global step: 7579,loss: 0.007123216\n",
            "\n",
            "Global step: 7580,loss: 0.006701025\n",
            "\n",
            "Global step: 7581,loss: 0.014116067\n",
            "\n",
            "Global step: 7582,loss: 0.0071811033\n",
            "\n",
            "Global step: 7583,loss: 0.008134279\n",
            "\n",
            "Global step: 7584,loss: 0.0072302194\n",
            "\n",
            "Global step: 7585,loss: 0.008293963\n",
            "\n",
            "Global step: 7586,loss: 0.010395796\n",
            "\n",
            "Global step: 7587,loss: 0.0076923603\n",
            "\n",
            "Global step: 7588,loss: 0.0073546306\n",
            "\n",
            "Global step: 7589,loss: 0.0069400957\n",
            "\n",
            "Global step: 7590,loss: 0.010937959\n",
            "\n",
            "Global step: 7591,loss: 0.0071567777\n",
            "\n",
            "Global step: 7592,loss: 0.006827369\n",
            "\n",
            "Global step: 7593,loss: 0.0072939103\n",
            "\n",
            "Global step: 7594,loss: 0.009221949\n",
            "\n",
            "Global step: 7595,loss: 0.009708732\n",
            "\n",
            "Global step: 7596,loss: 0.007392286\n",
            "\n",
            "Global step: 7597,loss: 0.0077055334\n",
            "\n",
            "Global step: 7598,loss: 0.008611463\n",
            "\n",
            "Global step: 7599,loss: 0.007067822\n",
            "\n",
            "Global step: 7600,loss: 0.007468363\n",
            "\n",
            "Global step: 7601,loss: 0.008526546\n",
            "\n",
            "Global step: 7602,loss: 0.007351443\n",
            "\n",
            "Global step: 7603,loss: 0.008376094\n",
            "\n",
            "Global step: 7604,loss: 0.008355007\n",
            "\n",
            "Global step: 7605,loss: 0.0099842325\n",
            "\n",
            "Global step: 7606,loss: 0.0072177835\n",
            "\n",
            "Global step: 7607,loss: 0.0070709297\n",
            "\n",
            "Global step: 7608,loss: 0.008630032\n",
            "\n",
            "Global step: 7609,loss: 0.019287938\n",
            "\n",
            "Global step: 7610,loss: 0.007924037\n",
            "\n",
            "Global step: 7611,loss: 0.009720523\n",
            "\n",
            "Global step: 7612,loss: 0.007301086\n",
            "\n",
            "Global step: 7613,loss: 0.007338709\n",
            "\n",
            "Global step: 7614,loss: 0.008219234\n",
            "\n",
            "Global step: 7615,loss: 0.0068612923\n",
            "\n",
            "Global step: 7616,loss: 0.007284288\n",
            "\n",
            "Global step: 7617,loss: 0.007546196\n",
            "\n",
            "Global step: 7618,loss: 0.007155226\n",
            "\n",
            "Global step: 7619,loss: 0.009824282\n",
            "\n",
            "Global step: 7620,loss: 0.007826755\n",
            "\n",
            "Global step: 7621,loss: 0.008025226\n",
            "\n",
            "Global step: 7622,loss: 0.0073468126\n",
            "\n",
            "Global step: 7623,loss: 0.0071625784\n",
            "\n",
            "Global step: 7624,loss: 0.0072792144\n",
            "\n",
            "Global step: 7625,loss: 0.008836526\n",
            "\n",
            "Global step: 7626,loss: 0.0064811236\n",
            "\n",
            "Global step: 7627,loss: 0.0077889655\n",
            "\n",
            "Global step: 7628,loss: 0.01237249\n",
            "\n",
            "Global step: 7629,loss: 0.007491003\n",
            "\n",
            "Global step: 7630,loss: 0.006869453\n",
            "\n",
            "Global step: 7631,loss: 0.0086066\n",
            "\n",
            "Global step: 7632,loss: 0.0073103234\n",
            "\n",
            "Global step: 7633,loss: 0.0072593163\n",
            "\n",
            "Global step: 7634,loss: 0.009663132\n",
            "\n",
            "Global step: 7635,loss: 0.007893071\n",
            "\n",
            "Global step: 7636,loss: 0.007296497\n",
            "\n",
            "Global step: 7637,loss: 0.007784431\n",
            "\n",
            "Global step: 7638,loss: 0.007952219\n",
            "\n",
            "Global step: 7639,loss: 0.014698543\n",
            "\n",
            "Global step: 7640,loss: 0.0074111824\n",
            "\n",
            "Global step: 7641,loss: 0.0085061025\n",
            "\n",
            "Global step: 7642,loss: 0.00679758\n",
            "\n",
            "Global step: 7643,loss: 0.008015115\n",
            "\n",
            "Global step: 7644,loss: 0.008970453\n",
            "\n",
            "Global step: 7645,loss: 0.008651538\n",
            "\n",
            "Global step: 7646,loss: 0.009073596\n",
            "\n",
            "Global step: 7647,loss: 0.008746343\n",
            "\n",
            "Global step: 7648,loss: 0.007365377\n",
            "\n",
            "Global step: 7649,loss: 0.0069828513\n",
            "\n",
            "Global step: 7650,loss: 0.010512434\n",
            "\n",
            "Global step: 7651,loss: 0.0069068526\n",
            "\n",
            "Global step: 7652,loss: 0.0072938753\n",
            "\n",
            "Global step: 7653,loss: 0.009175028\n",
            "\n",
            "Global step: 7654,loss: 0.0070991805\n",
            "\n",
            "Global step: 7655,loss: 0.010163842\n",
            "\n",
            "Global step: 7656,loss: 0.006772705\n",
            "\n",
            "Global step: 7657,loss: 0.009632094\n",
            "\n",
            "Global step: 7658,loss: 0.009820842\n",
            "\n",
            "Global step: 7659,loss: 0.0070466665\n",
            "\n",
            "Global step: 7660,loss: 0.007568102\n",
            "\n",
            "Global step: 7661,loss: 0.0071153888\n",
            "\n",
            "Global step: 7662,loss: 0.0066822064\n",
            "\n",
            "Global step: 7663,loss: 0.0075618513\n",
            "\n",
            "Global step: 7664,loss: 0.008920337\n",
            "\n",
            "Global step: 7665,loss: 0.006933428\n",
            "\n",
            "Global step: 7666,loss: 0.011847101\n",
            "\n",
            "Global step: 7667,loss: 0.008787103\n",
            "\n",
            "Global step: 7668,loss: 0.0074808877\n",
            "\n",
            "Global step: 7669,loss: 0.0075074565\n",
            "\n",
            "Global step: 7670,loss: 0.0073386147\n",
            "\n",
            "Global step: 7671,loss: 0.016413016\n",
            "\n",
            "Global step: 7672,loss: 0.0073396093\n",
            "\n",
            "Global step: 7673,loss: 0.007014402\n",
            "\n",
            "Global step: 7674,loss: 0.007256853\n",
            "\n",
            "Global step: 7675,loss: 0.007737312\n",
            "\n",
            "Global step: 7676,loss: 0.010527555\n",
            "\n",
            "Global step: 7677,loss: 0.007183156\n",
            "\n",
            "Global step: 7678,loss: 0.009283572\n",
            "\n",
            "Global step: 7679,loss: 0.010425663\n",
            "\n",
            "Global step: 7680,loss: 0.008488601\n",
            "\n",
            "Global step: 7681,loss: 0.0080211\n",
            "\n",
            "Global step: 7682,loss: 0.0077430317\n",
            "\n",
            "Global step: 7683,loss: 0.006752081\n",
            "\n",
            "Global step: 7684,loss: 0.011999536\n",
            "\n",
            "Global step: 7685,loss: 0.01111352\n",
            "\n",
            "Global step: 7686,loss: 0.008339203\n",
            "\n",
            "Global step: 7687,loss: 0.009060671\n",
            "\n",
            "Global step: 7688,loss: 0.010019824\n",
            "\n",
            "Global step: 7689,loss: 0.0072796415\n",
            "\n",
            "Global step: 7690,loss: 0.00812507\n",
            "\n",
            "Global step: 7691,loss: 0.008028437\n",
            "\n",
            "Global step: 7692,loss: 0.009146653\n",
            "\n",
            "Global step: 7693,loss: 0.008748638\n",
            "\n",
            "Global step: 7694,loss: 0.016754579\n",
            "\n",
            "Global step: 7695,loss: 0.006846066\n",
            "\n",
            "Global step: 7696,loss: 0.007824364\n",
            "\n",
            "Global step: 7697,loss: 0.008096518\n",
            "\n",
            "Global step: 7698,loss: 0.0076259254\n",
            "\n",
            "Global step: 7699,loss: 0.0066590183\n",
            "\n",
            "Global step: 7700,loss: 0.007527956\n",
            "\n",
            "Global step: 7701,loss: 0.007311075\n",
            "\n",
            "Global step: 7702,loss: 0.015137234\n",
            "\n",
            "Global step: 7703,loss: 0.007437732\n",
            "\n",
            "Global step: 7704,loss: 0.0075862287\n",
            "\n",
            "Global step: 7705,loss: 0.0114109935\n",
            "\n",
            "Global step: 7706,loss: 0.008256607\n",
            "\n",
            "Global step: 7707,loss: 0.008816269\n",
            "\n",
            "Global step: 7708,loss: 0.0124478005\n",
            "\n",
            "Global step: 7709,loss: 0.008607848\n",
            "\n",
            "Global step: 7710,loss: 0.007869202\n",
            "\n",
            "Global step: 7711,loss: 0.008152416\n",
            "\n",
            "Global step: 7712,loss: 0.00714944\n",
            "\n",
            "Global step: 7713,loss: 0.008943649\n",
            "\n",
            "Global step: 7714,loss: 0.008018711\n",
            "\n",
            "Global step: 7715,loss: 0.008642091\n",
            "\n",
            "Global step: 7716,loss: 0.008984809\n",
            "\n",
            "Global step: 7717,loss: 0.0072486023\n",
            "\n",
            "Global step: 7718,loss: 0.00829095\n",
            "\n",
            "Global step: 7719,loss: 0.007175767\n",
            "\n",
            "Global step: 7720,loss: 0.007897159\n",
            "\n",
            "Global step: 7721,loss: 0.007913407\n",
            "\n",
            "Global step: 7722,loss: 0.008798454\n",
            "\n",
            "Global step: 7723,loss: 0.008738039\n",
            "\n",
            "Global step: 7724,loss: 0.009209836\n",
            "\n",
            "Global step: 7725,loss: 0.0072504347\n",
            "\n",
            "Global step: 7726,loss: 0.008035585\n",
            "\n",
            "Global step: 7727,loss: 0.016213331\n",
            "\n",
            "Global step: 7728,loss: 0.0070379567\n",
            "\n",
            "Global step: 7729,loss: 0.008311521\n",
            "\n",
            "Global step: 7730,loss: 0.007074024\n",
            "\n",
            "Global step: 7731,loss: 0.008554556\n",
            "\n",
            "Global step: 7732,loss: 0.006915319\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 7732,val_loss: 0.009207269622602014\n",
            "\n",
            "Training for epoch 12/15:\n",
            "Global step: 7733,loss: 0.008149052\n",
            "\n",
            "Global step: 7734,loss: 0.0075970935\n",
            "\n",
            "Global step: 7735,loss: 0.008852644\n",
            "\n",
            "Global step: 7736,loss: 0.012860501\n",
            "\n",
            "Global step: 7737,loss: 0.0074053714\n",
            "\n",
            "Global step: 7738,loss: 0.008222448\n",
            "\n",
            "Global step: 7739,loss: 0.007404092\n",
            "\n",
            "Global step: 7740,loss: 0.008262546\n",
            "\n",
            "Global step: 7741,loss: 0.007390262\n",
            "\n",
            "Global step: 7742,loss: 0.007654001\n",
            "\n",
            "Global step: 7743,loss: 0.0070970715\n",
            "\n",
            "Global step: 7744,loss: 0.0072239693\n",
            "\n",
            "Global step: 7745,loss: 0.010514788\n",
            "\n",
            "Global step: 7746,loss: 0.008321649\n",
            "\n",
            "Global step: 7747,loss: 0.0081922095\n",
            "\n",
            "Global step: 7748,loss: 0.00667612\n",
            "\n",
            "Global step: 7749,loss: 0.008188997\n",
            "\n",
            "Global step: 7750,loss: 0.01345163\n",
            "\n",
            "Global step: 7751,loss: 0.0077225426\n",
            "\n",
            "Global step: 7752,loss: 0.007148394\n",
            "\n",
            "Global step: 7753,loss: 0.007960761\n",
            "\n",
            "Global step: 7754,loss: 0.011465419\n",
            "\n",
            "Global step: 7755,loss: 0.013154748\n",
            "\n",
            "Global step: 7756,loss: 0.0071401\n",
            "\n",
            "Global step: 7757,loss: 0.007864827\n",
            "\n",
            "Global step: 7758,loss: 0.008884301\n",
            "\n",
            "Global step: 7759,loss: 0.0072946837\n",
            "\n",
            "Global step: 7760,loss: 0.0075907344\n",
            "\n",
            "Global step: 7761,loss: 0.0072944574\n",
            "\n",
            "Global step: 7762,loss: 0.008066823\n",
            "\n",
            "Global step: 7763,loss: 0.010040906\n",
            "\n",
            "Global step: 7764,loss: 0.008679659\n",
            "\n",
            "Global step: 7765,loss: 0.0068613226\n",
            "\n",
            "Global step: 7766,loss: 0.00684262\n",
            "\n",
            "Global step: 7767,loss: 0.0071231844\n",
            "\n",
            "Global step: 7768,loss: 0.007615003\n",
            "\n",
            "Global step: 7769,loss: 0.006371391\n",
            "\n",
            "Global step: 7770,loss: 0.007278376\n",
            "\n",
            "Global step: 7771,loss: 0.0072904844\n",
            "\n",
            "Global step: 7772,loss: 0.007814693\n",
            "\n",
            "Global step: 7773,loss: 0.0072072316\n",
            "\n",
            "Global step: 7774,loss: 0.007928699\n",
            "\n",
            "Global step: 7775,loss: 0.0061038714\n",
            "\n",
            "Global step: 7776,loss: 0.006741242\n",
            "\n",
            "Global step: 7777,loss: 0.009086506\n",
            "\n",
            "Global step: 7778,loss: 0.0069176373\n",
            "\n",
            "Global step: 7779,loss: 0.0072807088\n",
            "\n",
            "Global step: 7780,loss: 0.00692061\n",
            "\n",
            "Global step: 7781,loss: 0.007516239\n",
            "\n",
            "Global step: 7782,loss: 0.008734701\n",
            "\n",
            "Global step: 7783,loss: 0.0062478986\n",
            "\n",
            "Global step: 7784,loss: 0.006720139\n",
            "\n",
            "Global step: 7785,loss: 0.006887705\n",
            "\n",
            "Global step: 7786,loss: 0.0070258547\n",
            "\n",
            "Global step: 7787,loss: 0.0069481423\n",
            "\n",
            "Global step: 7788,loss: 0.0069836727\n",
            "\n",
            "Global step: 7789,loss: 0.0072042886\n",
            "\n",
            "Global step: 7790,loss: 0.0067241453\n",
            "\n",
            "Global step: 7791,loss: 0.0071101356\n",
            "\n",
            "Global step: 7792,loss: 0.0071742777\n",
            "\n",
            "Global step: 7793,loss: 0.006407887\n",
            "\n",
            "Global step: 7794,loss: 0.0063013732\n",
            "\n",
            "Global step: 7795,loss: 0.0064234957\n",
            "\n",
            "Global step: 7796,loss: 0.0068213176\n",
            "\n",
            "Global step: 7797,loss: 0.0070234323\n",
            "\n",
            "Global step: 7798,loss: 0.0063099395\n",
            "\n",
            "Global step: 7799,loss: 0.006241378\n",
            "\n",
            "Global step: 7800,loss: 0.006694815\n",
            "\n",
            "Global step: 7801,loss: 0.0072665894\n",
            "\n",
            "Global step: 7802,loss: 0.0066070436\n",
            "\n",
            "Global step: 7803,loss: 0.0078094774\n",
            "\n",
            "Global step: 7804,loss: 0.0067608324\n",
            "\n",
            "Global step: 7805,loss: 0.0068271244\n",
            "\n",
            "Global step: 7806,loss: 0.006875546\n",
            "\n",
            "Global step: 7807,loss: 0.0067492747\n",
            "\n",
            "Global step: 7808,loss: 0.0069231475\n",
            "\n",
            "Global step: 7809,loss: 0.00622945\n",
            "\n",
            "Global step: 7810,loss: 0.006982522\n",
            "\n",
            "Global step: 7811,loss: 0.006785104\n",
            "\n",
            "Global step: 7812,loss: 0.007525091\n",
            "\n",
            "Global step: 7813,loss: 0.010395559\n",
            "\n",
            "Global step: 7814,loss: 0.006653359\n",
            "\n",
            "Global step: 7815,loss: 0.006481836\n",
            "\n",
            "Global step: 7816,loss: 0.006430272\n",
            "\n",
            "Global step: 7817,loss: 0.007619729\n",
            "\n",
            "Global step: 7818,loss: 0.006954765\n",
            "\n",
            "Global step: 7819,loss: 0.0061808964\n",
            "\n",
            "Global step: 7820,loss: 0.007980042\n",
            "\n",
            "Global step: 7821,loss: 0.0065511246\n",
            "\n",
            "Global step: 7822,loss: 0.006909939\n",
            "\n",
            "Global step: 7823,loss: 0.0062096394\n",
            "\n",
            "Global step: 7824,loss: 0.0064457897\n",
            "\n",
            "Global step: 7825,loss: 0.0071810116\n",
            "\n",
            "Global step: 7826,loss: 0.007041586\n",
            "\n",
            "Global step: 7827,loss: 0.008699794\n",
            "\n",
            "Global step: 7828,loss: 0.0068428623\n",
            "\n",
            "Global step: 7829,loss: 0.0063471077\n",
            "\n",
            "Global step: 7830,loss: 0.006420821\n",
            "\n",
            "Global step: 7831,loss: 0.0063784\n",
            "\n",
            "Global step: 7832,loss: 0.0068836254\n",
            "\n",
            "Global step: 7833,loss: 0.009470352\n",
            "\n",
            "Global step: 7834,loss: 0.006165084\n",
            "\n",
            "Global step: 7835,loss: 0.0070018936\n",
            "\n",
            "Global step: 7836,loss: 0.0070228935\n",
            "\n",
            "Global step: 7837,loss: 0.0066419546\n",
            "\n",
            "Global step: 7838,loss: 0.0066562104\n",
            "\n",
            "Global step: 7839,loss: 0.0072541097\n",
            "\n",
            "Global step: 7840,loss: 0.006556922\n",
            "\n",
            "Global step: 7841,loss: 0.006531349\n",
            "\n",
            "Global step: 7842,loss: 0.006740021\n",
            "\n",
            "Global step: 7843,loss: 0.007332686\n",
            "\n",
            "Global step: 7844,loss: 0.006668903\n",
            "\n",
            "Global step: 7845,loss: 0.006339995\n",
            "\n",
            "Global step: 7846,loss: 0.006131177\n",
            "\n",
            "Global step: 7847,loss: 0.006503081\n",
            "\n",
            "Global step: 7848,loss: 0.006621706\n",
            "\n",
            "Global step: 7849,loss: 0.0066656657\n",
            "\n",
            "Global step: 7850,loss: 0.00731628\n",
            "\n",
            "Global step: 7851,loss: 0.0071374793\n",
            "\n",
            "Global step: 7852,loss: 0.0064261444\n",
            "\n",
            "Global step: 7853,loss: 0.0076937773\n",
            "\n",
            "Global step: 7854,loss: 0.0069007818\n",
            "\n",
            "Global step: 7855,loss: 0.007364994\n",
            "\n",
            "Global step: 7856,loss: 0.00726351\n",
            "\n",
            "Global step: 7857,loss: 0.006179142\n",
            "\n",
            "Global step: 7858,loss: 0.0066911047\n",
            "\n",
            "Global step: 7859,loss: 0.0069663087\n",
            "\n",
            "Global step: 7860,loss: 0.0067815864\n",
            "\n",
            "Global step: 7861,loss: 0.006450702\n",
            "\n",
            "Global step: 7862,loss: 0.006454241\n",
            "\n",
            "Global step: 7863,loss: 0.006855461\n",
            "\n",
            "Global step: 7864,loss: 0.0067912186\n",
            "\n",
            "Global step: 7865,loss: 0.006882241\n",
            "\n",
            "Global step: 7866,loss: 0.006398123\n",
            "\n",
            "Global step: 7867,loss: 0.0063447566\n",
            "\n",
            "Global step: 7868,loss: 0.0060879574\n",
            "\n",
            "Global step: 7869,loss: 0.0062871035\n",
            "\n",
            "Global step: 7870,loss: 0.0067244037\n",
            "\n",
            "Global step: 7871,loss: 0.0066037104\n",
            "\n",
            "Global step: 7872,loss: 0.006872479\n",
            "\n",
            "Global step: 7873,loss: 0.0069304956\n",
            "\n",
            "Global step: 7874,loss: 0.0058936663\n",
            "\n",
            "Global step: 7875,loss: 0.006735622\n",
            "\n",
            "Global step: 7876,loss: 0.0063650403\n",
            "\n",
            "Global step: 7877,loss: 0.0069660265\n",
            "\n",
            "Global step: 7878,loss: 0.0065994738\n",
            "\n",
            "Global step: 7879,loss: 0.0060030953\n",
            "\n",
            "Global step: 7880,loss: 0.0064484407\n",
            "\n",
            "Global step: 7881,loss: 0.006359018\n",
            "\n",
            "Global step: 7882,loss: 0.008534145\n",
            "\n",
            "Global step: 7883,loss: 0.0066851387\n",
            "\n",
            "Global step: 7884,loss: 0.006220248\n",
            "\n",
            "Global step: 7885,loss: 0.0060991566\n",
            "\n",
            "Global step: 7886,loss: 0.0064408043\n",
            "\n",
            "Global step: 7887,loss: 0.008066105\n",
            "\n",
            "Global step: 7888,loss: 0.0062348284\n",
            "\n",
            "Global step: 7889,loss: 0.006366966\n",
            "\n",
            "Global step: 7890,loss: 0.0063699684\n",
            "\n",
            "Global step: 7891,loss: 0.006038659\n",
            "\n",
            "Global step: 7892,loss: 0.007400497\n",
            "\n",
            "Global step: 7893,loss: 0.0078032743\n",
            "\n",
            "Global step: 7894,loss: 0.006820083\n",
            "\n",
            "Global step: 7895,loss: 0.0069832318\n",
            "\n",
            "Global step: 7896,loss: 0.0067721796\n",
            "\n",
            "Global step: 7897,loss: 0.0061897635\n",
            "\n",
            "Global step: 7898,loss: 0.006525312\n",
            "\n",
            "Global step: 7899,loss: 0.0069367727\n",
            "\n",
            "Global step: 7900,loss: 0.006723592\n",
            "\n",
            "Global step: 7901,loss: 0.006702291\n",
            "\n",
            "Global step: 7902,loss: 0.0061210427\n",
            "\n",
            "Global step: 7903,loss: 0.00704544\n",
            "\n",
            "Global step: 7904,loss: 0.006612313\n",
            "\n",
            "Global step: 7905,loss: 0.006786132\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 7906.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:02:35.223371 139837023905536 supervisor.py:1050] Recording summary at step 7906.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 7906,loss: 0.006997424\n",
            "\n",
            "Global step: 7907,loss: 0.006585049\n",
            "\n",
            "Global step: 7908,loss: 0.006258129\n",
            "\n",
            "Global step: 7909,loss: 0.0060442463\n",
            "\n",
            "Global step: 7910,loss: 0.0063020415\n",
            "\n",
            "Global step: 7911,loss: 0.00652477\n",
            "\n",
            "Global step: 7912,loss: 0.008003969\n",
            "\n",
            "Global step: 7913,loss: 0.006328123\n",
            "\n",
            "Global step: 7914,loss: 0.006273996\n",
            "\n",
            "Global step: 7915,loss: 0.0070958175\n",
            "\n",
            "Global step: 7916,loss: 0.00620518\n",
            "\n",
            "Global step: 7917,loss: 0.0065937885\n",
            "\n",
            "Global step: 7918,loss: 0.0076373927\n",
            "\n",
            "Global step: 7919,loss: 0.006633788\n",
            "\n",
            "Global step: 7920,loss: 0.006400811\n",
            "\n",
            "Global step: 7921,loss: 0.006225598\n",
            "\n",
            "Global step: 7922,loss: 0.0066997004\n",
            "\n",
            "Global step: 7923,loss: 0.0063474206\n",
            "\n",
            "Global step: 7924,loss: 0.006027093\n",
            "\n",
            "Global step: 7925,loss: 0.0068989038\n",
            "\n",
            "Global step: 7926,loss: 0.0067348285\n",
            "\n",
            "Global step: 7927,loss: 0.0057838266\n",
            "\n",
            "Global step: 7928,loss: 0.0068035675\n",
            "\n",
            "Global step: 7929,loss: 0.0060654753\n",
            "\n",
            "Global step: 7930,loss: 0.006324215\n",
            "\n",
            "Global step: 7931,loss: 0.006247167\n",
            "\n",
            "Global step: 7932,loss: 0.0065338793\n",
            "\n",
            "Global step: 7933,loss: 0.0069301263\n",
            "\n",
            "Global step: 7934,loss: 0.005711434\n",
            "\n",
            "Global step: 7935,loss: 0.006716199\n",
            "\n",
            "Global step: 7936,loss: 0.0060915295\n",
            "\n",
            "Global step: 7937,loss: 0.006797852\n",
            "\n",
            "Global step: 7938,loss: 0.006497267\n",
            "\n",
            "Global step: 7939,loss: 0.0072616786\n",
            "\n",
            "Global step: 7940,loss: 0.006325335\n",
            "\n",
            "Global step: 7941,loss: 0.0063598626\n",
            "\n",
            "Global step: 7942,loss: 0.0060979524\n",
            "\n",
            "Global step: 7943,loss: 0.006320013\n",
            "\n",
            "Global step: 7944,loss: 0.006253973\n",
            "\n",
            "Global step: 7945,loss: 0.0061907824\n",
            "\n",
            "Global step: 7946,loss: 0.0061003747\n",
            "\n",
            "Global step: 7947,loss: 0.006901767\n",
            "\n",
            "Global step: 7948,loss: 0.007703849\n",
            "\n",
            "Global step: 7949,loss: 0.0062444503\n",
            "\n",
            "Global step: 7950,loss: 0.006501347\n",
            "\n",
            "Global step: 7951,loss: 0.0069156117\n",
            "\n",
            "Global step: 7952,loss: 0.0059721433\n",
            "\n",
            "Global step: 7953,loss: 0.007057206\n",
            "\n",
            "Global step: 7954,loss: 0.006545179\n",
            "\n",
            "Global step: 7955,loss: 0.006705723\n",
            "\n",
            "Global step: 7956,loss: 0.006935835\n",
            "\n",
            "Global step: 7957,loss: 0.0061827004\n",
            "\n",
            "Global step: 7958,loss: 0.007908834\n",
            "\n",
            "Global step: 7959,loss: 0.006577864\n",
            "\n",
            "Global step: 7960,loss: 0.006618545\n",
            "\n",
            "Global step: 7961,loss: 0.006196745\n",
            "\n",
            "Global step: 7962,loss: 0.0071736025\n",
            "\n",
            "Global step: 7963,loss: 0.006010005\n",
            "\n",
            "Global step: 7964,loss: 0.0064831544\n",
            "\n",
            "Global step: 7965,loss: 0.006906111\n",
            "\n",
            "Global step: 7966,loss: 0.006973332\n",
            "\n",
            "Global step: 7967,loss: 0.0064036227\n",
            "\n",
            "Global step: 7968,loss: 0.006363721\n",
            "\n",
            "Global step: 7969,loss: 0.006390903\n",
            "\n",
            "Global step: 7970,loss: 0.0070566055\n",
            "\n",
            "Global step: 7971,loss: 0.0059711416\n",
            "\n",
            "Global step: 7972,loss: 0.0068261824\n",
            "\n",
            "Global step: 7973,loss: 0.0066657197\n",
            "\n",
            "Global step: 7974,loss: 0.006854393\n",
            "\n",
            "Global step: 7975,loss: 0.0071303756\n",
            "\n",
            "Global step: 7976,loss: 0.006915616\n",
            "\n",
            "Global step: 7977,loss: 0.005721596\n",
            "\n",
            "Global step: 7978,loss: 0.010361016\n",
            "\n",
            "Global step: 7979,loss: 0.0061784773\n",
            "\n",
            "Global step: 7980,loss: 0.0066012107\n",
            "\n",
            "Global step: 7981,loss: 0.006158582\n",
            "\n",
            "Global step: 7982,loss: 0.0061440654\n",
            "\n",
            "Global step: 7983,loss: 0.0070473733\n",
            "\n",
            "Global step: 7984,loss: 0.005965058\n",
            "\n",
            "Global step: 7985,loss: 0.0062296824\n",
            "\n",
            "Global step: 7986,loss: 0.00592085\n",
            "\n",
            "Global step: 7987,loss: 0.0066602146\n",
            "\n",
            "Global step: 7988,loss: 0.00662737\n",
            "\n",
            "Global step: 7989,loss: 0.0063445414\n",
            "\n",
            "Global step: 7990,loss: 0.006292848\n",
            "\n",
            "Global step: 7991,loss: 0.0059521096\n",
            "\n",
            "Global step: 7992,loss: 0.0065065227\n",
            "\n",
            "Global step: 7993,loss: 0.009060971\n",
            "\n",
            "Global step: 7994,loss: 0.0059609795\n",
            "\n",
            "Global step: 7995,loss: 0.0066737365\n",
            "\n",
            "Global step: 7996,loss: 0.0065902914\n",
            "\n",
            "Global step: 7997,loss: 0.006055939\n",
            "\n",
            "Global step: 7998,loss: 0.0066634\n",
            "\n",
            "Global step: 7999,loss: 0.0068868594\n",
            "\n",
            "Global step: 8000,loss: 0.006416899\n",
            "\n",
            "Global step: 8001,loss: 0.0068951263\n",
            "\n",
            "Global step: 8002,loss: 0.0063192365\n",
            "\n",
            "Global step: 8003,loss: 0.0065270634\n",
            "\n",
            "Global step: 8004,loss: 0.0067533986\n",
            "\n",
            "Global step: 8005,loss: 0.0062980712\n",
            "\n",
            "Global step: 8006,loss: 0.00707031\n",
            "\n",
            "Global step: 8007,loss: 0.0062302905\n",
            "\n",
            "Global step: 8008,loss: 0.006429379\n",
            "\n",
            "Global step: 8009,loss: 0.006752633\n",
            "\n",
            "Global step: 8010,loss: 0.006916806\n",
            "\n",
            "Global step: 8011,loss: 0.006274458\n",
            "\n",
            "Global step: 8012,loss: 0.0064599128\n",
            "\n",
            "Global step: 8013,loss: 0.0070075276\n",
            "\n",
            "Global step: 8014,loss: 0.0063303555\n",
            "\n",
            "Global step: 8015,loss: 0.0061622118\n",
            "\n",
            "Global step: 8016,loss: 0.0059246114\n",
            "\n",
            "Global step: 8017,loss: 0.006671493\n",
            "\n",
            "Global step: 8018,loss: 0.006219903\n",
            "\n",
            "Global step: 8019,loss: 0.0071913525\n",
            "\n",
            "Global step: 8020,loss: 0.008351985\n",
            "\n",
            "Global step: 8021,loss: 0.0057741613\n",
            "\n",
            "Global step: 8022,loss: 0.006512356\n",
            "\n",
            "Global step: 8023,loss: 0.0067005996\n",
            "\n",
            "Global step: 8024,loss: 0.0065217162\n",
            "\n",
            "Global step: 8025,loss: 0.006469977\n",
            "\n",
            "Global step: 8026,loss: 0.0073271464\n",
            "\n",
            "Global step: 8027,loss: 0.0058001005\n",
            "\n",
            "Global step: 8028,loss: 0.0062507438\n",
            "\n",
            "Global step: 8029,loss: 0.007993111\n",
            "\n",
            "Global step: 8030,loss: 0.008187389\n",
            "\n",
            "Global step: 8031,loss: 0.007053554\n",
            "\n",
            "Global step: 8032,loss: 0.006429998\n",
            "\n",
            "Global step: 8033,loss: 0.006227745\n",
            "\n",
            "Global step: 8034,loss: 0.006071719\n",
            "\n",
            "Global step: 8035,loss: 0.0070266835\n",
            "\n",
            "Global step: 8036,loss: 0.0063485643\n",
            "\n",
            "Global step: 8037,loss: 0.0067217313\n",
            "\n",
            "Global step: 8038,loss: 0.009738208\n",
            "\n",
            "Global step: 8039,loss: 0.006354184\n",
            "\n",
            "Global step: 8040,loss: 0.0055245743\n",
            "\n",
            "Global step: 8041,loss: 0.0063215243\n",
            "\n",
            "Global step: 8042,loss: 0.008550253\n",
            "\n",
            "Global step: 8043,loss: 0.0061278692\n",
            "\n",
            "Global step: 8044,loss: 0.007434423\n",
            "\n",
            "Global step: 8045,loss: 0.006979914\n",
            "\n",
            "Global step: 8046,loss: 0.0062903226\n",
            "\n",
            "Global step: 8047,loss: 0.0066423328\n",
            "\n",
            "Global step: 8048,loss: 0.0064332155\n",
            "\n",
            "Global step: 8049,loss: 0.0063332655\n",
            "\n",
            "Global step: 8050,loss: 0.0060229897\n",
            "\n",
            "Global step: 8051,loss: 0.008516157\n",
            "\n",
            "Global step: 8052,loss: 0.0068292688\n",
            "\n",
            "Global step: 8053,loss: 0.006105769\n",
            "\n",
            "Global step: 8054,loss: 0.006241028\n",
            "\n",
            "Global step: 8055,loss: 0.006716458\n",
            "\n",
            "Global step: 8056,loss: 0.006760237\n",
            "\n",
            "Global step: 8057,loss: 0.0068830634\n",
            "\n",
            "Global step: 8058,loss: 0.007018375\n",
            "\n",
            "Global step: 8059,loss: 0.007230659\n",
            "\n",
            "Global step: 8060,loss: 0.0070813647\n",
            "\n",
            "Global step: 8061,loss: 0.006510756\n",
            "\n",
            "Global step: 8062,loss: 0.0065505775\n",
            "\n",
            "Global step: 8063,loss: 0.00580959\n",
            "\n",
            "Global step: 8064,loss: 0.006114667\n",
            "\n",
            "Global step: 8065,loss: 0.0066160928\n",
            "\n",
            "Global step: 8066,loss: 0.006230586\n",
            "\n",
            "Global step: 8067,loss: 0.0064525194\n",
            "\n",
            "Global step: 8068,loss: 0.0067445207\n",
            "\n",
            "Global step: 8069,loss: 0.006885256\n",
            "\n",
            "Global step: 8070,loss: 0.007398824\n",
            "\n",
            "Global step: 8071,loss: 0.006296578\n",
            "\n",
            "Global step: 8072,loss: 0.006785814\n",
            "\n",
            "Global step: 8073,loss: 0.0064208326\n",
            "\n",
            "Global step: 8074,loss: 0.0062301853\n",
            "\n",
            "Global step: 8075,loss: 0.0060122395\n",
            "\n",
            "Global step: 8076,loss: 0.005748271\n",
            "\n",
            "Global step: 8077,loss: 0.0075115287\n",
            "\n",
            "Global step: 8078,loss: 0.006357799\n",
            "\n",
            "Global step: 8079,loss: 0.006854157\n",
            "\n",
            "Global step: 8080,loss: 0.0064078853\n",
            "\n",
            "Global step: 8081,loss: 0.0063232556\n",
            "\n",
            "Global step: 8082,loss: 0.006346276\n",
            "\n",
            "Global step: 8083,loss: 0.0071664685\n",
            "\n",
            "Global step: 8084,loss: 0.006637869\n",
            "\n",
            "Global step: 8085,loss: 0.0065026795\n",
            "\n",
            "Global step: 8086,loss: 0.0068871933\n",
            "\n",
            "Global step: 8087,loss: 0.006355496\n",
            "\n",
            "Global step: 8088,loss: 0.005761404\n",
            "\n",
            "Global step: 8089,loss: 0.005883438\n",
            "\n",
            "Global step: 8090,loss: 0.0066820346\n",
            "\n",
            "Global step: 8091,loss: 0.0066643246\n",
            "\n",
            "Global step: 8092,loss: 0.0063605667\n",
            "\n",
            "Global step: 8093,loss: 0.0066214222\n",
            "\n",
            "Global step: 8094,loss: 0.0061136466\n",
            "\n",
            "Global step: 8095,loss: 0.005941948\n",
            "\n",
            "Global step: 8096,loss: 0.006888251\n",
            "\n",
            "Global step: 8097,loss: 0.006595808\n",
            "\n",
            "Global step: 8098,loss: 0.00595369\n",
            "\n",
            "Global step: 8099,loss: 0.0063582934\n",
            "\n",
            "Global step: 8100,loss: 0.0062231263\n",
            "\n",
            "Global step: 8101,loss: 0.006238681\n",
            "\n",
            "Global step: 8102,loss: 0.006432425\n",
            "\n",
            "Global step: 8103,loss: 0.0071487417\n",
            "\n",
            "Global step: 8104,loss: 0.006859866\n",
            "\n",
            "Global step: 8105,loss: 0.0061225467\n",
            "\n",
            "Global step: 8106,loss: 0.0065925247\n",
            "\n",
            "Global step: 8107,loss: 0.0059100264\n",
            "\n",
            "Global step: 8108,loss: 0.0060186596\n",
            "\n",
            "Global step: 8109,loss: 0.0058561135\n",
            "\n",
            "Global step: 8110,loss: 0.006427168\n",
            "\n",
            "Global step: 8111,loss: 0.005959402\n",
            "\n",
            "Global step: 8112,loss: 0.0067441477\n",
            "\n",
            "Global step: 8113,loss: 0.0063507254\n",
            "\n",
            "Global step: 8114,loss: 0.006410524\n",
            "\n",
            "Global step: 8115,loss: 0.0059768413\n",
            "\n",
            "Global step: 8116,loss: 0.006271652\n",
            "\n",
            "Global step: 8117,loss: 0.006384592\n",
            "\n",
            "Global step: 8118,loss: 0.0064384104\n",
            "\n",
            "Global step: 8119,loss: 0.006424763\n",
            "\n",
            "Global step: 8120,loss: 0.006267967\n",
            "\n",
            "Global step: 8121,loss: 0.0064923093\n",
            "\n",
            "Global step: 8122,loss: 0.0063223387\n",
            "\n",
            "Global step: 8123,loss: 0.006878618\n",
            "\n",
            "Global step: 8124,loss: 0.0059658615\n",
            "\n",
            "Global step: 8125,loss: 0.0065042246\n",
            "\n",
            "Global step: 8126,loss: 0.0074429014\n",
            "\n",
            "Global step: 8127,loss: 0.0062671974\n",
            "\n",
            "Global step: 8128,loss: 0.0059163063\n",
            "\n",
            "Global step: 8129,loss: 0.006176307\n",
            "\n",
            "Global step: 8130,loss: 0.006336964\n",
            "\n",
            "Global step: 8131,loss: 0.007100822\n",
            "\n",
            "Global step: 8132,loss: 0.0056670895\n",
            "\n",
            "Global step: 8133,loss: 0.00591758\n",
            "\n",
            "Global step: 8134,loss: 0.0065034213\n",
            "\n",
            "Global step: 8135,loss: 0.005851639\n",
            "\n",
            "Global step: 8136,loss: 0.0066451863\n",
            "\n",
            "Global step: 8137,loss: 0.0067727445\n",
            "\n",
            "Global step: 8138,loss: 0.006177492\n",
            "\n",
            "Global step: 8139,loss: 0.016795885\n",
            "\n",
            "Global step: 8140,loss: 0.006154433\n",
            "\n",
            "Global step: 8141,loss: 0.0061294674\n",
            "\n",
            "Global step: 8142,loss: 0.0064326758\n",
            "\n",
            "Global step: 8143,loss: 0.0055513424\n",
            "\n",
            "Global step: 8144,loss: 0.006937647\n",
            "\n",
            "Global step: 8145,loss: 0.008484043\n",
            "\n",
            "Global step: 8146,loss: 0.006466144\n",
            "\n",
            "Global step: 8147,loss: 0.0062969397\n",
            "\n",
            "Global step: 8148,loss: 0.0062254053\n",
            "\n",
            "Global step: 8149,loss: 0.0063092876\n",
            "\n",
            "Global step: 8150,loss: 0.0067352396\n",
            "\n",
            "Global step: 8151,loss: 0.0061950074\n",
            "\n",
            "Global step: 8152,loss: 0.0067351167\n",
            "\n",
            "Global step: 8153,loss: 0.0059571215\n",
            "\n",
            "Global step: 8154,loss: 0.0066154264\n",
            "\n",
            "Global step: 8155,loss: 0.0076628956\n",
            "\n",
            "Global step: 8156,loss: 0.006126044\n",
            "\n",
            "Global step: 8157,loss: 0.0065061403\n",
            "\n",
            "Global step: 8158,loss: 0.006315722\n",
            "\n",
            "Global step: 8159,loss: 0.006036781\n",
            "\n",
            "Global step: 8160,loss: 0.013416018\n",
            "\n",
            "Global step: 8161,loss: 0.0062258346\n",
            "\n",
            "Global step: 8162,loss: 0.0064903307\n",
            "\n",
            "Global step: 8163,loss: 0.0063614715\n",
            "\n",
            "Global step: 8164,loss: 0.0063591246\n",
            "\n",
            "Global step: 8165,loss: 0.0060514975\n",
            "\n",
            "Global step: 8166,loss: 0.0061334\n",
            "\n",
            "Global step: 8167,loss: 0.0067542735\n",
            "\n",
            "Global step: 8168,loss: 0.006880498\n",
            "\n",
            "Global step: 8169,loss: 0.006187999\n",
            "\n",
            "Global step: 8170,loss: 0.006315896\n",
            "\n",
            "Global step: 8171,loss: 0.0064324625\n",
            "\n",
            "Global step: 8172,loss: 0.006439464\n",
            "\n",
            "Global step: 8173,loss: 0.0060437387\n",
            "\n",
            "Global step: 8174,loss: 0.007065761\n",
            "\n",
            "Global step: 8175,loss: 0.0069079404\n",
            "\n",
            "Global step: 8176,loss: 0.006111048\n",
            "\n",
            "Global step: 8177,loss: 0.006412867\n",
            "\n",
            "Global step: 8178,loss: 0.006627842\n",
            "\n",
            "Global step: 8179,loss: 0.0062261927\n",
            "\n",
            "Global step: 8180,loss: 0.0065291426\n",
            "\n",
            "Global step: 8181,loss: 0.007805326\n",
            "\n",
            "Global step: 8182,loss: 0.005912241\n",
            "\n",
            "Global step: 8183,loss: 0.0061430624\n",
            "\n",
            "Global step: 8184,loss: 0.005969071\n",
            "\n",
            "Global step: 8185,loss: 0.006110794\n",
            "\n",
            "Global step: 8186,loss: 0.0064803977\n",
            "\n",
            "Global step: 8187,loss: 0.0060128095\n",
            "\n",
            "Global step: 8188,loss: 0.007005129\n",
            "\n",
            "Global step: 8189,loss: 0.0064730006\n",
            "\n",
            "Global step: 8190,loss: 0.0073363017\n",
            "\n",
            "Global step: 8191,loss: 0.0071269018\n",
            "\n",
            "Global step: 8192,loss: 0.006306009\n",
            "\n",
            "Global step: 8193,loss: 0.0061221784\n",
            "\n",
            "Global step: 8194,loss: 0.006182379\n",
            "\n",
            "Global step: 8195,loss: 0.00648828\n",
            "\n",
            "Global step: 8196,loss: 0.006313659\n",
            "\n",
            "Global step: 8197,loss: 0.005979998\n",
            "\n",
            "Global step: 8198,loss: 0.007244902\n",
            "\n",
            "Global step: 8199,loss: 0.005696737\n",
            "\n",
            "Global step: 8200,loss: 0.0063855774\n",
            "\n",
            "Global step: 8201,loss: 0.006162669\n",
            "\n",
            "Global step: 8202,loss: 0.0058893417\n",
            "\n",
            "Global step: 8203,loss: 0.0061517125\n",
            "\n",
            "Global step: 8204,loss: 0.006015093\n",
            "\n",
            "Global step: 8205,loss: 0.006883237\n",
            "\n",
            "Global step: 8206,loss: 0.005796296\n",
            "\n",
            "Global step: 8207,loss: 0.006216729\n",
            "\n",
            "Global step: 8208,loss: 0.00663007\n",
            "\n",
            "Global step: 8209,loss: 0.006266934\n",
            "\n",
            "Global step: 8210,loss: 0.00658023\n",
            "\n",
            "Global step: 8211,loss: 0.0058374703\n",
            "\n",
            "Global step: 8212,loss: 0.008442912\n",
            "\n",
            "Global step: 8213,loss: 0.0060048643\n",
            "\n",
            "Global step: 8214,loss: 0.0062739886\n",
            "\n",
            "Global step: 8215,loss: 0.006560587\n",
            "\n",
            "Global step: 8216,loss: 0.0068790508\n",
            "\n",
            "Global step: 8217,loss: 0.0058735595\n",
            "\n",
            "Global step: 8218,loss: 0.0061834278\n",
            "\n",
            "Global step: 8219,loss: 0.0062205005\n",
            "\n",
            "Global step: 8220,loss: 0.0060805446\n",
            "\n",
            "Global step: 8221,loss: 0.0060699265\n",
            "\n",
            "Global step: 8222,loss: 0.005851209\n",
            "\n",
            "Global step: 8223,loss: 0.011228625\n",
            "\n",
            "Global step: 8224,loss: 0.006737709\n",
            "\n",
            "Global step: 8225,loss: 0.0064158035\n",
            "\n",
            "Global step: 8226,loss: 0.0061887014\n",
            "\n",
            "Global step: 8227,loss: 0.006732628\n",
            "\n",
            "Global step: 8228,loss: 0.0061647436\n",
            "\n",
            "Global step: 8229,loss: 0.006459205\n",
            "\n",
            "Global step: 8230,loss: 0.0065105143\n",
            "\n",
            "Global step: 8231,loss: 0.005979083\n",
            "\n",
            "Global step: 8232,loss: 0.0060062497\n",
            "\n",
            "Global step: 8233,loss: 0.005724001\n",
            "\n",
            "Global step: 8234,loss: 0.006334288\n",
            "\n",
            "Global step: 8235,loss: 0.005948576\n",
            "\n",
            "Global step: 8236,loss: 0.005793913\n",
            "\n",
            "Global step: 8237,loss: 0.005652365\n",
            "\n",
            "Global step: 8238,loss: 0.006313442\n",
            "\n",
            "Global step: 8239,loss: 0.006273006\n",
            "\n",
            "Global step: 8240,loss: 0.0057412367\n",
            "\n",
            "Global step: 8241,loss: 0.006343698\n",
            "\n",
            "Global step: 8242,loss: 0.0061080814\n",
            "\n",
            "Global step: 8243,loss: 0.006723206\n",
            "\n",
            "Global step: 8244,loss: 0.0060008653\n",
            "\n",
            "Global step: 8245,loss: 0.0065450985\n",
            "\n",
            "Global step: 8246,loss: 0.0059077092\n",
            "\n",
            "Global step: 8247,loss: 0.0060038595\n",
            "\n",
            "Global step: 8248,loss: 0.006188419\n",
            "\n",
            "Global step: 8249,loss: 0.0065975133\n",
            "\n",
            "Global step: 8250,loss: 0.0064018504\n",
            "\n",
            "Global step: 8251,loss: 0.0062646735\n",
            "\n",
            "Global step: 8252,loss: 0.006177288\n",
            "\n",
            "Global step: 8253,loss: 0.0063701957\n",
            "\n",
            "Global step: 8254,loss: 0.006281248\n",
            "\n",
            "Global step: 8255,loss: 0.0057897745\n",
            "\n",
            "Global step: 8256,loss: 0.006029417\n",
            "\n",
            "Global step: 8257,loss: 0.0062906947\n",
            "\n",
            "Global step: 8258,loss: 0.006581926\n",
            "\n",
            "Global step: 8259,loss: 0.006145906\n",
            "\n",
            "Global step: 8260,loss: 0.0067265728\n",
            "\n",
            "Global step: 8261,loss: 0.0062654945\n",
            "\n",
            "Global step: 8262,loss: 0.005961005\n",
            "\n",
            "Global step: 8263,loss: 0.0063619837\n",
            "\n",
            "Global step: 8264,loss: 0.006284811\n",
            "\n",
            "Global step: 8265,loss: 0.0060045365\n",
            "\n",
            "Global step: 8266,loss: 0.006324278\n",
            "\n",
            "Global step: 8267,loss: 0.006266307\n",
            "\n",
            "Global step: 8268,loss: 0.0060950452\n",
            "\n",
            "Global step: 8269,loss: 0.005992111\n",
            "\n",
            "Global step: 8270,loss: 0.0061732447\n",
            "\n",
            "Global step: 8271,loss: 0.0062496397\n",
            "\n",
            "Global step: 8272,loss: 0.005906121\n",
            "\n",
            "Global step: 8273,loss: 0.0065867393\n",
            "\n",
            "Global step: 8274,loss: 0.006155487\n",
            "\n",
            "Global step: 8275,loss: 0.0055189757\n",
            "\n",
            "Global step: 8276,loss: 0.0066481666\n",
            "\n",
            "Global step: 8277,loss: 0.0059809373\n",
            "\n",
            "Global step: 8278,loss: 0.0059347088\n",
            "\n",
            "Global step: 8279,loss: 0.006116107\n",
            "\n",
            "Global step: 8280,loss: 0.0058270507\n",
            "\n",
            "Global step: 8281,loss: 0.0063424604\n",
            "\n",
            "Global step: 8282,loss: 0.005742953\n",
            "\n",
            "Global step: 8283,loss: 0.006945718\n",
            "\n",
            "Global step: 8284,loss: 0.006806066\n",
            "\n",
            "Global step: 8285,loss: 0.006003032\n",
            "\n",
            "Global step: 8286,loss: 0.00562486\n",
            "\n",
            "Global step: 8287,loss: 0.006593624\n",
            "\n",
            "Global step: 8288,loss: 0.005851099\n",
            "\n",
            "Global step: 8289,loss: 0.0059275986\n",
            "\n",
            "Global step: 8290,loss: 0.0060414122\n",
            "\n",
            "Global step: 8291,loss: 0.0064395573\n",
            "\n",
            "Global step: 8292,loss: 0.0061272862\n",
            "\n",
            "Global step: 8293,loss: 0.00627221\n",
            "\n",
            "Global step: 8294,loss: 0.0058343164\n",
            "\n",
            "Global step: 8295,loss: 0.0057475194\n",
            "\n",
            "Global step: 8296,loss: 0.0065480615\n",
            "\n",
            "Global step: 8297,loss: 0.0059334086\n",
            "\n",
            "Global step: 8298,loss: 0.0061466563\n",
            "\n",
            "Global step: 8299,loss: 0.006173018\n",
            "\n",
            "Global step: 8300,loss: 0.0062119123\n",
            "\n",
            "Global step: 8301,loss: 0.005872904\n",
            "\n",
            "Global step: 8302,loss: 0.006155957\n",
            "\n",
            "Global step: 8303,loss: 0.0063384604\n",
            "\n",
            "Global step: 8304,loss: 0.0066110357\n",
            "\n",
            "Global step: 8305,loss: 0.006305621\n",
            "\n",
            "Global step: 8306,loss: 0.0067654904\n",
            "\n",
            "Global step: 8307,loss: 0.0060707843\n",
            "\n",
            "Global step: 8308,loss: 0.006024885\n",
            "\n",
            "Global step: 8309,loss: 0.0061895875\n",
            "\n",
            "Global step: 8310,loss: 0.006175837\n",
            "\n",
            "Global step: 8311,loss: 0.0061465665\n",
            "\n",
            "Global step: 8312,loss: 0.0063964757\n",
            "\n",
            "Global step: 8313,loss: 0.0060826764\n",
            "\n",
            "Global step: 8314,loss: 0.0060452917\n",
            "\n",
            "Global step: 8315,loss: 0.00633725\n",
            "\n",
            "Global step: 8316,loss: 0.0064296527\n",
            "\n",
            "Global step: 8317,loss: 0.0056137145\n",
            "\n",
            "Global step: 8318,loss: 0.005946137\n",
            "\n",
            "Global step: 8319,loss: 0.006089231\n",
            "\n",
            "Global step: 8320,loss: 0.0059616994\n",
            "\n",
            "Global step: 8321,loss: 0.0060687046\n",
            "\n",
            "Global step: 8322,loss: 0.0061112805\n",
            "\n",
            "Global step: 8323,loss: 0.005660368\n",
            "\n",
            "Global step: 8324,loss: 0.0061206343\n",
            "\n",
            "Global step: 8325,loss: 0.0066122846\n",
            "\n",
            "Global step: 8326,loss: 0.005981923\n",
            "\n",
            "Global step: 8327,loss: 0.0064312294\n",
            "\n",
            "Global step: 8328,loss: 0.006013306\n",
            "\n",
            "Global step: 8329,loss: 0.00619952\n",
            "\n",
            "Global step: 8330,loss: 0.0058057653\n",
            "\n",
            "Global step: 8331,loss: 0.0062452503\n",
            "\n",
            "Global step: 8332,loss: 0.005853285\n",
            "\n",
            "Global step: 8333,loss: 0.0060236426\n",
            "\n",
            "Global step: 8334,loss: 0.006589646\n",
            "\n",
            "Global step: 8335,loss: 0.006113688\n",
            "\n",
            "Global step: 8336,loss: 0.006045969\n",
            "\n",
            "Global step: 8337,loss: 0.0062209554\n",
            "\n",
            "Global step: 8338,loss: 0.0056386087\n",
            "\n",
            "Global step: 8339,loss: 0.0058876052\n",
            "\n",
            "Global step: 8340,loss: 0.0068196477\n",
            "\n",
            "Global step: 8341,loss: 0.0064216177\n",
            "\n",
            "Global step: 8342,loss: 0.0062951366\n",
            "\n",
            "Global step: 8343,loss: 0.007221637\n",
            "\n",
            "Global step: 8344,loss: 0.0063951695\n",
            "\n",
            "Global step: 8345,loss: 0.006151103\n",
            "\n",
            "Global step: 8346,loss: 0.006508053\n",
            "\n",
            "Global step: 8347,loss: 0.006600116\n",
            "\n",
            "Global step: 8348,loss: 0.0062227254\n",
            "\n",
            "Global step: 8349,loss: 0.0067097275\n",
            "\n",
            "Global step: 8350,loss: 0.0062914416\n",
            "\n",
            "Global step: 8351,loss: 0.0064365733\n",
            "\n",
            "Global step: 8352,loss: 0.006727305\n",
            "\n",
            "Global step: 8353,loss: 0.0059642587\n",
            "\n",
            "Global step: 8354,loss: 0.0065894434\n",
            "\n",
            "Global step: 8355,loss: 0.006124523\n",
            "\n",
            "Global step: 8356,loss: 0.0059994995\n",
            "\n",
            "Global step: 8357,loss: 0.0066964678\n",
            "\n",
            "Global step: 8358,loss: 0.010113721\n",
            "\n",
            "Global step: 8359,loss: 0.0064349836\n",
            "\n",
            "Global step: 8360,loss: 0.0063279746\n",
            "\n",
            "Global step: 8361,loss: 0.006591511\n",
            "\n",
            "Global step: 8362,loss: 0.0067088054\n",
            "\n",
            "Global step: 8363,loss: 0.006293894\n",
            "\n",
            "Global step: 8364,loss: 0.0063110404\n",
            "\n",
            "Global step: 8365,loss: 0.0063393\n",
            "\n",
            "Global step: 8366,loss: 0.005598758\n",
            "\n",
            "Global step: 8367,loss: 0.00600563\n",
            "\n",
            "Global step: 8368,loss: 0.0069179204\n",
            "\n",
            "Global step: 8369,loss: 0.0064863055\n",
            "\n",
            "Global step: 8370,loss: 0.006103237\n",
            "\n",
            "Global step: 8371,loss: 0.0066195754\n",
            "\n",
            "Global step: 8372,loss: 0.006598274\n",
            "\n",
            "Global step: 8373,loss: 0.006031911\n",
            "\n",
            "Global step: 8374,loss: 0.0063169473\n",
            "\n",
            "Global step: 8375,loss: 0.006445694\n",
            "\n",
            "Global step: 8376,loss: 0.006396967\n",
            "\n",
            "Global step: 8377,loss: 0.00627777\n",
            "\n",
            "Global step: 8378,loss: 0.006043691\n",
            "\n",
            "Global step: 8379,loss: 0.0062060347\n",
            "\n",
            "Global step: 8380,loss: 0.0058874246\n",
            "\n",
            "Global step: 8381,loss: 0.006072887\n",
            "\n",
            "Global step: 8382,loss: 0.006184574\n",
            "\n",
            "Global step: 8383,loss: 0.005502042\n",
            "\n",
            "Global step: 8384,loss: 0.006360402\n",
            "\n",
            "Global step: 8385,loss: 0.006156476\n",
            "\n",
            "Global step: 8386,loss: 0.0061600283\n",
            "\n",
            "Global step: 8387,loss: 0.0060231136\n",
            "\n",
            "Global step: 8388,loss: 0.006663442\n",
            "\n",
            "Global step: 8389,loss: 0.0060557043\n",
            "\n",
            "Global step: 8390,loss: 0.0061737057\n",
            "\n",
            "Global step: 8391,loss: 0.006163229\n",
            "\n",
            "Global step: 8392,loss: 0.006624394\n",
            "\n",
            "Global step: 8393,loss: 0.0059718085\n",
            "\n",
            "Global step: 8394,loss: 0.0059898496\n",
            "\n",
            "Global step: 8395,loss: 0.0063105053\n",
            "\n",
            "Global step: 8396,loss: 0.0060050352\n",
            "\n",
            "Global step: 8397,loss: 0.006014385\n",
            "\n",
            "Global step: 8398,loss: 0.0061692907\n",
            "\n",
            "Global step: 8399,loss: 0.0063341865\n",
            "\n",
            "Global step: 8400,loss: 0.006060655\n",
            "\n",
            "Global step: 8401,loss: 0.006375659\n",
            "\n",
            "Global step: 8402,loss: 0.006209274\n",
            "\n",
            "Global step: 8403,loss: 0.005565669\n",
            "\n",
            "Global step: 8404,loss: 0.005804302\n",
            "\n",
            "Global step: 8405,loss: 0.0060937116\n",
            "\n",
            "Global step: 8406,loss: 0.006302656\n",
            "\n",
            "Global step: 8407,loss: 0.005857928\n",
            "\n",
            "Global step: 8408,loss: 0.0059615863\n",
            "\n",
            "Global step: 8409,loss: 0.0061125453\n",
            "\n",
            "Global step: 8410,loss: 0.006154099\n",
            "\n",
            "Global step: 8411,loss: 0.0058066812\n",
            "\n",
            "Global step: 8412,loss: 0.006928444\n",
            "\n",
            "Global step: 8413,loss: 0.0059163803\n",
            "\n",
            "Global step: 8414,loss: 0.0063109747\n",
            "\n",
            "Global step: 8415,loss: 0.0060264003\n",
            "\n",
            "Global step: 8416,loss: 0.0055033294\n",
            "\n",
            "Global step: 8417,loss: 0.005511878\n",
            "\n",
            "Global step: 8418,loss: 0.0060565434\n",
            "\n",
            "Global step: 8419,loss: 0.005890675\n",
            "\n",
            "Global step: 8420,loss: 0.0061721383\n",
            "\n",
            "Global step: 8421,loss: 0.008372762\n",
            "\n",
            "Global step: 8422,loss: 0.00673535\n",
            "\n",
            "Global step: 8423,loss: 0.005436088\n",
            "\n",
            "Global step: 8424,loss: 0.0059661567\n",
            "\n",
            "Global step: 8425,loss: 0.0058747176\n",
            "\n",
            "Global step: 8426,loss: 0.0063572912\n",
            "\n",
            "Global step: 8427,loss: 0.00619863\n",
            "\n",
            "Global step: 8428,loss: 0.0065184464\n",
            "\n",
            "Global step: 8429,loss: 0.0059440653\n",
            "\n",
            "Global step: 8430,loss: 0.006092627\n",
            "\n",
            "Global step: 8431,loss: 0.005627831\n",
            "\n",
            "Global step: 8432,loss: 0.0057590036\n",
            "\n",
            "Global step: 8433,loss: 0.006139534\n",
            "\n",
            "Global step: 8434,loss: 0.0073274155\n",
            "\n",
            "Global step: 8435,loss: 0.00586915\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 8435,Val_Loss: 0.007975088201590583,  Val_acc: 0.9992654914529915 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 13:03:56.191580 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 13/15:\n",
            "Global step: 8436,loss: 0.005989437\n",
            "\n",
            "Global step: 8437,loss: 0.005961397\n",
            "\n",
            "Global step: 8438,loss: 0.005852556\n",
            "\n",
            "Global step: 8439,loss: 0.0057773823\n",
            "\n",
            "Global step: 8440,loss: 0.0070538586\n",
            "\n",
            "Global step: 8441,loss: 0.0058397027\n",
            "\n",
            "Global step: 8442,loss: 0.005979898\n",
            "\n",
            "Global step: 8443,loss: 0.0056431317\n",
            "\n",
            "Global step: 8444,loss: 0.0062167533\n",
            "\n",
            "Global step: 8445,loss: 0.006411112\n",
            "\n",
            "Global step: 8446,loss: 0.006062377\n",
            "\n",
            "Global step: 8447,loss: 0.0061246683\n",
            "\n",
            "Global step: 8448,loss: 0.005996612\n",
            "\n",
            "Global step: 8449,loss: 0.0060773543\n",
            "\n",
            "Global step: 8450,loss: 0.0060323193\n",
            "\n",
            "Global step: 8451,loss: 0.0056255027\n",
            "\n",
            "Global step: 8452,loss: 0.005614501\n",
            "\n",
            "Global step: 8453,loss: 0.0057448135\n",
            "\n",
            "Global step: 8454,loss: 0.0062264497\n",
            "\n",
            "Global step: 8455,loss: 0.005959426\n",
            "\n",
            "Global step: 8456,loss: 0.0057799146\n",
            "\n",
            "Global step: 8457,loss: 0.0058397055\n",
            "\n",
            "Global step: 8458,loss: 0.006600543\n",
            "\n",
            "Global step: 8459,loss: 0.0052970694\n",
            "\n",
            "Global step: 8460,loss: 0.006554689\n",
            "\n",
            "Global step: 8461,loss: 0.00588968\n",
            "\n",
            "Global step: 8462,loss: 0.0056954543\n",
            "\n",
            "Global step: 8463,loss: 0.005863166\n",
            "\n",
            "Global step: 8464,loss: 0.006164015\n",
            "\n",
            "Global step: 8465,loss: 0.006115869\n",
            "\n",
            "Global step: 8466,loss: 0.005922515\n",
            "\n",
            "Global step: 8467,loss: 0.0059112497\n",
            "\n",
            "Global step: 8468,loss: 0.0055739563\n",
            "\n",
            "Global step: 8469,loss: 0.0058460063\n",
            "\n",
            "Global step: 8470,loss: 0.0056506074\n",
            "\n",
            "Global step: 8471,loss: 0.0064432123\n",
            "\n",
            "Global step: 8472,loss: 0.006652799\n",
            "\n",
            "Global step: 8473,loss: 0.0058779526\n",
            "\n",
            "Global step: 8474,loss: 0.006184903\n",
            "\n",
            "Global step: 8475,loss: 0.006726056\n",
            "\n",
            "Global step: 8476,loss: 0.005602437\n",
            "\n",
            "Global step: 8477,loss: 0.0067576882\n",
            "\n",
            "Global step: 8478,loss: 0.005635359\n",
            "\n",
            "Global step: 8479,loss: 0.0060539646\n",
            "\n",
            "Global step: 8480,loss: 0.0061446293\n",
            "\n",
            "Global step: 8481,loss: 0.0062912954\n",
            "\n",
            "Global step: 8482,loss: 0.006427182\n",
            "\n",
            "Global step: 8483,loss: 0.006107149\n",
            "\n",
            "Global step: 8484,loss: 0.005772144\n",
            "\n",
            "Global step: 8485,loss: 0.0058996934\n",
            "\n",
            "Global step: 8486,loss: 0.0061720503\n",
            "\n",
            "Global step: 8487,loss: 0.006181508\n",
            "\n",
            "Global step: 8488,loss: 0.0060115997\n",
            "\n",
            "Global step: 8489,loss: 0.0056868065\n",
            "\n",
            "Global step: 8490,loss: 0.0065350723\n",
            "\n",
            "Global step: 8491,loss: 0.005936057\n",
            "\n",
            "Global step: 8492,loss: 0.0056538205\n",
            "\n",
            "Global step: 8493,loss: 0.0055822274\n",
            "\n",
            "Global step: 8494,loss: 0.006372506\n",
            "\n",
            "Global step: 8495,loss: 0.0059597073\n",
            "\n",
            "Global step: 8496,loss: 0.0056437356\n",
            "\n",
            "Global step: 8497,loss: 0.005983797\n",
            "\n",
            "Global step: 8498,loss: 0.006057894\n",
            "\n",
            "Global step: 8499,loss: 0.0060103694\n",
            "\n",
            "Global step: 8500,loss: 0.0059989495\n",
            "\n",
            "Global step: 8501,loss: 0.0059338\n",
            "\n",
            "Global step: 8502,loss: 0.0055734157\n",
            "\n",
            "Global step: 8503,loss: 0.0066919685\n",
            "\n",
            "Global step: 8504,loss: 0.0061973627\n",
            "\n",
            "Global step: 8505,loss: 0.005228321\n",
            "\n",
            "Global step: 8506,loss: 0.005931864\n",
            "\n",
            "Global step: 8507,loss: 0.006287177\n",
            "\n",
            "Global step: 8508,loss: 0.006479606\n",
            "\n",
            "Global step: 8509,loss: 0.0064193006\n",
            "\n",
            "Global step: 8510,loss: 0.0056711165\n",
            "\n",
            "Global step: 8511,loss: 0.0059085577\n",
            "\n",
            "Global step: 8512,loss: 0.0059020068\n",
            "\n",
            "Global step: 8513,loss: 0.0056556487\n",
            "\n",
            "Global step: 8514,loss: 0.005843845\n",
            "\n",
            "Global step: 8515,loss: 0.006110716\n",
            "\n",
            "Global step: 8516,loss: 0.00633552\n",
            "\n",
            "Global step: 8517,loss: 0.0058683883\n",
            "\n",
            "Global step: 8518,loss: 0.0059979786\n",
            "\n",
            "Global step: 8519,loss: 0.006106615\n",
            "\n",
            "Global step: 8520,loss: 0.0065449197\n",
            "\n",
            "Global step: 8521,loss: 0.006040519\n",
            "\n",
            "Global step: 8522,loss: 0.005864326\n",
            "\n",
            "Global step: 8523,loss: 0.0064148614\n",
            "\n",
            "Global step: 8524,loss: 0.0055110403\n",
            "\n",
            "Global step: 8525,loss: 0.0058341864\n",
            "\n",
            "Global step: 8526,loss: 0.0055040675\n",
            "\n",
            "Global step: 8527,loss: 0.005771826\n",
            "\n",
            "Global step: 8528,loss: 0.005947846\n",
            "\n",
            "Global step: 8529,loss: 0.005843416\n",
            "\n",
            "Global step: 8530,loss: 0.005824276\n",
            "\n",
            "Global step: 8531,loss: 0.0057258657\n",
            "\n",
            "Global step: 8532,loss: 0.0058229673\n",
            "\n",
            "Global step: 8533,loss: 0.0059260726\n",
            "\n",
            "Global step: 8534,loss: 0.006347825\n",
            "\n",
            "Global step: 8535,loss: 0.0058838273\n",
            "\n",
            "Global step: 8536,loss: 0.006196579\n",
            "\n",
            "Global step: 8537,loss: 0.0062026675\n",
            "\n",
            "Global step: 8538,loss: 0.005744069\n",
            "\n",
            "Global step: 8539,loss: 0.0055060624\n",
            "\n",
            "Global step: 8540,loss: 0.0054425234\n",
            "\n",
            "Global step: 8541,loss: 0.0054603964\n",
            "\n",
            "Global step: 8542,loss: 0.0056610242\n",
            "\n",
            "Global step: 8543,loss: 0.0057550697\n",
            "\n",
            "Global step: 8544,loss: 0.005790993\n",
            "\n",
            "Global step: 8545,loss: 0.006468315\n",
            "\n",
            "Global step: 8546,loss: 0.006326024\n",
            "\n",
            "Global step: 8547,loss: 0.006274287\n",
            "\n",
            "Global step: 8548,loss: 0.0058291005\n",
            "\n",
            "Global step: 8549,loss: 0.0055033956\n",
            "\n",
            "Global step: 8550,loss: 0.0056820395\n",
            "\n",
            "Global step: 8551,loss: 0.0061242627\n",
            "\n",
            "Global step: 8552,loss: 0.0058943243\n",
            "\n",
            "Global step: 8553,loss: 0.005555722\n",
            "\n",
            "Global step: 8554,loss: 0.0057727057\n",
            "\n",
            "Global step: 8555,loss: 0.005415745\n",
            "\n",
            "Global step: 8556,loss: 0.005841236\n",
            "\n",
            "Global step: 8557,loss: 0.0065612327\n",
            "\n",
            "Global step: 8558,loss: 0.005765787\n",
            "\n",
            "Global step: 8559,loss: 0.005933631\n",
            "\n",
            "Global step: 8560,loss: 0.006377612\n",
            "\n",
            "Global step: 8561,loss: 0.005952718\n",
            "\n",
            "Global step: 8562,loss: 0.006417003\n",
            "\n",
            "Global step: 8563,loss: 0.0057806373\n",
            "\n",
            "Global step: 8564,loss: 0.0060364734\n",
            "\n",
            "Global step: 8565,loss: 0.0055302624\n",
            "\n",
            "Global step: 8566,loss: 0.0062807556\n",
            "\n",
            "Global step: 8567,loss: 0.006457747\n",
            "\n",
            "Global step: 8568,loss: 0.0057053277\n",
            "\n",
            "Global step: 8569,loss: 0.005810153\n",
            "\n",
            "Global step: 8570,loss: 0.0061519495\n",
            "\n",
            "Global step: 8571,loss: 0.00556274\n",
            "\n",
            "Global step: 8572,loss: 0.005950877\n",
            "\n",
            "Global step: 8573,loss: 0.005831866\n",
            "\n",
            "Global step: 8574,loss: 0.0061525274\n",
            "\n",
            "Global step: 8575,loss: 0.005936018\n",
            "\n",
            "Global step: 8576,loss: 0.005608503\n",
            "\n",
            "Global step: 8577,loss: 0.0060304375\n",
            "\n",
            "Global step: 8578,loss: 0.0056130122\n",
            "\n",
            "Global step: 8579,loss: 0.0058960193\n",
            "\n",
            "Global step: 8580,loss: 0.0059824716\n",
            "\n",
            "Global step: 8581,loss: 0.006036545\n",
            "\n",
            "Global step: 8582,loss: 0.0057196296\n",
            "\n",
            "Global step: 8583,loss: 0.0055550034\n",
            "\n",
            "Global step: 8584,loss: 0.00606031\n",
            "\n",
            "Global step: 8585,loss: 0.0059918873\n",
            "\n",
            "Global step: 8586,loss: 0.005933743\n",
            "\n",
            "Global step: 8587,loss: 0.006439009\n",
            "\n",
            "Global step: 8588,loss: 0.0060296473\n",
            "\n",
            "Global step: 8589,loss: 0.005396972\n",
            "\n",
            "Global step: 8590,loss: 0.006178891\n",
            "\n",
            "Global step: 8591,loss: 0.006017317\n",
            "\n",
            "Global step: 8592,loss: 0.0060424237\n",
            "\n",
            "Global step: 8593,loss: 0.006335839\n",
            "\n",
            "Global step: 8594,loss: 0.0057824505\n",
            "\n",
            "Global step: 8595,loss: 0.0061409976\n",
            "\n",
            "Global step: 8596,loss: 0.005491238\n",
            "\n",
            "Global step: 8597,loss: 0.0056208936\n",
            "\n",
            "Global step: 8598,loss: 0.0055768136\n",
            "\n",
            "Global step: 8599,loss: 0.0055389805\n",
            "\n",
            "Global step: 8600,loss: 0.005602613\n",
            "\n",
            "Global step: 8601,loss: 0.005671112\n",
            "\n",
            "Global step: 8602,loss: 0.010346503\n",
            "\n",
            "Global step: 8603,loss: 0.0063007674\n",
            "\n",
            "Global step: 8604,loss: 0.005647783\n",
            "\n",
            "Global step: 8605,loss: 0.005806235\n",
            "\n",
            "Global step: 8606,loss: 0.005363064\n",
            "\n",
            "Global step: 8607,loss: 0.005584124\n",
            "\n",
            "Global step: 8608,loss: 0.0057217656\n",
            "\n",
            "Global step: 8609,loss: 0.0062703043\n",
            "\n",
            "Global step: 8610,loss: 0.0060340688\n",
            "\n",
            "Global step: 8611,loss: 0.0062515587\n",
            "\n",
            "Global step: 8612,loss: 0.005774004\n",
            "\n",
            "Global step: 8613,loss: 0.007286446\n",
            "\n",
            "Global step: 8614,loss: 0.005576689\n",
            "\n",
            "Global step: 8615,loss: 0.0061220923\n",
            "\n",
            "Global step: 8616,loss: 0.0057197167\n",
            "\n",
            "Global step: 8617,loss: 0.006206514\n",
            "\n",
            "Global step: 8618,loss: 0.006547126\n",
            "\n",
            "Global step: 8619,loss: 0.006097134\n",
            "\n",
            "Global step: 8620,loss: 0.006499517\n",
            "\n",
            "Global step: 8621,loss: 0.005894435\n",
            "\n",
            "Global step: 8622,loss: 0.0059752953\n",
            "\n",
            "Global step: 8623,loss: 0.0056749946\n",
            "\n",
            "Global step: 8624,loss: 0.006440682\n",
            "\n",
            "Global step: 8625,loss: 0.0060389983\n",
            "\n",
            "Global step: 8626,loss: 0.0057266434\n",
            "\n",
            "Global step: 8627,loss: 0.0059690694\n",
            "\n",
            "Global step: 8628,loss: 0.0060522305\n",
            "\n",
            "Global step: 8629,loss: 0.0054651857\n",
            "\n",
            "Global step: 8630,loss: 0.005781745\n",
            "\n",
            "Global step: 8631,loss: 0.005664646\n",
            "\n",
            "Global step: 8632,loss: 0.005530867\n",
            "\n",
            "Global step: 8633,loss: 0.0060462677\n",
            "\n",
            "Global step: 8634,loss: 0.005830075\n",
            "\n",
            "Global step: 8635,loss: 0.005506046\n",
            "\n",
            "Global step: 8636,loss: 0.0060027656\n",
            "\n",
            "Global step: 8637,loss: 0.006534232\n",
            "\n",
            "Global step: 8638,loss: 0.0061657806\n",
            "\n",
            "Global step: 8639,loss: 0.006176693\n",
            "\n",
            "Global step: 8640,loss: 0.0060572913\n",
            "\n",
            "Global step: 8641,loss: 0.0064947153\n",
            "\n",
            "Global step: 8642,loss: 0.00617223\n",
            "\n",
            "Global step: 8643,loss: 0.0057352195\n",
            "\n",
            "Global step: 8644,loss: 0.0055778353\n",
            "\n",
            "Global step: 8645,loss: 0.0055346214\n",
            "\n",
            "Global step: 8646,loss: 0.0060661444\n",
            "\n",
            "Global step: 8647,loss: 0.005733598\n",
            "\n",
            "Global step: 8648,loss: 0.006410928\n",
            "\n",
            "Global step: 8649,loss: 0.0059075127\n",
            "\n",
            "Global step: 8650,loss: 0.0052968822\n",
            "\n",
            "Global step: 8651,loss: 0.005705065\n",
            "\n",
            "Global step: 8652,loss: 0.005986382\n",
            "\n",
            "Global step: 8653,loss: 0.0072696297\n",
            "\n",
            "Global step: 8654,loss: 0.00609218\n",
            "\n",
            "Global step: 8655,loss: 0.005809645\n",
            "\n",
            "Global step: 8656,loss: 0.005897845\n",
            "\n",
            "Global step: 8657,loss: 0.0061183227\n",
            "\n",
            "Global step: 8658,loss: 0.0068867644\n",
            "\n",
            "Global step: 8659,loss: 0.006352586\n",
            "\n",
            "Global step: 8660,loss: 0.006327569\n",
            "\n",
            "Global step: 8661,loss: 0.0057021617\n",
            "\n",
            "Global step: 8662,loss: 0.0056229425\n",
            "\n",
            "Global step: 8663,loss: 0.0056898417\n",
            "\n",
            "Global step: 8664,loss: 0.005863347\n",
            "\n",
            "Global step: 8665,loss: 0.006033214\n",
            "\n",
            "Global step: 8666,loss: 0.0059377914\n",
            "\n",
            "Global step: 8667,loss: 0.0053853737\n",
            "\n",
            "Global step: 8668,loss: 0.0057503735\n",
            "\n",
            "Global step: 8669,loss: 0.00541671\n",
            "\n",
            "Global step: 8670,loss: 0.005525072\n",
            "\n",
            "Global step: 8671,loss: 0.0060192454\n",
            "\n",
            "Global step: 8672,loss: 0.0066043427\n",
            "\n",
            "Global step: 8673,loss: 0.006439172\n",
            "\n",
            "Global step: 8674,loss: 0.00533689\n",
            "\n",
            "Global step: 8675,loss: 0.0061190175\n",
            "\n",
            "Global step: 8676,loss: 0.0058876514\n",
            "\n",
            "Global step: 8677,loss: 0.006245691\n",
            "\n",
            "Global step: 8678,loss: 0.0061989687\n",
            "\n",
            "Global step: 8679,loss: 0.0055957357\n",
            "\n",
            "Global step: 8680,loss: 0.010367868\n",
            "\n",
            "Global step: 8681,loss: 0.005877672\n",
            "\n",
            "Global step: 8682,loss: 0.00579028\n",
            "\n",
            "Global step: 8683,loss: 0.005521391\n",
            "\n",
            "Global step: 8684,loss: 0.0061550913\n",
            "\n",
            "Global step: 8685,loss: 0.006402254\n",
            "\n",
            "Global step: 8686,loss: 0.005859877\n",
            "\n",
            "Global step: 8687,loss: 0.0058084875\n",
            "\n",
            "Global step: 8688,loss: 0.0058244923\n",
            "\n",
            "Global step: 8689,loss: 0.0063805627\n",
            "\n",
            "Global step: 8690,loss: 0.00558695\n",
            "\n",
            "Global step: 8691,loss: 0.0059199594\n",
            "\n",
            "Global step: 8692,loss: 0.006559814\n",
            "\n",
            "Global step: 8693,loss: 0.0059660445\n",
            "\n",
            "Global step: 8694,loss: 0.0058252644\n",
            "\n",
            "Global step: 8695,loss: 0.0059447093\n",
            "\n",
            "Global step: 8696,loss: 0.0057026725\n",
            "\n",
            "Global step: 8697,loss: 0.0063240477\n",
            "\n",
            "Global step: 8698,loss: 0.0061851353\n",
            "\n",
            "Global step: 8699,loss: 0.0056773587\n",
            "\n",
            "Global step: 8700,loss: 0.0059345164\n",
            "\n",
            "Global step: 8701,loss: 0.005474173\n",
            "\n",
            "Global step: 8702,loss: 0.005793341\n",
            "\n",
            "Global step: 8703,loss: 0.005709129\n",
            "\n",
            "Global step: 8704,loss: 0.005584929\n",
            "\n",
            "Global step: 8705,loss: 0.0055333907\n",
            "\n",
            "Global step: 8706,loss: 0.0054380395\n",
            "\n",
            "Global step: 8707,loss: 0.0058801062\n",
            "\n",
            "Global step: 8708,loss: 0.007854569\n",
            "\n",
            "Global step: 8709,loss: 0.0055664624\n",
            "\n",
            "Global step: 8710,loss: 0.0058611077\n",
            "\n",
            "Global step: 8711,loss: 0.005276906\n",
            "\n",
            "Global step: 8712,loss: 0.006771728\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 8713.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:04:35.200631 139837023905536 supervisor.py:1050] Recording summary at step 8713.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 8713,loss: 0.0053833374\n",
            "\n",
            "Global step: 8714,loss: 0.005564801\n",
            "\n",
            "Global step: 8715,loss: 0.005474535\n",
            "\n",
            "Global step: 8716,loss: 0.0062658624\n",
            "\n",
            "Global step: 8717,loss: 0.0064543164\n",
            "\n",
            "Global step: 8718,loss: 0.006894208\n",
            "\n",
            "Global step: 8719,loss: 0.005684534\n",
            "\n",
            "Global step: 8720,loss: 0.005791835\n",
            "\n",
            "Global step: 8721,loss: 0.00579326\n",
            "\n",
            "Global step: 8722,loss: 0.0057625985\n",
            "\n",
            "Global step: 8723,loss: 0.005678417\n",
            "\n",
            "Global step: 8724,loss: 0.006088874\n",
            "\n",
            "Global step: 8725,loss: 0.0062840884\n",
            "\n",
            "Global step: 8726,loss: 0.005758008\n",
            "\n",
            "Global step: 8727,loss: 0.0061339024\n",
            "\n",
            "Global step: 8728,loss: 0.005516432\n",
            "\n",
            "Global step: 8729,loss: 0.0057835155\n",
            "\n",
            "Global step: 8730,loss: 0.0057651354\n",
            "\n",
            "Global step: 8731,loss: 0.0056721503\n",
            "\n",
            "Global step: 8732,loss: 0.005794604\n",
            "\n",
            "Global step: 8733,loss: 0.006643684\n",
            "\n",
            "Global step: 8734,loss: 0.0060744323\n",
            "\n",
            "Global step: 8735,loss: 0.0058476645\n",
            "\n",
            "Global step: 8736,loss: 0.006158521\n",
            "\n",
            "Global step: 8737,loss: 0.0057914606\n",
            "\n",
            "Global step: 8738,loss: 0.0061195795\n",
            "\n",
            "Global step: 8739,loss: 0.005797445\n",
            "\n",
            "Global step: 8740,loss: 0.0062397313\n",
            "\n",
            "Global step: 8741,loss: 0.005771599\n",
            "\n",
            "Global step: 8742,loss: 0.006243196\n",
            "\n",
            "Global step: 8743,loss: 0.0063079055\n",
            "\n",
            "Global step: 8744,loss: 0.005384745\n",
            "\n",
            "Global step: 8745,loss: 0.0057000946\n",
            "\n",
            "Global step: 8746,loss: 0.0060175564\n",
            "\n",
            "Global step: 8747,loss: 0.005799682\n",
            "\n",
            "Global step: 8748,loss: 0.0057934383\n",
            "\n",
            "Global step: 8749,loss: 0.00583648\n",
            "\n",
            "Global step: 8750,loss: 0.006179754\n",
            "\n",
            "Global step: 8751,loss: 0.006168488\n",
            "\n",
            "Global step: 8752,loss: 0.0059261965\n",
            "\n",
            "Global step: 8753,loss: 0.005769544\n",
            "\n",
            "Global step: 8754,loss: 0.0056568426\n",
            "\n",
            "Global step: 8755,loss: 0.0058642793\n",
            "\n",
            "Global step: 8756,loss: 0.0056306263\n",
            "\n",
            "Global step: 8757,loss: 0.0063970257\n",
            "\n",
            "Global step: 8758,loss: 0.0057999217\n",
            "\n",
            "Global step: 8759,loss: 0.005551606\n",
            "\n",
            "Global step: 8760,loss: 0.006412646\n",
            "\n",
            "Global step: 8761,loss: 0.0059781717\n",
            "\n",
            "Global step: 8762,loss: 0.005692383\n",
            "\n",
            "Global step: 8763,loss: 0.005605297\n",
            "\n",
            "Global step: 8764,loss: 0.0055464036\n",
            "\n",
            "Global step: 8765,loss: 0.005938803\n",
            "\n",
            "Global step: 8766,loss: 0.0061530313\n",
            "\n",
            "Global step: 8767,loss: 0.006052645\n",
            "\n",
            "Global step: 8768,loss: 0.005989847\n",
            "\n",
            "Global step: 8769,loss: 0.006065276\n",
            "\n",
            "Global step: 8770,loss: 0.0059694685\n",
            "\n",
            "Global step: 8771,loss: 0.005836269\n",
            "\n",
            "Global step: 8772,loss: 0.0057416772\n",
            "\n",
            "Global step: 8773,loss: 0.0060833152\n",
            "\n",
            "Global step: 8774,loss: 0.0058290125\n",
            "\n",
            "Global step: 8775,loss: 0.006085908\n",
            "\n",
            "Global step: 8776,loss: 0.006110265\n",
            "\n",
            "Global step: 8777,loss: 0.006256176\n",
            "\n",
            "Global step: 8778,loss: 0.0056765205\n",
            "\n",
            "Global step: 8779,loss: 0.0058227065\n",
            "\n",
            "Global step: 8780,loss: 0.0061824936\n",
            "\n",
            "Global step: 8781,loss: 0.0058424296\n",
            "\n",
            "Global step: 8782,loss: 0.006229755\n",
            "\n",
            "Global step: 8783,loss: 0.005917987\n",
            "\n",
            "Global step: 8784,loss: 0.0056736143\n",
            "\n",
            "Global step: 8785,loss: 0.0064474475\n",
            "\n",
            "Global step: 8786,loss: 0.0057568518\n",
            "\n",
            "Global step: 8787,loss: 0.0062435665\n",
            "\n",
            "Global step: 8788,loss: 0.005901041\n",
            "\n",
            "Global step: 8789,loss: 0.0059133833\n",
            "\n",
            "Global step: 8790,loss: 0.0060793613\n",
            "\n",
            "Global step: 8791,loss: 0.005574505\n",
            "\n",
            "Global step: 8792,loss: 0.0057874876\n",
            "\n",
            "Global step: 8793,loss: 0.006067041\n",
            "\n",
            "Global step: 8794,loss: 0.0057672015\n",
            "\n",
            "Global step: 8795,loss: 0.005774297\n",
            "\n",
            "Global step: 8796,loss: 0.005998335\n",
            "\n",
            "Global step: 8797,loss: 0.0062074303\n",
            "\n",
            "Global step: 8798,loss: 0.0060249977\n",
            "\n",
            "Global step: 8799,loss: 0.005777955\n",
            "\n",
            "Global step: 8800,loss: 0.005778675\n",
            "\n",
            "Global step: 8801,loss: 0.0059912223\n",
            "\n",
            "Global step: 8802,loss: 0.006148477\n",
            "\n",
            "Global step: 8803,loss: 0.005722422\n",
            "\n",
            "Global step: 8804,loss: 0.0058273873\n",
            "\n",
            "Global step: 8805,loss: 0.0056022923\n",
            "\n",
            "Global step: 8806,loss: 0.005708802\n",
            "\n",
            "Global step: 8807,loss: 0.005868619\n",
            "\n",
            "Global step: 8808,loss: 0.0058458266\n",
            "\n",
            "Global step: 8809,loss: 0.00569431\n",
            "\n",
            "Global step: 8810,loss: 0.005628338\n",
            "\n",
            "Global step: 8811,loss: 0.005749882\n",
            "\n",
            "Global step: 8812,loss: 0.0057944097\n",
            "\n",
            "Global step: 8813,loss: 0.0054879105\n",
            "\n",
            "Global step: 8814,loss: 0.0057728034\n",
            "\n",
            "Global step: 8815,loss: 0.005939042\n",
            "\n",
            "Global step: 8816,loss: 0.0064623356\n",
            "\n",
            "Global step: 8817,loss: 0.0059382254\n",
            "\n",
            "Global step: 8818,loss: 0.0060329977\n",
            "\n",
            "Global step: 8819,loss: 0.005844251\n",
            "\n",
            "Global step: 8820,loss: 0.0060282424\n",
            "\n",
            "Global step: 8821,loss: 0.006073843\n",
            "\n",
            "Global step: 8822,loss: 0.005826921\n",
            "\n",
            "Global step: 8823,loss: 0.0057709306\n",
            "\n",
            "Global step: 8824,loss: 0.006367638\n",
            "\n",
            "Global step: 8825,loss: 0.0059513897\n",
            "\n",
            "Global step: 8826,loss: 0.0058073653\n",
            "\n",
            "Global step: 8827,loss: 0.0058785337\n",
            "\n",
            "Global step: 8828,loss: 0.006102556\n",
            "\n",
            "Global step: 8829,loss: 0.0059802607\n",
            "\n",
            "Global step: 8830,loss: 0.0061063757\n",
            "\n",
            "Global step: 8831,loss: 0.0060584527\n",
            "\n",
            "Global step: 8832,loss: 0.006043586\n",
            "\n",
            "Global step: 8833,loss: 0.005678243\n",
            "\n",
            "Global step: 8834,loss: 0.006243586\n",
            "\n",
            "Global step: 8835,loss: 0.006188984\n",
            "\n",
            "Global step: 8836,loss: 0.005999549\n",
            "\n",
            "Global step: 8837,loss: 0.0054149083\n",
            "\n",
            "Global step: 8838,loss: 0.006492569\n",
            "\n",
            "Global step: 8839,loss: 0.005778868\n",
            "\n",
            "Global step: 8840,loss: 0.0062935892\n",
            "\n",
            "Global step: 8841,loss: 0.006215924\n",
            "\n",
            "Global step: 8842,loss: 0.0058701085\n",
            "\n",
            "Global step: 8843,loss: 0.0058212467\n",
            "\n",
            "Global step: 8844,loss: 0.005847121\n",
            "\n",
            "Global step: 8845,loss: 0.012674669\n",
            "\n",
            "Global step: 8846,loss: 0.0057854294\n",
            "\n",
            "Global step: 8847,loss: 0.005902585\n",
            "\n",
            "Global step: 8848,loss: 0.006518172\n",
            "\n",
            "Global step: 8849,loss: 0.005965438\n",
            "\n",
            "Global step: 8850,loss: 0.0051960023\n",
            "\n",
            "Global step: 8851,loss: 0.0060348995\n",
            "\n",
            "Global step: 8852,loss: 0.0057551987\n",
            "\n",
            "Global step: 8853,loss: 0.005489987\n",
            "\n",
            "Global step: 8854,loss: 0.006184479\n",
            "\n",
            "Global step: 8855,loss: 0.0059865625\n",
            "\n",
            "Global step: 8856,loss: 0.0061634434\n",
            "\n",
            "Global step: 8857,loss: 0.005648743\n",
            "\n",
            "Global step: 8858,loss: 0.0060330406\n",
            "\n",
            "Global step: 8859,loss: 0.005503256\n",
            "\n",
            "Global step: 8860,loss: 0.006112613\n",
            "\n",
            "Global step: 8861,loss: 0.006090326\n",
            "\n",
            "Global step: 8862,loss: 0.005932105\n",
            "\n",
            "Global step: 8863,loss: 0.0056522023\n",
            "\n",
            "Global step: 8864,loss: 0.0061456366\n",
            "\n",
            "Global step: 8865,loss: 0.0059588933\n",
            "\n",
            "Global step: 8866,loss: 0.005927068\n",
            "\n",
            "Global step: 8867,loss: 0.0056000873\n",
            "\n",
            "Global step: 8868,loss: 0.0056259953\n",
            "\n",
            "Global step: 8869,loss: 0.005658108\n",
            "\n",
            "Global step: 8870,loss: 0.005844742\n",
            "\n",
            "Global step: 8871,loss: 0.005948315\n",
            "\n",
            "Global step: 8872,loss: 0.0061536636\n",
            "\n",
            "Global step: 8873,loss: 0.0056431936\n",
            "\n",
            "Global step: 8874,loss: 0.0056647803\n",
            "\n",
            "Global step: 8875,loss: 0.005775534\n",
            "\n",
            "Global step: 8876,loss: 0.0058833584\n",
            "\n",
            "Global step: 8877,loss: 0.006015722\n",
            "\n",
            "Global step: 8878,loss: 0.0059806556\n",
            "\n",
            "Global step: 8879,loss: 0.005512549\n",
            "\n",
            "Global step: 8880,loss: 0.006208737\n",
            "\n",
            "Global step: 8881,loss: 0.0058289566\n",
            "\n",
            "Global step: 8882,loss: 0.005555987\n",
            "\n",
            "Global step: 8883,loss: 0.0060277283\n",
            "\n",
            "Global step: 8884,loss: 0.005621768\n",
            "\n",
            "Global step: 8885,loss: 0.0059127235\n",
            "\n",
            "Global step: 8886,loss: 0.0054546488\n",
            "\n",
            "Global step: 8887,loss: 0.0057818913\n",
            "\n",
            "Global step: 8888,loss: 0.005704934\n",
            "\n",
            "Global step: 8889,loss: 0.0055438937\n",
            "\n",
            "Global step: 8890,loss: 0.006007317\n",
            "\n",
            "Global step: 8891,loss: 0.0062773204\n",
            "\n",
            "Global step: 8892,loss: 0.005644649\n",
            "\n",
            "Global step: 8893,loss: 0.007446506\n",
            "\n",
            "Global step: 8894,loss: 0.0056521646\n",
            "\n",
            "Global step: 8895,loss: 0.005775847\n",
            "\n",
            "Global step: 8896,loss: 0.0062038233\n",
            "\n",
            "Global step: 8897,loss: 0.0065487255\n",
            "\n",
            "Global step: 8898,loss: 0.006026875\n",
            "\n",
            "Global step: 8899,loss: 0.005615288\n",
            "\n",
            "Global step: 8900,loss: 0.006985949\n",
            "\n",
            "Global step: 8901,loss: 0.0064421264\n",
            "\n",
            "Global step: 8902,loss: 0.006006101\n",
            "\n",
            "Global step: 8903,loss: 0.0064796936\n",
            "\n",
            "Global step: 8904,loss: 0.005913711\n",
            "\n",
            "Global step: 8905,loss: 0.0061256327\n",
            "\n",
            "Global step: 8906,loss: 0.0058491924\n",
            "\n",
            "Global step: 8907,loss: 0.005647677\n",
            "\n",
            "Global step: 8908,loss: 0.0058807074\n",
            "\n",
            "Global step: 8909,loss: 0.006284225\n",
            "\n",
            "Global step: 8910,loss: 0.0056568435\n",
            "\n",
            "Global step: 8911,loss: 0.006124874\n",
            "\n",
            "Global step: 8912,loss: 0.005662864\n",
            "\n",
            "Global step: 8913,loss: 0.005713794\n",
            "\n",
            "Global step: 8914,loss: 0.006226149\n",
            "\n",
            "Global step: 8915,loss: 0.005638668\n",
            "\n",
            "Global step: 8916,loss: 0.006064265\n",
            "\n",
            "Global step: 8917,loss: 0.0058594416\n",
            "\n",
            "Global step: 8918,loss: 0.005512971\n",
            "\n",
            "Global step: 8919,loss: 0.006514892\n",
            "\n",
            "Global step: 8920,loss: 0.0060902634\n",
            "\n",
            "Global step: 8921,loss: 0.005738774\n",
            "\n",
            "Global step: 8922,loss: 0.005414349\n",
            "\n",
            "Global step: 8923,loss: 0.009457299\n",
            "\n",
            "Global step: 8924,loss: 0.006307458\n",
            "\n",
            "Global step: 8925,loss: 0.0059393817\n",
            "\n",
            "Global step: 8926,loss: 0.0063290712\n",
            "\n",
            "Global step: 8927,loss: 0.0062731113\n",
            "\n",
            "Global step: 8928,loss: 0.0069558015\n",
            "\n",
            "Global step: 8929,loss: 0.006394484\n",
            "\n",
            "Global step: 8930,loss: 0.0055423304\n",
            "\n",
            "Global step: 8931,loss: 0.0058242846\n",
            "\n",
            "Global step: 8932,loss: 0.006047469\n",
            "\n",
            "Global step: 8933,loss: 0.005780742\n",
            "\n",
            "Global step: 8934,loss: 0.005300572\n",
            "\n",
            "Global step: 8935,loss: 0.0061209244\n",
            "\n",
            "Global step: 8936,loss: 0.005652972\n",
            "\n",
            "Global step: 8937,loss: 0.0061334907\n",
            "\n",
            "Global step: 8938,loss: 0.005846925\n",
            "\n",
            "Global step: 8939,loss: 0.0054088617\n",
            "\n",
            "Global step: 8940,loss: 0.0062334402\n",
            "\n",
            "Global step: 8941,loss: 0.0054238676\n",
            "\n",
            "Global step: 8942,loss: 0.005784927\n",
            "\n",
            "Global step: 8943,loss: 0.0061068903\n",
            "\n",
            "Global step: 8944,loss: 0.006240184\n",
            "\n",
            "Global step: 8945,loss: 0.0058434466\n",
            "\n",
            "Global step: 8946,loss: 0.0057984814\n",
            "\n",
            "Global step: 8947,loss: 0.0061372505\n",
            "\n",
            "Global step: 8948,loss: 0.005158311\n",
            "\n",
            "Global step: 8949,loss: 0.0066102026\n",
            "\n",
            "Global step: 8950,loss: 0.0061886087\n",
            "\n",
            "Global step: 8951,loss: 0.005778504\n",
            "\n",
            "Global step: 8952,loss: 0.005947862\n",
            "\n",
            "Global step: 8953,loss: 0.006074226\n",
            "\n",
            "Global step: 8954,loss: 0.0056310995\n",
            "\n",
            "Global step: 8955,loss: 0.0060298997\n",
            "\n",
            "Global step: 8956,loss: 0.0058781714\n",
            "\n",
            "Global step: 8957,loss: 0.0061506974\n",
            "\n",
            "Global step: 8958,loss: 0.005546311\n",
            "\n",
            "Global step: 8959,loss: 0.0057846503\n",
            "\n",
            "Global step: 8960,loss: 0.006031297\n",
            "\n",
            "Global step: 8961,loss: 0.005529751\n",
            "\n",
            "Global step: 8962,loss: 0.0057604285\n",
            "\n",
            "Global step: 8963,loss: 0.006095632\n",
            "\n",
            "Global step: 8964,loss: 0.006262843\n",
            "\n",
            "Global step: 8965,loss: 0.0055424585\n",
            "\n",
            "Global step: 8966,loss: 0.00566158\n",
            "\n",
            "Global step: 8967,loss: 0.0060823583\n",
            "\n",
            "Global step: 8968,loss: 0.0056492435\n",
            "\n",
            "Global step: 8969,loss: 0.005621562\n",
            "\n",
            "Global step: 8970,loss: 0.0058474\n",
            "\n",
            "Global step: 8971,loss: 0.005531008\n",
            "\n",
            "Global step: 8972,loss: 0.005285459\n",
            "\n",
            "Global step: 8973,loss: 0.0056689507\n",
            "\n",
            "Global step: 8974,loss: 0.0054696347\n",
            "\n",
            "Global step: 8975,loss: 0.005828565\n",
            "\n",
            "Global step: 8976,loss: 0.006172357\n",
            "\n",
            "Global step: 8977,loss: 0.006827421\n",
            "\n",
            "Global step: 8978,loss: 0.005850045\n",
            "\n",
            "Global step: 8979,loss: 0.005869176\n",
            "\n",
            "Global step: 8980,loss: 0.00531614\n",
            "\n",
            "Global step: 8981,loss: 0.005682396\n",
            "\n",
            "Global step: 8982,loss: 0.0060336553\n",
            "\n",
            "Global step: 8983,loss: 0.005726821\n",
            "\n",
            "Global step: 8984,loss: 0.0059524584\n",
            "\n",
            "Global step: 8985,loss: 0.014319129\n",
            "\n",
            "Global step: 8986,loss: 0.005572406\n",
            "\n",
            "Global step: 8987,loss: 0.005927776\n",
            "\n",
            "Global step: 8988,loss: 0.006516046\n",
            "\n",
            "Global step: 8989,loss: 0.0063275523\n",
            "\n",
            "Global step: 8990,loss: 0.0061900895\n",
            "\n",
            "Global step: 8991,loss: 0.005822798\n",
            "\n",
            "Global step: 8992,loss: 0.0062453374\n",
            "\n",
            "Global step: 8993,loss: 0.0057225847\n",
            "\n",
            "Global step: 8994,loss: 0.006334943\n",
            "\n",
            "Global step: 8995,loss: 0.005952318\n",
            "\n",
            "Global step: 8996,loss: 0.005751672\n",
            "\n",
            "Global step: 8997,loss: 0.0071558864\n",
            "\n",
            "Global step: 8998,loss: 0.0057537104\n",
            "\n",
            "Global step: 8999,loss: 0.005721785\n",
            "\n",
            "Global step: 9000,loss: 0.0060094185\n",
            "\n",
            "Global step: 9001,loss: 0.0058042947\n",
            "\n",
            "Global step: 9002,loss: 0.005359542\n",
            "\n",
            "Global step: 9003,loss: 0.005964861\n",
            "\n",
            "Global step: 9004,loss: 0.0057918252\n",
            "\n",
            "Global step: 9005,loss: 0.006265882\n",
            "\n",
            "Global step: 9006,loss: 0.0061548585\n",
            "\n",
            "Global step: 9007,loss: 0.007125302\n",
            "\n",
            "Global step: 9008,loss: 0.005804836\n",
            "\n",
            "Global step: 9009,loss: 0.005894091\n",
            "\n",
            "Global step: 9010,loss: 0.0060274317\n",
            "\n",
            "Global step: 9011,loss: 0.005921833\n",
            "\n",
            "Global step: 9012,loss: 0.005729463\n",
            "\n",
            "Global step: 9013,loss: 0.0059299045\n",
            "\n",
            "Global step: 9014,loss: 0.005570916\n",
            "\n",
            "Global step: 9015,loss: 0.0060906624\n",
            "\n",
            "Global step: 9016,loss: 0.0057803723\n",
            "\n",
            "Global step: 9017,loss: 0.006434855\n",
            "\n",
            "Global step: 9018,loss: 0.005817955\n",
            "\n",
            "Global step: 9019,loss: 0.005785546\n",
            "\n",
            "Global step: 9020,loss: 0.0057754843\n",
            "\n",
            "Global step: 9021,loss: 0.007975544\n",
            "\n",
            "Global step: 9022,loss: 0.0063519543\n",
            "\n",
            "Global step: 9023,loss: 0.0059348173\n",
            "\n",
            "Global step: 9024,loss: 0.0063441475\n",
            "\n",
            "Global step: 9025,loss: 0.0060179913\n",
            "\n",
            "Global step: 9026,loss: 0.00685747\n",
            "\n",
            "Global step: 9027,loss: 0.0057806512\n",
            "\n",
            "Global step: 9028,loss: 0.006106463\n",
            "\n",
            "Global step: 9029,loss: 0.0063971207\n",
            "\n",
            "Global step: 9030,loss: 0.0058083492\n",
            "\n",
            "Global step: 9031,loss: 0.006227379\n",
            "\n",
            "Global step: 9032,loss: 0.0059798416\n",
            "\n",
            "Global step: 9033,loss: 0.0069261263\n",
            "\n",
            "Global step: 9034,loss: 0.00569703\n",
            "\n",
            "Global step: 9035,loss: 0.007267475\n",
            "\n",
            "Global step: 9036,loss: 0.0062043653\n",
            "\n",
            "Global step: 9037,loss: 0.0065267077\n",
            "\n",
            "Global step: 9038,loss: 0.0056724916\n",
            "\n",
            "Global step: 9039,loss: 0.006107243\n",
            "\n",
            "Global step: 9040,loss: 0.0063846125\n",
            "\n",
            "Global step: 9041,loss: 0.006377747\n",
            "\n",
            "Global step: 9042,loss: 0.006236758\n",
            "\n",
            "Global step: 9043,loss: 0.006851076\n",
            "\n",
            "Global step: 9044,loss: 0.010317488\n",
            "\n",
            "Global step: 9045,loss: 0.0062359255\n",
            "\n",
            "Global step: 9046,loss: 0.006921635\n",
            "\n",
            "Global step: 9047,loss: 0.0058030104\n",
            "\n",
            "Global step: 9048,loss: 0.0055338806\n",
            "\n",
            "Global step: 9049,loss: 0.0062271575\n",
            "\n",
            "Global step: 9050,loss: 0.005880851\n",
            "\n",
            "Global step: 9051,loss: 0.0062276446\n",
            "\n",
            "Global step: 9052,loss: 0.0058898623\n",
            "\n",
            "Global step: 9053,loss: 0.00616401\n",
            "\n",
            "Global step: 9054,loss: 0.005386279\n",
            "\n",
            "Global step: 9055,loss: 0.005763702\n",
            "\n",
            "Global step: 9056,loss: 0.005974982\n",
            "\n",
            "Global step: 9057,loss: 0.006369402\n",
            "\n",
            "Global step: 9058,loss: 0.0058180867\n",
            "\n",
            "Global step: 9059,loss: 0.0058486657\n",
            "\n",
            "Global step: 9060,loss: 0.0060865288\n",
            "\n",
            "Global step: 9061,loss: 0.0057649957\n",
            "\n",
            "Global step: 9062,loss: 0.0066736415\n",
            "\n",
            "Global step: 9063,loss: 0.006103706\n",
            "\n",
            "Global step: 9064,loss: 0.0062888353\n",
            "\n",
            "Global step: 9065,loss: 0.005646092\n",
            "\n",
            "Global step: 9066,loss: 0.005561136\n",
            "\n",
            "Global step: 9067,loss: 0.005559527\n",
            "\n",
            "Global step: 9068,loss: 0.005811251\n",
            "\n",
            "Global step: 9069,loss: 0.00552423\n",
            "\n",
            "Global step: 9070,loss: 0.005833983\n",
            "\n",
            "Global step: 9071,loss: 0.0060149822\n",
            "\n",
            "Global step: 9072,loss: 0.00595922\n",
            "\n",
            "Global step: 9073,loss: 0.005962853\n",
            "\n",
            "Global step: 9074,loss: 0.0056880163\n",
            "\n",
            "Global step: 9075,loss: 0.0063031614\n",
            "\n",
            "Global step: 9076,loss: 0.0054961527\n",
            "\n",
            "Global step: 9077,loss: 0.0052472325\n",
            "\n",
            "Global step: 9078,loss: 0.0056213927\n",
            "\n",
            "Global step: 9079,loss: 0.005841981\n",
            "\n",
            "Global step: 9080,loss: 0.0066391784\n",
            "\n",
            "Global step: 9081,loss: 0.005626935\n",
            "\n",
            "Global step: 9082,loss: 0.006657432\n",
            "\n",
            "Global step: 9083,loss: 0.0064686085\n",
            "\n",
            "Global step: 9084,loss: 0.0055635315\n",
            "\n",
            "Global step: 9085,loss: 0.005370449\n",
            "\n",
            "Global step: 9086,loss: 0.006035153\n",
            "\n",
            "Global step: 9087,loss: 0.006438605\n",
            "\n",
            "Global step: 9088,loss: 0.006073822\n",
            "\n",
            "Global step: 9089,loss: 0.0057881866\n",
            "\n",
            "Global step: 9090,loss: 0.0062966025\n",
            "\n",
            "Global step: 9091,loss: 0.0060987957\n",
            "\n",
            "Global step: 9092,loss: 0.006189036\n",
            "\n",
            "Global step: 9093,loss: 0.005879254\n",
            "\n",
            "Global step: 9094,loss: 0.0059690764\n",
            "\n",
            "Global step: 9095,loss: 0.006463389\n",
            "\n",
            "Global step: 9096,loss: 0.006035079\n",
            "\n",
            "Global step: 9097,loss: 0.0060485043\n",
            "\n",
            "Global step: 9098,loss: 0.006342422\n",
            "\n",
            "Global step: 9099,loss: 0.005643483\n",
            "\n",
            "Global step: 9100,loss: 0.005895718\n",
            "\n",
            "Global step: 9101,loss: 0.0060274675\n",
            "\n",
            "Global step: 9102,loss: 0.0053533954\n",
            "\n",
            "Global step: 9103,loss: 0.005522647\n",
            "\n",
            "Global step: 9104,loss: 0.0061096516\n",
            "\n",
            "Global step: 9105,loss: 0.0068107154\n",
            "\n",
            "Global step: 9106,loss: 0.005854484\n",
            "\n",
            "Global step: 9107,loss: 0.006662458\n",
            "\n",
            "Global step: 9108,loss: 0.005492628\n",
            "\n",
            "Global step: 9109,loss: 0.0069450624\n",
            "\n",
            "Global step: 9110,loss: 0.0057367273\n",
            "\n",
            "Global step: 9111,loss: 0.0063317586\n",
            "\n",
            "Global step: 9112,loss: 0.0077239443\n",
            "\n",
            "Global step: 9113,loss: 0.0059380364\n",
            "\n",
            "Global step: 9114,loss: 0.0058718557\n",
            "\n",
            "Global step: 9115,loss: 0.0062049096\n",
            "\n",
            "Global step: 9116,loss: 0.0056000696\n",
            "\n",
            "Global step: 9117,loss: 0.0054436126\n",
            "\n",
            "Global step: 9118,loss: 0.005470751\n",
            "\n",
            "Global step: 9119,loss: 0.0060958345\n",
            "\n",
            "Global step: 9120,loss: 0.005347283\n",
            "\n",
            "Global step: 9121,loss: 0.006011222\n",
            "\n",
            "Global step: 9122,loss: 0.0061816103\n",
            "\n",
            "Global step: 9123,loss: 0.0060331407\n",
            "\n",
            "Global step: 9124,loss: 0.0059355274\n",
            "\n",
            "Global step: 9125,loss: 0.005467569\n",
            "\n",
            "Global step: 9126,loss: 0.0057733264\n",
            "\n",
            "Global step: 9127,loss: 0.005547621\n",
            "\n",
            "Global step: 9128,loss: 0.0064449925\n",
            "\n",
            "Global step: 9129,loss: 0.005970661\n",
            "\n",
            "Global step: 9130,loss: 0.0052927462\n",
            "\n",
            "Global step: 9131,loss: 0.0056220265\n",
            "\n",
            "Global step: 9132,loss: 0.0057014804\n",
            "\n",
            "Global step: 9133,loss: 0.005668312\n",
            "\n",
            "Global step: 9134,loss: 0.0060970467\n",
            "\n",
            "Global step: 9135,loss: 0.005772247\n",
            "\n",
            "Global step: 9136,loss: 0.006236546\n",
            "\n",
            "Global step: 9137,loss: 0.0055337846\n",
            "\n",
            "Global step: 9138,loss: 0.0054434855\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 9138,val_loss: 0.007815819102315566\n",
            "\n",
            "Training for epoch 14/15:\n",
            "Global step: 9139,loss: 0.005278872\n",
            "\n",
            "Global step: 9140,loss: 0.0056167054\n",
            "\n",
            "Global step: 9141,loss: 0.0054927515\n",
            "\n",
            "Global step: 9142,loss: 0.0051914803\n",
            "\n",
            "Global step: 9143,loss: 0.0061305976\n",
            "\n",
            "Global step: 9144,loss: 0.005553213\n",
            "\n",
            "Global step: 9145,loss: 0.005623577\n",
            "\n",
            "Global step: 9146,loss: 0.009275606\n",
            "\n",
            "Global step: 9147,loss: 0.005101315\n",
            "\n",
            "Global step: 9148,loss: 0.0058053476\n",
            "\n",
            "Global step: 9149,loss: 0.0058131963\n",
            "\n",
            "Global step: 9150,loss: 0.0055238777\n",
            "\n",
            "Global step: 9151,loss: 0.0064302324\n",
            "\n",
            "Global step: 9152,loss: 0.0059534158\n",
            "\n",
            "Global step: 9153,loss: 0.0056499303\n",
            "\n",
            "Global step: 9154,loss: 0.0060677743\n",
            "\n",
            "Global step: 9155,loss: 0.005334537\n",
            "\n",
            "Global step: 9156,loss: 0.005324464\n",
            "\n",
            "Global step: 9157,loss: 0.005906533\n",
            "\n",
            "Global step: 9158,loss: 0.005499698\n",
            "\n",
            "Global step: 9159,loss: 0.0059410497\n",
            "\n",
            "Global step: 9160,loss: 0.005585542\n",
            "\n",
            "Global step: 9161,loss: 0.0057683913\n",
            "\n",
            "Global step: 9162,loss: 0.005403948\n",
            "\n",
            "Global step: 9163,loss: 0.005451809\n",
            "\n",
            "Global step: 9164,loss: 0.005255951\n",
            "\n",
            "Global step: 9165,loss: 0.0059341313\n",
            "\n",
            "Global step: 9166,loss: 0.0055658235\n",
            "\n",
            "Global step: 9167,loss: 0.0056240186\n",
            "\n",
            "Global step: 9168,loss: 0.0054558683\n",
            "\n",
            "Global step: 9169,loss: 0.005930995\n",
            "\n",
            "Global step: 9170,loss: 0.0059013683\n",
            "\n",
            "Global step: 9171,loss: 0.0059648594\n",
            "\n",
            "Global step: 9172,loss: 0.0055621513\n",
            "\n",
            "Global step: 9173,loss: 0.005278412\n",
            "\n",
            "Global step: 9174,loss: 0.00534907\n",
            "\n",
            "Global step: 9175,loss: 0.0054730903\n",
            "\n",
            "Global step: 9176,loss: 0.0064732567\n",
            "\n",
            "Global step: 9177,loss: 0.0059945774\n",
            "\n",
            "Global step: 9178,loss: 0.005775976\n",
            "\n",
            "Global step: 9179,loss: 0.0055681816\n",
            "\n",
            "Global step: 9180,loss: 0.005455363\n",
            "\n",
            "Global step: 9181,loss: 0.0067388513\n",
            "\n",
            "Global step: 9182,loss: 0.0055353194\n",
            "\n",
            "Global step: 9183,loss: 0.005533216\n",
            "\n",
            "Global step: 9184,loss: 0.005802696\n",
            "\n",
            "Global step: 9185,loss: 0.006395286\n",
            "\n",
            "Global step: 9186,loss: 0.0058501814\n",
            "\n",
            "Global step: 9187,loss: 0.0057898685\n",
            "\n",
            "Global step: 9188,loss: 0.0053406395\n",
            "\n",
            "Global step: 9189,loss: 0.008189497\n",
            "\n",
            "Global step: 9190,loss: 0.0053500356\n",
            "\n",
            "Global step: 9191,loss: 0.0057174773\n",
            "\n",
            "Global step: 9192,loss: 0.0056290915\n",
            "\n",
            "Global step: 9193,loss: 0.005744418\n",
            "\n",
            "Global step: 9194,loss: 0.0059243096\n",
            "\n",
            "Global step: 9195,loss: 0.0062518045\n",
            "\n",
            "Global step: 9196,loss: 0.005323858\n",
            "\n",
            "Global step: 9197,loss: 0.00605411\n",
            "\n",
            "Global step: 9198,loss: 0.0060048993\n",
            "\n",
            "Global step: 9199,loss: 0.0056415866\n",
            "\n",
            "Global step: 9200,loss: 0.0058446936\n",
            "\n",
            "Global step: 9201,loss: 0.0055152965\n",
            "\n",
            "Global step: 9202,loss: 0.0053613745\n",
            "\n",
            "Global step: 9203,loss: 0.0051600193\n",
            "\n",
            "Global step: 9204,loss: 0.005910272\n",
            "\n",
            "Global step: 9205,loss: 0.0057142666\n",
            "\n",
            "Global step: 9206,loss: 0.0053818924\n",
            "\n",
            "Global step: 9207,loss: 0.0055117332\n",
            "\n",
            "Global step: 9208,loss: 0.0053032725\n",
            "\n",
            "Global step: 9209,loss: 0.00569764\n",
            "\n",
            "Global step: 9210,loss: 0.0052623195\n",
            "\n",
            "Global step: 9211,loss: 0.0059661623\n",
            "\n",
            "Global step: 9212,loss: 0.005459125\n",
            "\n",
            "Global step: 9213,loss: 0.008616834\n",
            "\n",
            "Global step: 9214,loss: 0.005432513\n",
            "\n",
            "Global step: 9215,loss: 0.005962026\n",
            "\n",
            "Global step: 9216,loss: 0.0060928324\n",
            "\n",
            "Global step: 9217,loss: 0.005650458\n",
            "\n",
            "Global step: 9218,loss: 0.005464046\n",
            "\n",
            "Global step: 9219,loss: 0.0055037052\n",
            "\n",
            "Global step: 9220,loss: 0.0071697813\n",
            "\n",
            "Global step: 9221,loss: 0.005472074\n",
            "\n",
            "Global step: 9222,loss: 0.005907121\n",
            "\n",
            "Global step: 9223,loss: 0.006180361\n",
            "\n",
            "Global step: 9224,loss: 0.0058201477\n",
            "\n",
            "Global step: 9225,loss: 0.005702175\n",
            "\n",
            "Global step: 9226,loss: 0.0059011425\n",
            "\n",
            "Global step: 9227,loss: 0.005048814\n",
            "\n",
            "Global step: 9228,loss: 0.0058210148\n",
            "\n",
            "Global step: 9229,loss: 0.005964825\n",
            "\n",
            "Global step: 9230,loss: 0.0065707434\n",
            "\n",
            "Global step: 9231,loss: 0.005587218\n",
            "\n",
            "Global step: 9232,loss: 0.0058692764\n",
            "\n",
            "Global step: 9233,loss: 0.005089023\n",
            "\n",
            "Global step: 9234,loss: 0.008359313\n",
            "\n",
            "Global step: 9235,loss: 0.006204769\n",
            "\n",
            "Global step: 9236,loss: 0.0059341537\n",
            "\n",
            "Global step: 9237,loss: 0.0059812255\n",
            "\n",
            "Global step: 9238,loss: 0.0053527965\n",
            "\n",
            "Global step: 9239,loss: 0.005607136\n",
            "\n",
            "Global step: 9240,loss: 0.005367443\n",
            "\n",
            "Global step: 9241,loss: 0.0054034106\n",
            "\n",
            "Global step: 9242,loss: 0.005507784\n",
            "\n",
            "Global step: 9243,loss: 0.005620118\n",
            "\n",
            "Global step: 9244,loss: 0.0053320206\n",
            "\n",
            "Global step: 9245,loss: 0.005672713\n",
            "\n",
            "Global step: 9246,loss: 0.0055582817\n",
            "\n",
            "Global step: 9247,loss: 0.006008172\n",
            "\n",
            "Global step: 9248,loss: 0.0056175697\n",
            "\n",
            "Global step: 9249,loss: 0.0058066123\n",
            "\n",
            "Global step: 9250,loss: 0.0057565495\n",
            "\n",
            "Global step: 9251,loss: 0.0057452535\n",
            "\n",
            "Global step: 9252,loss: 0.0055129817\n",
            "\n",
            "Global step: 9253,loss: 0.0052560973\n",
            "\n",
            "Global step: 9254,loss: 0.0056114085\n",
            "\n",
            "Global step: 9255,loss: 0.0056517054\n",
            "\n",
            "Global step: 9256,loss: 0.006027188\n",
            "\n",
            "Global step: 9257,loss: 0.0062997853\n",
            "\n",
            "Global step: 9258,loss: 0.0062022004\n",
            "\n",
            "Global step: 9259,loss: 0.0057070176\n",
            "\n",
            "Global step: 9260,loss: 0.0057203704\n",
            "\n",
            "Global step: 9261,loss: 0.0059624584\n",
            "\n",
            "Global step: 9262,loss: 0.005446318\n",
            "\n",
            "Global step: 9263,loss: 0.005504234\n",
            "\n",
            "Global step: 9264,loss: 0.005738921\n",
            "\n",
            "Global step: 9265,loss: 0.0058415\n",
            "\n",
            "Global step: 9266,loss: 0.0062434953\n",
            "\n",
            "Global step: 9267,loss: 0.0055835233\n",
            "\n",
            "Global step: 9268,loss: 0.005694179\n",
            "\n",
            "Global step: 9269,loss: 0.0057970555\n",
            "\n",
            "Global step: 9270,loss: 0.00567842\n",
            "\n",
            "Global step: 9271,loss: 0.005399097\n",
            "\n",
            "Global step: 9272,loss: 0.0057685324\n",
            "\n",
            "Global step: 9273,loss: 0.0055836\n",
            "\n",
            "Global step: 9274,loss: 0.005663232\n",
            "\n",
            "Global step: 9275,loss: 0.0059834016\n",
            "\n",
            "Global step: 9276,loss: 0.0058573876\n",
            "\n",
            "Global step: 9277,loss: 0.0057134912\n",
            "\n",
            "Global step: 9278,loss: 0.0053712414\n",
            "\n",
            "Global step: 9279,loss: 0.00531773\n",
            "\n",
            "Global step: 9280,loss: 0.0060293865\n",
            "\n",
            "Global step: 9281,loss: 0.0054417737\n",
            "\n",
            "Global step: 9282,loss: 0.0053417836\n",
            "\n",
            "Global step: 9283,loss: 0.0054040276\n",
            "\n",
            "Global step: 9284,loss: 0.0056710797\n",
            "\n",
            "Global step: 9285,loss: 0.005614673\n",
            "\n",
            "Global step: 9286,loss: 0.005207843\n",
            "\n",
            "Global step: 9287,loss: 0.005460128\n",
            "\n",
            "Global step: 9288,loss: 0.0051167915\n",
            "\n",
            "Global step: 9289,loss: 0.0053234347\n",
            "\n",
            "Global step: 9290,loss: 0.012262644\n",
            "\n",
            "Global step: 9291,loss: 0.005640865\n",
            "\n",
            "Global step: 9292,loss: 0.006032906\n",
            "\n",
            "Global step: 9293,loss: 0.0054287896\n",
            "\n",
            "Global step: 9294,loss: 0.0053623705\n",
            "\n",
            "Global step: 9295,loss: 0.005698671\n",
            "\n",
            "Global step: 9296,loss: 0.005591907\n",
            "\n",
            "Global step: 9297,loss: 0.005948971\n",
            "\n",
            "Global step: 9298,loss: 0.0055944566\n",
            "\n",
            "Global step: 9299,loss: 0.0056650788\n",
            "\n",
            "Global step: 9300,loss: 0.005404116\n",
            "\n",
            "Global step: 9301,loss: 0.005199444\n",
            "\n",
            "Global step: 9302,loss: 0.005955582\n",
            "\n",
            "Global step: 9303,loss: 0.0055928207\n",
            "\n",
            "Global step: 9304,loss: 0.0057062465\n",
            "\n",
            "Global step: 9305,loss: 0.0055110254\n",
            "\n",
            "Global step: 9306,loss: 0.005808767\n",
            "\n",
            "Global step: 9307,loss: 0.0057308497\n",
            "\n",
            "Global step: 9308,loss: 0.0054172925\n",
            "\n",
            "Global step: 9309,loss: 0.00557548\n",
            "\n",
            "Global step: 9310,loss: 0.0055395532\n",
            "\n",
            "Global step: 9311,loss: 0.0059894994\n",
            "\n",
            "Global step: 9312,loss: 0.005745441\n",
            "\n",
            "Global step: 9313,loss: 0.0057881367\n",
            "\n",
            "Global step: 9314,loss: 0.005914625\n",
            "\n",
            "Global step: 9315,loss: 0.0057793255\n",
            "\n",
            "Global step: 9316,loss: 0.0060783885\n",
            "\n",
            "Global step: 9317,loss: 0.0063381847\n",
            "\n",
            "Global step: 9318,loss: 0.0051426683\n",
            "\n",
            "Global step: 9319,loss: 0.005836268\n",
            "\n",
            "Global step: 9320,loss: 0.005609807\n",
            "\n",
            "Global step: 9321,loss: 0.005743821\n",
            "\n",
            "Global step: 9322,loss: 0.0056457\n",
            "\n",
            "Global step: 9323,loss: 0.006143078\n",
            "\n",
            "Global step: 9324,loss: 0.005558004\n",
            "\n",
            "Global step: 9325,loss: 0.0056151897\n",
            "\n",
            "Global step: 9326,loss: 0.005732018\n",
            "\n",
            "Global step: 9327,loss: 0.00548324\n",
            "\n",
            "Global step: 9328,loss: 0.0056416374\n",
            "\n",
            "Global step: 9329,loss: 0.0052037654\n",
            "\n",
            "Global step: 9330,loss: 0.005343589\n",
            "\n",
            "Global step: 9331,loss: 0.005416855\n",
            "\n",
            "Global step: 9332,loss: 0.005558632\n",
            "\n",
            "Global step: 9333,loss: 0.005357321\n",
            "\n",
            "Global step: 9334,loss: 0.0052970564\n",
            "\n",
            "Global step: 9335,loss: 0.0056364695\n",
            "\n",
            "Global step: 9336,loss: 0.005691676\n",
            "\n",
            "Global step: 9337,loss: 0.0056229145\n",
            "\n",
            "Global step: 9338,loss: 0.005845774\n",
            "\n",
            "Global step: 9339,loss: 0.006147004\n",
            "\n",
            "Global step: 9340,loss: 0.009862013\n",
            "\n",
            "Global step: 9341,loss: 0.006229557\n",
            "\n",
            "Global step: 9342,loss: 0.005499396\n",
            "\n",
            "Global step: 9343,loss: 0.0054946956\n",
            "\n",
            "Global step: 9344,loss: 0.0056668934\n",
            "\n",
            "Global step: 9345,loss: 0.0060049514\n",
            "\n",
            "Global step: 9346,loss: 0.005485727\n",
            "\n",
            "Global step: 9347,loss: 0.005810128\n",
            "\n",
            "Global step: 9348,loss: 0.005512091\n",
            "\n",
            "Global step: 9349,loss: 0.0054055564\n",
            "\n",
            "Global step: 9350,loss: 0.0057914057\n",
            "\n",
            "Global step: 9351,loss: 0.005915237\n",
            "\n",
            "Global step: 9352,loss: 0.005649434\n",
            "\n",
            "Global step: 9353,loss: 0.005405495\n",
            "\n",
            "Global step: 9354,loss: 0.0051967707\n",
            "\n",
            "Global step: 9355,loss: 0.0063328524\n",
            "\n",
            "Global step: 9356,loss: 0.005651005\n",
            "\n",
            "Global step: 9357,loss: 0.0052760644\n",
            "\n",
            "Global step: 9358,loss: 0.0057899924\n",
            "\n",
            "Global step: 9359,loss: 0.0064137396\n",
            "\n",
            "Global step: 9360,loss: 0.0059135268\n",
            "\n",
            "Global step: 9361,loss: 0.005509119\n",
            "\n",
            "Global step: 9362,loss: 0.0056442814\n",
            "\n",
            "Global step: 9363,loss: 0.0055456287\n",
            "\n",
            "Global step: 9364,loss: 0.005459925\n",
            "\n",
            "Global step: 9365,loss: 0.0058660535\n",
            "\n",
            "Global step: 9366,loss: 0.0056217345\n",
            "\n",
            "Global step: 9367,loss: 0.0055149114\n",
            "\n",
            "Global step: 9368,loss: 0.0054476224\n",
            "\n",
            "Global step: 9369,loss: 0.005428808\n",
            "\n",
            "Global step: 9370,loss: 0.0057608765\n",
            "\n",
            "Global step: 9371,loss: 0.005813065\n",
            "\n",
            "Global step: 9372,loss: 0.0056080446\n",
            "\n",
            "Global step: 9373,loss: 0.0054267175\n",
            "\n",
            "Global step: 9374,loss: 0.0056387517\n",
            "\n",
            "Global step: 9375,loss: 0.005548549\n",
            "\n",
            "Global step: 9376,loss: 0.005567561\n",
            "\n",
            "Global step: 9377,loss: 0.005501428\n",
            "\n",
            "Global step: 9378,loss: 0.005559753\n",
            "\n",
            "Global step: 9379,loss: 0.005445684\n",
            "\n",
            "Global step: 9380,loss: 0.0057216464\n",
            "\n",
            "Global step: 9381,loss: 0.005732797\n",
            "\n",
            "Global step: 9382,loss: 0.005632419\n",
            "\n",
            "Global step: 9383,loss: 0.0054000663\n",
            "\n",
            "Global step: 9384,loss: 0.0056154123\n",
            "\n",
            "Global step: 9385,loss: 0.0052688466\n",
            "\n",
            "Global step: 9386,loss: 0.005038319\n",
            "\n",
            "Global step: 9387,loss: 0.0062607382\n",
            "\n",
            "Global step: 9388,loss: 0.005775486\n",
            "\n",
            "Global step: 9389,loss: 0.005805746\n",
            "\n",
            "Global step: 9390,loss: 0.0053929565\n",
            "\n",
            "Global step: 9391,loss: 0.007878794\n",
            "\n",
            "Global step: 9392,loss: 0.006071226\n",
            "\n",
            "Global step: 9393,loss: 0.005785645\n",
            "\n",
            "Global step: 9394,loss: 0.005403748\n",
            "\n",
            "Global step: 9395,loss: 0.0055240253\n",
            "\n",
            "Global step: 9396,loss: 0.0054145656\n",
            "\n",
            "Global step: 9397,loss: 0.005629821\n",
            "\n",
            "Global step: 9398,loss: 0.0056695556\n",
            "\n",
            "Global step: 9399,loss: 0.005819612\n",
            "\n",
            "Global step: 9400,loss: 0.005740308\n",
            "\n",
            "Global step: 9401,loss: 0.0056366585\n",
            "\n",
            "Global step: 9402,loss: 0.00598832\n",
            "\n",
            "Global step: 9403,loss: 0.0054341615\n",
            "\n",
            "Global step: 9404,loss: 0.007518797\n",
            "\n",
            "Global step: 9405,loss: 0.0060992963\n",
            "\n",
            "Global step: 9406,loss: 0.005777964\n",
            "\n",
            "Global step: 9407,loss: 0.0085763205\n",
            "\n",
            "Global step: 9408,loss: 0.0057497425\n",
            "\n",
            "Global step: 9409,loss: 0.0056027584\n",
            "\n",
            "Global step: 9410,loss: 0.0053644567\n",
            "\n",
            "Global step: 9411,loss: 0.005851684\n",
            "\n",
            "Global step: 9412,loss: 0.0062725474\n",
            "\n",
            "Global step: 9413,loss: 0.0057938825\n",
            "\n",
            "Global step: 9414,loss: 0.005189345\n",
            "\n",
            "Global step: 9415,loss: 0.0061480883\n",
            "\n",
            "Global step: 9416,loss: 0.0059584575\n",
            "\n",
            "Global step: 9417,loss: 0.005538505\n",
            "\n",
            "Global step: 9418,loss: 0.0062796073\n",
            "\n",
            "Global step: 9419,loss: 0.0055611287\n",
            "\n",
            "Global step: 9420,loss: 0.005582224\n",
            "\n",
            "Global step: 9421,loss: 0.0057995534\n",
            "\n",
            "Global step: 9422,loss: 0.006243329\n",
            "\n",
            "Global step: 9423,loss: 0.0053053424\n",
            "\n",
            "Global step: 9424,loss: 0.00615074\n",
            "\n",
            "Global step: 9425,loss: 0.0061646756\n",
            "\n",
            "Global step: 9426,loss: 0.0056037763\n",
            "\n",
            "Global step: 9427,loss: 0.005709036\n",
            "\n",
            "Global step: 9428,loss: 0.005724588\n",
            "\n",
            "Global step: 9429,loss: 0.0055668713\n",
            "\n",
            "Global step: 9430,loss: 0.0058520166\n",
            "\n",
            "Global step: 9431,loss: 0.005612021\n",
            "\n",
            "Global step: 9432,loss: 0.005717721\n",
            "\n",
            "Global step: 9433,loss: 0.006142508\n",
            "\n",
            "Global step: 9434,loss: 0.005931857\n",
            "\n",
            "Global step: 9435,loss: 0.0061397706\n",
            "\n",
            "Global step: 9436,loss: 0.0051487912\n",
            "\n",
            "Global step: 9437,loss: 0.005565432\n",
            "\n",
            "Global step: 9438,loss: 0.005001857\n",
            "\n",
            "Global step: 9439,loss: 0.0058081285\n",
            "\n",
            "Global step: 9440,loss: 0.006199391\n",
            "\n",
            "Global step: 9441,loss: 0.005470971\n",
            "\n",
            "Global step: 9442,loss: 0.010921961\n",
            "\n",
            "Global step: 9443,loss: 0.0057731355\n",
            "\n",
            "Global step: 9444,loss: 0.008188072\n",
            "\n",
            "Global step: 9445,loss: 0.0057778643\n",
            "\n",
            "Global step: 9446,loss: 0.005670829\n",
            "\n",
            "Global step: 9447,loss: 0.0060012834\n",
            "\n",
            "Global step: 9448,loss: 0.006022054\n",
            "\n",
            "Global step: 9449,loss: 0.005567487\n",
            "\n",
            "Global step: 9450,loss: 0.0074398816\n",
            "\n",
            "Global step: 9451,loss: 0.0059529217\n",
            "\n",
            "Global step: 9452,loss: 0.008434588\n",
            "\n",
            "Global step: 9453,loss: 0.00808154\n",
            "\n",
            "Global step: 9454,loss: 0.005934825\n",
            "\n",
            "Global step: 9455,loss: 0.0062534907\n",
            "\n",
            "Global step: 9456,loss: 0.0057450873\n",
            "\n",
            "Global step: 9457,loss: 0.0057430756\n",
            "\n",
            "Global step: 9458,loss: 0.0066178236\n",
            "\n",
            "Global step: 9459,loss: 0.005844552\n",
            "\n",
            "Global step: 9460,loss: 0.0060715475\n",
            "\n",
            "Global step: 9461,loss: 0.005742639\n",
            "\n",
            "Global step: 9462,loss: 0.009021572\n",
            "\n",
            "Global step: 9463,loss: 0.0060212035\n",
            "\n",
            "Global step: 9464,loss: 0.006044348\n",
            "\n",
            "Global step: 9465,loss: 0.005428345\n",
            "\n",
            "Global step: 9466,loss: 0.006157152\n",
            "\n",
            "Global step: 9467,loss: 0.006312151\n",
            "\n",
            "Global step: 9468,loss: 0.0061926264\n",
            "\n",
            "Global step: 9469,loss: 0.005724598\n",
            "\n",
            "Global step: 9470,loss: 0.00560176\n",
            "\n",
            "Global step: 9471,loss: 0.005730379\n",
            "\n",
            "Global step: 9472,loss: 0.0069005806\n",
            "\n",
            "Global step: 9473,loss: 0.006123865\n",
            "\n",
            "Global step: 9474,loss: 0.0059545515\n",
            "\n",
            "Global step: 9475,loss: 0.0056965393\n",
            "\n",
            "Global step: 9476,loss: 0.008568565\n",
            "\n",
            "Global step: 9477,loss: 0.005627079\n",
            "\n",
            "Global step: 9478,loss: 0.005340085\n",
            "\n",
            "Global step: 9479,loss: 0.006263357\n",
            "\n",
            "Global step: 9480,loss: 0.0059024324\n",
            "\n",
            "Global step: 9481,loss: 0.005884506\n",
            "\n",
            "Global step: 9482,loss: 0.005363564\n",
            "\n",
            "Global step: 9483,loss: 0.005743455\n",
            "\n",
            "Global step: 9484,loss: 0.005684716\n",
            "\n",
            "Global step: 9485,loss: 0.005622019\n",
            "\n",
            "Global step: 9486,loss: 0.005959523\n",
            "\n",
            "Global step: 9487,loss: 0.0062384624\n",
            "\n",
            "Global step: 9488,loss: 0.0062358347\n",
            "\n",
            "Global step: 9489,loss: 0.0058480743\n",
            "\n",
            "Global step: 9490,loss: 0.0057938895\n",
            "\n",
            "Global step: 9491,loss: 0.007884\n",
            "\n",
            "Global step: 9492,loss: 0.0060356236\n",
            "\n",
            "Global step: 9493,loss: 0.00515074\n",
            "\n",
            "Global step: 9494,loss: 0.0055268733\n",
            "\n",
            "Global step: 9495,loss: 0.0057869684\n",
            "\n",
            "Global step: 9496,loss: 0.0058820196\n",
            "\n",
            "Global step: 9497,loss: 0.0073083276\n",
            "\n",
            "Global step: 9498,loss: 0.005805232\n",
            "\n",
            "Global step: 9499,loss: 0.0053420225\n",
            "\n",
            "Global step: 9500,loss: 0.0054004816\n",
            "\n",
            "Global step: 9501,loss: 0.0065585696\n",
            "\n",
            "Global step: 9502,loss: 0.005631344\n",
            "\n",
            "Global step: 9503,loss: 0.005464508\n",
            "\n",
            "Global step: 9504,loss: 0.0055372403\n",
            "\n",
            "Global step: 9505,loss: 0.006277325\n",
            "\n",
            "Global step: 9506,loss: 0.005935347\n",
            "\n",
            "Global step: 9507,loss: 0.006595748\n",
            "\n",
            "Global step: 9508,loss: 0.0057318774\n",
            "\n",
            "Global step: 9509,loss: 0.0059535704\n",
            "\n",
            "Global step: 9510,loss: 0.0055714045\n",
            "\n",
            "Global step: 9511,loss: 0.005769291\n",
            "\n",
            "Global step: 9512,loss: 0.005636898\n",
            "\n",
            "Global step: 9513,loss: 0.0056162253\n",
            "\n",
            "Global step: 9514,loss: 0.0061758235\n",
            "\n",
            "Global step: 9515,loss: 0.0057103313\n",
            "\n",
            "Global step: 9516,loss: 0.005237372\n",
            "\n",
            "Global step: 9517,loss: 0.0057875887\n",
            "\n",
            "Global step: 9518,loss: 0.0053873556\n",
            "\n",
            "Global step: 9519,loss: 0.005901682\n",
            "\n",
            "Global step: 9520,loss: 0.0056136227\n",
            "\n",
            "Global step: 9521,loss: 0.0060873018\n",
            "\n",
            "Global step: 9522,loss: 0.0071635433\n",
            "\n",
            "Global step: 9523,loss: 0.0054336926\n",
            "\n",
            "Global step: 9524,loss: 0.005455316\n",
            "\n",
            "Global step: 9525,loss: 0.005527487\n",
            "\n",
            "Global step: 9526,loss: 0.006033615\n",
            "\n",
            "Global step: 9527,loss: 0.00595835\n",
            "\n",
            "Global step: 9528,loss: 0.006140074\n",
            "\n",
            "Global step: 9529,loss: 0.0060302783\n",
            "\n",
            "Global step: 9530,loss: 0.005971805\n",
            "\n",
            "Global step: 9531,loss: 0.0053994777\n",
            "\n",
            "Global step: 9532,loss: 0.005574097\n",
            "\n",
            "Global step: 9533,loss: 0.0060715675\n",
            "\n",
            "Global step: 9534,loss: 0.005979686\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 9535.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:06:35.208107 139837023905536 supervisor.py:1050] Recording summary at step 9535.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 9535,loss: 0.00554217\n",
            "\n",
            "Global step: 9536,loss: 0.0053873826\n",
            "\n",
            "Global step: 9537,loss: 0.0057089212\n",
            "\n",
            "Global step: 9538,loss: 0.0053637517\n",
            "\n",
            "Global step: 9539,loss: 0.012225958\n",
            "\n",
            "Global step: 9540,loss: 0.005675709\n",
            "\n",
            "Global step: 9541,loss: 0.0052516796\n",
            "\n",
            "Global step: 9542,loss: 0.0058695152\n",
            "\n",
            "Global step: 9543,loss: 0.005398188\n",
            "\n",
            "Global step: 9544,loss: 0.005764511\n",
            "\n",
            "Global step: 9545,loss: 0.005853495\n",
            "\n",
            "Global step: 9546,loss: 0.0060790577\n",
            "\n",
            "Global step: 9547,loss: 0.005569404\n",
            "\n",
            "Global step: 9548,loss: 0.005670288\n",
            "\n",
            "Global step: 9549,loss: 0.0059473896\n",
            "\n",
            "Global step: 9550,loss: 0.006196016\n",
            "\n",
            "Global step: 9551,loss: 0.005922743\n",
            "\n",
            "Global step: 9552,loss: 0.0062992796\n",
            "\n",
            "Global step: 9553,loss: 0.005876051\n",
            "\n",
            "Global step: 9554,loss: 0.005819502\n",
            "\n",
            "Global step: 9555,loss: 0.0053987196\n",
            "\n",
            "Global step: 9556,loss: 0.0060515967\n",
            "\n",
            "Global step: 9557,loss: 0.005897133\n",
            "\n",
            "Global step: 9558,loss: 0.005754381\n",
            "\n",
            "Global step: 9559,loss: 0.005614713\n",
            "\n",
            "Global step: 9560,loss: 0.0057372972\n",
            "\n",
            "Global step: 9561,loss: 0.005877139\n",
            "\n",
            "Global step: 9562,loss: 0.005385128\n",
            "\n",
            "Global step: 9563,loss: 0.0058677057\n",
            "\n",
            "Global step: 9564,loss: 0.005695001\n",
            "\n",
            "Global step: 9565,loss: 0.005882705\n",
            "\n",
            "Global step: 9566,loss: 0.0065027135\n",
            "\n",
            "Global step: 9567,loss: 0.0055738855\n",
            "\n",
            "Global step: 9568,loss: 0.005880547\n",
            "\n",
            "Global step: 9569,loss: 0.0056017963\n",
            "\n",
            "Global step: 9570,loss: 0.005911555\n",
            "\n",
            "Global step: 9571,loss: 0.005747698\n",
            "\n",
            "Global step: 9572,loss: 0.0055991667\n",
            "\n",
            "Global step: 9573,loss: 0.0058853664\n",
            "\n",
            "Global step: 9574,loss: 0.0056636953\n",
            "\n",
            "Global step: 9575,loss: 0.0060787606\n",
            "\n",
            "Global step: 9576,loss: 0.005787782\n",
            "\n",
            "Global step: 9577,loss: 0.0057649333\n",
            "\n",
            "Global step: 9578,loss: 0.005752271\n",
            "\n",
            "Global step: 9579,loss: 0.005299956\n",
            "\n",
            "Global step: 9580,loss: 0.0054020095\n",
            "\n",
            "Global step: 9581,loss: 0.0052866754\n",
            "\n",
            "Global step: 9582,loss: 0.006287175\n",
            "\n",
            "Global step: 9583,loss: 0.005158718\n",
            "\n",
            "Global step: 9584,loss: 0.005852856\n",
            "\n",
            "Global step: 9585,loss: 0.00528574\n",
            "\n",
            "Global step: 9586,loss: 0.0051631345\n",
            "\n",
            "Global step: 9587,loss: 0.005877314\n",
            "\n",
            "Global step: 9588,loss: 0.005468887\n",
            "\n",
            "Global step: 9589,loss: 0.0055985544\n",
            "\n",
            "Global step: 9590,loss: 0.0060757273\n",
            "\n",
            "Global step: 9591,loss: 0.0058097825\n",
            "\n",
            "Global step: 9592,loss: 0.006016714\n",
            "\n",
            "Global step: 9593,loss: 0.0058589424\n",
            "\n",
            "Global step: 9594,loss: 0.00549875\n",
            "\n",
            "Global step: 9595,loss: 0.005435994\n",
            "\n",
            "Global step: 9596,loss: 0.005212099\n",
            "\n",
            "Global step: 9597,loss: 0.0059241\n",
            "\n",
            "Global step: 9598,loss: 0.005920247\n",
            "\n",
            "Global step: 9599,loss: 0.005519393\n",
            "\n",
            "Global step: 9600,loss: 0.0057858615\n",
            "\n",
            "Global step: 9601,loss: 0.0056709684\n",
            "\n",
            "Global step: 9602,loss: 0.005790501\n",
            "\n",
            "Global step: 9603,loss: 0.0054282453\n",
            "\n",
            "Global step: 9604,loss: 0.0053862096\n",
            "\n",
            "Global step: 9605,loss: 0.0053873025\n",
            "\n",
            "Global step: 9606,loss: 0.0057909354\n",
            "\n",
            "Global step: 9607,loss: 0.006000499\n",
            "\n",
            "Global step: 9608,loss: 0.0054780375\n",
            "\n",
            "Global step: 9609,loss: 0.0060696276\n",
            "\n",
            "Global step: 9610,loss: 0.0060515436\n",
            "\n",
            "Global step: 9611,loss: 0.0054527563\n",
            "\n",
            "Global step: 9612,loss: 0.005455909\n",
            "\n",
            "Global step: 9613,loss: 0.005076384\n",
            "\n",
            "Global step: 9614,loss: 0.0055813408\n",
            "\n",
            "Global step: 9615,loss: 0.005361739\n",
            "\n",
            "Global step: 9616,loss: 0.005511125\n",
            "\n",
            "Global step: 9617,loss: 0.0059762397\n",
            "\n",
            "Global step: 9618,loss: 0.0054014726\n",
            "\n",
            "Global step: 9619,loss: 0.005427842\n",
            "\n",
            "Global step: 9620,loss: 0.005227781\n",
            "\n",
            "Global step: 9621,loss: 0.005788092\n",
            "\n",
            "Global step: 9622,loss: 0.0050512366\n",
            "\n",
            "Global step: 9623,loss: 0.005952254\n",
            "\n",
            "Global step: 9624,loss: 0.0060928706\n",
            "\n",
            "Global step: 9625,loss: 0.005553332\n",
            "\n",
            "Global step: 9626,loss: 0.0055173147\n",
            "\n",
            "Global step: 9627,loss: 0.0055138655\n",
            "\n",
            "Global step: 9628,loss: 0.0055643083\n",
            "\n",
            "Global step: 9629,loss: 0.0051451833\n",
            "\n",
            "Global step: 9630,loss: 0.005399428\n",
            "\n",
            "Global step: 9631,loss: 0.0060008243\n",
            "\n",
            "Global step: 9632,loss: 0.00547453\n",
            "\n",
            "Global step: 9633,loss: 0.006431562\n",
            "\n",
            "Global step: 9634,loss: 0.005769061\n",
            "\n",
            "Global step: 9635,loss: 0.0062104706\n",
            "\n",
            "Global step: 9636,loss: 0.005681935\n",
            "\n",
            "Global step: 9637,loss: 0.005196725\n",
            "\n",
            "Global step: 9638,loss: 0.005333255\n",
            "\n",
            "Global step: 9639,loss: 0.0059736436\n",
            "\n",
            "Global step: 9640,loss: 0.0057683084\n",
            "\n",
            "Global step: 9641,loss: 0.0059128115\n",
            "\n",
            "Global step: 9642,loss: 0.005437822\n",
            "\n",
            "Global step: 9643,loss: 0.005653384\n",
            "\n",
            "Global step: 9644,loss: 0.0059991577\n",
            "\n",
            "Global step: 9645,loss: 0.005290378\n",
            "\n",
            "Global step: 9646,loss: 0.005369341\n",
            "\n",
            "Global step: 9647,loss: 0.005674607\n",
            "\n",
            "Global step: 9648,loss: 0.006205771\n",
            "\n",
            "Global step: 9649,loss: 0.005244464\n",
            "\n",
            "Global step: 9650,loss: 0.0064800302\n",
            "\n",
            "Global step: 9651,loss: 0.0054126047\n",
            "\n",
            "Global step: 9652,loss: 0.0059238514\n",
            "\n",
            "Global step: 9653,loss: 0.0053047314\n",
            "\n",
            "Global step: 9654,loss: 0.005291441\n",
            "\n",
            "Global step: 9655,loss: 0.005445346\n",
            "\n",
            "Global step: 9656,loss: 0.0058744755\n",
            "\n",
            "Global step: 9657,loss: 0.005473575\n",
            "\n",
            "Global step: 9658,loss: 0.0057375263\n",
            "\n",
            "Global step: 9659,loss: 0.0061560078\n",
            "\n",
            "Global step: 9660,loss: 0.0053145243\n",
            "\n",
            "Global step: 9661,loss: 0.0052835066\n",
            "\n",
            "Global step: 9662,loss: 0.005770899\n",
            "\n",
            "Global step: 9663,loss: 0.0054346803\n",
            "\n",
            "Global step: 9664,loss: 0.005910817\n",
            "\n",
            "Global step: 9665,loss: 0.0051702596\n",
            "\n",
            "Global step: 9666,loss: 0.005698159\n",
            "\n",
            "Global step: 9667,loss: 0.006078372\n",
            "\n",
            "Global step: 9668,loss: 0.005963933\n",
            "\n",
            "Global step: 9669,loss: 0.008429669\n",
            "\n",
            "Global step: 9670,loss: 0.00517537\n",
            "\n",
            "Global step: 9671,loss: 0.005568285\n",
            "\n",
            "Global step: 9672,loss: 0.0054794936\n",
            "\n",
            "Global step: 9673,loss: 0.005917548\n",
            "\n",
            "Global step: 9674,loss: 0.0054142475\n",
            "\n",
            "Global step: 9675,loss: 0.0053896885\n",
            "\n",
            "Global step: 9676,loss: 0.006158497\n",
            "\n",
            "Global step: 9677,loss: 0.0058409953\n",
            "\n",
            "Global step: 9678,loss: 0.005642132\n",
            "\n",
            "Global step: 9679,loss: 0.0057063224\n",
            "\n",
            "Global step: 9680,loss: 0.0056682075\n",
            "\n",
            "Global step: 9681,loss: 0.0057068756\n",
            "\n",
            "Global step: 9682,loss: 0.006069271\n",
            "\n",
            "Global step: 9683,loss: 0.0054602525\n",
            "\n",
            "Global step: 9684,loss: 0.0057125795\n",
            "\n",
            "Global step: 9685,loss: 0.0058838776\n",
            "\n",
            "Global step: 9686,loss: 0.0050966963\n",
            "\n",
            "Global step: 9687,loss: 0.005795997\n",
            "\n",
            "Global step: 9688,loss: 0.0055306638\n",
            "\n",
            "Global step: 9689,loss: 0.0061179423\n",
            "\n",
            "Global step: 9690,loss: 0.0057308455\n",
            "\n",
            "Global step: 9691,loss: 0.005597801\n",
            "\n",
            "Global step: 9692,loss: 0.0059283962\n",
            "\n",
            "Global step: 9693,loss: 0.0057530073\n",
            "\n",
            "Global step: 9694,loss: 0.0068156053\n",
            "\n",
            "Global step: 9695,loss: 0.0054910015\n",
            "\n",
            "Global step: 9696,loss: 0.0059517315\n",
            "\n",
            "Global step: 9697,loss: 0.0055387164\n",
            "\n",
            "Global step: 9698,loss: 0.005671929\n",
            "\n",
            "Global step: 9699,loss: 0.005521406\n",
            "\n",
            "Global step: 9700,loss: 0.005456288\n",
            "\n",
            "Global step: 9701,loss: 0.0053570443\n",
            "\n",
            "Global step: 9702,loss: 0.00563404\n",
            "\n",
            "Global step: 9703,loss: 0.005547072\n",
            "\n",
            "Global step: 9704,loss: 0.00591462\n",
            "\n",
            "Global step: 9705,loss: 0.005701044\n",
            "\n",
            "Global step: 9706,loss: 0.005916791\n",
            "\n",
            "Global step: 9707,loss: 0.006496407\n",
            "\n",
            "Global step: 9708,loss: 0.005823721\n",
            "\n",
            "Global step: 9709,loss: 0.0053045275\n",
            "\n",
            "Global step: 9710,loss: 0.005635234\n",
            "\n",
            "Global step: 9711,loss: 0.005231889\n",
            "\n",
            "Global step: 9712,loss: 0.005412628\n",
            "\n",
            "Global step: 9713,loss: 0.005591005\n",
            "\n",
            "Global step: 9714,loss: 0.005048914\n",
            "\n",
            "Global step: 9715,loss: 0.005426414\n",
            "\n",
            "Global step: 9716,loss: 0.0059149787\n",
            "\n",
            "Global step: 9717,loss: 0.0052779433\n",
            "\n",
            "Global step: 9718,loss: 0.0052721538\n",
            "\n",
            "Global step: 9719,loss: 0.0051088305\n",
            "\n",
            "Global step: 9720,loss: 0.005429075\n",
            "\n",
            "Global step: 9721,loss: 0.0053016106\n",
            "\n",
            "Global step: 9722,loss: 0.005681336\n",
            "\n",
            "Global step: 9723,loss: 0.005462824\n",
            "\n",
            "Global step: 9724,loss: 0.0052326834\n",
            "\n",
            "Global step: 9725,loss: 0.005266467\n",
            "\n",
            "Global step: 9726,loss: 0.0056822374\n",
            "\n",
            "Global step: 9727,loss: 0.0052322014\n",
            "\n",
            "Global step: 9728,loss: 0.0053002276\n",
            "\n",
            "Global step: 9729,loss: 0.0054906365\n",
            "\n",
            "Global step: 9730,loss: 0.005545775\n",
            "\n",
            "Global step: 9731,loss: 0.0052077207\n",
            "\n",
            "Global step: 9732,loss: 0.0051406655\n",
            "\n",
            "Global step: 9733,loss: 0.0061775264\n",
            "\n",
            "Global step: 9734,loss: 0.0049513364\n",
            "\n",
            "Global step: 9735,loss: 0.005377327\n",
            "\n",
            "Global step: 9736,loss: 0.0048504355\n",
            "\n",
            "Global step: 9737,loss: 0.0054802205\n",
            "\n",
            "Global step: 9738,loss: 0.0058720065\n",
            "\n",
            "Global step: 9739,loss: 0.005225827\n",
            "\n",
            "Global step: 9740,loss: 0.005547955\n",
            "\n",
            "Global step: 9741,loss: 0.0056925914\n",
            "\n",
            "Global step: 9742,loss: 0.0053995764\n",
            "\n",
            "Global step: 9743,loss: 0.0059206546\n",
            "\n",
            "Global step: 9744,loss: 0.005307349\n",
            "\n",
            "Global step: 9745,loss: 0.005389083\n",
            "\n",
            "Global step: 9746,loss: 0.0053125946\n",
            "\n",
            "Global step: 9747,loss: 0.0056292326\n",
            "\n",
            "Global step: 9748,loss: 0.004828905\n",
            "\n",
            "Global step: 9749,loss: 0.005078023\n",
            "\n",
            "Global step: 9750,loss: 0.005605807\n",
            "\n",
            "Global step: 9751,loss: 0.0056607756\n",
            "\n",
            "Global step: 9752,loss: 0.005580712\n",
            "\n",
            "Global step: 9753,loss: 0.0053036325\n",
            "\n",
            "Global step: 9754,loss: 0.0054382724\n",
            "\n",
            "Global step: 9755,loss: 0.005685448\n",
            "\n",
            "Global step: 9756,loss: 0.005530276\n",
            "\n",
            "Global step: 9757,loss: 0.00587766\n",
            "\n",
            "Global step: 9758,loss: 0.005094964\n",
            "\n",
            "Global step: 9759,loss: 0.0059795026\n",
            "\n",
            "Global step: 9760,loss: 0.005785938\n",
            "\n",
            "Global step: 9761,loss: 0.0061848015\n",
            "\n",
            "Global step: 9762,loss: 0.008743512\n",
            "\n",
            "Global step: 9763,loss: 0.0056685745\n",
            "\n",
            "Global step: 9764,loss: 0.005767675\n",
            "\n",
            "Global step: 9765,loss: 0.0059363022\n",
            "\n",
            "Global step: 9766,loss: 0.006288195\n",
            "\n",
            "Global step: 9767,loss: 0.0062582847\n",
            "\n",
            "Global step: 9768,loss: 0.007935016\n",
            "\n",
            "Global step: 9769,loss: 0.005403496\n",
            "\n",
            "Global step: 9770,loss: 0.0055507408\n",
            "\n",
            "Global step: 9771,loss: 0.0060366127\n",
            "\n",
            "Global step: 9772,loss: 0.005757274\n",
            "\n",
            "Global step: 9773,loss: 0.005996996\n",
            "\n",
            "Global step: 9774,loss: 0.0061094784\n",
            "\n",
            "Global step: 9775,loss: 0.0059290123\n",
            "\n",
            "Global step: 9776,loss: 0.005682799\n",
            "\n",
            "Global step: 9777,loss: 0.005569709\n",
            "\n",
            "Global step: 9778,loss: 0.0056628226\n",
            "\n",
            "Global step: 9779,loss: 0.0068907784\n",
            "\n",
            "Global step: 9780,loss: 0.005734718\n",
            "\n",
            "Global step: 9781,loss: 0.005613656\n",
            "\n",
            "Global step: 9782,loss: 0.0053535346\n",
            "\n",
            "Global step: 9783,loss: 0.0057994444\n",
            "\n",
            "Global step: 9784,loss: 0.005761096\n",
            "\n",
            "Global step: 9785,loss: 0.005691352\n",
            "\n",
            "Global step: 9786,loss: 0.006130183\n",
            "\n",
            "Global step: 9787,loss: 0.006565725\n",
            "\n",
            "Global step: 9788,loss: 0.006040125\n",
            "\n",
            "Global step: 9789,loss: 0.0060291756\n",
            "\n",
            "Global step: 9790,loss: 0.005652378\n",
            "\n",
            "Global step: 9791,loss: 0.0052284747\n",
            "\n",
            "Global step: 9792,loss: 0.0059861643\n",
            "\n",
            "Global step: 9793,loss: 0.0056203324\n",
            "\n",
            "Global step: 9794,loss: 0.0053370274\n",
            "\n",
            "Global step: 9795,loss: 0.0055719777\n",
            "\n",
            "Global step: 9796,loss: 0.006755565\n",
            "\n",
            "Global step: 9797,loss: 0.0055797026\n",
            "\n",
            "Global step: 9798,loss: 0.0063974834\n",
            "\n",
            "Global step: 9799,loss: 0.0057573332\n",
            "\n",
            "Global step: 9800,loss: 0.0059869885\n",
            "\n",
            "Global step: 9801,loss: 0.005852526\n",
            "\n",
            "Global step: 9802,loss: 0.0058689523\n",
            "\n",
            "Global step: 9803,loss: 0.0056079933\n",
            "\n",
            "Global step: 9804,loss: 0.005923128\n",
            "\n",
            "Global step: 9805,loss: 0.0060106837\n",
            "\n",
            "Global step: 9806,loss: 0.005669485\n",
            "\n",
            "Global step: 9807,loss: 0.0059914626\n",
            "\n",
            "Global step: 9808,loss: 0.0055108834\n",
            "\n",
            "Global step: 9809,loss: 0.0057443543\n",
            "\n",
            "Global step: 9810,loss: 0.0058078594\n",
            "\n",
            "Global step: 9811,loss: 0.0057325447\n",
            "\n",
            "Global step: 9812,loss: 0.005558404\n",
            "\n",
            "Global step: 9813,loss: 0.0058294046\n",
            "\n",
            "Global step: 9814,loss: 0.005322764\n",
            "\n",
            "Global step: 9815,loss: 0.00581709\n",
            "\n",
            "Global step: 9816,loss: 0.005779993\n",
            "\n",
            "Global step: 9817,loss: 0.005435053\n",
            "\n",
            "Global step: 9818,loss: 0.0054116263\n",
            "\n",
            "Global step: 9819,loss: 0.0060078967\n",
            "\n",
            "Global step: 9820,loss: 0.0050098384\n",
            "\n",
            "Global step: 9821,loss: 0.007909751\n",
            "\n",
            "Global step: 9822,loss: 0.00558529\n",
            "\n",
            "Global step: 9823,loss: 0.0057251574\n",
            "\n",
            "Global step: 9824,loss: 0.0053841732\n",
            "\n",
            "Global step: 9825,loss: 0.0060493974\n",
            "\n",
            "Global step: 9826,loss: 0.0058945487\n",
            "\n",
            "Global step: 9827,loss: 0.005795909\n",
            "\n",
            "Global step: 9828,loss: 0.005726732\n",
            "\n",
            "Global step: 9829,loss: 0.005376918\n",
            "\n",
            "Global step: 9830,loss: 0.006037615\n",
            "\n",
            "Global step: 9831,loss: 0.005217815\n",
            "\n",
            "Global step: 9832,loss: 0.0056612557\n",
            "\n",
            "Global step: 9833,loss: 0.005269251\n",
            "\n",
            "Global step: 9834,loss: 0.005344272\n",
            "\n",
            "Global step: 9835,loss: 0.0054033427\n",
            "\n",
            "Global step: 9836,loss: 0.005898888\n",
            "\n",
            "Global step: 9837,loss: 0.005507984\n",
            "\n",
            "Global step: 9838,loss: 0.0054417187\n",
            "\n",
            "Global step: 9839,loss: 0.005610829\n",
            "\n",
            "Global step: 9840,loss: 0.0053859665\n",
            "\n",
            "Global step: 9841,loss: 0.0054162974\n",
            "\n",
            "\n",
            "######  NOT SAVING MODEL  #########\n",
            "\n",
            "Global Step: 9841,val_loss: 0.007458170804266746\n",
            "\n",
            "Training for epoch 15/15:\n",
            "Global step: 9842,loss: 0.005634173\n",
            "\n",
            "Global step: 9843,loss: 0.0055832127\n",
            "\n",
            "Global step: 9844,loss: 0.005588317\n",
            "\n",
            "Global step: 9845,loss: 0.0054729944\n",
            "\n",
            "Global step: 9846,loss: 0.005997302\n",
            "\n",
            "Global step: 9847,loss: 0.0054887226\n",
            "\n",
            "Global step: 9848,loss: 0.005388212\n",
            "\n",
            "Global step: 9849,loss: 0.0053760996\n",
            "\n",
            "Global step: 9850,loss: 0.0054452736\n",
            "\n",
            "Global step: 9851,loss: 0.005614909\n",
            "\n",
            "Global step: 9852,loss: 0.0053913463\n",
            "\n",
            "Global step: 9853,loss: 0.0053628823\n",
            "\n",
            "Global step: 9854,loss: 0.0055895722\n",
            "\n",
            "Global step: 9855,loss: 0.0055898395\n",
            "\n",
            "Global step: 9856,loss: 0.005269941\n",
            "\n",
            "Global step: 9857,loss: 0.0054144515\n",
            "\n",
            "Global step: 9858,loss: 0.004984586\n",
            "\n",
            "Global step: 9859,loss: 0.005436331\n",
            "\n",
            "Global step: 9860,loss: 0.0055641453\n",
            "\n",
            "Global step: 9861,loss: 0.005482944\n",
            "\n",
            "Global step: 9862,loss: 0.0052054715\n",
            "\n",
            "Global step: 9863,loss: 0.0052937656\n",
            "\n",
            "Global step: 9864,loss: 0.0052220915\n",
            "\n",
            "Global step: 9865,loss: 0.0054118787\n",
            "\n",
            "Global step: 9866,loss: 0.0052939504\n",
            "\n",
            "Global step: 9867,loss: 0.0056730565\n",
            "\n",
            "Global step: 9868,loss: 0.0052010487\n",
            "\n",
            "Global step: 9869,loss: 0.004864894\n",
            "\n",
            "Global step: 9870,loss: 0.005572121\n",
            "\n",
            "Global step: 9871,loss: 0.0050402987\n",
            "\n",
            "Global step: 9872,loss: 0.006842484\n",
            "\n",
            "Global step: 9873,loss: 0.0054053194\n",
            "\n",
            "Global step: 9874,loss: 0.005374512\n",
            "\n",
            "Global step: 9875,loss: 0.0053617116\n",
            "\n",
            "Global step: 9876,loss: 0.0055779275\n",
            "\n",
            "Global step: 9877,loss: 0.0061421013\n",
            "\n",
            "Global step: 9878,loss: 0.004911877\n",
            "\n",
            "Global step: 9879,loss: 0.005195813\n",
            "\n",
            "Global step: 9880,loss: 0.005356808\n",
            "\n",
            "Global step: 9881,loss: 0.005101749\n",
            "\n",
            "Global step: 9882,loss: 0.0052746604\n",
            "\n",
            "Global step: 9883,loss: 0.005440928\n",
            "\n",
            "Global step: 9884,loss: 0.0051981863\n",
            "\n",
            "Global step: 9885,loss: 0.0052978424\n",
            "\n",
            "Global step: 9886,loss: 0.0053402986\n",
            "\n",
            "Global step: 9887,loss: 0.006262432\n",
            "\n",
            "Global step: 9888,loss: 0.005477278\n",
            "\n",
            "Global step: 9889,loss: 0.0059186476\n",
            "\n",
            "Global step: 9890,loss: 0.0055339057\n",
            "\n",
            "Global step: 9891,loss: 0.005125417\n",
            "\n",
            "Global step: 9892,loss: 0.0057974304\n",
            "\n",
            "Global step: 9893,loss: 0.0050393376\n",
            "\n",
            "Global step: 9894,loss: 0.005471649\n",
            "\n",
            "Global step: 9895,loss: 0.005379263\n",
            "\n",
            "Global step: 9896,loss: 0.0054730694\n",
            "\n",
            "Global step: 9897,loss: 0.0055487086\n",
            "\n",
            "Global step: 9898,loss: 0.00552319\n",
            "\n",
            "Global step: 9899,loss: 0.005338619\n",
            "\n",
            "Global step: 9900,loss: 0.0050140126\n",
            "\n",
            "Global step: 9901,loss: 0.005792217\n",
            "\n",
            "Global step: 9902,loss: 0.005219757\n",
            "\n",
            "Global step: 9903,loss: 0.0052818884\n",
            "\n",
            "Global step: 9904,loss: 0.0056892503\n",
            "\n",
            "Global step: 9905,loss: 0.0048485226\n",
            "\n",
            "Global step: 9906,loss: 0.005319919\n",
            "\n",
            "Global step: 9907,loss: 0.005127481\n",
            "\n",
            "Global step: 9908,loss: 0.005779042\n",
            "\n",
            "Global step: 9909,loss: 0.0050944006\n",
            "\n",
            "Global step: 9910,loss: 0.008765632\n",
            "\n",
            "Global step: 9911,loss: 0.0052191853\n",
            "\n",
            "Global step: 9912,loss: 0.0054098014\n",
            "\n",
            "Global step: 9913,loss: 0.005114692\n",
            "\n",
            "Global step: 9914,loss: 0.005310934\n",
            "\n",
            "Global step: 9915,loss: 0.005321646\n",
            "\n",
            "Global step: 9916,loss: 0.005637107\n",
            "\n",
            "Global step: 9917,loss: 0.005461823\n",
            "\n",
            "Global step: 9918,loss: 0.005286411\n",
            "\n",
            "Global step: 9919,loss: 0.0053592585\n",
            "\n",
            "Global step: 9920,loss: 0.005331292\n",
            "\n",
            "Global step: 9921,loss: 0.005107771\n",
            "\n",
            "Global step: 9922,loss: 0.005819379\n",
            "\n",
            "Global step: 9923,loss: 0.004780254\n",
            "\n",
            "Global step: 9924,loss: 0.0057539474\n",
            "\n",
            "Global step: 9925,loss: 0.005692709\n",
            "\n",
            "Global step: 9926,loss: 0.005120907\n",
            "\n",
            "Global step: 9927,loss: 0.005951857\n",
            "\n",
            "Global step: 9928,loss: 0.005175672\n",
            "\n",
            "Global step: 9929,loss: 0.0054316334\n",
            "\n",
            "Global step: 9930,loss: 0.0051834076\n",
            "\n",
            "Global step: 9931,loss: 0.005260838\n",
            "\n",
            "Global step: 9932,loss: 0.0052614883\n",
            "\n",
            "Global step: 9933,loss: 0.0056533744\n",
            "\n",
            "Global step: 9934,loss: 0.0056520873\n",
            "\n",
            "Global step: 9935,loss: 0.0054642353\n",
            "\n",
            "Global step: 9936,loss: 0.005413349\n",
            "\n",
            "Global step: 9937,loss: 0.005116518\n",
            "\n",
            "Global step: 9938,loss: 0.0059761433\n",
            "\n",
            "Global step: 9939,loss: 0.005358705\n",
            "\n",
            "Global step: 9940,loss: 0.0052814493\n",
            "\n",
            "Global step: 9941,loss: 0.0057197674\n",
            "\n",
            "Global step: 9942,loss: 0.0054806336\n",
            "\n",
            "Global step: 9943,loss: 0.0059011052\n",
            "\n",
            "Global step: 9944,loss: 0.005494194\n",
            "\n",
            "Global step: 9945,loss: 0.0055310847\n",
            "\n",
            "Global step: 9946,loss: 0.0053952835\n",
            "\n",
            "Global step: 9947,loss: 0.0060041747\n",
            "\n",
            "Global step: 9948,loss: 0.0052905614\n",
            "\n",
            "Global step: 9949,loss: 0.0052589136\n",
            "\n",
            "Global step: 9950,loss: 0.005908468\n",
            "\n",
            "Global step: 9951,loss: 0.0057946094\n",
            "\n",
            "Global step: 9952,loss: 0.0051283576\n",
            "\n",
            "Global step: 9953,loss: 0.008137979\n",
            "\n",
            "Global step: 9954,loss: 0.0054754554\n",
            "\n",
            "Global step: 9955,loss: 0.005422257\n",
            "\n",
            "Global step: 9956,loss: 0.0058531547\n",
            "\n",
            "Global step: 9957,loss: 0.005580118\n",
            "\n",
            "Global step: 9958,loss: 0.0051284055\n",
            "\n",
            "Global step: 9959,loss: 0.005551146\n",
            "\n",
            "Global step: 9960,loss: 0.005681501\n",
            "\n",
            "Global step: 9961,loss: 0.0074600796\n",
            "\n",
            "Global step: 9962,loss: 0.005532133\n",
            "\n",
            "Global step: 9963,loss: 0.005802819\n",
            "\n",
            "Global step: 9964,loss: 0.0060382388\n",
            "\n",
            "Global step: 9965,loss: 0.0059812535\n",
            "\n",
            "Global step: 9966,loss: 0.0049799373\n",
            "\n",
            "Global step: 9967,loss: 0.0052311188\n",
            "\n",
            "Global step: 9968,loss: 0.006336315\n",
            "\n",
            "Global step: 9969,loss: 0.011672188\n",
            "\n",
            "Global step: 9970,loss: 0.005632273\n",
            "\n",
            "Global step: 9971,loss: 0.0056464868\n",
            "\n",
            "Global step: 9972,loss: 0.0057149804\n",
            "\n",
            "Global step: 9973,loss: 0.005555024\n",
            "\n",
            "Global step: 9974,loss: 0.0053932937\n",
            "\n",
            "Global step: 9975,loss: 0.0055716103\n",
            "\n",
            "Global step: 9976,loss: 0.005310509\n",
            "\n",
            "Global step: 9977,loss: 0.005254973\n",
            "\n",
            "Global step: 9978,loss: 0.008067139\n",
            "\n",
            "Global step: 9979,loss: 0.0050677597\n",
            "\n",
            "Global step: 9980,loss: 0.005499728\n",
            "\n",
            "Global step: 9981,loss: 0.006237047\n",
            "\n",
            "Global step: 9982,loss: 0.005649295\n",
            "\n",
            "Global step: 9983,loss: 0.005991808\n",
            "\n",
            "Global step: 9984,loss: 0.0073265512\n",
            "\n",
            "Global step: 9985,loss: 0.005503129\n",
            "\n",
            "Global step: 9986,loss: 0.006101876\n",
            "\n",
            "Global step: 9987,loss: 0.007222968\n",
            "\n",
            "Global step: 9988,loss: 0.0061507407\n",
            "\n",
            "Global step: 9989,loss: 0.0054591876\n",
            "\n",
            "Global step: 9990,loss: 0.005758136\n",
            "\n",
            "Global step: 9991,loss: 0.006027388\n",
            "\n",
            "Global step: 9992,loss: 0.0052884747\n",
            "\n",
            "Global step: 9993,loss: 0.0054567484\n",
            "\n",
            "Global step: 9994,loss: 0.0059986385\n",
            "\n",
            "Global step: 9995,loss: 0.0057510603\n",
            "\n",
            "Global step: 9996,loss: 0.0076279165\n",
            "\n",
            "Global step: 9997,loss: 0.0054851864\n",
            "\n",
            "Global step: 9998,loss: 0.0061257742\n",
            "\n",
            "Global step: 9999,loss: 0.006372289\n",
            "\n",
            "Global step: 10000,loss: 0.0057417983\n",
            "\n",
            "Global step: 10001,loss: 0.0050856294\n",
            "\n",
            "Global step: 10002,loss: 0.005346672\n",
            "\n",
            "Global step: 10003,loss: 0.005734444\n",
            "\n",
            "Global step: 10004,loss: 0.00602819\n",
            "\n",
            "Global step: 10005,loss: 0.005952283\n",
            "\n",
            "Global step: 10006,loss: 0.0055854414\n",
            "\n",
            "Global step: 10007,loss: 0.006793383\n",
            "\n",
            "Global step: 10008,loss: 0.0063324547\n",
            "\n",
            "Global step: 10009,loss: 0.005997668\n",
            "\n",
            "Global step: 10010,loss: 0.005534769\n",
            "\n",
            "Global step: 10011,loss: 0.006121115\n",
            "\n",
            "Global step: 10012,loss: 0.005429757\n",
            "\n",
            "Global step: 10013,loss: 0.0058716107\n",
            "\n",
            "Global step: 10014,loss: 0.0057781544\n",
            "\n",
            "Global step: 10015,loss: 0.0057921465\n",
            "\n",
            "Global step: 10016,loss: 0.0057460517\n",
            "\n",
            "Global step: 10017,loss: 0.0056116013\n",
            "\n",
            "Global step: 10018,loss: 0.007819738\n",
            "\n",
            "Global step: 10019,loss: 0.0056309635\n",
            "\n",
            "Global step: 10020,loss: 0.010616462\n",
            "\n",
            "Global step: 10021,loss: 0.0057405843\n",
            "\n",
            "Global step: 10022,loss: 0.0057597356\n",
            "\n",
            "Global step: 10023,loss: 0.006381121\n",
            "\n",
            "Global step: 10024,loss: 0.0058710016\n",
            "\n",
            "Global step: 10025,loss: 0.0059113554\n",
            "\n",
            "Global step: 10026,loss: 0.0064730975\n",
            "\n",
            "Global step: 10027,loss: 0.0065712165\n",
            "\n",
            "Global step: 10028,loss: 0.007117493\n",
            "\n",
            "Global step: 10029,loss: 0.0056448164\n",
            "\n",
            "Global step: 10030,loss: 0.0065717623\n",
            "\n",
            "Global step: 10031,loss: 0.006304439\n",
            "\n",
            "Global step: 10032,loss: 0.006945792\n",
            "\n",
            "Global step: 10033,loss: 0.005889819\n",
            "\n",
            "Global step: 10034,loss: 0.00582496\n",
            "\n",
            "Global step: 10035,loss: 0.0065891505\n",
            "\n",
            "Global step: 10036,loss: 0.0065870937\n",
            "\n",
            "Global step: 10037,loss: 0.007736897\n",
            "\n",
            "Global step: 10038,loss: 0.0081852265\n",
            "\n",
            "Global step: 10039,loss: 0.0060274457\n",
            "\n",
            "Global step: 10040,loss: 0.0053993394\n",
            "\n",
            "Global step: 10041,loss: 0.006404384\n",
            "\n",
            "Global step: 10042,loss: 0.0061646756\n",
            "\n",
            "Global step: 10043,loss: 0.0061007612\n",
            "\n",
            "Global step: 10044,loss: 0.0064232107\n",
            "\n",
            "Global step: 10045,loss: 0.0050997464\n",
            "\n",
            "Global step: 10046,loss: 0.006520798\n",
            "\n",
            "Global step: 10047,loss: 0.006520277\n",
            "\n",
            "Global step: 10048,loss: 0.006271235\n",
            "\n",
            "Global step: 10049,loss: 0.0060914624\n",
            "\n",
            "Global step: 10050,loss: 0.0067641125\n",
            "\n",
            "Global step: 10051,loss: 0.006455185\n",
            "\n",
            "Global step: 10052,loss: 0.0064610485\n",
            "\n",
            "Global step: 10053,loss: 0.0058882786\n",
            "\n",
            "Global step: 10054,loss: 0.006108492\n",
            "\n",
            "Global step: 10055,loss: 0.0063528745\n",
            "\n",
            "Global step: 10056,loss: 0.0058715604\n",
            "\n",
            "Global step: 10057,loss: 0.006163763\n",
            "\n",
            "Global step: 10058,loss: 0.0065820185\n",
            "\n",
            "Global step: 10059,loss: 0.0057444377\n",
            "\n",
            "Global step: 10060,loss: 0.007058105\n",
            "\n",
            "Global step: 10061,loss: 0.008312721\n",
            "\n",
            "Global step: 10062,loss: 0.005783085\n",
            "\n",
            "Global step: 10063,loss: 0.0061253123\n",
            "\n",
            "Global step: 10064,loss: 0.0066675073\n",
            "\n",
            "Global step: 10065,loss: 0.0066562886\n",
            "\n",
            "Global step: 10066,loss: 0.005996267\n",
            "\n",
            "Global step: 10067,loss: 0.008682745\n",
            "\n",
            "Global step: 10068,loss: 0.0060812845\n",
            "\n",
            "Global step: 10069,loss: 0.006095954\n",
            "\n",
            "Global step: 10070,loss: 0.0056615863\n",
            "\n",
            "Global step: 10071,loss: 0.005566091\n",
            "\n",
            "Global step: 10072,loss: 0.0063005243\n",
            "\n",
            "Global step: 10073,loss: 0.0071218237\n",
            "\n",
            "Global step: 10074,loss: 0.0056719845\n",
            "\n",
            "Global step: 10075,loss: 0.0060925493\n",
            "\n",
            "Global step: 10076,loss: 0.0059378333\n",
            "\n",
            "Global step: 10077,loss: 0.0060435124\n",
            "\n",
            "Global step: 10078,loss: 0.006193089\n",
            "\n",
            "Global step: 10079,loss: 0.0065662772\n",
            "\n",
            "Global step: 10080,loss: 0.0058907084\n",
            "\n",
            "Global step: 10081,loss: 0.0057863784\n",
            "\n",
            "Global step: 10082,loss: 0.005944391\n",
            "\n",
            "Global step: 10083,loss: 0.0055947686\n",
            "\n",
            "Global step: 10084,loss: 0.0057507306\n",
            "\n",
            "Global step: 10085,loss: 0.0060331826\n",
            "\n",
            "Global step: 10086,loss: 0.007651707\n",
            "\n",
            "Global step: 10087,loss: 0.0057552005\n",
            "\n",
            "Global step: 10088,loss: 0.0063587045\n",
            "\n",
            "Global step: 10089,loss: 0.0063900105\n",
            "\n",
            "Global step: 10090,loss: 0.005934057\n",
            "\n",
            "Global step: 10091,loss: 0.005725047\n",
            "\n",
            "Global step: 10092,loss: 0.006569905\n",
            "\n",
            "Global step: 10093,loss: 0.006079223\n",
            "\n",
            "Global step: 10094,loss: 0.008522489\n",
            "\n",
            "Global step: 10095,loss: 0.005975064\n",
            "\n",
            "Global step: 10096,loss: 0.0064947335\n",
            "\n",
            "Global step: 10097,loss: 0.0069699585\n",
            "\n",
            "Global step: 10098,loss: 0.0054643974\n",
            "\n",
            "Global step: 10099,loss: 0.005864768\n",
            "\n",
            "Global step: 10100,loss: 0.005658263\n",
            "\n",
            "Global step: 10101,loss: 0.006059239\n",
            "\n",
            "Global step: 10102,loss: 0.005875777\n",
            "\n",
            "Global step: 10103,loss: 0.007654399\n",
            "\n",
            "Global step: 10104,loss: 0.0058759325\n",
            "\n",
            "Global step: 10105,loss: 0.006422915\n",
            "\n",
            "Global step: 10106,loss: 0.0052022873\n",
            "\n",
            "Global step: 10107,loss: 0.005699988\n",
            "\n",
            "Global step: 10108,loss: 0.0059223305\n",
            "\n",
            "Global step: 10109,loss: 0.00600906\n",
            "\n",
            "Global step: 10110,loss: 0.0060900976\n",
            "\n",
            "Global step: 10111,loss: 0.0054891286\n",
            "\n",
            "Global step: 10112,loss: 0.006530225\n",
            "\n",
            "Global step: 10113,loss: 0.0059035392\n",
            "\n",
            "Global step: 10114,loss: 0.005914513\n",
            "\n",
            "Global step: 10115,loss: 0.006474162\n",
            "\n",
            "Global step: 10116,loss: 0.006456726\n",
            "\n",
            "Global step: 10117,loss: 0.005847373\n",
            "\n",
            "Global step: 10118,loss: 0.0056325546\n",
            "\n",
            "Global step: 10119,loss: 0.005524703\n",
            "\n",
            "Global step: 10120,loss: 0.0059252637\n",
            "\n",
            "Global step: 10121,loss: 0.0053276205\n",
            "\n",
            "Global step: 10122,loss: 0.0083845835\n",
            "\n",
            "Global step: 10123,loss: 0.005939874\n",
            "\n",
            "Global step: 10124,loss: 0.005956866\n",
            "\n",
            "Global step: 10125,loss: 0.005957234\n",
            "\n",
            "Global step: 10126,loss: 0.0074363975\n",
            "\n",
            "Global step: 10127,loss: 0.006485569\n",
            "\n",
            "Global step: 10128,loss: 0.0068862727\n",
            "\n",
            "Global step: 10129,loss: 0.0058879666\n",
            "\n",
            "Global step: 10130,loss: 0.005204109\n",
            "\n",
            "Global step: 10131,loss: 0.005912601\n",
            "\n",
            "Global step: 10132,loss: 0.0064574406\n",
            "\n",
            "Global step: 10133,loss: 0.0059969034\n",
            "\n",
            "Global step: 10134,loss: 0.0059421356\n",
            "\n",
            "Global step: 10135,loss: 0.0060386104\n",
            "\n",
            "Global step: 10136,loss: 0.0065613016\n",
            "\n",
            "Global step: 10137,loss: 0.006320384\n",
            "\n",
            "Global step: 10138,loss: 0.0060632234\n",
            "\n",
            "Global step: 10139,loss: 0.0062502837\n",
            "\n",
            "Global step: 10140,loss: 0.005877016\n",
            "\n",
            "Global step: 10141,loss: 0.006429945\n",
            "\n",
            "Global step: 10142,loss: 0.005649081\n",
            "\n",
            "Global step: 10143,loss: 0.006571141\n",
            "\n",
            "Global step: 10144,loss: 0.0058826962\n",
            "\n",
            "Global step: 10145,loss: 0.0060671554\n",
            "\n",
            "Global step: 10146,loss: 0.006730047\n",
            "\n",
            "Global step: 10147,loss: 0.005779254\n",
            "\n",
            "Global step: 10148,loss: 0.0058530774\n",
            "\n",
            "Global step: 10149,loss: 0.0058584255\n",
            "\n",
            "Global step: 10150,loss: 0.005586717\n",
            "\n",
            "Global step: 10151,loss: 0.0060791075\n",
            "\n",
            "Global step: 10152,loss: 0.006409961\n",
            "\n",
            "Global step: 10153,loss: 0.0059265746\n",
            "\n",
            "Global step: 10154,loss: 0.006070887\n",
            "\n",
            "Global step: 10155,loss: 0.0056164362\n",
            "\n",
            "Global step: 10156,loss: 0.0056599253\n",
            "\n",
            "Global step: 10157,loss: 0.006376992\n",
            "\n",
            "Global step: 10158,loss: 0.0059720357\n",
            "\n",
            "Global step: 10159,loss: 0.0059742522\n",
            "\n",
            "Global step: 10160,loss: 0.0054190196\n",
            "\n",
            "Global step: 10161,loss: 0.005446125\n",
            "\n",
            "Global step: 10162,loss: 0.0056207757\n",
            "\n",
            "Global step: 10163,loss: 0.0057201344\n",
            "\n",
            "Global step: 10164,loss: 0.006248742\n",
            "\n",
            "Global step: 10165,loss: 0.0062492094\n",
            "\n",
            "Global step: 10166,loss: 0.0063119754\n",
            "\n",
            "Global step: 10167,loss: 0.006312907\n",
            "\n",
            "Global step: 10168,loss: 0.006004865\n",
            "\n",
            "Global step: 10169,loss: 0.0058907815\n",
            "\n",
            "Global step: 10170,loss: 0.0054415767\n",
            "\n",
            "Global step: 10171,loss: 0.005850722\n",
            "\n",
            "Global step: 10172,loss: 0.005666729\n",
            "\n",
            "Global step: 10173,loss: 0.0059772707\n",
            "\n",
            "Global step: 10174,loss: 0.005540658\n",
            "\n",
            "Global step: 10175,loss: 0.0059046787\n",
            "\n",
            "Global step: 10176,loss: 0.0054233186\n",
            "\n",
            "Global step: 10177,loss: 0.00602364\n",
            "\n",
            "Global step: 10178,loss: 0.005419823\n",
            "\n",
            "Global step: 10179,loss: 0.0056169694\n",
            "\n",
            "Global step: 10180,loss: 0.0059341574\n",
            "\n",
            "Global step: 10181,loss: 0.00506994\n",
            "\n",
            "Global step: 10182,loss: 0.00525798\n",
            "\n",
            "Global step: 10183,loss: 0.00574577\n",
            "\n",
            "Global step: 10184,loss: 0.0054577407\n",
            "\n",
            "Global step: 10185,loss: 0.005723192\n",
            "\n",
            "Global step: 10186,loss: 0.005733206\n",
            "\n",
            "Global step: 10187,loss: 0.0071547106\n",
            "\n",
            "Global step: 10188,loss: 0.0054949974\n",
            "\n",
            "Global step: 10189,loss: 0.0058784503\n",
            "\n",
            "Global step: 10190,loss: 0.006171223\n",
            "\n",
            "Global step: 10191,loss: 0.0054084575\n",
            "\n",
            "Global step: 10192,loss: 0.0059014037\n",
            "\n",
            "Global step: 10193,loss: 0.0061855614\n",
            "\n",
            "Global step: 10194,loss: 0.0060327398\n",
            "\n",
            "Global step: 10195,loss: 0.0061443467\n",
            "\n",
            "Global step: 10196,loss: 0.0059674466\n",
            "\n",
            "Global step: 10197,loss: 0.0056979377\n",
            "\n",
            "Global step: 10198,loss: 0.005986739\n",
            "\n",
            "Global step: 10199,loss: 0.0050946972\n",
            "\n",
            "Global step: 10200,loss: 0.0058401427\n",
            "\n",
            "Global step: 10201,loss: 0.00550083\n",
            "\n",
            "Global step: 10202,loss: 0.005599676\n",
            "\n",
            "Global step: 10203,loss: 0.005964353\n",
            "\n",
            "Global step: 10204,loss: 0.005542974\n",
            "\n",
            "Global step: 10205,loss: 0.005413606\n",
            "\n",
            "Global step: 10206,loss: 0.005827244\n",
            "\n",
            "Global step: 10207,loss: 0.0055940836\n",
            "\n",
            "Global step: 10208,loss: 0.005324747\n",
            "\n",
            "Global step: 10209,loss: 0.005383811\n",
            "\n",
            "Global step: 10210,loss: 0.0057472694\n",
            "\n",
            "Global step: 10211,loss: 0.005210259\n",
            "\n",
            "Global step: 10212,loss: 0.00591411\n",
            "\n",
            "Global step: 10213,loss: 0.00574097\n",
            "\n",
            "Global step: 10214,loss: 0.005282994\n",
            "\n",
            "Global step: 10215,loss: 0.0055517904\n",
            "\n",
            "Global step: 10216,loss: 0.006112436\n",
            "\n",
            "Global step: 10217,loss: 0.005691375\n",
            "\n",
            "Global step: 10218,loss: 0.0055673993\n",
            "\n",
            "Global step: 10219,loss: 0.0056388136\n",
            "\n",
            "Global step: 10220,loss: 0.005546656\n",
            "\n",
            "Global step: 10221,loss: 0.0056085354\n",
            "\n",
            "Global step: 10222,loss: 0.0060750674\n",
            "\n",
            "Global step: 10223,loss: 0.00518599\n",
            "\n",
            "Global step: 10224,loss: 0.0053997114\n",
            "\n",
            "Global step: 10225,loss: 0.006228152\n",
            "\n",
            "Global step: 10226,loss: 0.0053013028\n",
            "\n",
            "Global step: 10227,loss: 0.0053315745\n",
            "\n",
            "Global step: 10228,loss: 0.0056508384\n",
            "\n",
            "Global step: 10229,loss: 0.0050742766\n",
            "\n",
            "Global step: 10230,loss: 0.0054462557\n",
            "\n",
            "Global step: 10231,loss: 0.0055728476\n",
            "\n",
            "Global step: 10232,loss: 0.0055953274\n",
            "\n",
            "Global step: 10233,loss: 0.005562761\n",
            "\n",
            "Global step: 10234,loss: 0.005729471\n",
            "\n",
            "Global step: 10235,loss: 0.0056383493\n",
            "\n",
            "Global step: 10236,loss: 0.005794287\n",
            "\n",
            "Global step: 10237,loss: 0.005617976\n",
            "\n",
            "Global step: 10238,loss: 0.005244792\n",
            "\n",
            "Global step: 10239,loss: 0.005793575\n",
            "\n",
            "Global step: 10240,loss: 0.0052509373\n",
            "\n",
            "Global step: 10241,loss: 0.005603526\n",
            "\n",
            "Global step: 10242,loss: 0.0060593514\n",
            "\n",
            "Global step: 10243,loss: 0.0053335917\n",
            "\n",
            "Global step: 10244,loss: 0.005750676\n",
            "\n",
            "Global step: 10245,loss: 0.005630527\n",
            "\n",
            "Global step: 10246,loss: 0.0052586463\n",
            "\n",
            "Global step: 10247,loss: 0.0054603335\n",
            "\n",
            "Global step: 10248,loss: 0.00521074\n",
            "\n",
            "Global step: 10249,loss: 0.005972331\n",
            "\n",
            "Global step: 10250,loss: 0.0056856535\n",
            "\n",
            "Global step: 10251,loss: 0.005146526\n",
            "\n",
            "Global step: 10252,loss: 0.0058479654\n",
            "\n",
            "Global step: 10253,loss: 0.005585014\n",
            "\n",
            "Global step: 10254,loss: 0.0051873294\n",
            "\n",
            "Global step: 10255,loss: 0.0056519937\n",
            "\n",
            "Global step: 10256,loss: 0.005774112\n",
            "\n",
            "Global step: 10257,loss: 0.00507773\n",
            "\n",
            "Global step: 10258,loss: 0.005731466\n",
            "\n",
            "Global step: 10259,loss: 0.00560618\n",
            "\n",
            "Global step: 10260,loss: 0.005429989\n",
            "\n",
            "Global step: 10261,loss: 0.005255631\n",
            "\n",
            "Global step: 10262,loss: 0.0057671904\n",
            "\n",
            "Global step: 10263,loss: 0.005320975\n",
            "\n",
            "Global step: 10264,loss: 0.005727133\n",
            "\n",
            "Global step: 10265,loss: 0.0053497623\n",
            "\n",
            "Global step: 10266,loss: 0.0055401633\n",
            "\n",
            "Global step: 10267,loss: 0.005693791\n",
            "\n",
            "Global step: 10268,loss: 0.005202216\n",
            "\n",
            "Global step: 10269,loss: 0.005491349\n",
            "\n",
            "Global step: 10270,loss: 0.0055374317\n",
            "\n",
            "Global step: 10271,loss: 0.005695642\n",
            "\n",
            "Global step: 10272,loss: 0.00564337\n",
            "\n",
            "Global step: 10273,loss: 0.005436457\n",
            "\n",
            "Global step: 10274,loss: 0.005533461\n",
            "\n",
            "Global step: 10275,loss: 0.0052667954\n",
            "\n",
            "Global step: 10276,loss: 0.0050736917\n",
            "\n",
            "Global step: 10277,loss: 0.0054677324\n",
            "\n",
            "Global step: 10278,loss: 0.0055308593\n",
            "\n",
            "Global step: 10279,loss: 0.005869103\n",
            "\n",
            "Global step: 10280,loss: 0.0057201693\n",
            "\n",
            "Global step: 10281,loss: 0.0054865754\n",
            "\n",
            "Global step: 10282,loss: 0.005326793\n",
            "\n",
            "Global step: 10283,loss: 0.0052772397\n",
            "\n",
            "Global step: 10284,loss: 0.0053417766\n",
            "\n",
            "Global step: 10285,loss: 0.0058672465\n",
            "\n",
            "Global step: 10286,loss: 0.0052612685\n",
            "\n",
            "Global step: 10287,loss: 0.0056318673\n",
            "\n",
            "Global step: 10288,loss: 0.005894751\n",
            "\n",
            "Global step: 10289,loss: 0.0050344844\n",
            "\n",
            "Global step: 10290,loss: 0.0052805385\n",
            "\n",
            "Global step: 10291,loss: 0.0046907547\n",
            "\n",
            "Global step: 10292,loss: 0.0053479043\n",
            "\n",
            "Global step: 10293,loss: 0.0053563504\n",
            "\n",
            "Global step: 10294,loss: 0.005145341\n",
            "\n",
            "Global step: 10295,loss: 0.00514066\n",
            "\n",
            "Global step: 10296,loss: 0.005736738\n",
            "\n",
            "Global step: 10297,loss: 0.005408014\n",
            "\n",
            "Global step: 10298,loss: 0.0060000475\n",
            "\n",
            "Global step: 10299,loss: 0.0051383497\n",
            "\n",
            "Global step: 10300,loss: 0.0052081477\n",
            "\n",
            "Global step: 10301,loss: 0.0051508383\n",
            "\n",
            "Global step: 10302,loss: 0.0048877047\n",
            "\n",
            "Global step: 10303,loss: 0.0054175053\n",
            "\n",
            "Global step: 10304,loss: 0.0055276174\n",
            "\n",
            "Global step: 10305,loss: 0.005794036\n",
            "\n",
            "Global step: 10306,loss: 0.0049147834\n",
            "\n",
            "Global step: 10307,loss: 0.0054635596\n",
            "\n",
            "Global step: 10308,loss: 0.0053124237\n",
            "\n",
            "Global step: 10309,loss: 0.0052164732\n",
            "\n",
            "Global step: 10310,loss: 0.005014199\n",
            "\n",
            "Global step: 10311,loss: 0.0058248946\n",
            "\n",
            "Global step: 10312,loss: 0.0051998557\n",
            "\n",
            "Global step: 10313,loss: 0.005764258\n",
            "\n",
            "Global step: 10314,loss: 0.005263282\n",
            "\n",
            "Global step: 10315,loss: 0.0048009083\n",
            "\n",
            "Global step: 10316,loss: 0.0058087725\n",
            "\n",
            "Global step: 10317,loss: 0.00546771\n",
            "\n",
            "Global step: 10318,loss: 0.005697063\n",
            "\n",
            "Global step: 10319,loss: 0.005661192\n",
            "\n",
            "Global step: 10320,loss: 0.005404743\n",
            "\n",
            "Global step: 10321,loss: 0.0052125803\n",
            "\n",
            "Global step: 10322,loss: 0.01053918\n",
            "\n",
            "Global step: 10323,loss: 0.014223703\n",
            "\n",
            "Global step: 10324,loss: 0.005110429\n",
            "\n",
            "Global step: 10325,loss: 0.0054925093\n",
            "\n",
            "Global step: 10326,loss: 0.005306418\n",
            "\n",
            "Global step: 10327,loss: 0.005244021\n",
            "\n",
            "Global step: 10328,loss: 0.006102564\n",
            "\n",
            "Global step: 10329,loss: 0.0053207655\n",
            "\n",
            "Global step: 10330,loss: 0.006678825\n",
            "\n",
            "Global step: 10331,loss: 0.005416355\n",
            "\n",
            "Global step: 10332,loss: 0.005555475\n",
            "\n",
            "Global step: 10333,loss: 0.006165327\n",
            "\n",
            "Global step: 10334,loss: 0.005541533\n",
            "\n",
            "Global step: 10335,loss: 0.005350996\n",
            "\n",
            "Global step: 10336,loss: 0.0058010556\n",
            "\n",
            "Global step: 10337,loss: 0.006363678\n",
            "\n",
            "Global step: 10338,loss: 0.0070005637\n",
            "\n",
            "Global step: 10339,loss: 0.006219872\n",
            "\n",
            "Global step: 10340,loss: 0.0055505154\n",
            "\n",
            "Global step: 10341,loss: 0.0060932683\n",
            "\n",
            "Global step: 10342,loss: 0.0053889253\n",
            "\n",
            "Global step: 10343,loss: 0.005858609\n",
            "\n",
            "Global step: 10344,loss: 0.0053749774\n",
            "\n",
            "Global step: 10345,loss: 0.0054824157\n",
            "\n",
            "Global step: 10346,loss: 0.0051352438\n",
            "\n",
            "Global step: 10347,loss: 0.0057667783\n",
            "\n",
            "Global step: 10348,loss: 0.0069766617\n",
            "\n",
            "Global step: 10349,loss: 0.009074916\n",
            "\n",
            "Global step: 10350,loss: 0.0053245705\n",
            "\n",
            "Global step: 10351,loss: 0.005247645\n",
            "\n",
            "Global step: 10352,loss: 0.0056182686\n",
            "\n",
            "Global step: 10353,loss: 0.005720115\n",
            "\n",
            "Global step: 10354,loss: 0.0049871285\n",
            "\n",
            "Global step: 10355,loss: 0.0053633894\n",
            "\n",
            "Global step: 10356,loss: 0.005537761\n",
            "\n",
            "Global step: 10357,loss: 0.0052443403\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 10358.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:08:35.291955 139837023905536 supervisor.py:1050] Recording summary at step 10358.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 10358,loss: 0.006184402\n",
            "\n",
            "Global step: 10359,loss: 0.0055907792\n",
            "\n",
            "Global step: 10360,loss: 0.0053591365\n",
            "\n",
            "Global step: 10361,loss: 0.0049876315\n",
            "\n",
            "Global step: 10362,loss: 0.0059977104\n",
            "\n",
            "Global step: 10363,loss: 0.0049941572\n",
            "\n",
            "Global step: 10364,loss: 0.0055979067\n",
            "\n",
            "Global step: 10365,loss: 0.0054447837\n",
            "\n",
            "Global step: 10366,loss: 0.005434817\n",
            "\n",
            "Global step: 10367,loss: 0.0055447505\n",
            "\n",
            "Global step: 10368,loss: 0.0052673668\n",
            "\n",
            "Global step: 10369,loss: 0.0052001486\n",
            "\n",
            "Global step: 10370,loss: 0.0057052537\n",
            "\n",
            "Global step: 10371,loss: 0.005351979\n",
            "\n",
            "Global step: 10372,loss: 0.005827749\n",
            "\n",
            "Global step: 10373,loss: 0.0053729527\n",
            "\n",
            "Global step: 10374,loss: 0.0059074955\n",
            "\n",
            "Global step: 10375,loss: 0.0052517303\n",
            "\n",
            "Global step: 10376,loss: 0.005835766\n",
            "\n",
            "Global step: 10377,loss: 0.0057083587\n",
            "\n",
            "Global step: 10378,loss: 0.005498442\n",
            "\n",
            "Global step: 10379,loss: 0.00516032\n",
            "\n",
            "Global step: 10380,loss: 0.0055076713\n",
            "\n",
            "Global step: 10381,loss: 0.0056950087\n",
            "\n",
            "Global step: 10382,loss: 0.007772724\n",
            "\n",
            "Global step: 10383,loss: 0.005759906\n",
            "\n",
            "Global step: 10384,loss: 0.0053911502\n",
            "\n",
            "Global step: 10385,loss: 0.0052650827\n",
            "\n",
            "Global step: 10386,loss: 0.0050924355\n",
            "\n",
            "Global step: 10387,loss: 0.00557456\n",
            "\n",
            "Global step: 10388,loss: 0.0054764897\n",
            "\n",
            "Global step: 10389,loss: 0.005901253\n",
            "\n",
            "Global step: 10390,loss: 0.005860849\n",
            "\n",
            "Global step: 10391,loss: 0.005362352\n",
            "\n",
            "Global step: 10392,loss: 0.005470349\n",
            "\n",
            "Global step: 10393,loss: 0.0057240413\n",
            "\n",
            "Global step: 10394,loss: 0.005517468\n",
            "\n",
            "Global step: 10395,loss: 0.0058558304\n",
            "\n",
            "Global step: 10396,loss: 0.014003523\n",
            "\n",
            "Global step: 10397,loss: 0.0050502983\n",
            "\n",
            "Global step: 10398,loss: 0.0056036334\n",
            "\n",
            "Global step: 10399,loss: 0.005650935\n",
            "\n",
            "Global step: 10400,loss: 0.0057371138\n",
            "\n",
            "Global step: 10401,loss: 0.00551124\n",
            "\n",
            "Global step: 10402,loss: 0.005648979\n",
            "\n",
            "Global step: 10403,loss: 0.008138824\n",
            "\n",
            "Global step: 10404,loss: 0.0050747367\n",
            "\n",
            "Global step: 10405,loss: 0.005610093\n",
            "\n",
            "Global step: 10406,loss: 0.006122615\n",
            "\n",
            "Global step: 10407,loss: 0.005590423\n",
            "\n",
            "Global step: 10408,loss: 0.00527143\n",
            "\n",
            "Global step: 10409,loss: 0.0060366513\n",
            "\n",
            "Global step: 10410,loss: 0.0053427154\n",
            "\n",
            "Global step: 10411,loss: 0.005684725\n",
            "\n",
            "Global step: 10412,loss: 0.006014999\n",
            "\n",
            "Global step: 10413,loss: 0.0049953097\n",
            "\n",
            "Global step: 10414,loss: 0.005196533\n",
            "\n",
            "Global step: 10415,loss: 0.005679906\n",
            "\n",
            "Global step: 10416,loss: 0.005659153\n",
            "\n",
            "Global step: 10417,loss: 0.0056249076\n",
            "\n",
            "Global step: 10418,loss: 0.0055708615\n",
            "\n",
            "Global step: 10419,loss: 0.005377837\n",
            "\n",
            "Global step: 10420,loss: 0.0069552585\n",
            "\n",
            "Global step: 10421,loss: 0.0057860008\n",
            "\n",
            "Global step: 10422,loss: 0.0056501976\n",
            "\n",
            "Global step: 10423,loss: 0.00562312\n",
            "\n",
            "Global step: 10424,loss: 0.005779771\n",
            "\n",
            "Global step: 10425,loss: 0.005933582\n",
            "\n",
            "Global step: 10426,loss: 0.0051459135\n",
            "\n",
            "Global step: 10427,loss: 0.00561756\n",
            "\n",
            "Global step: 10428,loss: 0.005221445\n",
            "\n",
            "Global step: 10429,loss: 0.0058091707\n",
            "\n",
            "Global step: 10430,loss: 0.0055553745\n",
            "\n",
            "Global step: 10431,loss: 0.006397105\n",
            "\n",
            "Global step: 10432,loss: 0.0056043295\n",
            "\n",
            "Global step: 10433,loss: 0.0052423314\n",
            "\n",
            "Global step: 10434,loss: 0.005417096\n",
            "\n",
            "Global step: 10435,loss: 0.00548444\n",
            "\n",
            "Global step: 10436,loss: 0.0055068303\n",
            "\n",
            "Global step: 10437,loss: 0.005493664\n",
            "\n",
            "Global step: 10438,loss: 0.0055450913\n",
            "\n",
            "Global step: 10439,loss: 0.005891134\n",
            "\n",
            "Global step: 10440,loss: 0.0059407977\n",
            "\n",
            "Global step: 10441,loss: 0.0052819396\n",
            "\n",
            "Global step: 10442,loss: 0.005650571\n",
            "\n",
            "Global step: 10443,loss: 0.0051762783\n",
            "\n",
            "Global step: 10444,loss: 0.008498879\n",
            "\n",
            "Global step: 10445,loss: 0.0055480395\n",
            "\n",
            "Global step: 10446,loss: 0.00547739\n",
            "\n",
            "Global step: 10447,loss: 0.005299099\n",
            "\n",
            "Global step: 10448,loss: 0.0053473595\n",
            "\n",
            "Global step: 10449,loss: 0.0054415525\n",
            "\n",
            "Global step: 10450,loss: 0.0076105935\n",
            "\n",
            "Global step: 10451,loss: 0.0057787728\n",
            "\n",
            "Global step: 10452,loss: 0.005810844\n",
            "\n",
            "Global step: 10453,loss: 0.005292383\n",
            "\n",
            "Global step: 10454,loss: 0.0062940023\n",
            "\n",
            "Global step: 10455,loss: 0.005527572\n",
            "\n",
            "Global step: 10456,loss: 0.0056546186\n",
            "\n",
            "Global step: 10457,loss: 0.005620832\n",
            "\n",
            "Global step: 10458,loss: 0.0056143655\n",
            "\n",
            "Global step: 10459,loss: 0.005320907\n",
            "\n",
            "Global step: 10460,loss: 0.0060995724\n",
            "\n",
            "Global step: 10461,loss: 0.0057052574\n",
            "\n",
            "Global step: 10462,loss: 0.0053873477\n",
            "\n",
            "Global step: 10463,loss: 0.005650345\n",
            "\n",
            "Global step: 10464,loss: 0.0053185737\n",
            "\n",
            "Global step: 10465,loss: 0.0051463656\n",
            "\n",
            "Global step: 10466,loss: 0.006356562\n",
            "\n",
            "Global step: 10467,loss: 0.005238663\n",
            "\n",
            "Global step: 10468,loss: 0.0055860025\n",
            "\n",
            "Global step: 10469,loss: 0.0058777984\n",
            "\n",
            "Global step: 10470,loss: 0.0063895606\n",
            "\n",
            "Global step: 10471,loss: 0.005987051\n",
            "\n",
            "Global step: 10472,loss: 0.0054546646\n",
            "\n",
            "Global step: 10473,loss: 0.0058814012\n",
            "\n",
            "Global step: 10474,loss: 0.0061499462\n",
            "\n",
            "Global step: 10475,loss: 0.0062163575\n",
            "\n",
            "Global step: 10476,loss: 0.0051786373\n",
            "\n",
            "Global step: 10477,loss: 0.006441718\n",
            "\n",
            "Global step: 10478,loss: 0.005959169\n",
            "\n",
            "Global step: 10479,loss: 0.0059522\n",
            "\n",
            "Global step: 10480,loss: 0.0060368683\n",
            "\n",
            "Global step: 10481,loss: 0.0060851295\n",
            "\n",
            "Global step: 10482,loss: 0.0053141946\n",
            "\n",
            "Global step: 10483,loss: 0.006239447\n",
            "\n",
            "Global step: 10484,loss: 0.005779312\n",
            "\n",
            "Global step: 10485,loss: 0.0056721717\n",
            "\n",
            "Global step: 10486,loss: 0.0065837754\n",
            "\n",
            "Global step: 10487,loss: 0.0061098197\n",
            "\n",
            "Global step: 10488,loss: 0.0055955844\n",
            "\n",
            "Global step: 10489,loss: 0.006675803\n",
            "\n",
            "Global step: 10490,loss: 0.00608717\n",
            "\n",
            "Global step: 10491,loss: 0.0055238428\n",
            "\n",
            "Global step: 10492,loss: 0.006404805\n",
            "\n",
            "Global step: 10493,loss: 0.0053851106\n",
            "\n",
            "Global step: 10494,loss: 0.0050295186\n",
            "\n",
            "Global step: 10495,loss: 0.00574231\n",
            "\n",
            "Global step: 10496,loss: 0.0059923045\n",
            "\n",
            "Global step: 10497,loss: 0.006175754\n",
            "\n",
            "Global step: 10498,loss: 0.0055160727\n",
            "\n",
            "Global step: 10499,loss: 0.005803583\n",
            "\n",
            "Global step: 10500,loss: 0.0052114297\n",
            "\n",
            "Global step: 10501,loss: 0.0056108916\n",
            "\n",
            "Global step: 10502,loss: 0.006159319\n",
            "\n",
            "Global step: 10503,loss: 0.0056754407\n",
            "\n",
            "Global step: 10504,loss: 0.005227797\n",
            "\n",
            "Global step: 10505,loss: 0.005629989\n",
            "\n",
            "Global step: 10506,loss: 0.005763316\n",
            "\n",
            "Global step: 10507,loss: 0.0057300846\n",
            "\n",
            "Global step: 10508,loss: 0.005990758\n",
            "\n",
            "Global step: 10509,loss: 0.005707213\n",
            "\n",
            "Global step: 10510,loss: 0.0055860425\n",
            "\n",
            "Global step: 10511,loss: 0.0058949883\n",
            "\n",
            "Global step: 10512,loss: 0.005802315\n",
            "\n",
            "Global step: 10513,loss: 0.0055666748\n",
            "\n",
            "Global step: 10514,loss: 0.0059206206\n",
            "\n",
            "Global step: 10515,loss: 0.0052900314\n",
            "\n",
            "Global step: 10516,loss: 0.0054837135\n",
            "\n",
            "Global step: 10517,loss: 0.0052215955\n",
            "\n",
            "Global step: 10518,loss: 0.0056393603\n",
            "\n",
            "Global step: 10519,loss: 0.005279257\n",
            "\n",
            "Global step: 10520,loss: 0.0050358106\n",
            "\n",
            "Global step: 10521,loss: 0.0054800226\n",
            "\n",
            "Global step: 10522,loss: 0.005417541\n",
            "\n",
            "Global step: 10523,loss: 0.005510434\n",
            "\n",
            "Global step: 10524,loss: 0.005495944\n",
            "\n",
            "Global step: 10525,loss: 0.0058345078\n",
            "\n",
            "Global step: 10526,loss: 0.0053640134\n",
            "\n",
            "Global step: 10527,loss: 0.00508499\n",
            "\n",
            "Global step: 10528,loss: 0.0054843724\n",
            "\n",
            "Global step: 10529,loss: 0.005395693\n",
            "\n",
            "Global step: 10530,loss: 0.005237954\n",
            "\n",
            "Global step: 10531,loss: 0.0057899677\n",
            "\n",
            "Global step: 10532,loss: 0.0051916623\n",
            "\n",
            "Global step: 10533,loss: 0.0053578876\n",
            "\n",
            "Global step: 10534,loss: 0.005658479\n",
            "\n",
            "Global step: 10535,loss: 0.0055888533\n",
            "\n",
            "Global step: 10536,loss: 0.0052359183\n",
            "\n",
            "Global step: 10537,loss: 0.0050314884\n",
            "\n",
            "Global step: 10538,loss: 0.006035528\n",
            "\n",
            "Global step: 10539,loss: 0.005495045\n",
            "\n",
            "Global step: 10540,loss: 0.0052680094\n",
            "\n",
            "Global step: 10541,loss: 0.005601882\n",
            "\n",
            "Global step: 10542,loss: 0.0048832693\n",
            "\n",
            "Global step: 10543,loss: 0.0052619595\n",
            "\n",
            "Global step: 10544,loss: 0.0051076175\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 10544,Val_Loss: 0.007221731249816143,  Val_acc: 0.9993322649572649 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 13:09:08.670149 139840769816448 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:10.028231 139840769816448 <ipython-input-10-deb5ab324590>:16] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "outputId": "8e33b3ed-930c-4ae8-bc9e-566ab0c409a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "import time\n",
        "cfg.is_training=False\n",
        "st = time.time()\n",
        "try:\n",
        "  def main(_):\n",
        "      tf.logging.info(' Loading Graph...')\n",
        "      num_label = 10\n",
        "      model = CapsNet()\n",
        "      tf.logging.info(' Graph loaded')\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "      sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      if cfg.is_training:\n",
        "          tf.logging.info(' Start training...')\n",
        "          train(model, sv, num_label)\n",
        "          tf.logging.info('Training done')\n",
        "      else:\n",
        "          evaluation(model, sv, num_label)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"\\Completed in: {}s\".format(time.time()-st))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:50.508725 139840769816448 <ipython-input-12-721f33898b86>:6]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:58.953831 139840769816448 <ipython-input-7-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:58.956373 139840769816448 <ipython-input-12-721f33898b86>:9]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0014_step_10544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:59.565832 139840769816448 saver.py:1284] Restoring parameters from logdir/model_epoch_0014_step_10544\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:59.964792 139840769816448 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:09:59.991783 139840769816448 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:10:59.355046 139840769816448 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:11:00.695210 139840769816448 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0014_step_10544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:11:00.715013 139840769816448 saver.py:1284] Restoring parameters from logdir/model_epoch_0014_step_10544\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Model restored!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:11:03.534671 139840769816448 <ipython-input-9-2370c34f435b>:119] Model restored!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy is 0.817578125:\n",
            "\n",
            "Test accuracy has been saved to results/test_acc\n",
            "INFO:tensorflow:Recording summary at step 10545.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 13:11:08.249065 139834225735424 supervisor.py:1050] Recording summary at step 10545.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\Completed in: 77.84705066680908s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIK29G4RIeCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}