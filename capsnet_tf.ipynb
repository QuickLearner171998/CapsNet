{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "534c5a53-659b-4dc6-923a-70491e22b364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "a36a9fe2-4533-432f-f46a-112555267c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   data     results\t\t  Try-1   Try-4\n",
            " capsnet_tf.py\t     logdir   tensorboard.ipynb   Try-3   try-5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "outputId": "39e53b83-c2ad-48da-dd74-da587b4432b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "        trX = trainX[:55000] / 255.\n",
        "        trY = trainY[:55000]\n",
        "\n",
        "        valX = trainX[55000:, ] / 255.\n",
        "        valY = trainY[55000:]\n",
        "\n",
        "        num_tr_batch = 55000 // batch_size\n",
        "        num_val_batch = 5000 // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 256, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 50, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.9,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir_try8', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "# ############################\n",
        "# #   distributed setting    #\n",
        "# ############################\n",
        "# flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "# flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "# flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                  best_val_loss = val_loss\n",
        "                  best_val_acc = val_acc\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######NOT SAVING MODEL #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "outputId": "8ed23b68-fd9b-417f-bda4-36385ba152cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cfg.is_training=True\n",
        "try:\n",
        "  def main(_):\n",
        "      tf.logging.info(' Loading Graph...')\n",
        "      num_label = 10\n",
        "      model = CapsNet()\n",
        "      tf.logging.info(' Graph loaded')\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "      sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      if cfg.is_training:\n",
        "          tf.logging.info(' Start training...')\n",
        "          train(model, sv, num_label)\n",
        "          tf.logging.info('Training done')\n",
        "      else:\n",
        "          evaluation(model, sv, num_label)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"\\nBeginning Eval\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:51.850574 140170173187968 <ipython-input-9-b2423ad1e833>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.134911 140170173187968 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.541454 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.545930 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.548533 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.552192 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.555091 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.566912 140170173187968 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:52.590324 140170173187968 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I1110 22:25:53.138016 140170173187968 utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:53.440546 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:53.612744 140170173187968 deprecation.py:323] From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:53.733023 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:54.366537 140170173187968 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:54.368334 140170173187968 <ipython-input-9-b2423ad1e833>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-b2423ad1e833>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:25:54.369946 140170173187968 deprecation.py:323] From <ipython-input-9-b2423ad1e833>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:54.806324 140170173187968 <ipython-input-9-b2423ad1e833>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:56.220710 140170173187968 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:25:56.246239 140170173187968 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:26:28.369992 140170173187968 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:26:29.071683 140170173187968 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/50:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:26:31.315027 140166520051456 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:26:34.586744 140167885084416 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.8638574\n",
            "\n",
            "Global step: 1,loss: 0.85389435\n",
            "\n",
            "Global step: 2,loss: 0.84202147\n",
            "\n",
            "Global step: 3,loss: 0.8064839\n",
            "\n",
            "Global step: 4,loss: 0.7706866\n",
            "\n",
            "Global step: 5,loss: 0.6735324\n",
            "\n",
            "Global step: 6,loss: 0.6162441\n",
            "\n",
            "Global step: 7,loss: 0.53766537\n",
            "\n",
            "Global step: 8,loss: 0.49536064\n",
            "\n",
            "Global step: 9,loss: 0.4243006\n",
            "\n",
            "Global step: 10,loss: 0.38935184\n",
            "\n",
            "Global step: 11,loss: 0.36572194\n",
            "\n",
            "Global step: 12,loss: 0.33769077\n",
            "\n",
            "Global step: 13,loss: 0.28950316\n",
            "\n",
            "Global step: 14,loss: 0.26903957\n",
            "\n",
            "Global step: 15,loss: 0.27810025\n",
            "\n",
            "Global step: 16,loss: 0.26258823\n",
            "\n",
            "Global step: 17,loss: 0.26827592\n",
            "\n",
            "Global step: 18,loss: 0.25030893\n",
            "\n",
            "Global step: 19,loss: 0.23702773\n",
            "\n",
            "Global step: 20,loss: 0.23114783\n",
            "\n",
            "Global step: 21,loss: 0.2157732\n",
            "\n",
            "Global step: 22,loss: 0.22780964\n",
            "\n",
            "Global step: 23,loss: 0.20338637\n",
            "\n",
            "Global step: 24,loss: 0.20108017\n",
            "\n",
            "Global step: 25,loss: 0.19723964\n",
            "\n",
            "Global step: 26,loss: 0.2000396\n",
            "\n",
            "Global step: 27,loss: 0.20466736\n",
            "\n",
            "Global step: 28,loss: 0.18029135\n",
            "\n",
            "Global step: 29,loss: 0.18438214\n",
            "\n",
            "Global step: 30,loss: 0.17666481\n",
            "\n",
            "Global step: 31,loss: 0.1659849\n",
            "\n",
            "Global step: 32,loss: 0.16045964\n",
            "\n",
            "Global step: 33,loss: 0.16134445\n",
            "\n",
            "Global step: 34,loss: 0.16294712\n",
            "\n",
            "Global step: 35,loss: 0.15962796\n",
            "\n",
            "Global step: 36,loss: 0.1535202\n",
            "\n",
            "Global step: 37,loss: 0.15621588\n",
            "\n",
            "Global step: 38,loss: 0.14324452\n",
            "\n",
            "Global step: 39,loss: 0.15152234\n",
            "\n",
            "Global step: 40,loss: 0.1516521\n",
            "\n",
            "Global step: 41,loss: 0.14923398\n",
            "\n",
            "Global step: 42,loss: 0.15352312\n",
            "\n",
            "Global step: 43,loss: 0.13037762\n",
            "\n",
            "Global step: 44,loss: 0.14119792\n",
            "\n",
            "Global step: 45,loss: 0.124546915\n",
            "\n",
            "Global step: 46,loss: 0.13637961\n",
            "\n",
            "Global step: 47,loss: 0.11763334\n",
            "\n",
            "Global step: 48,loss: 0.11478679\n",
            "\n",
            "Global step: 49,loss: 0.11046217\n",
            "\n",
            "Global step: 50,loss: 0.121233\n",
            "\n",
            "Global step: 51,loss: 0.106288366\n",
            "\n",
            "Global step: 52,loss: 0.11312468\n",
            "\n",
            "Global step: 53,loss: 0.12477568\n",
            "\n",
            "Global step: 54,loss: 0.118932694\n",
            "\n",
            "Global step: 55,loss: 0.13399681\n",
            "\n",
            "Global step: 56,loss: 0.11760646\n",
            "\n",
            "Global step: 57,loss: 0.11698785\n",
            "\n",
            "Global step: 58,loss: 0.10888607\n",
            "\n",
            "Global step: 59,loss: 0.104450956\n",
            "\n",
            "Global step: 60,loss: 0.104643285\n",
            "\n",
            "Global step: 61,loss: 0.105355546\n",
            "\n",
            "Global step: 62,loss: 0.09828071\n",
            "\n",
            "Global step: 63,loss: 0.11566975\n",
            "\n",
            "Global step: 64,loss: 0.085165136\n",
            "\n",
            "Global step: 65,loss: 0.09885916\n",
            "\n",
            "Global step: 66,loss: 0.096505605\n",
            "\n",
            "Global step: 67,loss: 0.09833689\n",
            "\n",
            "Global step: 68,loss: 0.104442164\n",
            "\n",
            "Global step: 69,loss: 0.09701356\n",
            "\n",
            "Global step: 70,loss: 0.10211797\n",
            "\n",
            "Global step: 71,loss: 0.10302614\n",
            "\n",
            "Global step: 72,loss: 0.10506469\n",
            "\n",
            "Global step: 73,loss: 0.11470415\n",
            "\n",
            "Global step: 74,loss: 0.10325866\n",
            "\n",
            "Global step: 75,loss: 0.09017573\n",
            "\n",
            "Global step: 76,loss: 0.09578134\n",
            "\n",
            "Global step: 77,loss: 0.0984958\n",
            "\n",
            "Global step: 78,loss: 0.08937757\n",
            "\n",
            "Global step: 79,loss: 0.08525418\n",
            "\n",
            "Global step: 80,loss: 0.08195728\n",
            "\n",
            "Global step: 81,loss: 0.12036735\n",
            "\n",
            "Global step: 82,loss: 0.088417016\n",
            "\n",
            "Global step: 83,loss: 0.092555225\n",
            "\n",
            "Global step: 84,loss: 0.09300104\n",
            "\n",
            "Global step: 85,loss: 0.08853954\n",
            "\n",
            "Global step: 86,loss: 0.080749586\n",
            "\n",
            "Global step: 87,loss: 0.08824483\n",
            "\n",
            "Global step: 88,loss: 0.09102863\n",
            "\n",
            "Global step: 89,loss: 0.089898854\n",
            "\n",
            "Global step: 90,loss: 0.09082547\n",
            "\n",
            "Global step: 91,loss: 0.087889515\n",
            "\n",
            "Global step: 92,loss: 0.08651969\n",
            "\n",
            "Global step: 93,loss: 0.08266779\n",
            "\n",
            "Global step: 94,loss: 0.0809589\n",
            "\n",
            "Global step: 95,loss: 0.09004039\n",
            "\n",
            "Global step: 96,loss: 0.07858676\n",
            "\n",
            "Global step: 97,loss: 0.08352285\n",
            "\n",
            "Global step: 98,loss: 0.08465962\n",
            "\n",
            "Global step: 99,loss: 0.076782465\n",
            "\n",
            "Global step: 100,loss: 0.09242581\n",
            "\n",
            "Global step: 101,loss: 0.08037849\n",
            "\n",
            "Global step: 102,loss: 0.075253874\n",
            "\n",
            "Global step: 103,loss: 0.08197659\n",
            "\n",
            "Global step: 104,loss: 0.08657619\n",
            "\n",
            "Global step: 105,loss: 0.07019977\n",
            "\n",
            "Global step: 106,loss: 0.07346592\n",
            "\n",
            "Global step: 107,loss: 0.07556768\n",
            "\n",
            "Global step: 108,loss: 0.08241308\n",
            "\n",
            "Global step: 109,loss: 0.07189867\n",
            "\n",
            "Global step: 110,loss: 0.078632474\n",
            "\n",
            "Global step: 111,loss: 0.07918191\n",
            "\n",
            "Global step: 112,loss: 0.08948749\n",
            "\n",
            "Global step: 113,loss: 0.07688842\n",
            "\n",
            "Global step: 114,loss: 0.07487093\n",
            "\n",
            "Global step: 115,loss: 0.084077016\n",
            "\n",
            "Global step: 116,loss: 0.07424041\n",
            "\n",
            "Global step: 117,loss: 0.07554832\n",
            "\n",
            "Global step: 118,loss: 0.0693616\n",
            "\n",
            "Global step: 119,loss: 0.06978148\n",
            "\n",
            "Global step: 120,loss: 0.07820737\n",
            "\n",
            "Global step: 121,loss: 0.07560392\n",
            "\n",
            "Global step: 122,loss: 0.068057366\n",
            "\n",
            "Global step: 123,loss: 0.0710915\n",
            "\n",
            "Global step: 124,loss: 0.0815389\n",
            "\n",
            "Global step: 125,loss: 0.073512\n",
            "\n",
            "Global step: 126,loss: 0.071852654\n",
            "\n",
            "Global step: 127,loss: 0.075606\n",
            "\n",
            "Global step: 128,loss: 0.074964896\n",
            "\n",
            "Global step: 129,loss: 0.07526044\n",
            "\n",
            "Global step: 130,loss: 0.07770912\n",
            "\n",
            "Global step: 131,loss: 0.08038655\n",
            "\n",
            "Global step: 132,loss: 0.074964315\n",
            "\n",
            "Global step: 133,loss: 0.070653714\n",
            "\n",
            "Global step: 134,loss: 0.078274325\n",
            "\n",
            "Global step: 135,loss: 0.076355025\n",
            "\n",
            "Global step: 136,loss: 0.06845346\n",
            "\n",
            "Global step: 137,loss: 0.073274985\n",
            "\n",
            "Global step: 138,loss: 0.080704585\n",
            "\n",
            "Global step: 139,loss: 0.07099201\n",
            "\n",
            "Global step: 140,loss: 0.07478641\n",
            "\n",
            "Global step: 141,loss: 0.06699385\n",
            "\n",
            "Global step: 142,loss: 0.08437422\n",
            "\n",
            "Global step: 143,loss: 0.060725756\n",
            "\n",
            "Global step: 144,loss: 0.06564395\n",
            "\n",
            "Global step: 145,loss: 0.07026572\n",
            "\n",
            "Global step: 146,loss: 0.0679753\n",
            "\n",
            "Global step: 147,loss: 0.06939058\n",
            "\n",
            "Global step: 148,loss: 0.06902444\n",
            "\n",
            "Global step: 149,loss: 0.06807312\n",
            "\n",
            "Global step: 150,loss: 0.080011785\n",
            "\n",
            "Global step: 151,loss: 0.05988623\n",
            "\n",
            "Global step: 152,loss: 0.06132162\n",
            "\n",
            "Global step: 153,loss: 0.064248025\n",
            "\n",
            "Global step: 154,loss: 0.055741917\n",
            "\n",
            "Global step: 155,loss: 0.07011238\n",
            "\n",
            "Global step: 156,loss: 0.07128355\n",
            "\n",
            "Global step: 157,loss: 0.060467646\n",
            "\n",
            "Global step: 158,loss: 0.061721317\n",
            "\n",
            "Global step: 159,loss: 0.06590462\n",
            "\n",
            "Global step: 160,loss: 0.061409727\n",
            "\n",
            "Global step: 161,loss: 0.055750474\n",
            "\n",
            "Global step: 162,loss: 0.06195994\n",
            "\n",
            "Global step: 163,loss: 0.070673235\n",
            "\n",
            "Global step: 164,loss: 0.061044566\n",
            "\n",
            "Global step: 165,loss: 0.05788596\n",
            "\n",
            "Global step: 166,loss: 0.06521839\n",
            "\n",
            "Global step: 167,loss: 0.0597253\n",
            "\n",
            "Global step: 168,loss: 0.064000905\n",
            "\n",
            "Global step: 169,loss: 0.05852115\n",
            "\n",
            "Global step: 170,loss: 0.06890105\n",
            "\n",
            "Global step: 171,loss: 0.057863913\n",
            "\n",
            "Global step: 172,loss: 0.065213226\n",
            "\n",
            "Global step: 173,loss: 0.06072668\n",
            "\n",
            "Global step: 174,loss: 0.060215436\n",
            "\n",
            "Global step: 175,loss: 0.06193395\n",
            "\n",
            "Global step: 176,loss: 0.061960224\n",
            "\n",
            "Global step: 177,loss: 0.06484238\n",
            "\n",
            "Global step: 178,loss: 0.06462222\n",
            "\n",
            "Global step: 179,loss: 0.07490921\n",
            "\n",
            "Global step: 180,loss: 0.061220646\n",
            "\n",
            "Global step: 181,loss: 0.06177618\n",
            "\n",
            "Global step: 182,loss: 0.06292659\n",
            "\n",
            "Global step: 183,loss: 0.059907563\n",
            "\n",
            "Global step: 184,loss: 0.073246896\n",
            "\n",
            "Global step: 185,loss: 0.07061872\n",
            "\n",
            "Global step: 186,loss: 0.054378256\n",
            "\n",
            "Global step: 187,loss: 0.06657368\n",
            "\n",
            "Global step: 188,loss: 0.0705188\n",
            "\n",
            "Global step: 189,loss: 0.0629732\n",
            "\n",
            "Global step: 190,loss: 0.054830622\n",
            "\n",
            "Global step: 191,loss: 0.057253394\n",
            "\n",
            "Global step: 192,loss: 0.06328683\n",
            "\n",
            "Global step: 193,loss: 0.062458992\n",
            "\n",
            "Global step: 194,loss: 0.06962333\n",
            "\n",
            "Global step: 195,loss: 0.06197337\n",
            "\n",
            "Global step: 196,loss: 0.05502252\n",
            "\n",
            "Global step: 197,loss: 0.05859811\n",
            "\n",
            "Global step: 198,loss: 0.054095842\n",
            "\n",
            "Global step: 199,loss: 0.05437668\n",
            "\n",
            "Global step: 200,loss: 0.05966243\n",
            "\n",
            "Global step: 201,loss: 0.0549115\n",
            "\n",
            "Global step: 202,loss: 0.058945704\n",
            "\n",
            "Global step: 203,loss: 0.054924317\n",
            "\n",
            "Global step: 204,loss: 0.060406484\n",
            "\n",
            "Global step: 205,loss: 0.06721178\n",
            "\n",
            "Global step: 206,loss: 0.059914723\n",
            "\n",
            "Global step: 207,loss: 0.058047913\n",
            "\n",
            "Global step: 208,loss: 0.060363248\n",
            "\n",
            "Global step: 209,loss: 0.057208866\n",
            "\n",
            "Global step: 210,loss: 0.062428206\n",
            "\n",
            "Global step: 211,loss: 0.056900326\n",
            "\n",
            "Global step: 212,loss: 0.055643946\n",
            "\n",
            "Global step: 213,loss: 0.05412499\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 213,Val_Loss: 0.054746153127206,  Val_acc: 0.9932154605263158 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:27:31.598361 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/50:\n",
            "Global step: 214,loss: 0.05959543\n",
            "\n",
            "Global step: 215,loss: 0.052360915\n",
            "\n",
            "Global step: 216,loss: 0.05816724\n",
            "\n",
            "Global step: 217,loss: 0.053587336\n",
            "\n",
            "Global step: 218,loss: 0.054754663\n",
            "\n",
            "Global step: 219,loss: 0.05867714\n",
            "\n",
            "Global step: 220,loss: 0.055508137\n",
            "\n",
            "Global step: 221,loss: 0.06812912\n",
            "\n",
            "Global step: 222,loss: 0.050669797\n",
            "\n",
            "Global step: 223,loss: 0.05787102\n",
            "\n",
            "Global step: 224,loss: 0.06295\n",
            "\n",
            "Global step: 225,loss: 0.05745403\n",
            "\n",
            "Global step: 226,loss: 0.059105106\n",
            "\n",
            "Global step: 227,loss: 0.05112632\n",
            "\n",
            "Global step: 228,loss: 0.05480706\n",
            "\n",
            "Global step: 229,loss: 0.06319239\n",
            "\n",
            "Global step: 230,loss: 0.054452978\n",
            "\n",
            "Global step: 231,loss: 0.04852128\n",
            "\n",
            "Global step: 232,loss: 0.04723479\n",
            "\n",
            "Global step: 233,loss: 0.057013478\n",
            "\n",
            "Global step: 234,loss: 0.05789566\n",
            "\n",
            "Global step: 235,loss: 0.06381959\n",
            "\n",
            "Global step: 236,loss: 0.054829016\n",
            "\n",
            "Global step: 237,loss: 0.0545867\n",
            "\n",
            "Global step: 238,loss: 0.05447821\n",
            "\n",
            "Global step: 239,loss: 0.057533607\n",
            "\n",
            "Global step: 240,loss: 0.05477044\n",
            "\n",
            "Global step: 241,loss: 0.068114474\n",
            "\n",
            "Global step: 242,loss: 0.052888624\n",
            "\n",
            "Global step: 243,loss: 0.056758665\n",
            "\n",
            "Global step: 244,loss: 0.05672833\n",
            "\n",
            "Global step: 245,loss: 0.055164523\n",
            "\n",
            "Global step: 246,loss: 0.05628636\n",
            "\n",
            "Global step: 247,loss: 0.053255543\n",
            "\n",
            "Global step: 248,loss: 0.05070149\n",
            "\n",
            "Global step: 249,loss: 0.05303788\n",
            "\n",
            "Global step: 250,loss: 0.05190231\n",
            "\n",
            "Global step: 251,loss: 0.050104145\n",
            "\n",
            "Global step: 252,loss: 0.05549954\n",
            "\n",
            "Global step: 253,loss: 0.059384294\n",
            "\n",
            "Global step: 254,loss: 0.05777345\n",
            "\n",
            "Global step: 255,loss: 0.050528117\n",
            "\n",
            "Global step: 256,loss: 0.056538563\n",
            "\n",
            "Global step: 257,loss: 0.06614299\n",
            "\n",
            "Global step: 258,loss: 0.055340946\n",
            "\n",
            "Global step: 259,loss: 0.052434213\n",
            "\n",
            "Global step: 260,loss: 0.050652042\n",
            "\n",
            "Global step: 261,loss: 0.051068723\n",
            "\n",
            "Global step: 262,loss: 0.054029148\n",
            "\n",
            "Global step: 263,loss: 0.054639\n",
            "\n",
            "Global step: 264,loss: 0.04581499\n",
            "\n",
            "Global step: 265,loss: 0.050814662\n",
            "\n",
            "Global step: 266,loss: 0.05403037\n",
            "\n",
            "Global step: 267,loss: 0.0523204\n",
            "\n",
            "Global step: 268,loss: 0.05077493\n",
            "\n",
            "Global step: 269,loss: 0.053058796\n",
            "\n",
            "Global step: 270,loss: 0.054388553\n",
            "\n",
            "Global step: 271,loss: 0.048535742\n",
            "\n",
            "Global step: 272,loss: 0.060871884\n",
            "\n",
            "Global step: 273,loss: 0.056406483\n",
            "\n",
            "Global step: 274,loss: 0.053502925\n",
            "\n",
            "Global step: 275,loss: 0.055921376\n",
            "\n",
            "Global step: 276,loss: 0.04840121\n",
            "\n",
            "Global step: 277,loss: 0.05276323\n",
            "\n",
            "Global step: 278,loss: 0.057492074\n",
            "\n",
            "Global step: 279,loss: 0.04923795\n",
            "\n",
            "Global step: 280,loss: 0.060792975\n",
            "\n",
            "Global step: 281,loss: 0.055190988\n",
            "\n",
            "Global step: 282,loss: 0.05903147\n",
            "\n",
            "Global step: 283,loss: 0.056033574\n",
            "\n",
            "Global step: 284,loss: 0.054809153\n",
            "\n",
            "Global step: 285,loss: 0.05495507\n",
            "\n",
            "Global step: 286,loss: 0.052658442\n",
            "\n",
            "Global step: 287,loss: 0.052152738\n",
            "\n",
            "Global step: 288,loss: 0.054741338\n",
            "\n",
            "Global step: 289,loss: 0.05343423\n",
            "\n",
            "Global step: 290,loss: 0.048032507\n",
            "\n",
            "Global step: 291,loss: 0.060707983\n",
            "\n",
            "Global step: 292,loss: 0.049689494\n",
            "\n",
            "Global step: 293,loss: 0.055922277\n",
            "\n",
            "Global step: 294,loss: 0.05069261\n",
            "\n",
            "Global step: 295,loss: 0.047180157\n",
            "\n",
            "Global step: 296,loss: 0.054027285\n",
            "\n",
            "Global step: 297,loss: 0.059594307\n",
            "\n",
            "Global step: 298,loss: 0.05417449\n",
            "\n",
            "Global step: 299,loss: 0.04916698\n",
            "\n",
            "Global step: 300,loss: 0.05830142\n",
            "\n",
            "Global step: 301,loss: 0.054636095\n",
            "\n",
            "Global step: 302,loss: 0.05115998\n",
            "\n",
            "Global step: 303,loss: 0.04457691\n",
            "\n",
            "Global step: 304,loss: 0.05324466\n",
            "\n",
            "Global step: 305,loss: 0.05608301\n",
            "\n",
            "Global step: 306,loss: 0.046110943\n",
            "\n",
            "Global step: 307,loss: 0.044153735\n",
            "\n",
            "Global step: 308,loss: 0.055636965\n",
            "\n",
            "Global step: 309,loss: 0.049190454\n",
            "\n",
            "Global step: 310,loss: 0.050567552\n",
            "\n",
            "Global step: 311,loss: 0.04905673\n",
            "\n",
            "Global step: 312,loss: 0.051325567\n",
            "\n",
            "Global step: 313,loss: 0.04989782\n",
            "\n",
            "Global step: 314,loss: 0.047578014\n",
            "\n",
            "Global step: 315,loss: 0.059136137\n",
            "\n",
            "Global step: 316,loss: 0.0494731\n",
            "\n",
            "Global step: 317,loss: 0.04722624\n",
            "\n",
            "Global step: 318,loss: 0.052634627\n",
            "\n",
            "Global step: 319,loss: 0.049130887\n",
            "\n",
            "Global step: 320,loss: 0.05317358\n",
            "\n",
            "Global step: 321,loss: 0.048336215\n",
            "\n",
            "Global step: 322,loss: 0.045200452\n",
            "\n",
            "Global step: 323,loss: 0.051456943\n",
            "\n",
            "Global step: 324,loss: 0.04765984\n",
            "\n",
            "Global step: 325,loss: 0.047701146\n",
            "\n",
            "Global step: 326,loss: 0.05490867\n",
            "\n",
            "Global step: 327,loss: 0.05172152\n",
            "\n",
            "Global step: 328,loss: 0.0485591\n",
            "\n",
            "Global step: 329,loss: 0.051468037\n",
            "\n",
            "Global step: 330,loss: 0.050164048\n",
            "\n",
            "Global step: 331,loss: 0.04262285\n",
            "\n",
            "Global step: 332,loss: 0.041686155\n",
            "\n",
            "Global step: 333,loss: 0.05100935\n",
            "\n",
            "Global step: 334,loss: 0.04712811\n",
            "\n",
            "Global step: 335,loss: 0.045251973\n",
            "\n",
            "Global step: 336,loss: 0.051761843\n",
            "\n",
            "Global step: 337,loss: 0.049764246\n",
            "\n",
            "Global step: 338,loss: 0.04564211\n",
            "\n",
            "Global step: 339,loss: 0.054622512\n",
            "\n",
            "Global step: 340,loss: 0.042474\n",
            "\n",
            "Global step: 341,loss: 0.04658708\n",
            "\n",
            "Global step: 342,loss: 0.04412469\n",
            "\n",
            "Global step: 343,loss: 0.046447027\n",
            "\n",
            "Global step: 344,loss: 0.04912331\n",
            "\n",
            "Global step: 345,loss: 0.044026442\n",
            "\n",
            "Global step: 346,loss: 0.062191173\n",
            "\n",
            "Global step: 347,loss: 0.04987214\n",
            "\n",
            "Global step: 348,loss: 0.04723572\n",
            "\n",
            "Global step: 349,loss: 0.044459462\n",
            "\n",
            "Global step: 350,loss: 0.05117251\n",
            "\n",
            "Global step: 351,loss: 0.050481454\n",
            "\n",
            "Global step: 352,loss: 0.051458642\n",
            "\n",
            "Global step: 353,loss: 0.0508626\n",
            "\n",
            "Global step: 354,loss: 0.0468001\n",
            "\n",
            "Global step: 355,loss: 0.056201406\n",
            "\n",
            "Global step: 356,loss: 0.047046624\n",
            "\n",
            "Global step: 357,loss: 0.05156059\n",
            "\n",
            "Global step: 358,loss: 0.050081294\n",
            "\n",
            "Global step: 359,loss: 0.052472472\n",
            "\n",
            "Global step: 360,loss: 0.048317198\n",
            "\n",
            "Global step: 361,loss: 0.047433082\n",
            "\n",
            "Global step: 362,loss: 0.04658398\n",
            "\n",
            "Global step: 363,loss: 0.045027398\n",
            "\n",
            "Global step: 364,loss: 0.045725048\n",
            "\n",
            "Global step: 365,loss: 0.055294637\n",
            "\n",
            "Global step: 366,loss: 0.04999938\n",
            "\n",
            "Global step: 367,loss: 0.04668634\n",
            "\n",
            "Global step: 368,loss: 0.047715586\n",
            "\n",
            "Global step: 369,loss: 0.045988575\n",
            "\n",
            "Global step: 370,loss: 0.04990553\n",
            "\n",
            "Global step: 371,loss: 0.04870395\n",
            "\n",
            "Global step: 372,loss: 0.05425629\n",
            "\n",
            "Global step: 373,loss: 0.054676227\n",
            "\n",
            "Global step: 374,loss: 0.04784747\n",
            "\n",
            "Global step: 375,loss: 0.046888135\n",
            "\n",
            "Global step: 376,loss: 0.055720255\n",
            "\n",
            "Global step: 377,loss: 0.042870842\n",
            "\n",
            "Global step: 378,loss: 0.0465038\n",
            "\n",
            "Global step: 379,loss: 0.051911883\n",
            "\n",
            "Global step: 380,loss: 0.050893463\n",
            "\n",
            "Global step: 381,loss: 0.046969295\n",
            "\n",
            "Global step: 382,loss: 0.050492346\n",
            "\n",
            "Global step: 383,loss: 0.045420416\n",
            "\n",
            "Global step: 384,loss: 0.050724164\n",
            "\n",
            "Global step: 385,loss: 0.048225112\n",
            "\n",
            "Global step: 386,loss: 0.04881789\n",
            "\n",
            "Global step: 387,loss: 0.046845168\n",
            "\n",
            "Global step: 388,loss: 0.05003439\n",
            "\n",
            "Global step: 389,loss: 0.040302925\n",
            "\n",
            "Global step: 390,loss: 0.05810409\n",
            "\n",
            "Global step: 391,loss: 0.046402723\n",
            "\n",
            "Global step: 392,loss: 0.04336021\n",
            "\n",
            "Global step: 393,loss: 0.044745907\n",
            "\n",
            "Global step: 394,loss: 0.047702003\n",
            "\n",
            "Global step: 395,loss: 0.048045795\n",
            "\n",
            "Global step: 396,loss: 0.04354886\n",
            "\n",
            "Global step: 397,loss: 0.039266262\n",
            "\n",
            "Global step: 398,loss: 0.044419218\n",
            "\n",
            "Global step: 399,loss: 0.046068583\n",
            "\n",
            "Global step: 400,loss: 0.053841263\n",
            "\n",
            "Global step: 401,loss: 0.046350423\n",
            "\n",
            "Global step: 402,loss: 0.047467917\n",
            "\n",
            "Global step: 403,loss: 0.043986753\n",
            "\n",
            "Global step: 404,loss: 0.04213374\n",
            "\n",
            "Global step: 405,loss: 0.04287055\n",
            "\n",
            "Global step: 406,loss: 0.046784543\n",
            "\n",
            "Global step: 407,loss: 0.047753997\n",
            "\n",
            "Global step: 408,loss: 0.0481677\n",
            "\n",
            "Global step: 409,loss: 0.048372537\n",
            "\n",
            "Global step: 410,loss: 0.05273329\n",
            "\n",
            "Global step: 411,loss: 0.04992365\n",
            "\n",
            "Global step: 412,loss: 0.045150507\n",
            "\n",
            "Global step: 413,loss: 0.04355564\n",
            "\n",
            "Global step: 414,loss: 0.049420822\n",
            "\n",
            "Global step: 415,loss: 0.04094419\n",
            "\n",
            "Global step: 416,loss: 0.047363717\n",
            "\n",
            "Global step: 417,loss: 0.046866342\n",
            "\n",
            "Global step: 418,loss: 0.04985159\n",
            "\n",
            "Global step: 419,loss: 0.052562043\n",
            "\n",
            "Global step: 420,loss: 0.04103843\n",
            "\n",
            "Global step: 421,loss: 0.044718906\n",
            "\n",
            "Global step: 422,loss: 0.041152973\n",
            "\n",
            "Global step: 423,loss: 0.047002107\n",
            "\n",
            "Global step: 424,loss: 0.04057993\n",
            "\n",
            "Global step: 425,loss: 0.043293856\n",
            "\n",
            "Global step: 426,loss: 0.046533544\n",
            "\n",
            "Global step: 427,loss: 0.047218677\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 427,Val_Loss: 0.04193032552537165,  Val_acc: 0.9967105263157895 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:28:25.388901 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/50:\n",
            "Global step: 428,loss: 0.04978162\n",
            "\n",
            "Global step: 429,loss: 0.04968156\n",
            "\n",
            "Global step: 430,loss: 0.04517305\n",
            "\n",
            "Global step: 431,loss: 0.050451145\n",
            "\n",
            "Global step: 432,loss: 0.044617295\n",
            "\n",
            "Global step: 433,loss: 0.04333218\n",
            "\n",
            "Global step: 434,loss: 0.04821676\n",
            "\n",
            "Global step: 435,loss: 0.047432695\n",
            "\n",
            "Global step: 436,loss: 0.051027842\n",
            "\n",
            "Global step: 437,loss: 0.044410903\n",
            "\n",
            "Global step: 438,loss: 0.04512761\n",
            "\n",
            "Global step: 439,loss: 0.04150056\n",
            "\n",
            "Global step: 440,loss: 0.05256778\n",
            "\n",
            "Global step: 441,loss: 0.047550254\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 442.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:28:29.341350 140167885084416 supervisor.py:1050] Recording summary at step 442.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.7445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:28:29.354610 140166520051456 supervisor.py:1099] global_step/sec: 3.7445\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 442,loss: 0.040937643\n",
            "\n",
            "Global step: 443,loss: 0.044468556\n",
            "\n",
            "Global step: 444,loss: 0.04474867\n",
            "\n",
            "Global step: 445,loss: 0.045710973\n",
            "\n",
            "Global step: 446,loss: 0.04681582\n",
            "\n",
            "Global step: 447,loss: 0.040363744\n",
            "\n",
            "Global step: 448,loss: 0.043703727\n",
            "\n",
            "Global step: 449,loss: 0.040943168\n",
            "\n",
            "Global step: 450,loss: 0.05189881\n",
            "\n",
            "Global step: 451,loss: 0.037589118\n",
            "\n",
            "Global step: 452,loss: 0.038587786\n",
            "\n",
            "Global step: 453,loss: 0.043740675\n",
            "\n",
            "Global step: 454,loss: 0.043859754\n",
            "\n",
            "Global step: 455,loss: 0.049060717\n",
            "\n",
            "Global step: 456,loss: 0.042331338\n",
            "\n",
            "Global step: 457,loss: 0.045061488\n",
            "\n",
            "Global step: 458,loss: 0.045678753\n",
            "\n",
            "Global step: 459,loss: 0.050313536\n",
            "\n",
            "Global step: 460,loss: 0.046922557\n",
            "\n",
            "Global step: 461,loss: 0.044033118\n",
            "\n",
            "Global step: 462,loss: 0.040823646\n",
            "\n",
            "Global step: 463,loss: 0.04184168\n",
            "\n",
            "Global step: 464,loss: 0.047165\n",
            "\n",
            "Global step: 465,loss: 0.046777196\n",
            "\n",
            "Global step: 466,loss: 0.04399483\n",
            "\n",
            "Global step: 467,loss: 0.042013656\n",
            "\n",
            "Global step: 468,loss: 0.0416895\n",
            "\n",
            "Global step: 469,loss: 0.047361586\n",
            "\n",
            "Global step: 470,loss: 0.044782415\n",
            "\n",
            "Global step: 471,loss: 0.041313335\n",
            "\n",
            "Global step: 472,loss: 0.043104324\n",
            "\n",
            "Global step: 473,loss: 0.044266894\n",
            "\n",
            "Global step: 474,loss: 0.048875928\n",
            "\n",
            "Global step: 475,loss: 0.041940093\n",
            "\n",
            "Global step: 476,loss: 0.043216344\n",
            "\n",
            "Global step: 477,loss: 0.04510166\n",
            "\n",
            "Global step: 478,loss: 0.041142743\n",
            "\n",
            "Global step: 479,loss: 0.04380398\n",
            "\n",
            "Global step: 480,loss: 0.04305224\n",
            "\n",
            "Global step: 481,loss: 0.041228224\n",
            "\n",
            "Global step: 482,loss: 0.044696588\n",
            "\n",
            "Global step: 483,loss: 0.03933785\n",
            "\n",
            "Global step: 484,loss: 0.039753854\n",
            "\n",
            "Global step: 485,loss: 0.037489735\n",
            "\n",
            "Global step: 486,loss: 0.040681437\n",
            "\n",
            "Global step: 487,loss: 0.039264344\n",
            "\n",
            "Global step: 488,loss: 0.0371864\n",
            "\n",
            "Global step: 489,loss: 0.038700685\n",
            "\n",
            "Global step: 490,loss: 0.03780763\n",
            "\n",
            "Global step: 491,loss: 0.03827401\n",
            "\n",
            "Global step: 492,loss: 0.040716022\n",
            "\n",
            "Global step: 493,loss: 0.041014567\n",
            "\n",
            "Global step: 494,loss: 0.041821904\n",
            "\n",
            "Global step: 495,loss: 0.04176823\n",
            "\n",
            "Global step: 496,loss: 0.048608042\n",
            "\n",
            "Global step: 497,loss: 0.041808955\n",
            "\n",
            "Global step: 498,loss: 0.037002474\n",
            "\n",
            "Global step: 499,loss: 0.04071036\n",
            "\n",
            "Global step: 500,loss: 0.039896026\n",
            "\n",
            "Global step: 501,loss: 0.05346793\n",
            "\n",
            "Global step: 502,loss: 0.040461756\n",
            "\n",
            "Global step: 503,loss: 0.04521565\n",
            "\n",
            "Global step: 504,loss: 0.047193296\n",
            "\n",
            "Global step: 505,loss: 0.03906573\n",
            "\n",
            "Global step: 506,loss: 0.04392406\n",
            "\n",
            "Global step: 507,loss: 0.039805047\n",
            "\n",
            "Global step: 508,loss: 0.040253103\n",
            "\n",
            "Global step: 509,loss: 0.041390136\n",
            "\n",
            "Global step: 510,loss: 0.03751337\n",
            "\n",
            "Global step: 511,loss: 0.0419875\n",
            "\n",
            "Global step: 512,loss: 0.036851082\n",
            "\n",
            "Global step: 513,loss: 0.038490556\n",
            "\n",
            "Global step: 514,loss: 0.044297706\n",
            "\n",
            "Global step: 515,loss: 0.040623926\n",
            "\n",
            "Global step: 516,loss: 0.039804976\n",
            "\n",
            "Global step: 517,loss: 0.03836224\n",
            "\n",
            "Global step: 518,loss: 0.042022474\n",
            "\n",
            "Global step: 519,loss: 0.043260574\n",
            "\n",
            "Global step: 520,loss: 0.04396119\n",
            "\n",
            "Global step: 521,loss: 0.04337758\n",
            "\n",
            "Global step: 522,loss: 0.04127019\n",
            "\n",
            "Global step: 523,loss: 0.038127676\n",
            "\n",
            "Global step: 524,loss: 0.03768775\n",
            "\n",
            "Global step: 525,loss: 0.0410295\n",
            "\n",
            "Global step: 526,loss: 0.03967104\n",
            "\n",
            "Global step: 527,loss: 0.03487825\n",
            "\n",
            "Global step: 528,loss: 0.039332055\n",
            "\n",
            "Global step: 529,loss: 0.041330595\n",
            "\n",
            "Global step: 530,loss: 0.041223396\n",
            "\n",
            "Global step: 531,loss: 0.043670233\n",
            "\n",
            "Global step: 532,loss: 0.041146472\n",
            "\n",
            "Global step: 533,loss: 0.045607723\n",
            "\n",
            "Global step: 534,loss: 0.045607895\n",
            "\n",
            "Global step: 535,loss: 0.045060523\n",
            "\n",
            "Global step: 536,loss: 0.049630243\n",
            "\n",
            "Global step: 537,loss: 0.03694609\n",
            "\n",
            "Global step: 538,loss: 0.04478212\n",
            "\n",
            "Global step: 539,loss: 0.039843336\n",
            "\n",
            "Global step: 540,loss: 0.046161212\n",
            "\n",
            "Global step: 541,loss: 0.037801594\n",
            "\n",
            "Global step: 542,loss: 0.041724965\n",
            "\n",
            "Global step: 543,loss: 0.039732497\n",
            "\n",
            "Global step: 544,loss: 0.035921827\n",
            "\n",
            "Global step: 545,loss: 0.043433115\n",
            "\n",
            "Global step: 546,loss: 0.037555687\n",
            "\n",
            "Global step: 547,loss: 0.042421486\n",
            "\n",
            "Global step: 548,loss: 0.042049576\n",
            "\n",
            "Global step: 549,loss: 0.040704273\n",
            "\n",
            "Global step: 550,loss: 0.04632631\n",
            "\n",
            "Global step: 551,loss: 0.03646093\n",
            "\n",
            "Global step: 552,loss: 0.03803215\n",
            "\n",
            "Global step: 553,loss: 0.042956635\n",
            "\n",
            "Global step: 554,loss: 0.04252688\n",
            "\n",
            "Global step: 555,loss: 0.04250294\n",
            "\n",
            "Global step: 556,loss: 0.04139676\n",
            "\n",
            "Global step: 557,loss: 0.051137686\n",
            "\n",
            "Global step: 558,loss: 0.051455196\n",
            "\n",
            "Global step: 559,loss: 0.04315806\n",
            "\n",
            "Global step: 560,loss: 0.04895632\n",
            "\n",
            "Global step: 561,loss: 0.037851732\n",
            "\n",
            "Global step: 562,loss: 0.040588442\n",
            "\n",
            "Global step: 563,loss: 0.041212\n",
            "\n",
            "Global step: 564,loss: 0.04724396\n",
            "\n",
            "Global step: 565,loss: 0.04169268\n",
            "\n",
            "Global step: 566,loss: 0.0406411\n",
            "\n",
            "Global step: 567,loss: 0.04233138\n",
            "\n",
            "Global step: 568,loss: 0.045752127\n",
            "\n",
            "Global step: 569,loss: 0.042423118\n",
            "\n",
            "Global step: 570,loss: 0.039892156\n",
            "\n",
            "Global step: 571,loss: 0.046814412\n",
            "\n",
            "Global step: 572,loss: 0.04679042\n",
            "\n",
            "Global step: 573,loss: 0.037874702\n",
            "\n",
            "Global step: 574,loss: 0.03979414\n",
            "\n",
            "Global step: 575,loss: 0.039874997\n",
            "\n",
            "Global step: 576,loss: 0.041625503\n",
            "\n",
            "Global step: 577,loss: 0.041962262\n",
            "\n",
            "Global step: 578,loss: 0.03914521\n",
            "\n",
            "Global step: 579,loss: 0.034516096\n",
            "\n",
            "Global step: 580,loss: 0.04177744\n",
            "\n",
            "Global step: 581,loss: 0.047287814\n",
            "\n",
            "Global step: 582,loss: 0.043360002\n",
            "\n",
            "Global step: 583,loss: 0.037700027\n",
            "\n",
            "Global step: 584,loss: 0.036290053\n",
            "\n",
            "Global step: 585,loss: 0.039351724\n",
            "\n",
            "Global step: 586,loss: 0.039014164\n",
            "\n",
            "Global step: 587,loss: 0.047704864\n",
            "\n",
            "Global step: 588,loss: 0.039883807\n",
            "\n",
            "Global step: 589,loss: 0.040464893\n",
            "\n",
            "Global step: 590,loss: 0.037910234\n",
            "\n",
            "Global step: 591,loss: 0.038619477\n",
            "\n",
            "Global step: 592,loss: 0.042399324\n",
            "\n",
            "Global step: 593,loss: 0.037662983\n",
            "\n",
            "Global step: 594,loss: 0.036809772\n",
            "\n",
            "Global step: 595,loss: 0.040769592\n",
            "\n",
            "Global step: 596,loss: 0.042346306\n",
            "\n",
            "Global step: 597,loss: 0.03370221\n",
            "\n",
            "Global step: 598,loss: 0.037773643\n",
            "\n",
            "Global step: 599,loss: 0.035509676\n",
            "\n",
            "Global step: 600,loss: 0.035436895\n",
            "\n",
            "Global step: 601,loss: 0.041057657\n",
            "\n",
            "Global step: 602,loss: 0.0466604\n",
            "\n",
            "Global step: 603,loss: 0.040704176\n",
            "\n",
            "Global step: 604,loss: 0.04086476\n",
            "\n",
            "Global step: 605,loss: 0.039085533\n",
            "\n",
            "Global step: 606,loss: 0.036023926\n",
            "\n",
            "Global step: 607,loss: 0.037231352\n",
            "\n",
            "Global step: 608,loss: 0.03850282\n",
            "\n",
            "Global step: 609,loss: 0.03812783\n",
            "\n",
            "Global step: 610,loss: 0.039569814\n",
            "\n",
            "Global step: 611,loss: 0.0417129\n",
            "\n",
            "Global step: 612,loss: 0.04091564\n",
            "\n",
            "Global step: 613,loss: 0.03776809\n",
            "\n",
            "Global step: 614,loss: 0.04317607\n",
            "\n",
            "Global step: 615,loss: 0.035307057\n",
            "\n",
            "Global step: 616,loss: 0.038615517\n",
            "\n",
            "Global step: 617,loss: 0.035212263\n",
            "\n",
            "Global step: 618,loss: 0.036824428\n",
            "\n",
            "Global step: 619,loss: 0.03643585\n",
            "\n",
            "Global step: 620,loss: 0.037192445\n",
            "\n",
            "Global step: 621,loss: 0.037320152\n",
            "\n",
            "Global step: 622,loss: 0.035495937\n",
            "\n",
            "Global step: 623,loss: 0.041429132\n",
            "\n",
            "Global step: 624,loss: 0.035404947\n",
            "\n",
            "Global step: 625,loss: 0.03650796\n",
            "\n",
            "Global step: 626,loss: 0.040767357\n",
            "\n",
            "Global step: 627,loss: 0.036846437\n",
            "\n",
            "Global step: 628,loss: 0.03606291\n",
            "\n",
            "Global step: 629,loss: 0.03645122\n",
            "\n",
            "Global step: 630,loss: 0.03900096\n",
            "\n",
            "Global step: 631,loss: 0.037060946\n",
            "\n",
            "Global step: 632,loss: 0.037118547\n",
            "\n",
            "Global step: 633,loss: 0.037261296\n",
            "\n",
            "Global step: 634,loss: 0.036936626\n",
            "\n",
            "Global step: 635,loss: 0.03473848\n",
            "\n",
            "Global step: 636,loss: 0.043779075\n",
            "\n",
            "Global step: 637,loss: 0.042495716\n",
            "\n",
            "Global step: 638,loss: 0.041529547\n",
            "\n",
            "Global step: 639,loss: 0.044681866\n",
            "\n",
            "Global step: 640,loss: 0.03799839\n",
            "\n",
            "Global step: 641,loss: 0.035948984\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 641,Val_Loss: 0.03725499542135941,  Val_acc: 0.9975328947368421 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:29:19.769909 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/50:\n",
            "Global step: 642,loss: 0.03801746\n",
            "\n",
            "Global step: 643,loss: 0.034025844\n",
            "\n",
            "Global step: 644,loss: 0.040739324\n",
            "\n",
            "Global step: 645,loss: 0.03878324\n",
            "\n",
            "Global step: 646,loss: 0.037756253\n",
            "\n",
            "Global step: 647,loss: 0.039392225\n",
            "\n",
            "Global step: 648,loss: 0.03942418\n",
            "\n",
            "Global step: 649,loss: 0.039789513\n",
            "\n",
            "Global step: 650,loss: 0.039904468\n",
            "\n",
            "Global step: 651,loss: 0.03700246\n",
            "\n",
            "Global step: 652,loss: 0.040274553\n",
            "\n",
            "Global step: 653,loss: 0.033640794\n",
            "\n",
            "Global step: 654,loss: 0.042266905\n",
            "\n",
            "Global step: 655,loss: 0.03770303\n",
            "\n",
            "Global step: 656,loss: 0.041859694\n",
            "\n",
            "Global step: 657,loss: 0.036744606\n",
            "\n",
            "Global step: 658,loss: 0.037378144\n",
            "\n",
            "Global step: 659,loss: 0.033195466\n",
            "\n",
            "Global step: 660,loss: 0.038026355\n",
            "\n",
            "Global step: 661,loss: 0.037414797\n",
            "\n",
            "Global step: 662,loss: 0.03692426\n",
            "\n",
            "Global step: 663,loss: 0.034268025\n",
            "\n",
            "Global step: 664,loss: 0.038704813\n",
            "\n",
            "Global step: 665,loss: 0.03517256\n",
            "\n",
            "Global step: 666,loss: 0.038833812\n",
            "\n",
            "Global step: 667,loss: 0.044889845\n",
            "\n",
            "Global step: 668,loss: 0.037637934\n",
            "\n",
            "Global step: 669,loss: 0.03883939\n",
            "\n",
            "Global step: 670,loss: 0.03393937\n",
            "\n",
            "Global step: 671,loss: 0.036325682\n",
            "\n",
            "Global step: 672,loss: 0.045095317\n",
            "\n",
            "Global step: 673,loss: 0.03713433\n",
            "\n",
            "Global step: 674,loss: 0.034760855\n",
            "\n",
            "Global step: 675,loss: 0.03656575\n",
            "\n",
            "Global step: 676,loss: 0.03763104\n",
            "\n",
            "Global step: 677,loss: 0.03975972\n",
            "\n",
            "Global step: 678,loss: 0.035379514\n",
            "\n",
            "Global step: 679,loss: 0.039247893\n",
            "\n",
            "Global step: 680,loss: 0.04082047\n",
            "\n",
            "Global step: 681,loss: 0.042536516\n",
            "\n",
            "Global step: 682,loss: 0.03768057\n",
            "\n",
            "Global step: 683,loss: 0.03865516\n",
            "\n",
            "Global step: 684,loss: 0.035570055\n",
            "\n",
            "Global step: 685,loss: 0.033320326\n",
            "\n",
            "Global step: 686,loss: 0.037583657\n",
            "\n",
            "Global step: 687,loss: 0.033383302\n",
            "\n",
            "Global step: 688,loss: 0.035951227\n",
            "\n",
            "Global step: 689,loss: 0.03572742\n",
            "\n",
            "Global step: 690,loss: 0.033573832\n",
            "\n",
            "Global step: 691,loss: 0.03634372\n",
            "\n",
            "Global step: 692,loss: 0.037962172\n",
            "\n",
            "Global step: 693,loss: 0.045534194\n",
            "\n",
            "Global step: 694,loss: 0.03390232\n",
            "\n",
            "Global step: 695,loss: 0.0360139\n",
            "\n",
            "Global step: 696,loss: 0.036394462\n",
            "\n",
            "Global step: 697,loss: 0.032757144\n",
            "\n",
            "Global step: 698,loss: 0.03772134\n",
            "\n",
            "Global step: 699,loss: 0.03846491\n",
            "\n",
            "Global step: 700,loss: 0.035379805\n",
            "\n",
            "Global step: 701,loss: 0.03844983\n",
            "\n",
            "Global step: 702,loss: 0.039677203\n",
            "\n",
            "Global step: 703,loss: 0.04005887\n",
            "\n",
            "Global step: 704,loss: 0.033812184\n",
            "\n",
            "Global step: 705,loss: 0.039303347\n",
            "\n",
            "Global step: 706,loss: 0.0341651\n",
            "\n",
            "Global step: 707,loss: 0.03574107\n",
            "\n",
            "Global step: 708,loss: 0.037426755\n",
            "\n",
            "Global step: 709,loss: 0.039290927\n",
            "\n",
            "Global step: 710,loss: 0.03633228\n",
            "\n",
            "Global step: 711,loss: 0.037050318\n",
            "\n",
            "Global step: 712,loss: 0.036368847\n",
            "\n",
            "Global step: 713,loss: 0.034095228\n",
            "\n",
            "Global step: 714,loss: 0.039449982\n",
            "\n",
            "Global step: 715,loss: 0.037552726\n",
            "\n",
            "Global step: 716,loss: 0.039461225\n",
            "\n",
            "Global step: 717,loss: 0.037395805\n",
            "\n",
            "Global step: 718,loss: 0.037075076\n",
            "\n",
            "Global step: 719,loss: 0.03571193\n",
            "\n",
            "Global step: 720,loss: 0.03224055\n",
            "\n",
            "Global step: 721,loss: 0.04121305\n",
            "\n",
            "Global step: 722,loss: 0.04066936\n",
            "\n",
            "Global step: 723,loss: 0.03648748\n",
            "\n",
            "Global step: 724,loss: 0.037324168\n",
            "\n",
            "Global step: 725,loss: 0.033462223\n",
            "\n",
            "Global step: 726,loss: 0.035407055\n",
            "\n",
            "Global step: 727,loss: 0.035603873\n",
            "\n",
            "Global step: 728,loss: 0.036143433\n",
            "\n",
            "Global step: 729,loss: 0.037139628\n",
            "\n",
            "Global step: 730,loss: 0.037040435\n",
            "\n",
            "Global step: 731,loss: 0.034972366\n",
            "\n",
            "Global step: 732,loss: 0.03568125\n",
            "\n",
            "Global step: 733,loss: 0.041485947\n",
            "\n",
            "Global step: 734,loss: 0.035784595\n",
            "\n",
            "Global step: 735,loss: 0.035429526\n",
            "\n",
            "Global step: 736,loss: 0.032627825\n",
            "\n",
            "Global step: 737,loss: 0.0353814\n",
            "\n",
            "Global step: 738,loss: 0.03504552\n",
            "\n",
            "Global step: 739,loss: 0.03929256\n",
            "\n",
            "Global step: 740,loss: 0.03450483\n",
            "\n",
            "Global step: 741,loss: 0.0339376\n",
            "\n",
            "Global step: 742,loss: 0.033670485\n",
            "\n",
            "Global step: 743,loss: 0.038347475\n",
            "\n",
            "Global step: 744,loss: 0.033723142\n",
            "\n",
            "Global step: 745,loss: 0.036697228\n",
            "\n",
            "Global step: 746,loss: 0.034952406\n",
            "\n",
            "Global step: 747,loss: 0.03633901\n",
            "\n",
            "Global step: 748,loss: 0.04113256\n",
            "\n",
            "Global step: 749,loss: 0.04007882\n",
            "\n",
            "Global step: 750,loss: 0.03682526\n",
            "\n",
            "Global step: 751,loss: 0.034361776\n",
            "\n",
            "Global step: 752,loss: 0.038254537\n",
            "\n",
            "Global step: 753,loss: 0.04030744\n",
            "\n",
            "Global step: 754,loss: 0.034021866\n",
            "\n",
            "Global step: 755,loss: 0.03241027\n",
            "\n",
            "Global step: 756,loss: 0.03736431\n",
            "\n",
            "Global step: 757,loss: 0.035900176\n",
            "\n",
            "Global step: 758,loss: 0.035293855\n",
            "\n",
            "Global step: 759,loss: 0.03742673\n",
            "\n",
            "Global step: 760,loss: 0.033002883\n",
            "\n",
            "Global step: 761,loss: 0.03210207\n",
            "\n",
            "Global step: 762,loss: 0.041691456\n",
            "\n",
            "Global step: 763,loss: 0.039289072\n",
            "\n",
            "Global step: 764,loss: 0.03770084\n",
            "\n",
            "Global step: 765,loss: 0.039225712\n",
            "\n",
            "Global step: 766,loss: 0.03324407\n",
            "\n",
            "Global step: 767,loss: 0.03865908\n",
            "\n",
            "Global step: 768,loss: 0.035040908\n",
            "\n",
            "Global step: 769,loss: 0.037028864\n",
            "\n",
            "Global step: 770,loss: 0.035126034\n",
            "\n",
            "Global step: 771,loss: 0.036674738\n",
            "\n",
            "Global step: 772,loss: 0.035596672\n",
            "\n",
            "Global step: 773,loss: 0.033870135\n",
            "\n",
            "Global step: 774,loss: 0.033238877\n",
            "\n",
            "Global step: 775,loss: 0.03651691\n",
            "\n",
            "Global step: 776,loss: 0.037567325\n",
            "\n",
            "Global step: 777,loss: 0.03332778\n",
            "\n",
            "Global step: 778,loss: 0.033009566\n",
            "\n",
            "Global step: 779,loss: 0.03417975\n",
            "\n",
            "Global step: 780,loss: 0.03512478\n",
            "\n",
            "Global step: 781,loss: 0.035871636\n",
            "\n",
            "Global step: 782,loss: 0.038668223\n",
            "\n",
            "Global step: 783,loss: 0.031095143\n",
            "\n",
            "Global step: 784,loss: 0.03340463\n",
            "\n",
            "Global step: 785,loss: 0.035115402\n",
            "\n",
            "Global step: 786,loss: 0.041169293\n",
            "\n",
            "Global step: 787,loss: 0.037756737\n",
            "\n",
            "Global step: 788,loss: 0.034230705\n",
            "\n",
            "Global step: 789,loss: 0.035077125\n",
            "\n",
            "Global step: 790,loss: 0.03543242\n",
            "\n",
            "Global step: 791,loss: 0.03680069\n",
            "\n",
            "Global step: 792,loss: 0.035464726\n",
            "\n",
            "Global step: 793,loss: 0.033640638\n",
            "\n",
            "Global step: 794,loss: 0.036313258\n",
            "\n",
            "Global step: 795,loss: 0.03925947\n",
            "\n",
            "Global step: 796,loss: 0.038364857\n",
            "\n",
            "Global step: 797,loss: 0.034717713\n",
            "\n",
            "Global step: 798,loss: 0.03172388\n",
            "\n",
            "Global step: 799,loss: 0.031524967\n",
            "\n",
            "Global step: 800,loss: 0.03679644\n",
            "\n",
            "Global step: 801,loss: 0.035793506\n",
            "\n",
            "Global step: 802,loss: 0.036868133\n",
            "\n",
            "Global step: 803,loss: 0.034241598\n",
            "\n",
            "Global step: 804,loss: 0.037547696\n",
            "\n",
            "Global step: 805,loss: 0.03425851\n",
            "\n",
            "Global step: 806,loss: 0.036761254\n",
            "\n",
            "Global step: 807,loss: 0.040800855\n",
            "\n",
            "Global step: 808,loss: 0.03388141\n",
            "\n",
            "Global step: 809,loss: 0.032379664\n",
            "\n",
            "Global step: 810,loss: 0.03867829\n",
            "\n",
            "Global step: 811,loss: 0.039791457\n",
            "\n",
            "Global step: 812,loss: 0.032463253\n",
            "\n",
            "Global step: 813,loss: 0.03136173\n",
            "\n",
            "Global step: 814,loss: 0.040250912\n",
            "\n",
            "Global step: 815,loss: 0.036997378\n",
            "\n",
            "Global step: 816,loss: 0.03761653\n",
            "\n",
            "Global step: 817,loss: 0.031043425\n",
            "\n",
            "Global step: 818,loss: 0.03574\n",
            "\n",
            "Global step: 819,loss: 0.03316918\n",
            "\n",
            "Global step: 820,loss: 0.033528615\n",
            "\n",
            "Global step: 821,loss: 0.03836189\n",
            "\n",
            "Global step: 822,loss: 0.038295895\n",
            "\n",
            "Global step: 823,loss: 0.033041544\n",
            "\n",
            "Global step: 824,loss: 0.039269555\n",
            "\n",
            "Global step: 825,loss: 0.03259153\n",
            "\n",
            "Global step: 826,loss: 0.037665527\n",
            "\n",
            "Global step: 827,loss: 0.0329611\n",
            "\n",
            "Global step: 828,loss: 0.03703524\n",
            "\n",
            "Global step: 829,loss: 0.03912619\n",
            "\n",
            "Global step: 830,loss: 0.03940768\n",
            "\n",
            "Global step: 831,loss: 0.03452545\n",
            "\n",
            "Global step: 832,loss: 0.03577109\n",
            "\n",
            "Global step: 833,loss: 0.037103307\n",
            "\n",
            "Global step: 834,loss: 0.03754941\n",
            "\n",
            "Global step: 835,loss: 0.031398006\n",
            "\n",
            "Global step: 836,loss: 0.03190419\n",
            "\n",
            "Global step: 837,loss: 0.035327334\n",
            "\n",
            "Global step: 838,loss: 0.040196195\n",
            "\n",
            "Global step: 839,loss: 0.032321934\n",
            "\n",
            "Global step: 840,loss: 0.032485522\n",
            "\n",
            "Global step: 841,loss: 0.036549177\n",
            "\n",
            "Global step: 842,loss: 0.034425374\n",
            "\n",
            "Global step: 843,loss: 0.03515443\n",
            "\n",
            "Global step: 844,loss: 0.037858292\n",
            "\n",
            "Global step: 845,loss: 0.04015274\n",
            "\n",
            "Global step: 846,loss: 0.03600175\n",
            "\n",
            "Global step: 847,loss: 0.038795255\n",
            "\n",
            "Global step: 848,loss: 0.032325834\n",
            "\n",
            "Global step: 849,loss: 0.037558284\n",
            "\n",
            "Global step: 850,loss: 0.03297743\n",
            "\n",
            "Global step: 851,loss: 0.033818737\n",
            "\n",
            "Global step: 852,loss: 0.031369936\n",
            "\n",
            "Global step: 853,loss: 0.033461213\n",
            "\n",
            "Global step: 854,loss: 0.034468416\n",
            "\n",
            "Global step: 855,loss: 0.037633915\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 855,Val_Loss: 0.0346172190222301,  Val_acc: 0.9969161184210527 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:30:14.175062 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/50:\n",
            "Global step: 856,loss: 0.033253133\n",
            "\n",
            "Global step: 857,loss: 0.039500497\n",
            "\n",
            "Global step: 858,loss: 0.03482012\n",
            "\n",
            "Global step: 859,loss: 0.03409271\n",
            "\n",
            "Global step: 860,loss: 0.03846267\n",
            "\n",
            "Global step: 861,loss: 0.031971935\n",
            "\n",
            "Global step: 862,loss: 0.032501273\n",
            "\n",
            "Global step: 863,loss: 0.03863397\n",
            "\n",
            "Global step: 864,loss: 0.03690748\n",
            "\n",
            "Global step: 865,loss: 0.035660736\n",
            "\n",
            "Global step: 866,loss: 0.03247676\n",
            "\n",
            "Global step: 867,loss: 0.033322908\n",
            "\n",
            "Global step: 868,loss: 0.031613152\n",
            "\n",
            "Global step: 869,loss: 0.03262567\n",
            "\n",
            "Global step: 870,loss: 0.034687072\n",
            "\n",
            "Global step: 871,loss: 0.033170898\n",
            "\n",
            "Global step: 872,loss: 0.033653617\n",
            "\n",
            "Global step: 873,loss: 0.03822965\n",
            "\n",
            "Global step: 874,loss: 0.036898978\n",
            "\n",
            "Global step: 875,loss: 0.034673057\n",
            "\n",
            "Global step: 876,loss: 0.035054408\n",
            "\n",
            "Global step: 877,loss: 0.032471195\n",
            "\n",
            "Global step: 878,loss: 0.032204214\n",
            "\n",
            "Global step: 879,loss: 0.033596512\n",
            "\n",
            "Global step: 880,loss: 0.033966593\n",
            "\n",
            "Global step: 881,loss: 0.03439408\n",
            "\n",
            "Global step: 882,loss: 0.031821065\n",
            "\n",
            "Global step: 883,loss: 0.036020987\n",
            "\n",
            "Global step: 884,loss: 0.03785622\n",
            "\n",
            "Global step: 885,loss: 0.03670368\n",
            "\n",
            "Global step: 886,loss: 0.031401835\n",
            "\n",
            "Global step: 887,loss: 0.041943103\n",
            "\n",
            "Global step: 888,loss: 0.035102725\n",
            "\n",
            "Global step: 889,loss: 0.036659613\n",
            "\n",
            "Global step: 890,loss: 0.0372825\n",
            "\n",
            "Global step: 891,loss: 0.033141192\n",
            "\n",
            "Global step: 892,loss: 0.033904754\n",
            "\n",
            "Global step: 893,loss: 0.03314962\n",
            "\n",
            "Global step: 894,loss: 0.03378363\n",
            "\n",
            "Global step: 895,loss: 0.034952756\n",
            "\n",
            "Global step: 896,loss: 0.04048948\n",
            "\n",
            "Global step: 897,loss: 0.034486867\n",
            "\n",
            "Global step: 898,loss: 0.03383625\n",
            "\n",
            "Global step: 899,loss: 0.031994678\n",
            "\n",
            "Global step: 900,loss: 0.030986553\n",
            "\n",
            "Global step: 901,loss: 0.03164559\n",
            "\n",
            "Global step: 902,loss: 0.040351115\n",
            "\n",
            "Global step: 903,loss: 0.036705904\n",
            "\n",
            "Global step: 904,loss: 0.034752995\n",
            "\n",
            "Global step: 905,loss: 0.033791177\n",
            "\n",
            "Global step: 906,loss: 0.035341613\n",
            "\n",
            "Global step: 907,loss: 0.035050403\n",
            "\n",
            "Global step: 908,loss: 0.032432582\n",
            "\n",
            "Global step: 909,loss: 0.032990932\n",
            "\n",
            "Global step: 910,loss: 0.035669066\n",
            "\n",
            "Global step: 911,loss: 0.03417902\n",
            "\n",
            "Global step: 912,loss: 0.035849683\n",
            "\n",
            "Global step: 913,loss: 0.03514257\n",
            "\n",
            "Global step: 914,loss: 0.033669032\n",
            "\n",
            "Global step: 915,loss: 0.03618243\n",
            "\n",
            "Global step: 916,loss: 0.034662712\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 3.95567\n",
            "INFO:tensorflow:Recording summary at step 917.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:30:29.435606 140166520051456 supervisor.py:1099] global_step/sec: 3.95567\n",
            "I1110 22:30:29.435962 140167885084416 supervisor.py:1050] Recording summary at step 917.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 917,loss: 0.037547883\n",
            "\n",
            "Global step: 918,loss: 0.03281176\n",
            "\n",
            "Global step: 919,loss: 0.034881894\n",
            "\n",
            "Global step: 920,loss: 0.034081083\n",
            "\n",
            "Global step: 921,loss: 0.03847415\n",
            "\n",
            "Global step: 922,loss: 0.036015444\n",
            "\n",
            "Global step: 923,loss: 0.03240635\n",
            "\n",
            "Global step: 924,loss: 0.03149065\n",
            "\n",
            "Global step: 925,loss: 0.03369852\n",
            "\n",
            "Global step: 926,loss: 0.03157967\n",
            "\n",
            "Global step: 927,loss: 0.03370315\n",
            "\n",
            "Global step: 928,loss: 0.032475177\n",
            "\n",
            "Global step: 929,loss: 0.036774196\n",
            "\n",
            "Global step: 930,loss: 0.033005398\n",
            "\n",
            "Global step: 931,loss: 0.03047557\n",
            "\n",
            "Global step: 932,loss: 0.03471718\n",
            "\n",
            "Global step: 933,loss: 0.031772487\n",
            "\n",
            "Global step: 934,loss: 0.03082747\n",
            "\n",
            "Global step: 935,loss: 0.031267654\n",
            "\n",
            "Global step: 936,loss: 0.035928234\n",
            "\n",
            "Global step: 937,loss: 0.035764754\n",
            "\n",
            "Global step: 938,loss: 0.0316979\n",
            "\n",
            "Global step: 939,loss: 0.03158928\n",
            "\n",
            "Global step: 940,loss: 0.032112584\n",
            "\n",
            "Global step: 941,loss: 0.03420216\n",
            "\n",
            "Global step: 942,loss: 0.031754807\n",
            "\n",
            "Global step: 943,loss: 0.03220902\n",
            "\n",
            "Global step: 944,loss: 0.039633498\n",
            "\n",
            "Global step: 945,loss: 0.036387827\n",
            "\n",
            "Global step: 946,loss: 0.030772125\n",
            "\n",
            "Global step: 947,loss: 0.033455953\n",
            "\n",
            "Global step: 948,loss: 0.029547848\n",
            "\n",
            "Global step: 949,loss: 0.03135759\n",
            "\n",
            "Global step: 950,loss: 0.03206331\n",
            "\n",
            "Global step: 951,loss: 0.033534646\n",
            "\n",
            "Global step: 952,loss: 0.032381214\n",
            "\n",
            "Global step: 953,loss: 0.03259801\n",
            "\n",
            "Global step: 954,loss: 0.030419867\n",
            "\n",
            "Global step: 955,loss: 0.03533815\n",
            "\n",
            "Global step: 956,loss: 0.03736069\n",
            "\n",
            "Global step: 957,loss: 0.031026846\n",
            "\n",
            "Global step: 958,loss: 0.033324406\n",
            "\n",
            "Global step: 959,loss: 0.033556934\n",
            "\n",
            "Global step: 960,loss: 0.036900587\n",
            "\n",
            "Global step: 961,loss: 0.033357564\n",
            "\n",
            "Global step: 962,loss: 0.03275747\n",
            "\n",
            "Global step: 963,loss: 0.03214703\n",
            "\n",
            "Global step: 964,loss: 0.0316323\n",
            "\n",
            "Global step: 965,loss: 0.03326591\n",
            "\n",
            "Global step: 966,loss: 0.035049118\n",
            "\n",
            "Global step: 967,loss: 0.033520345\n",
            "\n",
            "Global step: 968,loss: 0.03269033\n",
            "\n",
            "Global step: 969,loss: 0.035743468\n",
            "\n",
            "Global step: 970,loss: 0.03290199\n",
            "\n",
            "Global step: 971,loss: 0.033040233\n",
            "\n",
            "Global step: 972,loss: 0.035875663\n",
            "\n",
            "Global step: 973,loss: 0.032617424\n",
            "\n",
            "Global step: 974,loss: 0.033783134\n",
            "\n",
            "Global step: 975,loss: 0.034969926\n",
            "\n",
            "Global step: 976,loss: 0.03279389\n",
            "\n",
            "Global step: 977,loss: 0.02995585\n",
            "\n",
            "Global step: 978,loss: 0.032112464\n",
            "\n",
            "Global step: 979,loss: 0.034401767\n",
            "\n",
            "Global step: 980,loss: 0.032161158\n",
            "\n",
            "Global step: 981,loss: 0.030646428\n",
            "\n",
            "Global step: 982,loss: 0.031418335\n",
            "\n",
            "Global step: 983,loss: 0.031864915\n",
            "\n",
            "Global step: 984,loss: 0.031182434\n",
            "\n",
            "Global step: 985,loss: 0.03138339\n",
            "\n",
            "Global step: 986,loss: 0.034250304\n",
            "\n",
            "Global step: 987,loss: 0.03234194\n",
            "\n",
            "Global step: 988,loss: 0.03226943\n",
            "\n",
            "Global step: 989,loss: 0.0313212\n",
            "\n",
            "Global step: 990,loss: 0.03171556\n",
            "\n",
            "Global step: 991,loss: 0.03285702\n",
            "\n",
            "Global step: 992,loss: 0.033825763\n",
            "\n",
            "Global step: 993,loss: 0.031164927\n",
            "\n",
            "Global step: 994,loss: 0.03243411\n",
            "\n",
            "Global step: 995,loss: 0.03306809\n",
            "\n",
            "Global step: 996,loss: 0.032688156\n",
            "\n",
            "Global step: 997,loss: 0.03220246\n",
            "\n",
            "Global step: 998,loss: 0.032369535\n",
            "\n",
            "Global step: 999,loss: 0.032970056\n",
            "\n",
            "Global step: 1000,loss: 0.029277802\n",
            "\n",
            "Global step: 1001,loss: 0.031507228\n",
            "\n",
            "Global step: 1002,loss: 0.036955535\n",
            "\n",
            "Global step: 1003,loss: 0.03319882\n",
            "\n",
            "Global step: 1004,loss: 0.03205835\n",
            "\n",
            "Global step: 1005,loss: 0.033112656\n",
            "\n",
            "Global step: 1006,loss: 0.033388097\n",
            "\n",
            "Global step: 1007,loss: 0.031179087\n",
            "\n",
            "Global step: 1008,loss: 0.030892164\n",
            "\n",
            "Global step: 1009,loss: 0.034157116\n",
            "\n",
            "Global step: 1010,loss: 0.030087784\n",
            "\n",
            "Global step: 1011,loss: 0.030968355\n",
            "\n",
            "Global step: 1012,loss: 0.03330095\n",
            "\n",
            "Global step: 1013,loss: 0.032042533\n",
            "\n",
            "Global step: 1014,loss: 0.031366855\n",
            "\n",
            "Global step: 1015,loss: 0.03207573\n",
            "\n",
            "Global step: 1016,loss: 0.03241548\n",
            "\n",
            "Global step: 1017,loss: 0.030538678\n",
            "\n",
            "Global step: 1018,loss: 0.031790778\n",
            "\n",
            "Global step: 1019,loss: 0.030690413\n",
            "\n",
            "Global step: 1020,loss: 0.030793374\n",
            "\n",
            "Global step: 1021,loss: 0.030619683\n",
            "\n",
            "Global step: 1022,loss: 0.03038864\n",
            "\n",
            "Global step: 1023,loss: 0.032928567\n",
            "\n",
            "Global step: 1024,loss: 0.03441571\n",
            "\n",
            "Global step: 1025,loss: 0.035438158\n",
            "\n",
            "Global step: 1026,loss: 0.036411718\n",
            "\n",
            "Global step: 1027,loss: 0.033808302\n",
            "\n",
            "Global step: 1028,loss: 0.032152575\n",
            "\n",
            "Global step: 1029,loss: 0.036872618\n",
            "\n",
            "Global step: 1030,loss: 0.034273647\n",
            "\n",
            "Global step: 1031,loss: 0.03150551\n",
            "\n",
            "Global step: 1032,loss: 0.030562554\n",
            "\n",
            "Global step: 1033,loss: 0.031810943\n",
            "\n",
            "Global step: 1034,loss: 0.03271704\n",
            "\n",
            "Global step: 1035,loss: 0.036405314\n",
            "\n",
            "Global step: 1036,loss: 0.03516297\n",
            "\n",
            "Global step: 1037,loss: 0.0330906\n",
            "\n",
            "Global step: 1038,loss: 0.030696927\n",
            "\n",
            "Global step: 1039,loss: 0.03076806\n",
            "\n",
            "Global step: 1040,loss: 0.030448454\n",
            "\n",
            "Global step: 1041,loss: 0.034433495\n",
            "\n",
            "Global step: 1042,loss: 0.03475096\n",
            "\n",
            "Global step: 1043,loss: 0.0343933\n",
            "\n",
            "Global step: 1044,loss: 0.032986086\n",
            "\n",
            "Global step: 1045,loss: 0.031439148\n",
            "\n",
            "Global step: 1046,loss: 0.038691454\n",
            "\n",
            "Global step: 1047,loss: 0.033706512\n",
            "\n",
            "Global step: 1048,loss: 0.032404307\n",
            "\n",
            "Global step: 1049,loss: 0.031034242\n",
            "\n",
            "Global step: 1050,loss: 0.036140285\n",
            "\n",
            "Global step: 1051,loss: 0.031591833\n",
            "\n",
            "Global step: 1052,loss: 0.03190178\n",
            "\n",
            "Global step: 1053,loss: 0.036236838\n",
            "\n",
            "Global step: 1054,loss: 0.031847976\n",
            "\n",
            "Global step: 1055,loss: 0.030300895\n",
            "\n",
            "Global step: 1056,loss: 0.029183708\n",
            "\n",
            "Global step: 1057,loss: 0.030573808\n",
            "\n",
            "Global step: 1058,loss: 0.03061294\n",
            "\n",
            "Global step: 1059,loss: 0.03142108\n",
            "\n",
            "Global step: 1060,loss: 0.03089452\n",
            "\n",
            "Global step: 1061,loss: 0.0295025\n",
            "\n",
            "Global step: 1062,loss: 0.030255545\n",
            "\n",
            "Global step: 1063,loss: 0.028854534\n",
            "\n",
            "Global step: 1064,loss: 0.03666755\n",
            "\n",
            "Global step: 1065,loss: 0.03053367\n",
            "\n",
            "Global step: 1066,loss: 0.031849485\n",
            "\n",
            "Global step: 1067,loss: 0.031014377\n",
            "\n",
            "Global step: 1068,loss: 0.033551127\n",
            "\n",
            "Global step: 1069,loss: 0.032136973\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1069,Val_Loss: 0.032555576707971726,  Val_acc: 0.9971217105263158 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:31:08.301869 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/50:\n",
            "Global step: 1070,loss: 0.028461404\n",
            "\n",
            "Global step: 1071,loss: 0.030044252\n",
            "\n",
            "Global step: 1072,loss: 0.034292527\n",
            "\n",
            "Global step: 1073,loss: 0.02898967\n",
            "\n",
            "Global step: 1074,loss: 0.029251384\n",
            "\n",
            "Global step: 1075,loss: 0.029860584\n",
            "\n",
            "Global step: 1076,loss: 0.033919312\n",
            "\n",
            "Global step: 1077,loss: 0.030924845\n",
            "\n",
            "Global step: 1078,loss: 0.0348665\n",
            "\n",
            "Global step: 1079,loss: 0.031438492\n",
            "\n",
            "Global step: 1080,loss: 0.031838257\n",
            "\n",
            "Global step: 1081,loss: 0.030456465\n",
            "\n",
            "Global step: 1082,loss: 0.03474468\n",
            "\n",
            "Global step: 1083,loss: 0.027873373\n",
            "\n",
            "Global step: 1084,loss: 0.032841027\n",
            "\n",
            "Global step: 1085,loss: 0.029761178\n",
            "\n",
            "Global step: 1086,loss: 0.03535109\n",
            "\n",
            "Global step: 1087,loss: 0.030509759\n",
            "\n",
            "Global step: 1088,loss: 0.03150154\n",
            "\n",
            "Global step: 1089,loss: 0.03270156\n",
            "\n",
            "Global step: 1090,loss: 0.0360697\n",
            "\n",
            "Global step: 1091,loss: 0.028636064\n",
            "\n",
            "Global step: 1092,loss: 0.02850806\n",
            "\n",
            "Global step: 1093,loss: 0.03150185\n",
            "\n",
            "Global step: 1094,loss: 0.036493964\n",
            "\n",
            "Global step: 1095,loss: 0.031446118\n",
            "\n",
            "Global step: 1096,loss: 0.03007102\n",
            "\n",
            "Global step: 1097,loss: 0.032527223\n",
            "\n",
            "Global step: 1098,loss: 0.03699978\n",
            "\n",
            "Global step: 1099,loss: 0.031767506\n",
            "\n",
            "Global step: 1100,loss: 0.030856645\n",
            "\n",
            "Global step: 1101,loss: 0.030318055\n",
            "\n",
            "Global step: 1102,loss: 0.029939236\n",
            "\n",
            "Global step: 1103,loss: 0.03118355\n",
            "\n",
            "Global step: 1104,loss: 0.033306774\n",
            "\n",
            "Global step: 1105,loss: 0.030043442\n",
            "\n",
            "Global step: 1106,loss: 0.031474758\n",
            "\n",
            "Global step: 1107,loss: 0.029800035\n",
            "\n",
            "Global step: 1108,loss: 0.034465536\n",
            "\n",
            "Global step: 1109,loss: 0.035036966\n",
            "\n",
            "Global step: 1110,loss: 0.031899802\n",
            "\n",
            "Global step: 1111,loss: 0.037894707\n",
            "\n",
            "Global step: 1112,loss: 0.034860812\n",
            "\n",
            "Global step: 1113,loss: 0.030187543\n",
            "\n",
            "Global step: 1114,loss: 0.028991127\n",
            "\n",
            "Global step: 1115,loss: 0.032583952\n",
            "\n",
            "Global step: 1116,loss: 0.031637996\n",
            "\n",
            "Global step: 1117,loss: 0.03134141\n",
            "\n",
            "Global step: 1118,loss: 0.030002015\n",
            "\n",
            "Global step: 1119,loss: 0.030347696\n",
            "\n",
            "Global step: 1120,loss: 0.031344038\n",
            "\n",
            "Global step: 1121,loss: 0.03069873\n",
            "\n",
            "Global step: 1122,loss: 0.034820504\n",
            "\n",
            "Global step: 1123,loss: 0.030613787\n",
            "\n",
            "Global step: 1124,loss: 0.031214008\n",
            "\n",
            "Global step: 1125,loss: 0.030713726\n",
            "\n",
            "Global step: 1126,loss: 0.02901429\n",
            "\n",
            "Global step: 1127,loss: 0.035092708\n",
            "\n",
            "Global step: 1128,loss: 0.032770276\n",
            "\n",
            "Global step: 1129,loss: 0.030372573\n",
            "\n",
            "Global step: 1130,loss: 0.032276355\n",
            "\n",
            "Global step: 1131,loss: 0.032144174\n",
            "\n",
            "Global step: 1132,loss: 0.031792108\n",
            "\n",
            "Global step: 1133,loss: 0.03095978\n",
            "\n",
            "Global step: 1134,loss: 0.03387312\n",
            "\n",
            "Global step: 1135,loss: 0.03257687\n",
            "\n",
            "Global step: 1136,loss: 0.03328199\n",
            "\n",
            "Global step: 1137,loss: 0.033083893\n",
            "\n",
            "Global step: 1138,loss: 0.029084034\n",
            "\n",
            "Global step: 1139,loss: 0.02994159\n",
            "\n",
            "Global step: 1140,loss: 0.03431561\n",
            "\n",
            "Global step: 1141,loss: 0.028752334\n",
            "\n",
            "Global step: 1142,loss: 0.0327849\n",
            "\n",
            "Global step: 1143,loss: 0.031506047\n",
            "\n",
            "Global step: 1144,loss: 0.030163493\n",
            "\n",
            "Global step: 1145,loss: 0.03382226\n",
            "\n",
            "Global step: 1146,loss: 0.03133726\n",
            "\n",
            "Global step: 1147,loss: 0.03159413\n",
            "\n",
            "Global step: 1148,loss: 0.029989522\n",
            "\n",
            "Global step: 1149,loss: 0.03083536\n",
            "\n",
            "Global step: 1150,loss: 0.03167666\n",
            "\n",
            "Global step: 1151,loss: 0.029657012\n",
            "\n",
            "Global step: 1152,loss: 0.027839202\n",
            "\n",
            "Global step: 1153,loss: 0.03477611\n",
            "\n",
            "Global step: 1154,loss: 0.028881945\n",
            "\n",
            "Global step: 1155,loss: 0.031701546\n",
            "\n",
            "Global step: 1156,loss: 0.03263499\n",
            "\n",
            "Global step: 1157,loss: 0.027732408\n",
            "\n",
            "Global step: 1158,loss: 0.030373137\n",
            "\n",
            "Global step: 1159,loss: 0.029654672\n",
            "\n",
            "Global step: 1160,loss: 0.030057538\n",
            "\n",
            "Global step: 1161,loss: 0.032597195\n",
            "\n",
            "Global step: 1162,loss: 0.02916318\n",
            "\n",
            "Global step: 1163,loss: 0.031575263\n",
            "\n",
            "Global step: 1164,loss: 0.029724509\n",
            "\n",
            "Global step: 1165,loss: 0.030684248\n",
            "\n",
            "Global step: 1166,loss: 0.030239336\n",
            "\n",
            "Global step: 1167,loss: 0.035480034\n",
            "\n",
            "Global step: 1168,loss: 0.028866347\n",
            "\n",
            "Global step: 1169,loss: 0.027981814\n",
            "\n",
            "Global step: 1170,loss: 0.029602574\n",
            "\n",
            "Global step: 1171,loss: 0.03290555\n",
            "\n",
            "Global step: 1172,loss: 0.030873552\n",
            "\n",
            "Global step: 1173,loss: 0.02997899\n",
            "\n",
            "Global step: 1174,loss: 0.028205477\n",
            "\n",
            "Global step: 1175,loss: 0.035948314\n",
            "\n",
            "Global step: 1176,loss: 0.031542983\n",
            "\n",
            "Global step: 1177,loss: 0.034525827\n",
            "\n",
            "Global step: 1178,loss: 0.032878634\n",
            "\n",
            "Global step: 1179,loss: 0.030117974\n",
            "\n",
            "Global step: 1180,loss: 0.028233645\n",
            "\n",
            "Global step: 1181,loss: 0.033376124\n",
            "\n",
            "Global step: 1182,loss: 0.0303879\n",
            "\n",
            "Global step: 1183,loss: 0.029269027\n",
            "\n",
            "Global step: 1184,loss: 0.028463112\n",
            "\n",
            "Global step: 1185,loss: 0.030074479\n",
            "\n",
            "Global step: 1186,loss: 0.02875109\n",
            "\n",
            "Global step: 1187,loss: 0.030839555\n",
            "\n",
            "Global step: 1188,loss: 0.029204117\n",
            "\n",
            "Global step: 1189,loss: 0.032193366\n",
            "\n",
            "Global step: 1190,loss: 0.034240052\n",
            "\n",
            "Global step: 1191,loss: 0.033363987\n",
            "\n",
            "Global step: 1192,loss: 0.03437239\n",
            "\n",
            "Global step: 1193,loss: 0.029498633\n",
            "\n",
            "Global step: 1194,loss: 0.02918058\n",
            "\n",
            "Global step: 1195,loss: 0.029286362\n",
            "\n",
            "Global step: 1196,loss: 0.032892253\n",
            "\n",
            "Global step: 1197,loss: 0.029885441\n",
            "\n",
            "Global step: 1198,loss: 0.031247074\n",
            "\n",
            "Global step: 1199,loss: 0.028178837\n",
            "\n",
            "Global step: 1200,loss: 0.032320958\n",
            "\n",
            "Global step: 1201,loss: 0.02970014\n",
            "\n",
            "Global step: 1202,loss: 0.0274393\n",
            "\n",
            "Global step: 1203,loss: 0.027116774\n",
            "\n",
            "Global step: 1204,loss: 0.028611524\n",
            "\n",
            "Global step: 1205,loss: 0.03938061\n",
            "\n",
            "Global step: 1206,loss: 0.028582979\n",
            "\n",
            "Global step: 1207,loss: 0.030197134\n",
            "\n",
            "Global step: 1208,loss: 0.029826263\n",
            "\n",
            "Global step: 1209,loss: 0.03766318\n",
            "\n",
            "Global step: 1210,loss: 0.031841\n",
            "\n",
            "Global step: 1211,loss: 0.02918882\n",
            "\n",
            "Global step: 1212,loss: 0.028324546\n",
            "\n",
            "Global step: 1213,loss: 0.029276852\n",
            "\n",
            "Global step: 1214,loss: 0.02890858\n",
            "\n",
            "Global step: 1215,loss: 0.028647745\n",
            "\n",
            "Global step: 1216,loss: 0.030271767\n",
            "\n",
            "Global step: 1217,loss: 0.029102176\n",
            "\n",
            "Global step: 1218,loss: 0.03479455\n",
            "\n",
            "Global step: 1219,loss: 0.031090384\n",
            "\n",
            "Global step: 1220,loss: 0.03253329\n",
            "\n",
            "Global step: 1221,loss: 0.029712243\n",
            "\n",
            "Global step: 1222,loss: 0.028967973\n",
            "\n",
            "Global step: 1223,loss: 0.033059265\n",
            "\n",
            "Global step: 1224,loss: 0.02852454\n",
            "\n",
            "Global step: 1225,loss: 0.029070545\n",
            "\n",
            "Global step: 1226,loss: 0.029587906\n",
            "\n",
            "Global step: 1227,loss: 0.032840714\n",
            "\n",
            "Global step: 1228,loss: 0.03106567\n",
            "\n",
            "Global step: 1229,loss: 0.029563945\n",
            "\n",
            "Global step: 1230,loss: 0.03110449\n",
            "\n",
            "Global step: 1231,loss: 0.030499801\n",
            "\n",
            "Global step: 1232,loss: 0.03259881\n",
            "\n",
            "Global step: 1233,loss: 0.030932503\n",
            "\n",
            "Global step: 1234,loss: 0.027601002\n",
            "\n",
            "Global step: 1235,loss: 0.028462196\n",
            "\n",
            "Global step: 1236,loss: 0.030634955\n",
            "\n",
            "Global step: 1237,loss: 0.030899297\n",
            "\n",
            "Global step: 1238,loss: 0.028820815\n",
            "\n",
            "Global step: 1239,loss: 0.029753165\n",
            "\n",
            "Global step: 1240,loss: 0.029435985\n",
            "\n",
            "Global step: 1241,loss: 0.03151449\n",
            "\n",
            "Global step: 1242,loss: 0.029313166\n",
            "\n",
            "Global step: 1243,loss: 0.03090337\n",
            "\n",
            "Global step: 1244,loss: 0.029631421\n",
            "\n",
            "Global step: 1245,loss: 0.029344454\n",
            "\n",
            "Global step: 1246,loss: 0.029327849\n",
            "\n",
            "Global step: 1247,loss: 0.027756987\n",
            "\n",
            "Global step: 1248,loss: 0.027561\n",
            "\n",
            "Global step: 1249,loss: 0.03000681\n",
            "\n",
            "Global step: 1250,loss: 0.028759742\n",
            "\n",
            "Global step: 1251,loss: 0.029525232\n",
            "\n",
            "Global step: 1252,loss: 0.027520437\n",
            "\n",
            "Global step: 1253,loss: 0.03192167\n",
            "\n",
            "Global step: 1254,loss: 0.030323477\n",
            "\n",
            "Global step: 1255,loss: 0.028307782\n",
            "\n",
            "Global step: 1256,loss: 0.040878925\n",
            "\n",
            "Global step: 1257,loss: 0.029094893\n",
            "\n",
            "Global step: 1258,loss: 0.031237897\n",
            "\n",
            "Global step: 1259,loss: 0.030568942\n",
            "\n",
            "Global step: 1260,loss: 0.028976927\n",
            "\n",
            "Global step: 1261,loss: 0.029735323\n",
            "\n",
            "Global step: 1262,loss: 0.03154136\n",
            "\n",
            "Global step: 1263,loss: 0.028124573\n",
            "\n",
            "Global step: 1264,loss: 0.032593846\n",
            "\n",
            "Global step: 1265,loss: 0.028626423\n",
            "\n",
            "Global step: 1266,loss: 0.029912515\n",
            "\n",
            "Global step: 1267,loss: 0.034455504\n",
            "\n",
            "Global step: 1268,loss: 0.030835157\n",
            "\n",
            "Global step: 1269,loss: 0.028222162\n",
            "\n",
            "Global step: 1270,loss: 0.032088827\n",
            "\n",
            "Global step: 1271,loss: 0.028607314\n",
            "\n",
            "Global step: 1272,loss: 0.028753629\n",
            "\n",
            "Global step: 1273,loss: 0.02926473\n",
            "\n",
            "Global step: 1274,loss: 0.02826013\n",
            "\n",
            "Global step: 1275,loss: 0.028746963\n",
            "\n",
            "Global step: 1276,loss: 0.02802648\n",
            "\n",
            "Global step: 1277,loss: 0.029533327\n",
            "\n",
            "Global step: 1278,loss: 0.030567173\n",
            "\n",
            "Global step: 1279,loss: 0.033489704\n",
            "\n",
            "Global step: 1280,loss: 0.028951516\n",
            "\n",
            "Global step: 1281,loss: 0.028761137\n",
            "\n",
            "Global step: 1282,loss: 0.029533448\n",
            "\n",
            "Global step: 1283,loss: 0.031147413\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1283,Val_Loss: 0.030712067786800235,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:32:02.118062 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:32:02.689256 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 7/50:\n",
            "Global step: 1284,loss: 0.0295955\n",
            "\n",
            "Global step: 1285,loss: 0.031772655\n",
            "\n",
            "Global step: 1286,loss: 0.03017366\n",
            "\n",
            "Global step: 1287,loss: 0.029204838\n",
            "\n",
            "Global step: 1288,loss: 0.029231068\n",
            "\n",
            "Global step: 1289,loss: 0.029778583\n",
            "\n",
            "Global step: 1290,loss: 0.028451122\n",
            "\n",
            "Global step: 1291,loss: 0.03404185\n",
            "\n",
            "Global step: 1292,loss: 0.031107774\n",
            "\n",
            "Global step: 1293,loss: 0.028549828\n",
            "\n",
            "Global step: 1294,loss: 0.03302275\n",
            "\n",
            "Global step: 1295,loss: 0.028625704\n",
            "\n",
            "Global step: 1296,loss: 0.029864091\n",
            "\n",
            "Global step: 1297,loss: 0.029954266\n",
            "\n",
            "Global step: 1298,loss: 0.033182487\n",
            "\n",
            "Global step: 1299,loss: 0.03187481\n",
            "\n",
            "Global step: 1300,loss: 0.027291114\n",
            "\n",
            "Global step: 1301,loss: 0.03187257\n",
            "\n",
            "Global step: 1302,loss: 0.03235979\n",
            "\n",
            "Global step: 1303,loss: 0.031589083\n",
            "\n",
            "Global step: 1304,loss: 0.030158568\n",
            "\n",
            "Global step: 1305,loss: 0.030462403\n",
            "\n",
            "Global step: 1306,loss: 0.029336093\n",
            "\n",
            "Global step: 1307,loss: 0.029620659\n",
            "\n",
            "Global step: 1308,loss: 0.026739715\n",
            "\n",
            "Global step: 1309,loss: 0.029495519\n",
            "\n",
            "Global step: 1310,loss: 0.03002832\n",
            "\n",
            "Global step: 1311,loss: 0.028015554\n",
            "\n",
            "Global step: 1312,loss: 0.030125933\n",
            "\n",
            "Global step: 1313,loss: 0.027507192\n",
            "\n",
            "Global step: 1314,loss: 0.02706974\n",
            "\n",
            "Global step: 1315,loss: 0.031120067\n",
            "\n",
            "Global step: 1316,loss: 0.028783038\n",
            "\n",
            "Global step: 1317,loss: 0.026893366\n",
            "\n",
            "Global step: 1318,loss: 0.02794839\n",
            "\n",
            "Global step: 1319,loss: 0.029695071\n",
            "\n",
            "Global step: 1320,loss: 0.02777894\n",
            "\n",
            "Global step: 1321,loss: 0.03029312\n",
            "\n",
            "Global step: 1322,loss: 0.026557127\n",
            "\n",
            "Global step: 1323,loss: 0.026208961\n",
            "\n",
            "Global step: 1324,loss: 0.0284651\n",
            "\n",
            "Global step: 1325,loss: 0.027756257\n",
            "\n",
            "Global step: 1326,loss: 0.029760063\n",
            "\n",
            "Global step: 1327,loss: 0.033979937\n",
            "\n",
            "Global step: 1328,loss: 0.030443259\n",
            "\n",
            "Global step: 1329,loss: 0.031287648\n",
            "\n",
            "Global step: 1330,loss: 0.030710056\n",
            "\n",
            "Global step: 1331,loss: 0.027147966\n",
            "\n",
            "Global step: 1332,loss: 0.030641478\n",
            "\n",
            "Global step: 1333,loss: 0.02932214\n",
            "\n",
            "Global step: 1334,loss: 0.026824404\n",
            "\n",
            "Global step: 1335,loss: 0.02882224\n",
            "\n",
            "Global step: 1336,loss: 0.027097408\n",
            "\n",
            "Global step: 1337,loss: 0.028979767\n",
            "\n",
            "Global step: 1338,loss: 0.030613098\n",
            "\n",
            "Global step: 1339,loss: 0.02720431\n",
            "\n",
            "Global step: 1340,loss: 0.02845332\n",
            "\n",
            "Global step: 1341,loss: 0.02711569\n",
            "\n",
            "Global step: 1342,loss: 0.02966986\n",
            "\n",
            "Global step: 1343,loss: 0.028110623\n",
            "\n",
            "Global step: 1344,loss: 0.027476043\n",
            "\n",
            "Global step: 1345,loss: 0.03242176\n",
            "\n",
            "Global step: 1346,loss: 0.027295664\n",
            "\n",
            "Global step: 1347,loss: 0.027160112\n",
            "\n",
            "Global step: 1348,loss: 0.026936295\n",
            "\n",
            "Global step: 1349,loss: 0.031044673\n",
            "\n",
            "Global step: 1350,loss: 0.030962054\n",
            "\n",
            "Global step: 1351,loss: 0.030617964\n",
            "\n",
            "Global step: 1352,loss: 0.03233739\n",
            "\n",
            "Global step: 1353,loss: 0.029702302\n",
            "\n",
            "Global step: 1354,loss: 0.02960069\n",
            "\n",
            "Global step: 1355,loss: 0.027539564\n",
            "\n",
            "Global step: 1356,loss: 0.030189974\n",
            "\n",
            "Global step: 1357,loss: 0.030008996\n",
            "\n",
            "Global step: 1358,loss: 0.03010896\n",
            "\n",
            "Global step: 1359,loss: 0.028910648\n",
            "\n",
            "Global step: 1360,loss: 0.0272551\n",
            "\n",
            "Global step: 1361,loss: 0.02690983\n",
            "\n",
            "Global step: 1362,loss: 0.02699898\n",
            "\n",
            "Global step: 1363,loss: 0.028690124\n",
            "\n",
            "Global step: 1364,loss: 0.027740085\n",
            "\n",
            "Global step: 1365,loss: 0.029312396\n",
            "\n",
            "Global step: 1366,loss: 0.0272856\n",
            "\n",
            "Global step: 1367,loss: 0.026522607\n",
            "\n",
            "Global step: 1368,loss: 0.028124725\n",
            "\n",
            "Global step: 1369,loss: 0.031060139\n",
            "\n",
            "Global step: 1370,loss: 0.030826187\n",
            "\n",
            "Global step: 1371,loss: 0.026146028\n",
            "\n",
            "Global step: 1372,loss: 0.0280326\n",
            "\n",
            "Global step: 1373,loss: 0.031295817\n",
            "\n",
            "Global step: 1374,loss: 0.029505715\n",
            "\n",
            "Global step: 1375,loss: 0.031081662\n",
            "\n",
            "Global step: 1376,loss: 0.0313095\n",
            "\n",
            "Global step: 1377,loss: 0.028079037\n",
            "\n",
            "Global step: 1378,loss: 0.028230727\n",
            "\n",
            "Global step: 1379,loss: 0.025944026\n",
            "\n",
            "Global step: 1380,loss: 0.028004318\n",
            "\n",
            "Global step: 1381,loss: 0.028851287\n",
            "\n",
            "Global step: 1382,loss: 0.030368324\n",
            "\n",
            "Global step: 1383,loss: 0.027281264\n",
            "\n",
            "Global step: 1384,loss: 0.02668755\n",
            "\n",
            "Global step: 1385,loss: 0.029247345\n",
            "\n",
            "Global step: 1386,loss: 0.028686285\n",
            "\n",
            "Global step: 1387,loss: 0.035611656\n",
            "\n",
            "Global step: 1388,loss: 0.027292913\n",
            "\n",
            "Global step: 1389,loss: 0.028076833\n",
            "\n",
            "Global step: 1390,loss: 0.027825767\n",
            "\n",
            "Global step: 1391,loss: 0.028451221\n",
            "\n",
            "Global step: 1392,loss: 0.027071668\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1393.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:32:29.318755 140167885084416 supervisor.py:1050] Recording summary at step 1393.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.96933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:32:29.355116 140166520051456 supervisor.py:1099] global_step/sec: 3.96933\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1393,loss: 0.027225269\n",
            "\n",
            "Global step: 1394,loss: 0.027788276\n",
            "\n",
            "Global step: 1395,loss: 0.025973596\n",
            "\n",
            "Global step: 1396,loss: 0.026813928\n",
            "\n",
            "Global step: 1397,loss: 0.028189233\n",
            "\n",
            "Global step: 1398,loss: 0.029746193\n",
            "\n",
            "Global step: 1399,loss: 0.03281161\n",
            "\n",
            "Global step: 1400,loss: 0.029271858\n",
            "\n",
            "Global step: 1401,loss: 0.029328033\n",
            "\n",
            "Global step: 1402,loss: 0.02584469\n",
            "\n",
            "Global step: 1403,loss: 0.02929741\n",
            "\n",
            "Global step: 1404,loss: 0.029531974\n",
            "\n",
            "Global step: 1405,loss: 0.033834305\n",
            "\n",
            "Global step: 1406,loss: 0.027296789\n",
            "\n",
            "Global step: 1407,loss: 0.02854285\n",
            "\n",
            "Global step: 1408,loss: 0.02863283\n",
            "\n",
            "Global step: 1409,loss: 0.027903516\n",
            "\n",
            "Global step: 1410,loss: 0.026268443\n",
            "\n",
            "Global step: 1411,loss: 0.027695104\n",
            "\n",
            "Global step: 1412,loss: 0.03028547\n",
            "\n",
            "Global step: 1413,loss: 0.026324034\n",
            "\n",
            "Global step: 1414,loss: 0.02599855\n",
            "\n",
            "Global step: 1415,loss: 0.026873508\n",
            "\n",
            "Global step: 1416,loss: 0.026359512\n",
            "\n",
            "Global step: 1417,loss: 0.02779496\n",
            "\n",
            "Global step: 1418,loss: 0.0273994\n",
            "\n",
            "Global step: 1419,loss: 0.030659292\n",
            "\n",
            "Global step: 1420,loss: 0.028270215\n",
            "\n",
            "Global step: 1421,loss: 0.027733162\n",
            "\n",
            "Global step: 1422,loss: 0.027002217\n",
            "\n",
            "Global step: 1423,loss: 0.026828833\n",
            "\n",
            "Global step: 1424,loss: 0.027135072\n",
            "\n",
            "Global step: 1425,loss: 0.026257828\n",
            "\n",
            "Global step: 1426,loss: 0.028841954\n",
            "\n",
            "Global step: 1427,loss: 0.027494414\n",
            "\n",
            "Global step: 1428,loss: 0.02728337\n",
            "\n",
            "Global step: 1429,loss: 0.026271202\n",
            "\n",
            "Global step: 1430,loss: 0.027991543\n",
            "\n",
            "Global step: 1431,loss: 0.028716892\n",
            "\n",
            "Global step: 1432,loss: 0.026721817\n",
            "\n",
            "Global step: 1433,loss: 0.026098253\n",
            "\n",
            "Global step: 1434,loss: 0.026580755\n",
            "\n",
            "Global step: 1435,loss: 0.03067194\n",
            "\n",
            "Global step: 1436,loss: 0.02558168\n",
            "\n",
            "Global step: 1437,loss: 0.02839608\n",
            "\n",
            "Global step: 1438,loss: 0.030075159\n",
            "\n",
            "Global step: 1439,loss: 0.029469652\n",
            "\n",
            "Global step: 1440,loss: 0.026640799\n",
            "\n",
            "Global step: 1441,loss: 0.03211326\n",
            "\n",
            "Global step: 1442,loss: 0.026077062\n",
            "\n",
            "Global step: 1443,loss: 0.028043162\n",
            "\n",
            "Global step: 1444,loss: 0.028278243\n",
            "\n",
            "Global step: 1445,loss: 0.028141588\n",
            "\n",
            "Global step: 1446,loss: 0.027198607\n",
            "\n",
            "Global step: 1447,loss: 0.025995454\n",
            "\n",
            "Global step: 1448,loss: 0.028660703\n",
            "\n",
            "Global step: 1449,loss: 0.030482017\n",
            "\n",
            "Global step: 1450,loss: 0.026397334\n",
            "\n",
            "Global step: 1451,loss: 0.02869495\n",
            "\n",
            "Global step: 1452,loss: 0.027614633\n",
            "\n",
            "Global step: 1453,loss: 0.028171025\n",
            "\n",
            "Global step: 1454,loss: 0.027791973\n",
            "\n",
            "Global step: 1455,loss: 0.028114982\n",
            "\n",
            "Global step: 1456,loss: 0.028521966\n",
            "\n",
            "Global step: 1457,loss: 0.027536616\n",
            "\n",
            "Global step: 1458,loss: 0.027406223\n",
            "\n",
            "Global step: 1459,loss: 0.032608654\n",
            "\n",
            "Global step: 1460,loss: 0.02881522\n",
            "\n",
            "Global step: 1461,loss: 0.026761888\n",
            "\n",
            "Global step: 1462,loss: 0.027134094\n",
            "\n",
            "Global step: 1463,loss: 0.028291477\n",
            "\n",
            "Global step: 1464,loss: 0.0268507\n",
            "\n",
            "Global step: 1465,loss: 0.030249938\n",
            "\n",
            "Global step: 1466,loss: 0.029649582\n",
            "\n",
            "Global step: 1467,loss: 0.028689548\n",
            "\n",
            "Global step: 1468,loss: 0.02678979\n",
            "\n",
            "Global step: 1469,loss: 0.030718133\n",
            "\n",
            "Global step: 1470,loss: 0.028902896\n",
            "\n",
            "Global step: 1471,loss: 0.027818518\n",
            "\n",
            "Global step: 1472,loss: 0.027017321\n",
            "\n",
            "Global step: 1473,loss: 0.027172945\n",
            "\n",
            "Global step: 1474,loss: 0.026940817\n",
            "\n",
            "Global step: 1475,loss: 0.027324397\n",
            "\n",
            "Global step: 1476,loss: 0.02939114\n",
            "\n",
            "Global step: 1477,loss: 0.026340062\n",
            "\n",
            "Global step: 1478,loss: 0.028099619\n",
            "\n",
            "Global step: 1479,loss: 0.025115712\n",
            "\n",
            "Global step: 1480,loss: 0.026595052\n",
            "\n",
            "Global step: 1481,loss: 0.025483755\n",
            "\n",
            "Global step: 1482,loss: 0.026248958\n",
            "\n",
            "Global step: 1483,loss: 0.026185129\n",
            "\n",
            "Global step: 1484,loss: 0.025935197\n",
            "\n",
            "Global step: 1485,loss: 0.03277452\n",
            "\n",
            "Global step: 1486,loss: 0.027353566\n",
            "\n",
            "Global step: 1487,loss: 0.027004685\n",
            "\n",
            "Global step: 1488,loss: 0.026054226\n",
            "\n",
            "Global step: 1489,loss: 0.025980435\n",
            "\n",
            "Global step: 1490,loss: 0.025619818\n",
            "\n",
            "Global step: 1491,loss: 0.027943747\n",
            "\n",
            "Global step: 1492,loss: 0.026936727\n",
            "\n",
            "Global step: 1493,loss: 0.027883042\n",
            "\n",
            "Global step: 1494,loss: 0.025619233\n",
            "\n",
            "Global step: 1495,loss: 0.026620902\n",
            "\n",
            "Global step: 1496,loss: 0.026609695\n",
            "\n",
            "Global step: 1497,loss: 0.025744773\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1497,Val_Loss: 0.029188842659718113,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:32:56.903360 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 8/50:\n",
            "Global step: 1498,loss: 0.027929436\n",
            "\n",
            "Global step: 1499,loss: 0.026933694\n",
            "\n",
            "Global step: 1500,loss: 0.028254261\n",
            "\n",
            "Global step: 1501,loss: 0.026899733\n",
            "\n",
            "Global step: 1502,loss: 0.02790874\n",
            "\n",
            "Global step: 1503,loss: 0.025976222\n",
            "\n",
            "Global step: 1504,loss: 0.026582737\n",
            "\n",
            "Global step: 1505,loss: 0.028666195\n",
            "\n",
            "Global step: 1506,loss: 0.027105201\n",
            "\n",
            "Global step: 1507,loss: 0.025587562\n",
            "\n",
            "Global step: 1508,loss: 0.026790945\n",
            "\n",
            "Global step: 1509,loss: 0.026326371\n",
            "\n",
            "Global step: 1510,loss: 0.025439108\n",
            "\n",
            "Global step: 1511,loss: 0.026945677\n",
            "\n",
            "Global step: 1512,loss: 0.026264472\n",
            "\n",
            "Global step: 1513,loss: 0.030442838\n",
            "\n",
            "Global step: 1514,loss: 0.026338175\n",
            "\n",
            "Global step: 1515,loss: 0.023838142\n",
            "\n",
            "Global step: 1516,loss: 0.026975833\n",
            "\n",
            "Global step: 1517,loss: 0.028755533\n",
            "\n",
            "Global step: 1518,loss: 0.026008021\n",
            "\n",
            "Global step: 1519,loss: 0.026085127\n",
            "\n",
            "Global step: 1520,loss: 0.025188131\n",
            "\n",
            "Global step: 1521,loss: 0.026090708\n",
            "\n",
            "Global step: 1522,loss: 0.02539791\n",
            "\n",
            "Global step: 1523,loss: 0.026738254\n",
            "\n",
            "Global step: 1524,loss: 0.027463857\n",
            "\n",
            "Global step: 1525,loss: 0.026758898\n",
            "\n",
            "Global step: 1526,loss: 0.027785458\n",
            "\n",
            "Global step: 1527,loss: 0.02794029\n",
            "\n",
            "Global step: 1528,loss: 0.02617529\n",
            "\n",
            "Global step: 1529,loss: 0.024879772\n",
            "\n",
            "Global step: 1530,loss: 0.025472462\n",
            "\n",
            "Global step: 1531,loss: 0.025238475\n",
            "\n",
            "Global step: 1532,loss: 0.024721691\n",
            "\n",
            "Global step: 1533,loss: 0.025794713\n",
            "\n",
            "Global step: 1534,loss: 0.02621496\n",
            "\n",
            "Global step: 1535,loss: 0.026578508\n",
            "\n",
            "Global step: 1536,loss: 0.027555738\n",
            "\n",
            "Global step: 1537,loss: 0.023602167\n",
            "\n",
            "Global step: 1538,loss: 0.02726956\n",
            "\n",
            "Global step: 1539,loss: 0.026416339\n",
            "\n",
            "Global step: 1540,loss: 0.02537755\n",
            "\n",
            "Global step: 1541,loss: 0.025854256\n",
            "\n",
            "Global step: 1542,loss: 0.025194079\n",
            "\n",
            "Global step: 1543,loss: 0.026454775\n",
            "\n",
            "Global step: 1544,loss: 0.02584186\n",
            "\n",
            "Global step: 1545,loss: 0.029759053\n",
            "\n",
            "Global step: 1546,loss: 0.02523242\n",
            "\n",
            "Global step: 1547,loss: 0.025423035\n",
            "\n",
            "Global step: 1548,loss: 0.025801688\n",
            "\n",
            "Global step: 1549,loss: 0.028383661\n",
            "\n",
            "Global step: 1550,loss: 0.0260829\n",
            "\n",
            "Global step: 1551,loss: 0.029218175\n",
            "\n",
            "Global step: 1552,loss: 0.02583159\n",
            "\n",
            "Global step: 1553,loss: 0.026678413\n",
            "\n",
            "Global step: 1554,loss: 0.026047913\n",
            "\n",
            "Global step: 1555,loss: 0.026429083\n",
            "\n",
            "Global step: 1556,loss: 0.025865106\n",
            "\n",
            "Global step: 1557,loss: 0.027991628\n",
            "\n",
            "Global step: 1558,loss: 0.025300318\n",
            "\n",
            "Global step: 1559,loss: 0.026472954\n",
            "\n",
            "Global step: 1560,loss: 0.02500151\n",
            "\n",
            "Global step: 1561,loss: 0.025157157\n",
            "\n",
            "Global step: 1562,loss: 0.026702618\n",
            "\n",
            "Global step: 1563,loss: 0.024826398\n",
            "\n",
            "Global step: 1564,loss: 0.025113244\n",
            "\n",
            "Global step: 1565,loss: 0.026047125\n",
            "\n",
            "Global step: 1566,loss: 0.025666405\n",
            "\n",
            "Global step: 1567,loss: 0.0272155\n",
            "\n",
            "Global step: 1568,loss: 0.025048744\n",
            "\n",
            "Global step: 1569,loss: 0.028013257\n",
            "\n",
            "Global step: 1570,loss: 0.030680154\n",
            "\n",
            "Global step: 1571,loss: 0.024626749\n",
            "\n",
            "Global step: 1572,loss: 0.0268441\n",
            "\n",
            "Global step: 1573,loss: 0.026441256\n",
            "\n",
            "Global step: 1574,loss: 0.026116831\n",
            "\n",
            "Global step: 1575,loss: 0.025765456\n",
            "\n",
            "Global step: 1576,loss: 0.024584074\n",
            "\n",
            "Global step: 1577,loss: 0.026000531\n",
            "\n",
            "Global step: 1578,loss: 0.026945263\n",
            "\n",
            "Global step: 1579,loss: 0.025964221\n",
            "\n",
            "Global step: 1580,loss: 0.027663562\n",
            "\n",
            "Global step: 1581,loss: 0.033585757\n",
            "\n",
            "Global step: 1582,loss: 0.029515777\n",
            "\n",
            "Global step: 1583,loss: 0.025891718\n",
            "\n",
            "Global step: 1584,loss: 0.025642486\n",
            "\n",
            "Global step: 1585,loss: 0.025717001\n",
            "\n",
            "Global step: 1586,loss: 0.031210048\n",
            "\n",
            "Global step: 1587,loss: 0.025178313\n",
            "\n",
            "Global step: 1588,loss: 0.025986554\n",
            "\n",
            "Global step: 1589,loss: 0.025870863\n",
            "\n",
            "Global step: 1590,loss: 0.025908867\n",
            "\n",
            "Global step: 1591,loss: 0.025692003\n",
            "\n",
            "Global step: 1592,loss: 0.024777455\n",
            "\n",
            "Global step: 1593,loss: 0.026229143\n",
            "\n",
            "Global step: 1594,loss: 0.027565468\n",
            "\n",
            "Global step: 1595,loss: 0.025639948\n",
            "\n",
            "Global step: 1596,loss: 0.02849984\n",
            "\n",
            "Global step: 1597,loss: 0.029786348\n",
            "\n",
            "Global step: 1598,loss: 0.026661875\n",
            "\n",
            "Global step: 1599,loss: 0.026163472\n",
            "\n",
            "Global step: 1600,loss: 0.027031705\n",
            "\n",
            "Global step: 1601,loss: 0.025379486\n",
            "\n",
            "Global step: 1602,loss: 0.026563024\n",
            "\n",
            "Global step: 1603,loss: 0.026617117\n",
            "\n",
            "Global step: 1604,loss: 0.02831469\n",
            "\n",
            "Global step: 1605,loss: 0.026223619\n",
            "\n",
            "Global step: 1606,loss: 0.025764007\n",
            "\n",
            "Global step: 1607,loss: 0.02484517\n",
            "\n",
            "Global step: 1608,loss: 0.029424759\n",
            "\n",
            "Global step: 1609,loss: 0.028051825\n",
            "\n",
            "Global step: 1610,loss: 0.02685641\n",
            "\n",
            "Global step: 1611,loss: 0.027437035\n",
            "\n",
            "Global step: 1612,loss: 0.023976037\n",
            "\n",
            "Global step: 1613,loss: 0.027830433\n",
            "\n",
            "Global step: 1614,loss: 0.0247464\n",
            "\n",
            "Global step: 1615,loss: 0.025085405\n",
            "\n",
            "Global step: 1616,loss: 0.027065787\n",
            "\n",
            "Global step: 1617,loss: 0.027287658\n",
            "\n",
            "Global step: 1618,loss: 0.025902493\n",
            "\n",
            "Global step: 1619,loss: 0.026931005\n",
            "\n",
            "Global step: 1620,loss: 0.025151787\n",
            "\n",
            "Global step: 1621,loss: 0.025003977\n",
            "\n",
            "Global step: 1622,loss: 0.025425712\n",
            "\n",
            "Global step: 1623,loss: 0.025238456\n",
            "\n",
            "Global step: 1624,loss: 0.02428689\n",
            "\n",
            "Global step: 1625,loss: 0.023802245\n",
            "\n",
            "Global step: 1626,loss: 0.027038403\n",
            "\n",
            "Global step: 1627,loss: 0.02559302\n",
            "\n",
            "Global step: 1628,loss: 0.030734044\n",
            "\n",
            "Global step: 1629,loss: 0.026927516\n",
            "\n",
            "Global step: 1630,loss: 0.026097622\n",
            "\n",
            "Global step: 1631,loss: 0.027287256\n",
            "\n",
            "Global step: 1632,loss: 0.028363075\n",
            "\n",
            "Global step: 1633,loss: 0.026558429\n",
            "\n",
            "Global step: 1634,loss: 0.024529248\n",
            "\n",
            "Global step: 1635,loss: 0.026164718\n",
            "\n",
            "Global step: 1636,loss: 0.025807992\n",
            "\n",
            "Global step: 1637,loss: 0.02629598\n",
            "\n",
            "Global step: 1638,loss: 0.025721965\n",
            "\n",
            "Global step: 1639,loss: 0.026511816\n",
            "\n",
            "Global step: 1640,loss: 0.027483923\n",
            "\n",
            "Global step: 1641,loss: 0.025614282\n",
            "\n",
            "Global step: 1642,loss: 0.02778329\n",
            "\n",
            "Global step: 1643,loss: 0.024370871\n",
            "\n",
            "Global step: 1644,loss: 0.027569156\n",
            "\n",
            "Global step: 1645,loss: 0.028083365\n",
            "\n",
            "Global step: 1646,loss: 0.023826117\n",
            "\n",
            "Global step: 1647,loss: 0.026640411\n",
            "\n",
            "Global step: 1648,loss: 0.0276856\n",
            "\n",
            "Global step: 1649,loss: 0.027866991\n",
            "\n",
            "Global step: 1650,loss: 0.025955716\n",
            "\n",
            "Global step: 1651,loss: 0.026231928\n",
            "\n",
            "Global step: 1652,loss: 0.02446048\n",
            "\n",
            "Global step: 1653,loss: 0.026560634\n",
            "\n",
            "Global step: 1654,loss: 0.02610872\n",
            "\n",
            "Global step: 1655,loss: 0.029160812\n",
            "\n",
            "Global step: 1656,loss: 0.02408509\n",
            "\n",
            "Global step: 1657,loss: 0.02639931\n",
            "\n",
            "Global step: 1658,loss: 0.026481662\n",
            "\n",
            "Global step: 1659,loss: 0.02454422\n",
            "\n",
            "Global step: 1660,loss: 0.025384638\n",
            "\n",
            "Global step: 1661,loss: 0.025098087\n",
            "\n",
            "Global step: 1662,loss: 0.024840852\n",
            "\n",
            "Global step: 1663,loss: 0.028875634\n",
            "\n",
            "Global step: 1664,loss: 0.024813306\n",
            "\n",
            "Global step: 1665,loss: 0.024695748\n",
            "\n",
            "Global step: 1666,loss: 0.025393983\n",
            "\n",
            "Global step: 1667,loss: 0.02435144\n",
            "\n",
            "Global step: 1668,loss: 0.028566057\n",
            "\n",
            "Global step: 1669,loss: 0.024763392\n",
            "\n",
            "Global step: 1670,loss: 0.025000794\n",
            "\n",
            "Global step: 1671,loss: 0.026595697\n",
            "\n",
            "Global step: 1672,loss: 0.024687454\n",
            "\n",
            "Global step: 1673,loss: 0.026134718\n",
            "\n",
            "Global step: 1674,loss: 0.026839167\n",
            "\n",
            "Global step: 1675,loss: 0.02517371\n",
            "\n",
            "Global step: 1676,loss: 0.02544391\n",
            "\n",
            "Global step: 1677,loss: 0.02583238\n",
            "\n",
            "Global step: 1678,loss: 0.025352376\n",
            "\n",
            "Global step: 1679,loss: 0.0249498\n",
            "\n",
            "Global step: 1680,loss: 0.026772572\n",
            "\n",
            "Global step: 1681,loss: 0.026000056\n",
            "\n",
            "Global step: 1682,loss: 0.029356705\n",
            "\n",
            "Global step: 1683,loss: 0.025890991\n",
            "\n",
            "Global step: 1684,loss: 0.02519839\n",
            "\n",
            "Global step: 1685,loss: 0.02501853\n",
            "\n",
            "Global step: 1686,loss: 0.027074475\n",
            "\n",
            "Global step: 1687,loss: 0.025735648\n",
            "\n",
            "Global step: 1688,loss: 0.024288252\n",
            "\n",
            "Global step: 1689,loss: 0.031115565\n",
            "\n",
            "Global step: 1690,loss: 0.024959734\n",
            "\n",
            "Global step: 1691,loss: 0.026900474\n",
            "\n",
            "Global step: 1692,loss: 0.029247638\n",
            "\n",
            "Global step: 1693,loss: 0.025249932\n",
            "\n",
            "Global step: 1694,loss: 0.024618069\n",
            "\n",
            "Global step: 1695,loss: 0.028970933\n",
            "\n",
            "Global step: 1696,loss: 0.027942495\n",
            "\n",
            "Global step: 1697,loss: 0.025302714\n",
            "\n",
            "Global step: 1698,loss: 0.024618093\n",
            "\n",
            "Global step: 1699,loss: 0.02481338\n",
            "\n",
            "Global step: 1700,loss: 0.026176305\n",
            "\n",
            "Global step: 1701,loss: 0.025466943\n",
            "\n",
            "Global step: 1702,loss: 0.023630759\n",
            "\n",
            "Global step: 1703,loss: 0.023856275\n",
            "\n",
            "Global step: 1704,loss: 0.024983253\n",
            "\n",
            "Global step: 1705,loss: 0.026342817\n",
            "\n",
            "Global step: 1706,loss: 0.024012512\n",
            "\n",
            "Global step: 1707,loss: 0.025391025\n",
            "\n",
            "Global step: 1708,loss: 0.028446488\n",
            "\n",
            "Global step: 1709,loss: 0.023622416\n",
            "\n",
            "Global step: 1710,loss: 0.026813075\n",
            "\n",
            "Global step: 1711,loss: 0.025315227\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1711,Val_Loss: 0.028337765485048294,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:33:51.012487 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 9/50:\n",
            "Global step: 1712,loss: 0.026851118\n",
            "\n",
            "Global step: 1713,loss: 0.023663457\n",
            "\n",
            "Global step: 1714,loss: 0.025849743\n",
            "\n",
            "Global step: 1715,loss: 0.02401292\n",
            "\n",
            "Global step: 1716,loss: 0.025522903\n",
            "\n",
            "Global step: 1717,loss: 0.023644708\n",
            "\n",
            "Global step: 1718,loss: 0.024275726\n",
            "\n",
            "Global step: 1719,loss: 0.024934648\n",
            "\n",
            "Global step: 1720,loss: 0.026182571\n",
            "\n",
            "Global step: 1721,loss: 0.026624782\n",
            "\n",
            "Global step: 1722,loss: 0.026613042\n",
            "\n",
            "Global step: 1723,loss: 0.025853766\n",
            "\n",
            "Global step: 1724,loss: 0.025582315\n",
            "\n",
            "Global step: 1725,loss: 0.0256208\n",
            "\n",
            "Global step: 1726,loss: 0.025104944\n",
            "\n",
            "Global step: 1727,loss: 0.023409339\n",
            "\n",
            "Global step: 1728,loss: 0.02763715\n",
            "\n",
            "Global step: 1729,loss: 0.025034897\n",
            "\n",
            "Global step: 1730,loss: 0.025471022\n",
            "\n",
            "Global step: 1731,loss: 0.024472753\n",
            "\n",
            "Global step: 1732,loss: 0.024819816\n",
            "\n",
            "Global step: 1733,loss: 0.023707217\n",
            "\n",
            "Global step: 1734,loss: 0.025329178\n",
            "\n",
            "Global step: 1735,loss: 0.02413681\n",
            "\n",
            "Global step: 1736,loss: 0.024387347\n",
            "\n",
            "Global step: 1737,loss: 0.025218477\n",
            "\n",
            "Global step: 1738,loss: 0.02428548\n",
            "\n",
            "Global step: 1739,loss: 0.023193885\n",
            "\n",
            "Global step: 1740,loss: 0.026617486\n",
            "\n",
            "Global step: 1741,loss: 0.024410881\n",
            "\n",
            "Global step: 1742,loss: 0.022823902\n",
            "\n",
            "Global step: 1743,loss: 0.026538095\n",
            "\n",
            "Global step: 1744,loss: 0.026294585\n",
            "\n",
            "Global step: 1745,loss: 0.023531284\n",
            "\n",
            "Global step: 1746,loss: 0.024255214\n",
            "\n",
            "Global step: 1747,loss: 0.02624495\n",
            "\n",
            "Global step: 1748,loss: 0.024738716\n",
            "\n",
            "Global step: 1749,loss: 0.024384521\n",
            "\n",
            "Global step: 1750,loss: 0.025211109\n",
            "\n",
            "Global step: 1751,loss: 0.024372227\n",
            "\n",
            "Global step: 1752,loss: 0.026040759\n",
            "\n",
            "Global step: 1753,loss: 0.025351046\n",
            "\n",
            "Global step: 1754,loss: 0.026842251\n",
            "\n",
            "Global step: 1755,loss: 0.025529047\n",
            "\n",
            "Global step: 1756,loss: 0.024267169\n",
            "\n",
            "Global step: 1757,loss: 0.025450861\n",
            "\n",
            "Global step: 1758,loss: 0.026104223\n",
            "\n",
            "Global step: 1759,loss: 0.025088118\n",
            "\n",
            "Global step: 1760,loss: 0.024089908\n",
            "\n",
            "Global step: 1761,loss: 0.024883835\n",
            "\n",
            "Global step: 1762,loss: 0.024972545\n",
            "\n",
            "Global step: 1763,loss: 0.023936832\n",
            "\n",
            "Global step: 1764,loss: 0.025529256\n",
            "\n",
            "Global step: 1765,loss: 0.024870314\n",
            "\n",
            "Global step: 1766,loss: 0.025052365\n",
            "\n",
            "Global step: 1767,loss: 0.028075999\n",
            "\n",
            "Global step: 1768,loss: 0.024200844\n",
            "\n",
            "Global step: 1769,loss: 0.026719868\n",
            "\n",
            "Global step: 1770,loss: 0.023574943\n",
            "\n",
            "Global step: 1771,loss: 0.025021834\n",
            "\n",
            "Global step: 1772,loss: 0.024415538\n",
            "\n",
            "Global step: 1773,loss: 0.025514511\n",
            "\n",
            "Global step: 1774,loss: 0.024282139\n",
            "\n",
            "Global step: 1775,loss: 0.025883367\n",
            "\n",
            "Global step: 1776,loss: 0.024309613\n",
            "\n",
            "Global step: 1777,loss: 0.025650507\n",
            "\n",
            "Global step: 1778,loss: 0.02471775\n",
            "\n",
            "Global step: 1779,loss: 0.024188958\n",
            "\n",
            "Global step: 1780,loss: 0.025336722\n",
            "\n",
            "Global step: 1781,loss: 0.024471613\n",
            "\n",
            "Global step: 1782,loss: 0.024800096\n",
            "\n",
            "Global step: 1783,loss: 0.028507186\n",
            "\n",
            "Global step: 1784,loss: 0.023499526\n",
            "\n",
            "Global step: 1785,loss: 0.02455613\n",
            "\n",
            "Global step: 1786,loss: 0.023237485\n",
            "\n",
            "Global step: 1787,loss: 0.025363082\n",
            "\n",
            "Global step: 1788,loss: 0.023715636\n",
            "\n",
            "Global step: 1789,loss: 0.024562933\n",
            "\n",
            "Global step: 1790,loss: 0.024976054\n",
            "\n",
            "Global step: 1791,loss: 0.025409333\n",
            "\n",
            "Global step: 1792,loss: 0.023845471\n",
            "\n",
            "Global step: 1793,loss: 0.023802564\n",
            "\n",
            "Global step: 1794,loss: 0.023855321\n",
            "\n",
            "Global step: 1795,loss: 0.02300727\n",
            "\n",
            "Global step: 1796,loss: 0.024782475\n",
            "\n",
            "Global step: 1797,loss: 0.025824083\n",
            "\n",
            "Global step: 1798,loss: 0.022841683\n",
            "\n",
            "Global step: 1799,loss: 0.02815783\n",
            "\n",
            "Global step: 1800,loss: 0.026174128\n",
            "\n",
            "Global step: 1801,loss: 0.024464512\n",
            "\n",
            "Global step: 1802,loss: 0.024362428\n",
            "\n",
            "Global step: 1803,loss: 0.023538562\n",
            "\n",
            "Global step: 1804,loss: 0.023371456\n",
            "\n",
            "Global step: 1805,loss: 0.024319252\n",
            "\n",
            "Global step: 1806,loss: 0.02584316\n",
            "\n",
            "Global step: 1807,loss: 0.022906573\n",
            "\n",
            "Global step: 1808,loss: 0.02443235\n",
            "\n",
            "Global step: 1809,loss: 0.024777386\n",
            "\n",
            "Global step: 1810,loss: 0.024282655\n",
            "\n",
            "Global step: 1811,loss: 0.02318486\n",
            "\n",
            "Global step: 1812,loss: 0.02368876\n",
            "\n",
            "Global step: 1813,loss: 0.024952995\n",
            "\n",
            "Global step: 1814,loss: 0.023392769\n",
            "\n",
            "Global step: 1815,loss: 0.02424707\n",
            "\n",
            "Global step: 1816,loss: 0.023369718\n",
            "\n",
            "Global step: 1817,loss: 0.025394872\n",
            "\n",
            "Global step: 1818,loss: 0.024272583\n",
            "\n",
            "Global step: 1819,loss: 0.024968766\n",
            "\n",
            "Global step: 1820,loss: 0.024194967\n",
            "\n",
            "Global step: 1821,loss: 0.02314369\n",
            "\n",
            "Global step: 1822,loss: 0.024963757\n",
            "\n",
            "Global step: 1823,loss: 0.023015019\n",
            "\n",
            "Global step: 1824,loss: 0.025063744\n",
            "\n",
            "Global step: 1825,loss: 0.024343582\n",
            "\n",
            "Global step: 1826,loss: 0.023942934\n",
            "\n",
            "Global step: 1827,loss: 0.024191402\n",
            "\n",
            "Global step: 1828,loss: 0.026422165\n",
            "\n",
            "Global step: 1829,loss: 0.023554029\n",
            "\n",
            "Global step: 1830,loss: 0.024602594\n",
            "\n",
            "Global step: 1831,loss: 0.026273254\n",
            "\n",
            "Global step: 1832,loss: 0.022928597\n",
            "\n",
            "Global step: 1833,loss: 0.025445614\n",
            "\n",
            "Global step: 1834,loss: 0.023852991\n",
            "\n",
            "Global step: 1835,loss: 0.024891008\n",
            "\n",
            "Global step: 1836,loss: 0.023909558\n",
            "\n",
            "Global step: 1837,loss: 0.024840115\n",
            "\n",
            "Global step: 1838,loss: 0.023578053\n",
            "\n",
            "Global step: 1839,loss: 0.027994279\n",
            "\n",
            "Global step: 1840,loss: 0.026196847\n",
            "\n",
            "Global step: 1841,loss: 0.023379747\n",
            "\n",
            "Global step: 1842,loss: 0.025414098\n",
            "\n",
            "Global step: 1843,loss: 0.023592897\n",
            "\n",
            "Global step: 1844,loss: 0.023655877\n",
            "\n",
            "Global step: 1845,loss: 0.025726171\n",
            "\n",
            "Global step: 1846,loss: 0.022731654\n",
            "\n",
            "Global step: 1847,loss: 0.023266101\n",
            "\n",
            "Global step: 1848,loss: 0.024394931\n",
            "\n",
            "Global step: 1849,loss: 0.027761184\n",
            "\n",
            "Global step: 1850,loss: 0.022456797\n",
            "\n",
            "Global step: 1851,loss: 0.023836222\n",
            "\n",
            "Global step: 1852,loss: 0.025798531\n",
            "\n",
            "Global step: 1853,loss: 0.025173225\n",
            "\n",
            "Global step: 1854,loss: 0.023221381\n",
            "\n",
            "Global step: 1855,loss: 0.024321653\n",
            "\n",
            "Global step: 1856,loss: 0.024218077\n",
            "\n",
            "Global step: 1857,loss: 0.024003627\n",
            "\n",
            "Global step: 1858,loss: 0.024441909\n",
            "\n",
            "Global step: 1859,loss: 0.02489655\n",
            "\n",
            "Global step: 1860,loss: 0.023832424\n",
            "\n",
            "Global step: 1861,loss: 0.023037508\n",
            "\n",
            "Global step: 1862,loss: 0.027479276\n",
            "\n",
            "Global step: 1863,loss: 0.026054688\n",
            "\n",
            "Global step: 1864,loss: 0.024501791\n",
            "\n",
            "Global step: 1865,loss: 0.023987329\n",
            "\n",
            "Global step: 1866,loss: 0.02496876\n",
            "\n",
            "Global step: 1867,loss: 0.025652021\n",
            "\n",
            "Global step: 1868,loss: 0.023837248\n",
            "\n",
            "Global step: 1869,loss: 0.023609392\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1870.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:34:29.348670 140167885084416 supervisor.py:1050] Recording summary at step 1870.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.97492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:34:29.357590 140166520051456 supervisor.py:1099] global_step/sec: 3.97492\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1870,loss: 0.022759492\n",
            "\n",
            "Global step: 1871,loss: 0.025281677\n",
            "\n",
            "Global step: 1872,loss: 0.024679022\n",
            "\n",
            "Global step: 1873,loss: 0.025822481\n",
            "\n",
            "Global step: 1874,loss: 0.023273757\n",
            "\n",
            "Global step: 1875,loss: 0.023408342\n",
            "\n",
            "Global step: 1876,loss: 0.023129957\n",
            "\n",
            "Global step: 1877,loss: 0.022931404\n",
            "\n",
            "Global step: 1878,loss: 0.023981206\n",
            "\n",
            "Global step: 1879,loss: 0.023029432\n",
            "\n",
            "Global step: 1880,loss: 0.024067417\n",
            "\n",
            "Global step: 1881,loss: 0.023768611\n",
            "\n",
            "Global step: 1882,loss: 0.022877092\n",
            "\n",
            "Global step: 1883,loss: 0.02310901\n",
            "\n",
            "Global step: 1884,loss: 0.024773367\n",
            "\n",
            "Global step: 1885,loss: 0.023906244\n",
            "\n",
            "Global step: 1886,loss: 0.027750552\n",
            "\n",
            "Global step: 1887,loss: 0.023547594\n",
            "\n",
            "Global step: 1888,loss: 0.024352614\n",
            "\n",
            "Global step: 1889,loss: 0.025362207\n",
            "\n",
            "Global step: 1890,loss: 0.024838202\n",
            "\n",
            "Global step: 1891,loss: 0.02304227\n",
            "\n",
            "Global step: 1892,loss: 0.02619841\n",
            "\n",
            "Global step: 1893,loss: 0.022809403\n",
            "\n",
            "Global step: 1894,loss: 0.023287505\n",
            "\n",
            "Global step: 1895,loss: 0.02350236\n",
            "\n",
            "Global step: 1896,loss: 0.024827825\n",
            "\n",
            "Global step: 1897,loss: 0.023070965\n",
            "\n",
            "Global step: 1898,loss: 0.02529601\n",
            "\n",
            "Global step: 1899,loss: 0.026414657\n",
            "\n",
            "Global step: 1900,loss: 0.024952902\n",
            "\n",
            "Global step: 1901,loss: 0.023912238\n",
            "\n",
            "Global step: 1902,loss: 0.0242911\n",
            "\n",
            "Global step: 1903,loss: 0.024210304\n",
            "\n",
            "Global step: 1904,loss: 0.025463495\n",
            "\n",
            "Global step: 1905,loss: 0.023157097\n",
            "\n",
            "Global step: 1906,loss: 0.025582552\n",
            "\n",
            "Global step: 1907,loss: 0.023101669\n",
            "\n",
            "Global step: 1908,loss: 0.025688482\n",
            "\n",
            "Global step: 1909,loss: 0.023526888\n",
            "\n",
            "Global step: 1910,loss: 0.025472846\n",
            "\n",
            "Global step: 1911,loss: 0.024629477\n",
            "\n",
            "Global step: 1912,loss: 0.023484878\n",
            "\n",
            "Global step: 1913,loss: 0.023537274\n",
            "\n",
            "Global step: 1914,loss: 0.022913214\n",
            "\n",
            "Global step: 1915,loss: 0.023938892\n",
            "\n",
            "Global step: 1916,loss: 0.022824634\n",
            "\n",
            "Global step: 1917,loss: 0.023148578\n",
            "\n",
            "Global step: 1918,loss: 0.022195287\n",
            "\n",
            "Global step: 1919,loss: 0.023683935\n",
            "\n",
            "Global step: 1920,loss: 0.022708105\n",
            "\n",
            "Global step: 1921,loss: 0.024226543\n",
            "\n",
            "Global step: 1922,loss: 0.023411779\n",
            "\n",
            "Global step: 1923,loss: 0.024607528\n",
            "\n",
            "Global step: 1924,loss: 0.02628864\n",
            "\n",
            "Global step: 1925,loss: 0.0249545\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1925,Val_Loss: 0.026931587704702428,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:34:45.175406 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 10/50:\n",
            "Global step: 1926,loss: 0.02448249\n",
            "\n",
            "Global step: 1927,loss: 0.024009122\n",
            "\n",
            "Global step: 1928,loss: 0.023710927\n",
            "\n",
            "Global step: 1929,loss: 0.024116347\n",
            "\n",
            "Global step: 1930,loss: 0.023799537\n",
            "\n",
            "Global step: 1931,loss: 0.022878008\n",
            "\n",
            "Global step: 1932,loss: 0.02192887\n",
            "\n",
            "Global step: 1933,loss: 0.024749689\n",
            "\n",
            "Global step: 1934,loss: 0.025223328\n",
            "\n",
            "Global step: 1935,loss: 0.023415908\n",
            "\n",
            "Global step: 1936,loss: 0.023939047\n",
            "\n",
            "Global step: 1937,loss: 0.026253447\n",
            "\n",
            "Global step: 1938,loss: 0.023893144\n",
            "\n",
            "Global step: 1939,loss: 0.022828268\n",
            "\n",
            "Global step: 1940,loss: 0.023431418\n",
            "\n",
            "Global step: 1941,loss: 0.024411472\n",
            "\n",
            "Global step: 1942,loss: 0.02214627\n",
            "\n",
            "Global step: 1943,loss: 0.023966156\n",
            "\n",
            "Global step: 1944,loss: 0.023566658\n",
            "\n",
            "Global step: 1945,loss: 0.024147484\n",
            "\n",
            "Global step: 1946,loss: 0.022048268\n",
            "\n",
            "Global step: 1947,loss: 0.02212598\n",
            "\n",
            "Global step: 1948,loss: 0.021870855\n",
            "\n",
            "Global step: 1949,loss: 0.024909368\n",
            "\n",
            "Global step: 1950,loss: 0.026025793\n",
            "\n",
            "Global step: 1951,loss: 0.023738142\n",
            "\n",
            "Global step: 1952,loss: 0.02176624\n",
            "\n",
            "Global step: 1953,loss: 0.024775023\n",
            "\n",
            "Global step: 1954,loss: 0.023293337\n",
            "\n",
            "Global step: 1955,loss: 0.023332324\n",
            "\n",
            "Global step: 1956,loss: 0.022658307\n",
            "\n",
            "Global step: 1957,loss: 0.021008369\n",
            "\n",
            "Global step: 1958,loss: 0.022693759\n",
            "\n",
            "Global step: 1959,loss: 0.022252554\n",
            "\n",
            "Global step: 1960,loss: 0.022490712\n",
            "\n",
            "Global step: 1961,loss: 0.021500222\n",
            "\n",
            "Global step: 1962,loss: 0.022873694\n",
            "\n",
            "Global step: 1963,loss: 0.026167963\n",
            "\n",
            "Global step: 1964,loss: 0.024894897\n",
            "\n",
            "Global step: 1965,loss: 0.021989152\n",
            "\n",
            "Global step: 1966,loss: 0.023027385\n",
            "\n",
            "Global step: 1967,loss: 0.02248136\n",
            "\n",
            "Global step: 1968,loss: 0.02329601\n",
            "\n",
            "Global step: 1969,loss: 0.022713691\n",
            "\n",
            "Global step: 1970,loss: 0.023525802\n",
            "\n",
            "Global step: 1971,loss: 0.023541482\n",
            "\n",
            "Global step: 1972,loss: 0.02285662\n",
            "\n",
            "Global step: 1973,loss: 0.021860816\n",
            "\n",
            "Global step: 1974,loss: 0.024325334\n",
            "\n",
            "Global step: 1975,loss: 0.023855243\n",
            "\n",
            "Global step: 1976,loss: 0.023730215\n",
            "\n",
            "Global step: 1977,loss: 0.024484975\n",
            "\n",
            "Global step: 1978,loss: 0.026834637\n",
            "\n",
            "Global step: 1979,loss: 0.023700932\n",
            "\n",
            "Global step: 1980,loss: 0.022366567\n",
            "\n",
            "Global step: 1981,loss: 0.021918802\n",
            "\n",
            "Global step: 1982,loss: 0.023251956\n",
            "\n",
            "Global step: 1983,loss: 0.022411514\n",
            "\n",
            "Global step: 1984,loss: 0.022096965\n",
            "\n",
            "Global step: 1985,loss: 0.023012284\n",
            "\n",
            "Global step: 1986,loss: 0.021940818\n",
            "\n",
            "Global step: 1987,loss: 0.022916706\n",
            "\n",
            "Global step: 1988,loss: 0.023389421\n",
            "\n",
            "Global step: 1989,loss: 0.022467395\n",
            "\n",
            "Global step: 1990,loss: 0.022509137\n",
            "\n",
            "Global step: 1991,loss: 0.023641609\n",
            "\n",
            "Global step: 1992,loss: 0.02294387\n",
            "\n",
            "Global step: 1993,loss: 0.021722907\n",
            "\n",
            "Global step: 1994,loss: 0.021738634\n",
            "\n",
            "Global step: 1995,loss: 0.021929046\n",
            "\n",
            "Global step: 1996,loss: 0.02609761\n",
            "\n",
            "Global step: 1997,loss: 0.022343341\n",
            "\n",
            "Global step: 1998,loss: 0.022883033\n",
            "\n",
            "Global step: 1999,loss: 0.02371562\n",
            "\n",
            "Global step: 2000,loss: 0.024775445\n",
            "\n",
            "Global step: 2001,loss: 0.022931144\n",
            "\n",
            "Global step: 2002,loss: 0.02524355\n",
            "\n",
            "Global step: 2003,loss: 0.025104295\n",
            "\n",
            "Global step: 2004,loss: 0.022063028\n",
            "\n",
            "Global step: 2005,loss: 0.024994574\n",
            "\n",
            "Global step: 2006,loss: 0.023247017\n",
            "\n",
            "Global step: 2007,loss: 0.021428257\n",
            "\n",
            "Global step: 2008,loss: 0.021404298\n",
            "\n",
            "Global step: 2009,loss: 0.021969084\n",
            "\n",
            "Global step: 2010,loss: 0.022885045\n",
            "\n",
            "Global step: 2011,loss: 0.023204442\n",
            "\n",
            "Global step: 2012,loss: 0.0229829\n",
            "\n",
            "Global step: 2013,loss: 0.023905978\n",
            "\n",
            "Global step: 2014,loss: 0.021372486\n",
            "\n",
            "Global step: 2015,loss: 0.0231624\n",
            "\n",
            "Global step: 2016,loss: 0.02480698\n",
            "\n",
            "Global step: 2017,loss: 0.02200129\n",
            "\n",
            "Global step: 2018,loss: 0.022736697\n",
            "\n",
            "Global step: 2019,loss: 0.023223067\n",
            "\n",
            "Global step: 2020,loss: 0.022483762\n",
            "\n",
            "Global step: 2021,loss: 0.02367117\n",
            "\n",
            "Global step: 2022,loss: 0.022778384\n",
            "\n",
            "Global step: 2023,loss: 0.023108512\n",
            "\n",
            "Global step: 2024,loss: 0.022124011\n",
            "\n",
            "Global step: 2025,loss: 0.02435922\n",
            "\n",
            "Global step: 2026,loss: 0.020546492\n",
            "\n",
            "Global step: 2027,loss: 0.021998815\n",
            "\n",
            "Global step: 2028,loss: 0.022513278\n",
            "\n",
            "Global step: 2029,loss: 0.021182392\n",
            "\n",
            "Global step: 2030,loss: 0.022139633\n",
            "\n",
            "Global step: 2031,loss: 0.02359622\n",
            "\n",
            "Global step: 2032,loss: 0.023889164\n",
            "\n",
            "Global step: 2033,loss: 0.022388268\n",
            "\n",
            "Global step: 2034,loss: 0.022882761\n",
            "\n",
            "Global step: 2035,loss: 0.022833576\n",
            "\n",
            "Global step: 2036,loss: 0.022840345\n",
            "\n",
            "Global step: 2037,loss: 0.023503069\n",
            "\n",
            "Global step: 2038,loss: 0.026696498\n",
            "\n",
            "Global step: 2039,loss: 0.023666339\n",
            "\n",
            "Global step: 2040,loss: 0.021446284\n",
            "\n",
            "Global step: 2041,loss: 0.022432908\n",
            "\n",
            "Global step: 2042,loss: 0.022874847\n",
            "\n",
            "Global step: 2043,loss: 0.023407249\n",
            "\n",
            "Global step: 2044,loss: 0.02418399\n",
            "\n",
            "Global step: 2045,loss: 0.021858599\n",
            "\n",
            "Global step: 2046,loss: 0.021562427\n",
            "\n",
            "Global step: 2047,loss: 0.022599764\n",
            "\n",
            "Global step: 2048,loss: 0.02190487\n",
            "\n",
            "Global step: 2049,loss: 0.022847882\n",
            "\n",
            "Global step: 2050,loss: 0.023833632\n",
            "\n",
            "Global step: 2051,loss: 0.021906924\n",
            "\n",
            "Global step: 2052,loss: 0.021593416\n",
            "\n",
            "Global step: 2053,loss: 0.023458889\n",
            "\n",
            "Global step: 2054,loss: 0.022720506\n",
            "\n",
            "Global step: 2055,loss: 0.022662727\n",
            "\n",
            "Global step: 2056,loss: 0.0217031\n",
            "\n",
            "Global step: 2057,loss: 0.023710411\n",
            "\n",
            "Global step: 2058,loss: 0.023474796\n",
            "\n",
            "Global step: 2059,loss: 0.024902804\n",
            "\n",
            "Global step: 2060,loss: 0.02280544\n",
            "\n",
            "Global step: 2061,loss: 0.023247635\n",
            "\n",
            "Global step: 2062,loss: 0.023605034\n",
            "\n",
            "Global step: 2063,loss: 0.02261812\n",
            "\n",
            "Global step: 2064,loss: 0.022974348\n",
            "\n",
            "Global step: 2065,loss: 0.022456264\n",
            "\n",
            "Global step: 2066,loss: 0.027749065\n",
            "\n",
            "Global step: 2067,loss: 0.020942545\n",
            "\n",
            "Global step: 2068,loss: 0.02266461\n",
            "\n",
            "Global step: 2069,loss: 0.02283787\n",
            "\n",
            "Global step: 2070,loss: 0.02333349\n",
            "\n",
            "Global step: 2071,loss: 0.021961378\n",
            "\n",
            "Global step: 2072,loss: 0.022235231\n",
            "\n",
            "Global step: 2073,loss: 0.021994617\n",
            "\n",
            "Global step: 2074,loss: 0.023528405\n",
            "\n",
            "Global step: 2075,loss: 0.022378258\n",
            "\n",
            "Global step: 2076,loss: 0.0218679\n",
            "\n",
            "Global step: 2077,loss: 0.021802949\n",
            "\n",
            "Global step: 2078,loss: 0.021890003\n",
            "\n",
            "Global step: 2079,loss: 0.022384156\n",
            "\n",
            "Global step: 2080,loss: 0.022570197\n",
            "\n",
            "Global step: 2081,loss: 0.022179797\n",
            "\n",
            "Global step: 2082,loss: 0.022210794\n",
            "\n",
            "Global step: 2083,loss: 0.022541357\n",
            "\n",
            "Global step: 2084,loss: 0.021818362\n",
            "\n",
            "Global step: 2085,loss: 0.022484304\n",
            "\n",
            "Global step: 2086,loss: 0.021123748\n",
            "\n",
            "Global step: 2087,loss: 0.023793397\n",
            "\n",
            "Global step: 2088,loss: 0.022268377\n",
            "\n",
            "Global step: 2089,loss: 0.021630125\n",
            "\n",
            "Global step: 2090,loss: 0.023093931\n",
            "\n",
            "Global step: 2091,loss: 0.021861294\n",
            "\n",
            "Global step: 2092,loss: 0.022899678\n",
            "\n",
            "Global step: 2093,loss: 0.021913227\n",
            "\n",
            "Global step: 2094,loss: 0.021994893\n",
            "\n",
            "Global step: 2095,loss: 0.021618504\n",
            "\n",
            "Global step: 2096,loss: 0.022306206\n",
            "\n",
            "Global step: 2097,loss: 0.023351962\n",
            "\n",
            "Global step: 2098,loss: 0.021242237\n",
            "\n",
            "Global step: 2099,loss: 0.022057712\n",
            "\n",
            "Global step: 2100,loss: 0.023329273\n",
            "\n",
            "Global step: 2101,loss: 0.022095669\n",
            "\n",
            "Global step: 2102,loss: 0.021219505\n",
            "\n",
            "Global step: 2103,loss: 0.02235833\n",
            "\n",
            "Global step: 2104,loss: 0.022709237\n",
            "\n",
            "Global step: 2105,loss: 0.021820717\n",
            "\n",
            "Global step: 2106,loss: 0.02378247\n",
            "\n",
            "Global step: 2107,loss: 0.021098329\n",
            "\n",
            "Global step: 2108,loss: 0.021612454\n",
            "\n",
            "Global step: 2109,loss: 0.021641685\n",
            "\n",
            "Global step: 2110,loss: 0.021688258\n",
            "\n",
            "Global step: 2111,loss: 0.023898307\n",
            "\n",
            "Global step: 2112,loss: 0.021227892\n",
            "\n",
            "Global step: 2113,loss: 0.021685204\n",
            "\n",
            "Global step: 2114,loss: 0.021964919\n",
            "\n",
            "Global step: 2115,loss: 0.021805206\n",
            "\n",
            "Global step: 2116,loss: 0.023575418\n",
            "\n",
            "Global step: 2117,loss: 0.021184906\n",
            "\n",
            "Global step: 2118,loss: 0.022426214\n",
            "\n",
            "Global step: 2119,loss: 0.022552753\n",
            "\n",
            "Global step: 2120,loss: 0.022540225\n",
            "\n",
            "Global step: 2121,loss: 0.022926938\n",
            "\n",
            "Global step: 2122,loss: 0.022675572\n",
            "\n",
            "Global step: 2123,loss: 0.021970592\n",
            "\n",
            "Global step: 2124,loss: 0.021678556\n",
            "\n",
            "Global step: 2125,loss: 0.021392861\n",
            "\n",
            "Global step: 2126,loss: 0.023679173\n",
            "\n",
            "Global step: 2127,loss: 0.024001751\n",
            "\n",
            "Global step: 2128,loss: 0.020916434\n",
            "\n",
            "Global step: 2129,loss: 0.021434687\n",
            "\n",
            "Global step: 2130,loss: 0.021433452\n",
            "\n",
            "Global step: 2131,loss: 0.02483387\n",
            "\n",
            "Global step: 2132,loss: 0.021576807\n",
            "\n",
            "Global step: 2133,loss: 0.022334427\n",
            "\n",
            "Global step: 2134,loss: 0.021138804\n",
            "\n",
            "Global step: 2135,loss: 0.021851426\n",
            "\n",
            "Global step: 2136,loss: 0.02167815\n",
            "\n",
            "Global step: 2137,loss: 0.021628298\n",
            "\n",
            "Global step: 2138,loss: 0.02133093\n",
            "\n",
            "Global step: 2139,loss: 0.021700615\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2139,Val_Loss: 0.02562586061264339,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:35:39.574835 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 11/50:\n",
            "Global step: 2140,loss: 0.021387016\n",
            "\n",
            "Global step: 2141,loss: 0.022226023\n",
            "\n",
            "Global step: 2142,loss: 0.021699984\n",
            "\n",
            "Global step: 2143,loss: 0.02154955\n",
            "\n",
            "Global step: 2144,loss: 0.023341091\n",
            "\n",
            "Global step: 2145,loss: 0.02064408\n",
            "\n",
            "Global step: 2146,loss: 0.025393\n",
            "\n",
            "Global step: 2147,loss: 0.025548883\n",
            "\n",
            "Global step: 2148,loss: 0.021870421\n",
            "\n",
            "Global step: 2149,loss: 0.021790748\n",
            "\n",
            "Global step: 2150,loss: 0.0219855\n",
            "\n",
            "Global step: 2151,loss: 0.021349143\n",
            "\n",
            "Global step: 2152,loss: 0.021489274\n",
            "\n",
            "Global step: 2153,loss: 0.022316221\n",
            "\n",
            "Global step: 2154,loss: 0.021572923\n",
            "\n",
            "Global step: 2155,loss: 0.023682315\n",
            "\n",
            "Global step: 2156,loss: 0.02111392\n",
            "\n",
            "Global step: 2157,loss: 0.023599429\n",
            "\n",
            "Global step: 2158,loss: 0.021337168\n",
            "\n",
            "Global step: 2159,loss: 0.021102723\n",
            "\n",
            "Global step: 2160,loss: 0.021389537\n",
            "\n",
            "Global step: 2161,loss: 0.021559576\n",
            "\n",
            "Global step: 2162,loss: 0.022915322\n",
            "\n",
            "Global step: 2163,loss: 0.02199603\n",
            "\n",
            "Global step: 2164,loss: 0.020686207\n",
            "\n",
            "Global step: 2165,loss: 0.020733176\n",
            "\n",
            "Global step: 2166,loss: 0.021330237\n",
            "\n",
            "Global step: 2167,loss: 0.02068397\n",
            "\n",
            "Global step: 2168,loss: 0.021584842\n",
            "\n",
            "Global step: 2169,loss: 0.022552533\n",
            "\n",
            "Global step: 2170,loss: 0.02295028\n",
            "\n",
            "Global step: 2171,loss: 0.021465585\n",
            "\n",
            "Global step: 2172,loss: 0.022617057\n",
            "\n",
            "Global step: 2173,loss: 0.02210768\n",
            "\n",
            "Global step: 2174,loss: 0.02124057\n",
            "\n",
            "Global step: 2175,loss: 0.02181188\n",
            "\n",
            "Global step: 2176,loss: 0.021836644\n",
            "\n",
            "Global step: 2177,loss: 0.0216841\n",
            "\n",
            "Global step: 2178,loss: 0.021631384\n",
            "\n",
            "Global step: 2179,loss: 0.020847067\n",
            "\n",
            "Global step: 2180,loss: 0.02135498\n",
            "\n",
            "Global step: 2181,loss: 0.022124348\n",
            "\n",
            "Global step: 2182,loss: 0.022501932\n",
            "\n",
            "Global step: 2183,loss: 0.022702795\n",
            "\n",
            "Global step: 2184,loss: 0.0210149\n",
            "\n",
            "Global step: 2185,loss: 0.02160545\n",
            "\n",
            "Global step: 2186,loss: 0.021068897\n",
            "\n",
            "Global step: 2187,loss: 0.020552067\n",
            "\n",
            "Global step: 2188,loss: 0.020707583\n",
            "\n",
            "Global step: 2189,loss: 0.02172249\n",
            "\n",
            "Global step: 2190,loss: 0.020759156\n",
            "\n",
            "Global step: 2191,loss: 0.021247676\n",
            "\n",
            "Global step: 2192,loss: 0.02171276\n",
            "\n",
            "Global step: 2193,loss: 0.021042947\n",
            "\n",
            "Global step: 2194,loss: 0.02164153\n",
            "\n",
            "Global step: 2195,loss: 0.020939393\n",
            "\n",
            "Global step: 2196,loss: 0.021164786\n",
            "\n",
            "Global step: 2197,loss: 0.023089573\n",
            "\n",
            "Global step: 2198,loss: 0.020942476\n",
            "\n",
            "Global step: 2199,loss: 0.020473525\n",
            "\n",
            "Global step: 2200,loss: 0.021718662\n",
            "\n",
            "Global step: 2201,loss: 0.022390906\n",
            "\n",
            "Global step: 2202,loss: 0.024162197\n",
            "\n",
            "Global step: 2203,loss: 0.023217564\n",
            "\n",
            "Global step: 2204,loss: 0.022670507\n",
            "\n",
            "Global step: 2205,loss: 0.020710023\n",
            "\n",
            "Global step: 2206,loss: 0.02158667\n",
            "\n",
            "Global step: 2207,loss: 0.022066578\n",
            "\n",
            "Global step: 2208,loss: 0.020229233\n",
            "\n",
            "Global step: 2209,loss: 0.020445928\n",
            "\n",
            "Global step: 2210,loss: 0.02183298\n",
            "\n",
            "Global step: 2211,loss: 0.021228721\n",
            "\n",
            "Global step: 2212,loss: 0.021078562\n",
            "\n",
            "Global step: 2213,loss: 0.02128873\n",
            "\n",
            "Global step: 2214,loss: 0.023375634\n",
            "\n",
            "Global step: 2215,loss: 0.020907586\n",
            "\n",
            "Global step: 2216,loss: 0.021007903\n",
            "\n",
            "Global step: 2217,loss: 0.022797408\n",
            "\n",
            "Global step: 2218,loss: 0.019889615\n",
            "\n",
            "Global step: 2219,loss: 0.021945305\n",
            "\n",
            "Global step: 2220,loss: 0.021500628\n",
            "\n",
            "Global step: 2221,loss: 0.020247277\n",
            "\n",
            "Global step: 2222,loss: 0.021084381\n",
            "\n",
            "Global step: 2223,loss: 0.021800917\n",
            "\n",
            "Global step: 2224,loss: 0.02222726\n",
            "\n",
            "Global step: 2225,loss: 0.019681316\n",
            "\n",
            "Global step: 2226,loss: 0.020479964\n",
            "\n",
            "Global step: 2227,loss: 0.022123449\n",
            "\n",
            "Global step: 2228,loss: 0.020952094\n",
            "\n",
            "Global step: 2229,loss: 0.020986643\n",
            "\n",
            "Global step: 2230,loss: 0.02137277\n",
            "\n",
            "Global step: 2231,loss: 0.02238367\n",
            "\n",
            "Global step: 2232,loss: 0.020786269\n",
            "\n",
            "Global step: 2233,loss: 0.02145736\n",
            "\n",
            "Global step: 2234,loss: 0.021798024\n",
            "\n",
            "Global step: 2235,loss: 0.02157158\n",
            "\n",
            "Global step: 2236,loss: 0.021204732\n",
            "\n",
            "Global step: 2237,loss: 0.021642102\n",
            "\n",
            "Global step: 2238,loss: 0.020981517\n",
            "\n",
            "Global step: 2239,loss: 0.022670353\n",
            "\n",
            "Global step: 2240,loss: 0.021335823\n",
            "\n",
            "Global step: 2241,loss: 0.02362468\n",
            "\n",
            "Global step: 2242,loss: 0.021385713\n",
            "\n",
            "Global step: 2243,loss: 0.020280803\n",
            "\n",
            "Global step: 2244,loss: 0.02252236\n",
            "\n",
            "Global step: 2245,loss: 0.024085209\n",
            "\n",
            "Global step: 2246,loss: 0.020183492\n",
            "\n",
            "Global step: 2247,loss: 0.020686982\n",
            "\n",
            "Global step: 2248,loss: 0.021997083\n",
            "\n",
            "Global step: 2249,loss: 0.021671355\n",
            "\n",
            "Global step: 2250,loss: 0.022751354\n",
            "\n",
            "Global step: 2251,loss: 0.02208213\n",
            "\n",
            "Global step: 2252,loss: 0.021036351\n",
            "\n",
            "Global step: 2253,loss: 0.020932239\n",
            "\n",
            "Global step: 2254,loss: 0.02112804\n",
            "\n",
            "Global step: 2255,loss: 0.022307016\n",
            "\n",
            "Global step: 2256,loss: 0.020214863\n",
            "\n",
            "Global step: 2257,loss: 0.022657502\n",
            "\n",
            "Global step: 2258,loss: 0.020195037\n",
            "\n",
            "Global step: 2259,loss: 0.021546744\n",
            "\n",
            "Global step: 2260,loss: 0.020793162\n",
            "\n",
            "Global step: 2261,loss: 0.019966405\n",
            "\n",
            "Global step: 2262,loss: 0.02142845\n",
            "\n",
            "Global step: 2263,loss: 0.020972528\n",
            "\n",
            "Global step: 2264,loss: 0.020405397\n",
            "\n",
            "Global step: 2265,loss: 0.021227006\n",
            "\n",
            "Global step: 2266,loss: 0.019673478\n",
            "\n",
            "Global step: 2267,loss: 0.020443\n",
            "\n",
            "Global step: 2268,loss: 0.020530907\n",
            "\n",
            "Global step: 2269,loss: 0.021496981\n",
            "\n",
            "Global step: 2270,loss: 0.020676732\n",
            "\n",
            "Global step: 2271,loss: 0.021016492\n",
            "\n",
            "Global step: 2272,loss: 0.020069223\n",
            "\n",
            "Global step: 2273,loss: 0.020807616\n",
            "\n",
            "Global step: 2274,loss: 0.02027863\n",
            "\n",
            "Global step: 2275,loss: 0.0219256\n",
            "\n",
            "Global step: 2276,loss: 0.020040505\n",
            "\n",
            "Global step: 2277,loss: 0.021952905\n",
            "\n",
            "Global step: 2278,loss: 0.02208103\n",
            "\n",
            "Global step: 2279,loss: 0.02215531\n",
            "\n",
            "Global step: 2280,loss: 0.020725783\n",
            "\n",
            "Global step: 2281,loss: 0.019847214\n",
            "\n",
            "Global step: 2282,loss: 0.021323826\n",
            "\n",
            "Global step: 2283,loss: 0.020878915\n",
            "\n",
            "Global step: 2284,loss: 0.02067597\n",
            "\n",
            "Global step: 2285,loss: 0.022010596\n",
            "\n",
            "Global step: 2286,loss: 0.022014787\n",
            "\n",
            "Global step: 2287,loss: 0.02238901\n",
            "\n",
            "Global step: 2288,loss: 0.020735087\n",
            "\n",
            "Global step: 2289,loss: 0.020341331\n",
            "\n",
            "Global step: 2290,loss: 0.020569533\n",
            "\n",
            "Global step: 2291,loss: 0.021246474\n",
            "\n",
            "Global step: 2292,loss: 0.021077147\n",
            "\n",
            "Global step: 2293,loss: 0.020762404\n",
            "\n",
            "Global step: 2294,loss: 0.02128726\n",
            "\n",
            "Global step: 2295,loss: 0.021762414\n",
            "\n",
            "Global step: 2296,loss: 0.021800958\n",
            "\n",
            "Global step: 2297,loss: 0.021311864\n",
            "\n",
            "Global step: 2298,loss: 0.02071105\n",
            "\n",
            "Global step: 2299,loss: 0.021352248\n",
            "\n",
            "Global step: 2300,loss: 0.019983146\n",
            "\n",
            "Global step: 2301,loss: 0.021804716\n",
            "\n",
            "Global step: 2302,loss: 0.021153675\n",
            "\n",
            "Global step: 2303,loss: 0.019696442\n",
            "\n",
            "Global step: 2304,loss: 0.021862414\n",
            "\n",
            "Global step: 2305,loss: 0.0212211\n",
            "\n",
            "Global step: 2306,loss: 0.020711511\n",
            "\n",
            "Global step: 2307,loss: 0.020865042\n",
            "\n",
            "Global step: 2308,loss: 0.021712538\n",
            "\n",
            "Global step: 2309,loss: 0.020300647\n",
            "\n",
            "Global step: 2310,loss: 0.02054557\n",
            "\n",
            "Global step: 2311,loss: 0.022354592\n",
            "\n",
            "Global step: 2312,loss: 0.02069137\n",
            "\n",
            "Global step: 2313,loss: 0.02315138\n",
            "\n",
            "Global step: 2314,loss: 0.01979332\n",
            "\n",
            "Global step: 2315,loss: 0.02099338\n",
            "\n",
            "Global step: 2316,loss: 0.021765612\n",
            "\n",
            "Global step: 2317,loss: 0.021679599\n",
            "\n",
            "Global step: 2318,loss: 0.020755997\n",
            "\n",
            "Global step: 2319,loss: 0.020604452\n",
            "\n",
            "Global step: 2320,loss: 0.02032681\n",
            "\n",
            "Global step: 2321,loss: 0.022931352\n",
            "\n",
            "Global step: 2322,loss: 0.020200137\n",
            "\n",
            "Global step: 2323,loss: 0.021549992\n",
            "\n",
            "Global step: 2324,loss: 0.021558385\n",
            "\n",
            "Global step: 2325,loss: 0.020252684\n",
            "\n",
            "Global step: 2326,loss: 0.020694653\n",
            "\n",
            "Global step: 2327,loss: 0.021011056\n",
            "\n",
            "Global step: 2328,loss: 0.020838562\n",
            "\n",
            "Global step: 2329,loss: 0.01998464\n",
            "\n",
            "Global step: 2330,loss: 0.021962447\n",
            "\n",
            "Global step: 2331,loss: 0.022242451\n",
            "\n",
            "Global step: 2332,loss: 0.02013839\n",
            "\n",
            "Global step: 2333,loss: 0.022945546\n",
            "\n",
            "Global step: 2334,loss: 0.02015895\n",
            "\n",
            "Global step: 2335,loss: 0.020251041\n",
            "\n",
            "Global step: 2336,loss: 0.020184044\n",
            "\n",
            "Global step: 2337,loss: 0.02221965\n",
            "\n",
            "Global step: 2338,loss: 0.020455254\n",
            "\n",
            "Global step: 2339,loss: 0.020066766\n",
            "\n",
            "Global step: 2340,loss: 0.020121818\n",
            "\n",
            "Global step: 2341,loss: 0.021303523\n",
            "\n",
            "Global step: 2342,loss: 0.0207813\n",
            "\n",
            "Global step: 2343,loss: 0.020917716\n",
            "\n",
            "Global step: 2344,loss: 0.02064835\n",
            "\n",
            "Global step: 2345,loss: 0.021531565\n",
            "\n",
            "Global step: 2346,loss: 0.020014819\n",
            "\n",
            "Global step: 2347,loss: 0.020306466\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2348.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:36:29.322643 140167885084416 supervisor.py:1050] Recording summary at step 2348.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.98342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:36:29.354915 140166520051456 supervisor.py:1099] global_step/sec: 3.98342\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2348,loss: 0.019735439\n",
            "\n",
            "Global step: 2349,loss: 0.020066883\n",
            "\n",
            "Global step: 2350,loss: 0.020192962\n",
            "\n",
            "Global step: 2351,loss: 0.022371875\n",
            "\n",
            "Global step: 2352,loss: 0.021212466\n",
            "\n",
            "Global step: 2353,loss: 0.020430934\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2353,Val_Loss: 0.024384867007795134,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:36:33.376662 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 12/50:\n",
            "Global step: 2354,loss: 0.021232136\n",
            "\n",
            "Global step: 2355,loss: 0.021832975\n",
            "\n",
            "Global step: 2356,loss: 0.020243583\n",
            "\n",
            "Global step: 2357,loss: 0.020210346\n",
            "\n",
            "Global step: 2358,loss: 0.02502194\n",
            "\n",
            "Global step: 2359,loss: 0.020854205\n",
            "\n",
            "Global step: 2360,loss: 0.022429796\n",
            "\n",
            "Global step: 2361,loss: 0.019957533\n",
            "\n",
            "Global step: 2362,loss: 0.020261329\n",
            "\n",
            "Global step: 2363,loss: 0.020394674\n",
            "\n",
            "Global step: 2364,loss: 0.019544614\n",
            "\n",
            "Global step: 2365,loss: 0.020304581\n",
            "\n",
            "Global step: 2366,loss: 0.020192305\n",
            "\n",
            "Global step: 2367,loss: 0.019911842\n",
            "\n",
            "Global step: 2368,loss: 0.020125702\n",
            "\n",
            "Global step: 2369,loss: 0.020600094\n",
            "\n",
            "Global step: 2370,loss: 0.020594606\n",
            "\n",
            "Global step: 2371,loss: 0.020596597\n",
            "\n",
            "Global step: 2372,loss: 0.021907734\n",
            "\n",
            "Global step: 2373,loss: 0.020524511\n",
            "\n",
            "Global step: 2374,loss: 0.021289522\n",
            "\n",
            "Global step: 2375,loss: 0.018956268\n",
            "\n",
            "Global step: 2376,loss: 0.020287136\n",
            "\n",
            "Global step: 2377,loss: 0.021036468\n",
            "\n",
            "Global step: 2378,loss: 0.020777319\n",
            "\n",
            "Global step: 2379,loss: 0.020399068\n",
            "\n",
            "Global step: 2380,loss: 0.020627055\n",
            "\n",
            "Global step: 2381,loss: 0.020936463\n",
            "\n",
            "Global step: 2382,loss: 0.020064656\n",
            "\n",
            "Global step: 2383,loss: 0.019680513\n",
            "\n",
            "Global step: 2384,loss: 0.019914698\n",
            "\n",
            "Global step: 2385,loss: 0.019325389\n",
            "\n",
            "Global step: 2386,loss: 0.01986986\n",
            "\n",
            "Global step: 2387,loss: 0.020515334\n",
            "\n",
            "Global step: 2388,loss: 0.02114603\n",
            "\n",
            "Global step: 2389,loss: 0.020145942\n",
            "\n",
            "Global step: 2390,loss: 0.020877665\n",
            "\n",
            "Global step: 2391,loss: 0.019791514\n",
            "\n",
            "Global step: 2392,loss: 0.01933555\n",
            "\n",
            "Global step: 2393,loss: 0.019958885\n",
            "\n",
            "Global step: 2394,loss: 0.020684486\n",
            "\n",
            "Global step: 2395,loss: 0.020165116\n",
            "\n",
            "Global step: 2396,loss: 0.021751847\n",
            "\n",
            "Global step: 2397,loss: 0.020024154\n",
            "\n",
            "Global step: 2398,loss: 0.019990087\n",
            "\n",
            "Global step: 2399,loss: 0.020426724\n",
            "\n",
            "Global step: 2400,loss: 0.020285662\n",
            "\n",
            "Global step: 2401,loss: 0.020156851\n",
            "\n",
            "Global step: 2402,loss: 0.021591302\n",
            "\n",
            "Global step: 2403,loss: 0.02083568\n",
            "\n",
            "Global step: 2404,loss: 0.02044877\n",
            "\n",
            "Global step: 2405,loss: 0.020372113\n",
            "\n",
            "Global step: 2406,loss: 0.02099672\n",
            "\n",
            "Global step: 2407,loss: 0.020246947\n",
            "\n",
            "Global step: 2408,loss: 0.021132393\n",
            "\n",
            "Global step: 2409,loss: 0.020024609\n",
            "\n",
            "Global step: 2410,loss: 0.021094354\n",
            "\n",
            "Global step: 2411,loss: 0.020248402\n",
            "\n",
            "Global step: 2412,loss: 0.019900385\n",
            "\n",
            "Global step: 2413,loss: 0.019581594\n",
            "\n",
            "Global step: 2414,loss: 0.01919162\n",
            "\n",
            "Global step: 2415,loss: 0.020933\n",
            "\n",
            "Global step: 2416,loss: 0.020431368\n",
            "\n",
            "Global step: 2417,loss: 0.020440632\n",
            "\n",
            "Global step: 2418,loss: 0.021138553\n",
            "\n",
            "Global step: 2419,loss: 0.020610396\n",
            "\n",
            "Global step: 2420,loss: 0.0199953\n",
            "\n",
            "Global step: 2421,loss: 0.019479677\n",
            "\n",
            "Global step: 2422,loss: 0.019670164\n",
            "\n",
            "Global step: 2423,loss: 0.019676672\n",
            "\n",
            "Global step: 2424,loss: 0.019886373\n",
            "\n",
            "Global step: 2425,loss: 0.019687515\n",
            "\n",
            "Global step: 2426,loss: 0.019296542\n",
            "\n",
            "Global step: 2427,loss: 0.020424757\n",
            "\n",
            "Global step: 2428,loss: 0.019707652\n",
            "\n",
            "Global step: 2429,loss: 0.019439457\n",
            "\n",
            "Global step: 2430,loss: 0.020048043\n",
            "\n",
            "Global step: 2431,loss: 0.02112619\n",
            "\n",
            "Global step: 2432,loss: 0.020239066\n",
            "\n",
            "Global step: 2433,loss: 0.0192723\n",
            "\n",
            "Global step: 2434,loss: 0.020325255\n",
            "\n",
            "Global step: 2435,loss: 0.019418623\n",
            "\n",
            "Global step: 2436,loss: 0.018608551\n",
            "\n",
            "Global step: 2437,loss: 0.020295955\n",
            "\n",
            "Global step: 2438,loss: 0.02072407\n",
            "\n",
            "Global step: 2439,loss: 0.019884244\n",
            "\n",
            "Global step: 2440,loss: 0.020428957\n",
            "\n",
            "Global step: 2441,loss: 0.022417683\n",
            "\n",
            "Global step: 2442,loss: 0.02119352\n",
            "\n",
            "Global step: 2443,loss: 0.019400023\n",
            "\n",
            "Global step: 2444,loss: 0.01901155\n",
            "\n",
            "Global step: 2445,loss: 0.020932594\n",
            "\n",
            "Global step: 2446,loss: 0.019260615\n",
            "\n",
            "Global step: 2447,loss: 0.020403728\n",
            "\n",
            "Global step: 2448,loss: 0.019032087\n",
            "\n",
            "Global step: 2449,loss: 0.020030798\n",
            "\n",
            "Global step: 2450,loss: 0.019800868\n",
            "\n",
            "Global step: 2451,loss: 0.019773034\n",
            "\n",
            "Global step: 2452,loss: 0.021205612\n",
            "\n",
            "Global step: 2453,loss: 0.018685311\n",
            "\n",
            "Global step: 2454,loss: 0.019485526\n",
            "\n",
            "Global step: 2455,loss: 0.022136042\n",
            "\n",
            "Global step: 2456,loss: 0.021091135\n",
            "\n",
            "Global step: 2457,loss: 0.020802677\n",
            "\n",
            "Global step: 2458,loss: 0.019937921\n",
            "\n",
            "Global step: 2459,loss: 0.02061319\n",
            "\n",
            "Global step: 2460,loss: 0.018909954\n",
            "\n",
            "Global step: 2461,loss: 0.020531936\n",
            "\n",
            "Global step: 2462,loss: 0.020802416\n",
            "\n",
            "Global step: 2463,loss: 0.019320229\n",
            "\n",
            "Global step: 2464,loss: 0.022256017\n",
            "\n",
            "Global step: 2465,loss: 0.019731166\n",
            "\n",
            "Global step: 2466,loss: 0.019191148\n",
            "\n",
            "Global step: 2467,loss: 0.020240495\n",
            "\n",
            "Global step: 2468,loss: 0.01998642\n",
            "\n",
            "Global step: 2469,loss: 0.020588266\n",
            "\n",
            "Global step: 2470,loss: 0.021368615\n",
            "\n",
            "Global step: 2471,loss: 0.02089271\n",
            "\n",
            "Global step: 2472,loss: 0.022325369\n",
            "\n",
            "Global step: 2473,loss: 0.020128876\n",
            "\n",
            "Global step: 2474,loss: 0.020865042\n",
            "\n",
            "Global step: 2475,loss: 0.020456381\n",
            "\n",
            "Global step: 2476,loss: 0.01978764\n",
            "\n",
            "Global step: 2477,loss: 0.020304304\n",
            "\n",
            "Global step: 2478,loss: 0.020714749\n",
            "\n",
            "Global step: 2479,loss: 0.019486653\n",
            "\n",
            "Global step: 2480,loss: 0.019065723\n",
            "\n",
            "Global step: 2481,loss: 0.019740792\n",
            "\n",
            "Global step: 2482,loss: 0.02013257\n",
            "\n",
            "Global step: 2483,loss: 0.021556178\n",
            "\n",
            "Global step: 2484,loss: 0.019407766\n",
            "\n",
            "Global step: 2485,loss: 0.019439558\n",
            "\n",
            "Global step: 2486,loss: 0.020554429\n",
            "\n",
            "Global step: 2487,loss: 0.020683022\n",
            "\n",
            "Global step: 2488,loss: 0.021397093\n",
            "\n",
            "Global step: 2489,loss: 0.019803258\n",
            "\n",
            "Global step: 2490,loss: 0.019654978\n",
            "\n",
            "Global step: 2491,loss: 0.018348737\n",
            "\n",
            "Global step: 2492,loss: 0.01973364\n",
            "\n",
            "Global step: 2493,loss: 0.0203584\n",
            "\n",
            "Global step: 2494,loss: 0.02106439\n",
            "\n",
            "Global step: 2495,loss: 0.019754643\n",
            "\n",
            "Global step: 2496,loss: 0.019334232\n",
            "\n",
            "Global step: 2497,loss: 0.020195952\n",
            "\n",
            "Global step: 2498,loss: 0.019550508\n",
            "\n",
            "Global step: 2499,loss: 0.020254679\n",
            "\n",
            "Global step: 2500,loss: 0.019553479\n",
            "\n",
            "Global step: 2501,loss: 0.019362865\n",
            "\n",
            "Global step: 2502,loss: 0.021036299\n",
            "\n",
            "Global step: 2503,loss: 0.01928744\n",
            "\n",
            "Global step: 2504,loss: 0.01945999\n",
            "\n",
            "Global step: 2505,loss: 0.019550372\n",
            "\n",
            "Global step: 2506,loss: 0.020240225\n",
            "\n",
            "Global step: 2507,loss: 0.01952827\n",
            "\n",
            "Global step: 2508,loss: 0.021346191\n",
            "\n",
            "Global step: 2509,loss: 0.01927339\n",
            "\n",
            "Global step: 2510,loss: 0.019979207\n",
            "\n",
            "Global step: 2511,loss: 0.019069187\n",
            "\n",
            "Global step: 2512,loss: 0.020235416\n",
            "\n",
            "Global step: 2513,loss: 0.019972848\n",
            "\n",
            "Global step: 2514,loss: 0.020651525\n",
            "\n",
            "Global step: 2515,loss: 0.019489653\n",
            "\n",
            "Global step: 2516,loss: 0.022464933\n",
            "\n",
            "Global step: 2517,loss: 0.019888464\n",
            "\n",
            "Global step: 2518,loss: 0.020370822\n",
            "\n",
            "Global step: 2519,loss: 0.020066256\n",
            "\n",
            "Global step: 2520,loss: 0.01970307\n",
            "\n",
            "Global step: 2521,loss: 0.020848721\n",
            "\n",
            "Global step: 2522,loss: 0.020557515\n",
            "\n",
            "Global step: 2523,loss: 0.019188682\n",
            "\n",
            "Global step: 2524,loss: 0.023418847\n",
            "\n",
            "Global step: 2525,loss: 0.019903585\n",
            "\n",
            "Global step: 2526,loss: 0.020508759\n",
            "\n",
            "Global step: 2527,loss: 0.02055454\n",
            "\n",
            "Global step: 2528,loss: 0.01950068\n",
            "\n",
            "Global step: 2529,loss: 0.019507231\n",
            "\n",
            "Global step: 2530,loss: 0.022461643\n",
            "\n",
            "Global step: 2531,loss: 0.020514539\n",
            "\n",
            "Global step: 2532,loss: 0.019682059\n",
            "\n",
            "Global step: 2533,loss: 0.019962111\n",
            "\n",
            "Global step: 2534,loss: 0.019840255\n",
            "\n",
            "Global step: 2535,loss: 0.02008281\n",
            "\n",
            "Global step: 2536,loss: 0.020559913\n",
            "\n",
            "Global step: 2537,loss: 0.020379653\n",
            "\n",
            "Global step: 2538,loss: 0.021644875\n",
            "\n",
            "Global step: 2539,loss: 0.021546679\n",
            "\n",
            "Global step: 2540,loss: 0.019881967\n",
            "\n",
            "Global step: 2541,loss: 0.021007326\n",
            "\n",
            "Global step: 2542,loss: 0.018698793\n",
            "\n",
            "Global step: 2543,loss: 0.022000518\n",
            "\n",
            "Global step: 2544,loss: 0.019160569\n",
            "\n",
            "Global step: 2545,loss: 0.018613935\n",
            "\n",
            "Global step: 2546,loss: 0.021106197\n",
            "\n",
            "Global step: 2547,loss: 0.019510187\n",
            "\n",
            "Global step: 2548,loss: 0.019177223\n",
            "\n",
            "Global step: 2549,loss: 0.020302508\n",
            "\n",
            "Global step: 2550,loss: 0.019582847\n",
            "\n",
            "Global step: 2551,loss: 0.01983702\n",
            "\n",
            "Global step: 2552,loss: 0.019154485\n",
            "\n",
            "Global step: 2553,loss: 0.020378526\n",
            "\n",
            "Global step: 2554,loss: 0.019005721\n",
            "\n",
            "Global step: 2555,loss: 0.020949122\n",
            "\n",
            "Global step: 2556,loss: 0.021559436\n",
            "\n",
            "Global step: 2557,loss: 0.019607669\n",
            "\n",
            "Global step: 2558,loss: 0.019476004\n",
            "\n",
            "Global step: 2559,loss: 0.019397091\n",
            "\n",
            "Global step: 2560,loss: 0.019585613\n",
            "\n",
            "Global step: 2561,loss: 0.019232921\n",
            "\n",
            "Global step: 2562,loss: 0.019172657\n",
            "\n",
            "Global step: 2563,loss: 0.021381196\n",
            "\n",
            "Global step: 2564,loss: 0.019213667\n",
            "\n",
            "Global step: 2565,loss: 0.021238808\n",
            "\n",
            "Global step: 2566,loss: 0.019373128\n",
            "\n",
            "Global step: 2567,loss: 0.019604895\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2567,Val_Loss: 0.02317980627872442,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:37:27.416066 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 13/50:\n",
            "Global step: 2568,loss: 0.019283496\n",
            "\n",
            "Global step: 2569,loss: 0.019655082\n",
            "\n",
            "Global step: 2570,loss: 0.019382443\n",
            "\n",
            "Global step: 2571,loss: 0.021085769\n",
            "\n",
            "Global step: 2572,loss: 0.02044113\n",
            "\n",
            "Global step: 2573,loss: 0.01798215\n",
            "\n",
            "Global step: 2574,loss: 0.01911417\n",
            "\n",
            "Global step: 2575,loss: 0.019861728\n",
            "\n",
            "Global step: 2576,loss: 0.019855067\n",
            "\n",
            "Global step: 2577,loss: 0.019138578\n",
            "\n",
            "Global step: 2578,loss: 0.019174319\n",
            "\n",
            "Global step: 2579,loss: 0.018368559\n",
            "\n",
            "Global step: 2580,loss: 0.020012768\n",
            "\n",
            "Global step: 2581,loss: 0.019830946\n",
            "\n",
            "Global step: 2582,loss: 0.01897087\n",
            "\n",
            "Global step: 2583,loss: 0.019230207\n",
            "\n",
            "Global step: 2584,loss: 0.0188095\n",
            "\n",
            "Global step: 2585,loss: 0.019728092\n",
            "\n",
            "Global step: 2586,loss: 0.018982638\n",
            "\n",
            "Global step: 2587,loss: 0.018588293\n",
            "\n",
            "Global step: 2588,loss: 0.019469537\n",
            "\n",
            "Global step: 2589,loss: 0.020383826\n",
            "\n",
            "Global step: 2590,loss: 0.018340347\n",
            "\n",
            "Global step: 2591,loss: 0.019386781\n",
            "\n",
            "Global step: 2592,loss: 0.01982113\n",
            "\n",
            "Global step: 2593,loss: 0.019729093\n",
            "\n",
            "Global step: 2594,loss: 0.019169718\n",
            "\n",
            "Global step: 2595,loss: 0.018786673\n",
            "\n",
            "Global step: 2596,loss: 0.020026\n",
            "\n",
            "Global step: 2597,loss: 0.01927972\n",
            "\n",
            "Global step: 2598,loss: 0.020905405\n",
            "\n",
            "Global step: 2599,loss: 0.019773986\n",
            "\n",
            "Global step: 2600,loss: 0.020054081\n",
            "\n",
            "Global step: 2601,loss: 0.019077169\n",
            "\n",
            "Global step: 2602,loss: 0.019741474\n",
            "\n",
            "Global step: 2603,loss: 0.020037813\n",
            "\n",
            "Global step: 2604,loss: 0.019223176\n",
            "\n",
            "Global step: 2605,loss: 0.0189328\n",
            "\n",
            "Global step: 2606,loss: 0.019061042\n",
            "\n",
            "Global step: 2607,loss: 0.019512177\n",
            "\n",
            "Global step: 2608,loss: 0.019204356\n",
            "\n",
            "Global step: 2609,loss: 0.01938961\n",
            "\n",
            "Global step: 2610,loss: 0.020048665\n",
            "\n",
            "Global step: 2611,loss: 0.020126965\n",
            "\n",
            "Global step: 2612,loss: 0.019183133\n",
            "\n",
            "Global step: 2613,loss: 0.019498866\n",
            "\n",
            "Global step: 2614,loss: 0.019814184\n",
            "\n",
            "Global step: 2615,loss: 0.018204318\n",
            "\n",
            "Global step: 2616,loss: 0.018908247\n",
            "\n",
            "Global step: 2617,loss: 0.019593464\n",
            "\n",
            "Global step: 2618,loss: 0.01963215\n",
            "\n",
            "Global step: 2619,loss: 0.018911991\n",
            "\n",
            "Global step: 2620,loss: 0.020163137\n",
            "\n",
            "Global step: 2621,loss: 0.018286489\n",
            "\n",
            "Global step: 2622,loss: 0.02016918\n",
            "\n",
            "Global step: 2623,loss: 0.018479127\n",
            "\n",
            "Global step: 2624,loss: 0.01860442\n",
            "\n",
            "Global step: 2625,loss: 0.022656558\n",
            "\n",
            "Global step: 2626,loss: 0.02054245\n",
            "\n",
            "Global step: 2627,loss: 0.019143634\n",
            "\n",
            "Global step: 2628,loss: 0.019847915\n",
            "\n",
            "Global step: 2629,loss: 0.019148117\n",
            "\n",
            "Global step: 2630,loss: 0.019505452\n",
            "\n",
            "Global step: 2631,loss: 0.019991029\n",
            "\n",
            "Global step: 2632,loss: 0.018819753\n",
            "\n",
            "Global step: 2633,loss: 0.01922906\n",
            "\n",
            "Global step: 2634,loss: 0.01930509\n",
            "\n",
            "Global step: 2635,loss: 0.019268677\n",
            "\n",
            "Global step: 2636,loss: 0.019282445\n",
            "\n",
            "Global step: 2637,loss: 0.01995955\n",
            "\n",
            "Global step: 2638,loss: 0.018656591\n",
            "\n",
            "Global step: 2639,loss: 0.01953018\n",
            "\n",
            "Global step: 2640,loss: 0.019660564\n",
            "\n",
            "Global step: 2641,loss: 0.019251488\n",
            "\n",
            "Global step: 2642,loss: 0.01995342\n",
            "\n",
            "Global step: 2643,loss: 0.018913861\n",
            "\n",
            "Global step: 2644,loss: 0.018559571\n",
            "\n",
            "Global step: 2645,loss: 0.01882816\n",
            "\n",
            "Global step: 2646,loss: 0.019935321\n",
            "\n",
            "Global step: 2647,loss: 0.018706711\n",
            "\n",
            "Global step: 2648,loss: 0.018731082\n",
            "\n",
            "Global step: 2649,loss: 0.018811755\n",
            "\n",
            "Global step: 2650,loss: 0.019676413\n",
            "\n",
            "Global step: 2651,loss: 0.01918955\n",
            "\n",
            "Global step: 2652,loss: 0.01883669\n",
            "\n",
            "Global step: 2653,loss: 0.017984463\n",
            "\n",
            "Global step: 2654,loss: 0.01888521\n",
            "\n",
            "Global step: 2655,loss: 0.01990416\n",
            "\n",
            "Global step: 2656,loss: 0.019019727\n",
            "\n",
            "Global step: 2657,loss: 0.01851958\n",
            "\n",
            "Global step: 2658,loss: 0.018593216\n",
            "\n",
            "Global step: 2659,loss: 0.018717466\n",
            "\n",
            "Global step: 2660,loss: 0.018503837\n",
            "\n",
            "Global step: 2661,loss: 0.019707406\n",
            "\n",
            "Global step: 2662,loss: 0.01890698\n",
            "\n",
            "Global step: 2663,loss: 0.019361215\n",
            "\n",
            "Global step: 2664,loss: 0.019073045\n",
            "\n",
            "Global step: 2665,loss: 0.020272663\n",
            "\n",
            "Global step: 2666,loss: 0.019061357\n",
            "\n",
            "Global step: 2667,loss: 0.018321011\n",
            "\n",
            "Global step: 2668,loss: 0.0186298\n",
            "\n",
            "Global step: 2669,loss: 0.018736009\n",
            "\n",
            "Global step: 2670,loss: 0.018846085\n",
            "\n",
            "Global step: 2671,loss: 0.018008541\n",
            "\n",
            "Global step: 2672,loss: 0.018310053\n",
            "\n",
            "Global step: 2673,loss: 0.019070571\n",
            "\n",
            "Global step: 2674,loss: 0.018314028\n",
            "\n",
            "Global step: 2675,loss: 0.020348368\n",
            "\n",
            "Global step: 2676,loss: 0.018542748\n",
            "\n",
            "Global step: 2677,loss: 0.018351614\n",
            "\n",
            "Global step: 2678,loss: 0.019730443\n",
            "\n",
            "Global step: 2679,loss: 0.019300414\n",
            "\n",
            "Global step: 2680,loss: 0.01913513\n",
            "\n",
            "Global step: 2681,loss: 0.01921136\n",
            "\n",
            "Global step: 2682,loss: 0.018786404\n",
            "\n",
            "Global step: 2683,loss: 0.019386655\n",
            "\n",
            "Global step: 2684,loss: 0.018840266\n",
            "\n",
            "Global step: 2685,loss: 0.019623717\n",
            "\n",
            "Global step: 2686,loss: 0.017782139\n",
            "\n",
            "Global step: 2687,loss: 0.019428734\n",
            "\n",
            "Global step: 2688,loss: 0.018579898\n",
            "\n",
            "Global step: 2689,loss: 0.018766526\n",
            "\n",
            "Global step: 2690,loss: 0.018961603\n",
            "\n",
            "Global step: 2691,loss: 0.019086529\n",
            "\n",
            "Global step: 2692,loss: 0.018143304\n",
            "\n",
            "Global step: 2693,loss: 0.018616745\n",
            "\n",
            "Global step: 2694,loss: 0.019424042\n",
            "\n",
            "Global step: 2695,loss: 0.01975261\n",
            "\n",
            "Global step: 2696,loss: 0.019283228\n",
            "\n",
            "Global step: 2697,loss: 0.019190583\n",
            "\n",
            "Global step: 2698,loss: 0.019113215\n",
            "\n",
            "Global step: 2699,loss: 0.0191462\n",
            "\n",
            "Global step: 2700,loss: 0.017820882\n",
            "\n",
            "Global step: 2701,loss: 0.022270385\n",
            "\n",
            "Global step: 2702,loss: 0.018367665\n",
            "\n",
            "Global step: 2703,loss: 0.021126889\n",
            "\n",
            "Global step: 2704,loss: 0.018266933\n",
            "\n",
            "Global step: 2705,loss: 0.01868002\n",
            "\n",
            "Global step: 2706,loss: 0.019189894\n",
            "\n",
            "Global step: 2707,loss: 0.019787868\n",
            "\n",
            "Global step: 2708,loss: 0.018676564\n",
            "\n",
            "Global step: 2709,loss: 0.018623998\n",
            "\n",
            "Global step: 2710,loss: 0.019596918\n",
            "\n",
            "Global step: 2711,loss: 0.018793441\n",
            "\n",
            "Global step: 2712,loss: 0.01855139\n",
            "\n",
            "Global step: 2713,loss: 0.018423488\n",
            "\n",
            "Global step: 2714,loss: 0.018771224\n",
            "\n",
            "Global step: 2715,loss: 0.018581433\n",
            "\n",
            "Global step: 2716,loss: 0.019544104\n",
            "\n",
            "Global step: 2717,loss: 0.018973125\n",
            "\n",
            "Global step: 2718,loss: 0.018485855\n",
            "\n",
            "Global step: 2719,loss: 0.01908201\n",
            "\n",
            "Global step: 2720,loss: 0.019418512\n",
            "\n",
            "Global step: 2721,loss: 0.01901975\n",
            "\n",
            "Global step: 2722,loss: 0.01968733\n",
            "\n",
            "Global step: 2723,loss: 0.017885284\n",
            "\n",
            "Global step: 2724,loss: 0.018206807\n",
            "\n",
            "Global step: 2725,loss: 0.01832985\n",
            "\n",
            "Global step: 2726,loss: 0.018623795\n",
            "\n",
            "Global step: 2727,loss: 0.019874401\n",
            "\n",
            "Global step: 2728,loss: 0.019227551\n",
            "\n",
            "Global step: 2729,loss: 0.018830154\n",
            "\n",
            "Global step: 2730,loss: 0.018930554\n",
            "\n",
            "Global step: 2731,loss: 0.020058492\n",
            "\n",
            "Global step: 2732,loss: 0.019259643\n",
            "\n",
            "Global step: 2733,loss: 0.019458931\n",
            "\n",
            "Global step: 2734,loss: 0.021037772\n",
            "\n",
            "Global step: 2735,loss: 0.01865173\n",
            "\n",
            "Global step: 2736,loss: 0.019284165\n",
            "\n",
            "Global step: 2737,loss: 0.019245889\n",
            "\n",
            "Global step: 2738,loss: 0.019139985\n",
            "\n",
            "Global step: 2739,loss: 0.019673264\n",
            "\n",
            "Global step: 2740,loss: 0.018428234\n",
            "\n",
            "Global step: 2741,loss: 0.01906518\n",
            "\n",
            "Global step: 2742,loss: 0.019674016\n",
            "\n",
            "Global step: 2743,loss: 0.018791208\n",
            "\n",
            "Global step: 2744,loss: 0.018058082\n",
            "\n",
            "Global step: 2745,loss: 0.019499201\n",
            "\n",
            "Global step: 2746,loss: 0.018080723\n",
            "\n",
            "Global step: 2747,loss: 0.018894363\n",
            "\n",
            "Global step: 2748,loss: 0.01855366\n",
            "\n",
            "Global step: 2749,loss: 0.018154558\n",
            "\n",
            "Global step: 2750,loss: 0.020369437\n",
            "\n",
            "Global step: 2751,loss: 0.018081428\n",
            "\n",
            "Global step: 2752,loss: 0.018983832\n",
            "\n",
            "Global step: 2753,loss: 0.018460976\n",
            "\n",
            "Global step: 2754,loss: 0.018726239\n",
            "\n",
            "Global step: 2755,loss: 0.022221647\n",
            "\n",
            "Global step: 2756,loss: 0.018382218\n",
            "\n",
            "Global step: 2757,loss: 0.018815288\n",
            "\n",
            "Global step: 2758,loss: 0.017629616\n",
            "\n",
            "Global step: 2759,loss: 0.019014526\n",
            "\n",
            "Global step: 2760,loss: 0.019708168\n",
            "\n",
            "Global step: 2761,loss: 0.019960728\n",
            "\n",
            "Global step: 2762,loss: 0.019503128\n",
            "\n",
            "Global step: 2763,loss: 0.017861739\n",
            "\n",
            "Global step: 2764,loss: 0.01835313\n",
            "\n",
            "Global step: 2765,loss: 0.018508274\n",
            "\n",
            "Global step: 2766,loss: 0.01821956\n",
            "\n",
            "Global step: 2767,loss: 0.01824468\n",
            "\n",
            "Global step: 2768,loss: 0.018433936\n",
            "\n",
            "Global step: 2769,loss: 0.019117508\n",
            "\n",
            "Global step: 2770,loss: 0.01903052\n",
            "\n",
            "Global step: 2771,loss: 0.018668344\n",
            "\n",
            "Global step: 2772,loss: 0.018700616\n",
            "\n",
            "Global step: 2773,loss: 0.017837184\n",
            "\n",
            "Global step: 2774,loss: 0.018422386\n",
            "\n",
            "Global step: 2775,loss: 0.01971334\n",
            "\n",
            "Global step: 2776,loss: 0.019271148\n",
            "\n",
            "Global step: 2777,loss: 0.017978398\n",
            "\n",
            "Global step: 2778,loss: 0.018014655\n",
            "\n",
            "Global step: 2779,loss: 0.019034952\n",
            "\n",
            "Global step: 2780,loss: 0.017928366\n",
            "\n",
            "Global step: 2781,loss: 0.017553061\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2781,Val_Loss: 0.022403678121535403,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:38:21.605304 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 14/50:\n",
            "Global step: 2782,loss: 0.017764537\n",
            "\n",
            "Global step: 2783,loss: 0.018246649\n",
            "\n",
            "Global step: 2784,loss: 0.018330082\n",
            "\n",
            "Global step: 2785,loss: 0.018197509\n",
            "\n",
            "Global step: 2786,loss: 0.019632827\n",
            "\n",
            "Global step: 2787,loss: 0.01897365\n",
            "\n",
            "Global step: 2788,loss: 0.018590987\n",
            "\n",
            "Global step: 2789,loss: 0.01718521\n",
            "\n",
            "Global step: 2790,loss: 0.018769711\n",
            "\n",
            "Global step: 2791,loss: 0.019008296\n",
            "\n",
            "Global step: 2792,loss: 0.018769\n",
            "\n",
            "Global step: 2793,loss: 0.018408243\n",
            "\n",
            "Global step: 2794,loss: 0.017939167\n",
            "\n",
            "Global step: 2795,loss: 0.017954672\n",
            "\n",
            "Global step: 2796,loss: 0.017804746\n",
            "\n",
            "Global step: 2797,loss: 0.018838895\n",
            "\n",
            "Global step: 2798,loss: 0.019729618\n",
            "\n",
            "Global step: 2799,loss: 0.019759145\n",
            "\n",
            "Global step: 2800,loss: 0.018602828\n",
            "\n",
            "Global step: 2801,loss: 0.018727602\n",
            "\n",
            "Global step: 2802,loss: 0.018993482\n",
            "\n",
            "Global step: 2803,loss: 0.018442024\n",
            "\n",
            "Global step: 2804,loss: 0.017440174\n",
            "\n",
            "Global step: 2805,loss: 0.018381638\n",
            "\n",
            "Global step: 2806,loss: 0.018010579\n",
            "\n",
            "Global step: 2807,loss: 0.018932808\n",
            "\n",
            "Global step: 2808,loss: 0.018484602\n",
            "\n",
            "Global step: 2809,loss: 0.018243022\n",
            "\n",
            "Global step: 2810,loss: 0.018732293\n",
            "\n",
            "Global step: 2811,loss: 0.019404214\n",
            "\n",
            "Global step: 2812,loss: 0.018048255\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 3.87501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:38:29.354784 140166520051456 supervisor.py:1099] global_step/sec: 3.87501\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2813.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:38:29.355097 140167885084416 supervisor.py:1050] Recording summary at step 2813.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2813,loss: 0.018149367\n",
            "\n",
            "Global step: 2814,loss: 0.018456565\n",
            "\n",
            "Global step: 2815,loss: 0.018575087\n",
            "\n",
            "Global step: 2816,loss: 0.017887976\n",
            "\n",
            "Global step: 2817,loss: 0.019588582\n",
            "\n",
            "Global step: 2818,loss: 0.018332109\n",
            "\n",
            "Global step: 2819,loss: 0.017682193\n",
            "\n",
            "Global step: 2820,loss: 0.019435944\n",
            "\n",
            "Global step: 2821,loss: 0.0187973\n",
            "\n",
            "Global step: 2822,loss: 0.018570276\n",
            "\n",
            "Global step: 2823,loss: 0.018705359\n",
            "\n",
            "Global step: 2824,loss: 0.017628903\n",
            "\n",
            "Global step: 2825,loss: 0.017903993\n",
            "\n",
            "Global step: 2826,loss: 0.01861116\n",
            "\n",
            "Global step: 2827,loss: 0.018981561\n",
            "\n",
            "Global step: 2828,loss: 0.018508852\n",
            "\n",
            "Global step: 2829,loss: 0.018795352\n",
            "\n",
            "Global step: 2830,loss: 0.01919018\n",
            "\n",
            "Global step: 2831,loss: 0.018286036\n",
            "\n",
            "Global step: 2832,loss: 0.020814382\n",
            "\n",
            "Global step: 2833,loss: 0.018501101\n",
            "\n",
            "Global step: 2834,loss: 0.018415526\n",
            "\n",
            "Global step: 2835,loss: 0.018416326\n",
            "\n",
            "Global step: 2836,loss: 0.017750075\n",
            "\n",
            "Global step: 2837,loss: 0.018580088\n",
            "\n",
            "Global step: 2838,loss: 0.018097904\n",
            "\n",
            "Global step: 2839,loss: 0.018766515\n",
            "\n",
            "Global step: 2840,loss: 0.016992329\n",
            "\n",
            "Global step: 2841,loss: 0.018903991\n",
            "\n",
            "Global step: 2842,loss: 0.018338488\n",
            "\n",
            "Global step: 2843,loss: 0.018010851\n",
            "\n",
            "Global step: 2844,loss: 0.018626912\n",
            "\n",
            "Global step: 2845,loss: 0.018866807\n",
            "\n",
            "Global step: 2846,loss: 0.01805012\n",
            "\n",
            "Global step: 2847,loss: 0.017702064\n",
            "\n",
            "Global step: 2848,loss: 0.0179857\n",
            "\n",
            "Global step: 2849,loss: 0.017504292\n",
            "\n",
            "Global step: 2850,loss: 0.01865806\n",
            "\n",
            "Global step: 2851,loss: 0.018455796\n",
            "\n",
            "Global step: 2852,loss: 0.018385032\n",
            "\n",
            "Global step: 2853,loss: 0.019325636\n",
            "\n",
            "Global step: 2854,loss: 0.017239965\n",
            "\n",
            "Global step: 2855,loss: 0.019063761\n",
            "\n",
            "Global step: 2856,loss: 0.018464401\n",
            "\n",
            "Global step: 2857,loss: 0.018788848\n",
            "\n",
            "Global step: 2858,loss: 0.017892322\n",
            "\n",
            "Global step: 2859,loss: 0.017809581\n",
            "\n",
            "Global step: 2860,loss: 0.019270977\n",
            "\n",
            "Global step: 2861,loss: 0.017962875\n",
            "\n",
            "Global step: 2862,loss: 0.01800013\n",
            "\n",
            "Global step: 2863,loss: 0.018289533\n",
            "\n",
            "Global step: 2864,loss: 0.019547317\n",
            "\n",
            "Global step: 2865,loss: 0.019791296\n",
            "\n",
            "Global step: 2866,loss: 0.018229185\n",
            "\n",
            "Global step: 2867,loss: 0.017845234\n",
            "\n",
            "Global step: 2868,loss: 0.018491087\n",
            "\n",
            "Global step: 2869,loss: 0.018443601\n",
            "\n",
            "Global step: 2870,loss: 0.019014819\n",
            "\n",
            "Global step: 2871,loss: 0.017812833\n",
            "\n",
            "Global step: 2872,loss: 0.017819982\n",
            "\n",
            "Global step: 2873,loss: 0.019132785\n",
            "\n",
            "Global step: 2874,loss: 0.01885921\n",
            "\n",
            "Global step: 2875,loss: 0.017383184\n",
            "\n",
            "Global step: 2876,loss: 0.018310482\n",
            "\n",
            "Global step: 2877,loss: 0.017985877\n",
            "\n",
            "Global step: 2878,loss: 0.018746154\n",
            "\n",
            "Global step: 2879,loss: 0.018092379\n",
            "\n",
            "Global step: 2880,loss: 0.017736733\n",
            "\n",
            "Global step: 2881,loss: 0.018656833\n",
            "\n",
            "Global step: 2882,loss: 0.017956905\n",
            "\n",
            "Global step: 2883,loss: 0.017726118\n",
            "\n",
            "Global step: 2884,loss: 0.018922051\n",
            "\n",
            "Global step: 2885,loss: 0.017369466\n",
            "\n",
            "Global step: 2886,loss: 0.01806887\n",
            "\n",
            "Global step: 2887,loss: 0.018317627\n",
            "\n",
            "Global step: 2888,loss: 0.018892482\n",
            "\n",
            "Global step: 2889,loss: 0.016912304\n",
            "\n",
            "Global step: 2890,loss: 0.019005006\n",
            "\n",
            "Global step: 2891,loss: 0.018081576\n",
            "\n",
            "Global step: 2892,loss: 0.019831078\n",
            "\n",
            "Global step: 2893,loss: 0.01870423\n",
            "\n",
            "Global step: 2894,loss: 0.018067926\n",
            "\n",
            "Global step: 2895,loss: 0.018083664\n",
            "\n",
            "Global step: 2896,loss: 0.018770268\n",
            "\n",
            "Global step: 2897,loss: 0.018274192\n",
            "\n",
            "Global step: 2898,loss: 0.018402157\n",
            "\n",
            "Global step: 2899,loss: 0.018365378\n",
            "\n",
            "Global step: 2900,loss: 0.017146852\n",
            "\n",
            "Global step: 2901,loss: 0.017608982\n",
            "\n",
            "Global step: 2902,loss: 0.017856907\n",
            "\n",
            "Global step: 2903,loss: 0.017801013\n",
            "\n",
            "Global step: 2904,loss: 0.01877479\n",
            "\n",
            "Global step: 2905,loss: 0.01747805\n",
            "\n",
            "Global step: 2906,loss: 0.018070966\n",
            "\n",
            "Global step: 2907,loss: 0.017391782\n",
            "\n",
            "Global step: 2908,loss: 0.019875228\n",
            "\n",
            "Global step: 2909,loss: 0.017804366\n",
            "\n",
            "Global step: 2910,loss: 0.018886043\n",
            "\n",
            "Global step: 2911,loss: 0.01890445\n",
            "\n",
            "Global step: 2912,loss: 0.017716134\n",
            "\n",
            "Global step: 2913,loss: 0.017500216\n",
            "\n",
            "Global step: 2914,loss: 0.017647658\n",
            "\n",
            "Global step: 2915,loss: 0.018460447\n",
            "\n",
            "Global step: 2916,loss: 0.017314753\n",
            "\n",
            "Global step: 2917,loss: 0.018003833\n",
            "\n",
            "Global step: 2918,loss: 0.018133804\n",
            "\n",
            "Global step: 2919,loss: 0.01871212\n",
            "\n",
            "Global step: 2920,loss: 0.018288326\n",
            "\n",
            "Global step: 2921,loss: 0.017696539\n",
            "\n",
            "Global step: 2922,loss: 0.018262593\n",
            "\n",
            "Global step: 2923,loss: 0.017404992\n",
            "\n",
            "Global step: 2924,loss: 0.0181735\n",
            "\n",
            "Global step: 2925,loss: 0.018440165\n",
            "\n",
            "Global step: 2926,loss: 0.01811402\n",
            "\n",
            "Global step: 2927,loss: 0.018030254\n",
            "\n",
            "Global step: 2928,loss: 0.018018294\n",
            "\n",
            "Global step: 2929,loss: 0.017488264\n",
            "\n",
            "Global step: 2930,loss: 0.017495686\n",
            "\n",
            "Global step: 2931,loss: 0.017997487\n",
            "\n",
            "Global step: 2932,loss: 0.017764403\n",
            "\n",
            "Global step: 2933,loss: 0.018136093\n",
            "\n",
            "Global step: 2934,loss: 0.018384606\n",
            "\n",
            "Global step: 2935,loss: 0.018443838\n",
            "\n",
            "Global step: 2936,loss: 0.018050302\n",
            "\n",
            "Global step: 2937,loss: 0.01828428\n",
            "\n",
            "Global step: 2938,loss: 0.017790945\n",
            "\n",
            "Global step: 2939,loss: 0.017826647\n",
            "\n",
            "Global step: 2940,loss: 0.01839852\n",
            "\n",
            "Global step: 2941,loss: 0.018122392\n",
            "\n",
            "Global step: 2942,loss: 0.017572274\n",
            "\n",
            "Global step: 2943,loss: 0.017918801\n",
            "\n",
            "Global step: 2944,loss: 0.018627938\n",
            "\n",
            "Global step: 2945,loss: 0.01810262\n",
            "\n",
            "Global step: 2946,loss: 0.018488854\n",
            "\n",
            "Global step: 2947,loss: 0.017750386\n",
            "\n",
            "Global step: 2948,loss: 0.018626668\n",
            "\n",
            "Global step: 2949,loss: 0.01861898\n",
            "\n",
            "Global step: 2950,loss: 0.019753844\n",
            "\n",
            "Global step: 2951,loss: 0.020308895\n",
            "\n",
            "Global step: 2952,loss: 0.018530745\n",
            "\n",
            "Global step: 2953,loss: 0.017987402\n",
            "\n",
            "Global step: 2954,loss: 0.017613862\n",
            "\n",
            "Global step: 2955,loss: 0.017633282\n",
            "\n",
            "Global step: 2956,loss: 0.018338636\n",
            "\n",
            "Global step: 2957,loss: 0.017934045\n",
            "\n",
            "Global step: 2958,loss: 0.017887771\n",
            "\n",
            "Global step: 2959,loss: 0.0183444\n",
            "\n",
            "Global step: 2960,loss: 0.01786142\n",
            "\n",
            "Global step: 2961,loss: 0.01703142\n",
            "\n",
            "Global step: 2962,loss: 0.017796515\n",
            "\n",
            "Global step: 2963,loss: 0.017663175\n",
            "\n",
            "Global step: 2964,loss: 0.017793534\n",
            "\n",
            "Global step: 2965,loss: 0.01958502\n",
            "\n",
            "Global step: 2966,loss: 0.018420244\n",
            "\n",
            "Global step: 2967,loss: 0.018241229\n",
            "\n",
            "Global step: 2968,loss: 0.017544437\n",
            "\n",
            "Global step: 2969,loss: 0.017603146\n",
            "\n",
            "Global step: 2970,loss: 0.018219916\n",
            "\n",
            "Global step: 2971,loss: 0.018985951\n",
            "\n",
            "Global step: 2972,loss: 0.018092353\n",
            "\n",
            "Global step: 2973,loss: 0.01719813\n",
            "\n",
            "Global step: 2974,loss: 0.019227648\n",
            "\n",
            "Global step: 2975,loss: 0.017855959\n",
            "\n",
            "Global step: 2976,loss: 0.018163648\n",
            "\n",
            "Global step: 2977,loss: 0.017308854\n",
            "\n",
            "Global step: 2978,loss: 0.01723736\n",
            "\n",
            "Global step: 2979,loss: 0.018065052\n",
            "\n",
            "Global step: 2980,loss: 0.018408487\n",
            "\n",
            "Global step: 2981,loss: 0.017572992\n",
            "\n",
            "Global step: 2982,loss: 0.018058931\n",
            "\n",
            "Global step: 2983,loss: 0.017771455\n",
            "\n",
            "Global step: 2984,loss: 0.018408896\n",
            "\n",
            "Global step: 2985,loss: 0.018408993\n",
            "\n",
            "Global step: 2986,loss: 0.017738942\n",
            "\n",
            "Global step: 2987,loss: 0.018145807\n",
            "\n",
            "Global step: 2988,loss: 0.017861731\n",
            "\n",
            "Global step: 2989,loss: 0.017581515\n",
            "\n",
            "Global step: 2990,loss: 0.017946947\n",
            "\n",
            "Global step: 2991,loss: 0.017614454\n",
            "\n",
            "Global step: 2992,loss: 0.01876108\n",
            "\n",
            "Global step: 2993,loss: 0.017744623\n",
            "\n",
            "Global step: 2994,loss: 0.017710976\n",
            "\n",
            "Global step: 2995,loss: 0.01744419\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2995,Val_Loss: 0.02175607140126981,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:39:15.459496 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 15/50:\n",
            "Global step: 2996,loss: 0.019219942\n",
            "\n",
            "Global step: 2997,loss: 0.017305447\n",
            "\n",
            "Global step: 2998,loss: 0.01784874\n",
            "\n",
            "Global step: 2999,loss: 0.017662352\n",
            "\n",
            "Global step: 3000,loss: 0.01834509\n",
            "\n",
            "Global step: 3001,loss: 0.01782392\n",
            "\n",
            "Global step: 3002,loss: 0.017627334\n",
            "\n",
            "Global step: 3003,loss: 0.0182794\n",
            "\n",
            "Global step: 3004,loss: 0.0176327\n",
            "\n",
            "Global step: 3005,loss: 0.01835526\n",
            "\n",
            "Global step: 3006,loss: 0.017127585\n",
            "\n",
            "Global step: 3007,loss: 0.01814417\n",
            "\n",
            "Global step: 3008,loss: 0.016539477\n",
            "\n",
            "Global step: 3009,loss: 0.01750972\n",
            "\n",
            "Global step: 3010,loss: 0.018467344\n",
            "\n",
            "Global step: 3011,loss: 0.0180964\n",
            "\n",
            "Global step: 3012,loss: 0.016090808\n",
            "\n",
            "Global step: 3013,loss: 0.01843464\n",
            "\n",
            "Global step: 3014,loss: 0.016877448\n",
            "\n",
            "Global step: 3015,loss: 0.01738683\n",
            "\n",
            "Global step: 3016,loss: 0.018383548\n",
            "\n",
            "Global step: 3017,loss: 0.016785495\n",
            "\n",
            "Global step: 3018,loss: 0.017756129\n",
            "\n",
            "Global step: 3019,loss: 0.01781439\n",
            "\n",
            "Global step: 3020,loss: 0.017581025\n",
            "\n",
            "Global step: 3021,loss: 0.017988576\n",
            "\n",
            "Global step: 3022,loss: 0.018040271\n",
            "\n",
            "Global step: 3023,loss: 0.01791826\n",
            "\n",
            "Global step: 3024,loss: 0.018237637\n",
            "\n",
            "Global step: 3025,loss: 0.017227108\n",
            "\n",
            "Global step: 3026,loss: 0.01830361\n",
            "\n",
            "Global step: 3027,loss: 0.018920137\n",
            "\n",
            "Global step: 3028,loss: 0.01725257\n",
            "\n",
            "Global step: 3029,loss: 0.017445551\n",
            "\n",
            "Global step: 3030,loss: 0.01707442\n",
            "\n",
            "Global step: 3031,loss: 0.017579544\n",
            "\n",
            "Global step: 3032,loss: 0.017513838\n",
            "\n",
            "Global step: 3033,loss: 0.017444856\n",
            "\n",
            "Global step: 3034,loss: 0.016579855\n",
            "\n",
            "Global step: 3035,loss: 0.017458549\n",
            "\n",
            "Global step: 3036,loss: 0.017244443\n",
            "\n",
            "Global step: 3037,loss: 0.017814888\n",
            "\n",
            "Global step: 3038,loss: 0.01712777\n",
            "\n",
            "Global step: 3039,loss: 0.016605144\n",
            "\n",
            "Global step: 3040,loss: 0.016962614\n",
            "\n",
            "Global step: 3041,loss: 0.016297352\n",
            "\n",
            "Global step: 3042,loss: 0.01758832\n",
            "\n",
            "Global step: 3043,loss: 0.017020326\n",
            "\n",
            "Global step: 3044,loss: 0.017498577\n",
            "\n",
            "Global step: 3045,loss: 0.017700609\n",
            "\n",
            "Global step: 3046,loss: 0.017793208\n",
            "\n",
            "Global step: 3047,loss: 0.0175015\n",
            "\n",
            "Global step: 3048,loss: 0.017877841\n",
            "\n",
            "Global step: 3049,loss: 0.018127754\n",
            "\n",
            "Global step: 3050,loss: 0.01729917\n",
            "\n",
            "Global step: 3051,loss: 0.018050157\n",
            "\n",
            "Global step: 3052,loss: 0.017005455\n",
            "\n",
            "Global step: 3053,loss: 0.017296256\n",
            "\n",
            "Global step: 3054,loss: 0.017493783\n",
            "\n",
            "Global step: 3055,loss: 0.017783895\n",
            "\n",
            "Global step: 3056,loss: 0.017294465\n",
            "\n",
            "Global step: 3057,loss: 0.017880471\n",
            "\n",
            "Global step: 3058,loss: 0.01712218\n",
            "\n",
            "Global step: 3059,loss: 0.016786505\n",
            "\n",
            "Global step: 3060,loss: 0.018162621\n",
            "\n",
            "Global step: 3061,loss: 0.017088905\n",
            "\n",
            "Global step: 3062,loss: 0.017212497\n",
            "\n",
            "Global step: 3063,loss: 0.016201695\n",
            "\n",
            "Global step: 3064,loss: 0.01655306\n",
            "\n",
            "Global step: 3065,loss: 0.017460432\n",
            "\n",
            "Global step: 3066,loss: 0.017633595\n",
            "\n",
            "Global step: 3067,loss: 0.01655479\n",
            "\n",
            "Global step: 3068,loss: 0.01703365\n",
            "\n",
            "Global step: 3069,loss: 0.019004766\n",
            "\n",
            "Global step: 3070,loss: 0.01701568\n",
            "\n",
            "Global step: 3071,loss: 0.017213322\n",
            "\n",
            "Global step: 3072,loss: 0.016368277\n",
            "\n",
            "Global step: 3073,loss: 0.017152483\n",
            "\n",
            "Global step: 3074,loss: 0.016555067\n",
            "\n",
            "Global step: 3075,loss: 0.017002052\n",
            "\n",
            "Global step: 3076,loss: 0.016810374\n",
            "\n",
            "Global step: 3077,loss: 0.017538173\n",
            "\n",
            "Global step: 3078,loss: 0.017341292\n",
            "\n",
            "Global step: 3079,loss: 0.018000267\n",
            "\n",
            "Global step: 3080,loss: 0.01712402\n",
            "\n",
            "Global step: 3081,loss: 0.017059498\n",
            "\n",
            "Global step: 3082,loss: 0.017683879\n",
            "\n",
            "Global step: 3083,loss: 0.016767245\n",
            "\n",
            "Global step: 3084,loss: 0.01795494\n",
            "\n",
            "Global step: 3085,loss: 0.016803477\n",
            "\n",
            "Global step: 3086,loss: 0.017307945\n",
            "\n",
            "Global step: 3087,loss: 0.01779435\n",
            "\n",
            "Global step: 3088,loss: 0.018573608\n",
            "\n",
            "Global step: 3089,loss: 0.017434372\n",
            "\n",
            "Global step: 3090,loss: 0.017048387\n",
            "\n",
            "Global step: 3091,loss: 0.01785707\n",
            "\n",
            "Global step: 3092,loss: 0.01802919\n",
            "\n",
            "Global step: 3093,loss: 0.018404782\n",
            "\n",
            "Global step: 3094,loss: 0.017377742\n",
            "\n",
            "Global step: 3095,loss: 0.017567527\n",
            "\n",
            "Global step: 3096,loss: 0.016602214\n",
            "\n",
            "Global step: 3097,loss: 0.01743283\n",
            "\n",
            "Global step: 3098,loss: 0.017242473\n",
            "\n",
            "Global step: 3099,loss: 0.018787589\n",
            "\n",
            "Global step: 3100,loss: 0.018004227\n",
            "\n",
            "Global step: 3101,loss: 0.01704744\n",
            "\n",
            "Global step: 3102,loss: 0.017536806\n",
            "\n",
            "Global step: 3103,loss: 0.01596898\n",
            "\n",
            "Global step: 3104,loss: 0.018066421\n",
            "\n",
            "Global step: 3105,loss: 0.017103773\n",
            "\n",
            "Global step: 3106,loss: 0.017339047\n",
            "\n",
            "Global step: 3107,loss: 0.018062718\n",
            "\n",
            "Global step: 3108,loss: 0.017624216\n",
            "\n",
            "Global step: 3109,loss: 0.01719013\n",
            "\n",
            "Global step: 3110,loss: 0.017243998\n",
            "\n",
            "Global step: 3111,loss: 0.016385281\n",
            "\n",
            "Global step: 3112,loss: 0.017424287\n",
            "\n",
            "Global step: 3113,loss: 0.016720975\n",
            "\n",
            "Global step: 3114,loss: 0.017436031\n",
            "\n",
            "Global step: 3115,loss: 0.018154612\n",
            "\n",
            "Global step: 3116,loss: 0.01703589\n",
            "\n",
            "Global step: 3117,loss: 0.017893633\n",
            "\n",
            "Global step: 3118,loss: 0.016902106\n",
            "\n",
            "Global step: 3119,loss: 0.018425275\n",
            "\n",
            "Global step: 3120,loss: 0.01775468\n",
            "\n",
            "Global step: 3121,loss: 0.01963437\n",
            "\n",
            "Global step: 3122,loss: 0.017377736\n",
            "\n",
            "Global step: 3123,loss: 0.017351933\n",
            "\n",
            "Global step: 3124,loss: 0.017260136\n",
            "\n",
            "Global step: 3125,loss: 0.017262995\n",
            "\n",
            "Global step: 3126,loss: 0.017115103\n",
            "\n",
            "Global step: 3127,loss: 0.016817667\n",
            "\n",
            "Global step: 3128,loss: 0.017742073\n",
            "\n",
            "Global step: 3129,loss: 0.01745617\n",
            "\n",
            "Global step: 3130,loss: 0.017443914\n",
            "\n",
            "Global step: 3131,loss: 0.016189462\n",
            "\n",
            "Global step: 3132,loss: 0.016524578\n",
            "\n",
            "Global step: 3133,loss: 0.017845007\n",
            "\n",
            "Global step: 3134,loss: 0.017737933\n",
            "\n",
            "Global step: 3135,loss: 0.016683612\n",
            "\n",
            "Global step: 3136,loss: 0.017086403\n",
            "\n",
            "Global step: 3137,loss: 0.017767694\n",
            "\n",
            "Global step: 3138,loss: 0.01815327\n",
            "\n",
            "Global step: 3139,loss: 0.01681699\n",
            "\n",
            "Global step: 3140,loss: 0.016670648\n",
            "\n",
            "Global step: 3141,loss: 0.016800653\n",
            "\n",
            "Global step: 3142,loss: 0.016593575\n",
            "\n",
            "Global step: 3143,loss: 0.017048955\n",
            "\n",
            "Global step: 3144,loss: 0.018974686\n",
            "\n",
            "Global step: 3145,loss: 0.017695036\n",
            "\n",
            "Global step: 3146,loss: 0.01652715\n",
            "\n",
            "Global step: 3147,loss: 0.018085545\n",
            "\n",
            "Global step: 3148,loss: 0.017087642\n",
            "\n",
            "Global step: 3149,loss: 0.01712552\n",
            "\n",
            "Global step: 3150,loss: 0.017921269\n",
            "\n",
            "Global step: 3151,loss: 0.017901208\n",
            "\n",
            "Global step: 3152,loss: 0.01809949\n",
            "\n",
            "Global step: 3153,loss: 0.016635312\n",
            "\n",
            "Global step: 3154,loss: 0.018184138\n",
            "\n",
            "Global step: 3155,loss: 0.01682361\n",
            "\n",
            "Global step: 3156,loss: 0.016720485\n",
            "\n",
            "Global step: 3157,loss: 0.01717849\n",
            "\n",
            "Global step: 3158,loss: 0.017156096\n",
            "\n",
            "Global step: 3159,loss: 0.016674249\n",
            "\n",
            "Global step: 3160,loss: 0.017313432\n",
            "\n",
            "Global step: 3161,loss: 0.01660126\n",
            "\n",
            "Global step: 3162,loss: 0.016685076\n",
            "\n",
            "Global step: 3163,loss: 0.016803177\n",
            "\n",
            "Global step: 3164,loss: 0.016883755\n",
            "\n",
            "Global step: 3165,loss: 0.017561464\n",
            "\n",
            "Global step: 3166,loss: 0.017819151\n",
            "\n",
            "Global step: 3167,loss: 0.016935771\n",
            "\n",
            "Global step: 3168,loss: 0.016570413\n",
            "\n",
            "Global step: 3169,loss: 0.017943338\n",
            "\n",
            "Global step: 3170,loss: 0.018633563\n",
            "\n",
            "Global step: 3171,loss: 0.017079953\n",
            "\n",
            "Global step: 3172,loss: 0.017567825\n",
            "\n",
            "Global step: 3173,loss: 0.016962737\n",
            "\n",
            "Global step: 3174,loss: 0.016297126\n",
            "\n",
            "Global step: 3175,loss: 0.017918067\n",
            "\n",
            "Global step: 3176,loss: 0.01734517\n",
            "\n",
            "Global step: 3177,loss: 0.016664078\n",
            "\n",
            "Global step: 3178,loss: 0.018317\n",
            "\n",
            "Global step: 3179,loss: 0.01702685\n",
            "\n",
            "Global step: 3180,loss: 0.017338213\n",
            "\n",
            "Global step: 3181,loss: 0.017958244\n",
            "\n",
            "Global step: 3182,loss: 0.01700638\n",
            "\n",
            "Global step: 3183,loss: 0.017633667\n",
            "\n",
            "Global step: 3184,loss: 0.01736023\n",
            "\n",
            "Global step: 3185,loss: 0.01732739\n",
            "\n",
            "Global step: 3186,loss: 0.017699542\n",
            "\n",
            "Global step: 3187,loss: 0.018395562\n",
            "\n",
            "Global step: 3188,loss: 0.018132582\n",
            "\n",
            "Global step: 3189,loss: 0.017225439\n",
            "\n",
            "Global step: 3190,loss: 0.016544675\n",
            "\n",
            "Global step: 3191,loss: 0.01663988\n",
            "\n",
            "Global step: 3192,loss: 0.016548838\n",
            "\n",
            "Global step: 3193,loss: 0.01744046\n",
            "\n",
            "Global step: 3194,loss: 0.017156502\n",
            "\n",
            "Global step: 3195,loss: 0.016774645\n",
            "\n",
            "Global step: 3196,loss: 0.017239247\n",
            "\n",
            "Global step: 3197,loss: 0.017286621\n",
            "\n",
            "Global step: 3198,loss: 0.017323928\n",
            "\n",
            "Global step: 3199,loss: 0.016915817\n",
            "\n",
            "Global step: 3200,loss: 0.016701778\n",
            "\n",
            "Global step: 3201,loss: 0.01707834\n",
            "\n",
            "Global step: 3202,loss: 0.01760621\n",
            "\n",
            "Global step: 3203,loss: 0.016939536\n",
            "\n",
            "Global step: 3204,loss: 0.017721144\n",
            "\n",
            "Global step: 3205,loss: 0.016528701\n",
            "\n",
            "Global step: 3206,loss: 0.016554303\n",
            "\n",
            "Global step: 3207,loss: 0.017401583\n",
            "\n",
            "Global step: 3208,loss: 0.017269675\n",
            "\n",
            "Global step: 3209,loss: 0.017023278\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3209,Val_Loss: 0.020838051544208275,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:40:09.501702 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 16/50:\n",
            "Global step: 3210,loss: 0.018093366\n",
            "\n",
            "Global step: 3211,loss: 0.018233057\n",
            "\n",
            "Global step: 3212,loss: 0.017090172\n",
            "\n",
            "Global step: 3213,loss: 0.017071804\n",
            "\n",
            "Global step: 3214,loss: 0.016784763\n",
            "\n",
            "Global step: 3215,loss: 0.017096467\n",
            "\n",
            "Global step: 3216,loss: 0.017142385\n",
            "\n",
            "Global step: 3217,loss: 0.01735648\n",
            "\n",
            "Global step: 3218,loss: 0.016431976\n",
            "\n",
            "Global step: 3219,loss: 0.017362457\n",
            "\n",
            "Global step: 3220,loss: 0.01699522\n",
            "\n",
            "Global step: 3221,loss: 0.017343022\n",
            "\n",
            "Global step: 3222,loss: 0.016651973\n",
            "\n",
            "Global step: 3223,loss: 0.01719673\n",
            "\n",
            "Global step: 3224,loss: 0.016407775\n",
            "\n",
            "Global step: 3225,loss: 0.016843231\n",
            "\n",
            "Global step: 3226,loss: 0.017098963\n",
            "\n",
            "Global step: 3227,loss: 0.017411165\n",
            "\n",
            "Global step: 3228,loss: 0.0171866\n",
            "\n",
            "Global step: 3229,loss: 0.017456621\n",
            "\n",
            "Global step: 3230,loss: 0.017343273\n",
            "\n",
            "Global step: 3231,loss: 0.016379822\n",
            "\n",
            "Global step: 3232,loss: 0.01683674\n",
            "\n",
            "Global step: 3233,loss: 0.017072037\n",
            "\n",
            "Global step: 3234,loss: 0.016238168\n",
            "\n",
            "Global step: 3235,loss: 0.016373603\n",
            "\n",
            "Global step: 3236,loss: 0.01739473\n",
            "\n",
            "Global step: 3237,loss: 0.017764626\n",
            "\n",
            "Global step: 3238,loss: 0.016686164\n",
            "\n",
            "Global step: 3239,loss: 0.017196534\n",
            "\n",
            "Global step: 3240,loss: 0.01663184\n",
            "\n",
            "Global step: 3241,loss: 0.016477209\n",
            "\n",
            "Global step: 3242,loss: 0.01693331\n",
            "\n",
            "Global step: 3243,loss: 0.017140798\n",
            "\n",
            "Global step: 3244,loss: 0.016377823\n",
            "\n",
            "Global step: 3245,loss: 0.01699784\n",
            "\n",
            "Global step: 3246,loss: 0.016912727\n",
            "\n",
            "Global step: 3247,loss: 0.017123181\n",
            "\n",
            "Global step: 3248,loss: 0.016102448\n",
            "\n",
            "Global step: 3249,loss: 0.01637723\n",
            "\n",
            "Global step: 3250,loss: 0.01677881\n",
            "\n",
            "Global step: 3251,loss: 0.01650512\n",
            "\n",
            "Global step: 3252,loss: 0.016714977\n",
            "\n",
            "Global step: 3253,loss: 0.017003546\n",
            "\n",
            "Global step: 3254,loss: 0.017590191\n",
            "\n",
            "Global step: 3255,loss: 0.017009404\n",
            "\n",
            "Global step: 3256,loss: 0.016696228\n",
            "\n",
            "Global step: 3257,loss: 0.016228836\n",
            "\n",
            "Global step: 3258,loss: 0.017381161\n",
            "\n",
            "Global step: 3259,loss: 0.017106514\n",
            "\n",
            "Global step: 3260,loss: 0.016388416\n",
            "\n",
            "Global step: 3261,loss: 0.016462699\n",
            "\n",
            "Global step: 3262,loss: 0.017083608\n",
            "\n",
            "Global step: 3263,loss: 0.016869072\n",
            "\n",
            "Global step: 3264,loss: 0.016671605\n",
            "\n",
            "Global step: 3265,loss: 0.016441103\n",
            "\n",
            "Global step: 3266,loss: 0.016455512\n",
            "\n",
            "Global step: 3267,loss: 0.016760416\n",
            "\n",
            "Global step: 3268,loss: 0.016785864\n",
            "\n",
            "Global step: 3269,loss: 0.016970221\n",
            "\n",
            "Global step: 3270,loss: 0.016803345\n",
            "\n",
            "Global step: 3271,loss: 0.017870076\n",
            "\n",
            "Global step: 3272,loss: 0.0171256\n",
            "\n",
            "Global step: 3273,loss: 0.015972115\n",
            "\n",
            "Global step: 3274,loss: 0.016602827\n",
            "\n",
            "Global step: 3275,loss: 0.01634592\n",
            "\n",
            "Global step: 3276,loss: 0.017378725\n",
            "\n",
            "Global step: 3277,loss: 0.016340863\n",
            "\n",
            "Global step: 3278,loss: 0.017398579\n",
            "\n",
            "Global step: 3279,loss: 0.016315445\n",
            "\n",
            "Global step: 3280,loss: 0.016230032\n",
            "\n",
            "Global step: 3281,loss: 0.017789915\n",
            "\n",
            "Global step: 3282,loss: 0.018443624\n",
            "\n",
            "Global step: 3283,loss: 0.016441487\n",
            "\n",
            "Global step: 3284,loss: 0.015846455\n",
            "\n",
            "Global step: 3285,loss: 0.016710859\n",
            "\n",
            "Global step: 3286,loss: 0.0169927\n",
            "\n",
            "Global step: 3287,loss: 0.017716726\n",
            "\n",
            "Global step: 3288,loss: 0.016689366\n",
            "\n",
            "Global step: 3289,loss: 0.017455313\n",
            "\n",
            "Global step: 3290,loss: 0.017366374\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 3.98303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:40:29.363896 140166520051456 supervisor.py:1099] global_step/sec: 3.98303\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 3291.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:40:29.372851 140167885084416 supervisor.py:1050] Recording summary at step 3291.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3291,loss: 0.017712105\n",
            "\n",
            "Global step: 3292,loss: 0.016189631\n",
            "\n",
            "Global step: 3293,loss: 0.016947372\n",
            "\n",
            "Global step: 3294,loss: 0.016540844\n",
            "\n",
            "Global step: 3295,loss: 0.016538266\n",
            "\n",
            "Global step: 3296,loss: 0.016031945\n",
            "\n",
            "Global step: 3297,loss: 0.015763624\n",
            "\n",
            "Global step: 3298,loss: 0.016871462\n",
            "\n",
            "Global step: 3299,loss: 0.016534366\n",
            "\n",
            "Global step: 3300,loss: 0.016266808\n",
            "\n",
            "Global step: 3301,loss: 0.01728351\n",
            "\n",
            "Global step: 3302,loss: 0.016201656\n",
            "\n",
            "Global step: 3303,loss: 0.01674963\n",
            "\n",
            "Global step: 3304,loss: 0.016632043\n",
            "\n",
            "Global step: 3305,loss: 0.016181635\n",
            "\n",
            "Global step: 3306,loss: 0.017037855\n",
            "\n",
            "Global step: 3307,loss: 0.016442783\n",
            "\n",
            "Global step: 3308,loss: 0.01752628\n",
            "\n",
            "Global step: 3309,loss: 0.016229343\n",
            "\n",
            "Global step: 3310,loss: 0.01606532\n",
            "\n",
            "Global step: 3311,loss: 0.016392043\n",
            "\n",
            "Global step: 3312,loss: 0.016823394\n",
            "\n",
            "Global step: 3313,loss: 0.016246026\n",
            "\n",
            "Global step: 3314,loss: 0.015638193\n",
            "\n",
            "Global step: 3315,loss: 0.017245062\n",
            "\n",
            "Global step: 3316,loss: 0.016215647\n",
            "\n",
            "Global step: 3317,loss: 0.017005527\n",
            "\n",
            "Global step: 3318,loss: 0.016804114\n",
            "\n",
            "Global step: 3319,loss: 0.01714409\n",
            "\n",
            "Global step: 3320,loss: 0.016582951\n",
            "\n",
            "Global step: 3321,loss: 0.017163623\n",
            "\n",
            "Global step: 3322,loss: 0.018212805\n",
            "\n",
            "Global step: 3323,loss: 0.016520586\n",
            "\n",
            "Global step: 3324,loss: 0.01576457\n",
            "\n",
            "Global step: 3325,loss: 0.016053876\n",
            "\n",
            "Global step: 3326,loss: 0.016731001\n",
            "\n",
            "Global step: 3327,loss: 0.016958952\n",
            "\n",
            "Global step: 3328,loss: 0.017046975\n",
            "\n",
            "Global step: 3329,loss: 0.016124483\n",
            "\n",
            "Global step: 3330,loss: 0.017652927\n",
            "\n",
            "Global step: 3331,loss: 0.017328106\n",
            "\n",
            "Global step: 3332,loss: 0.017026588\n",
            "\n",
            "Global step: 3333,loss: 0.016719114\n",
            "\n",
            "Global step: 3334,loss: 0.016390486\n",
            "\n",
            "Global step: 3335,loss: 0.016535355\n",
            "\n",
            "Global step: 3336,loss: 0.016843962\n",
            "\n",
            "Global step: 3337,loss: 0.016515333\n",
            "\n",
            "Global step: 3338,loss: 0.01619475\n",
            "\n",
            "Global step: 3339,loss: 0.017118735\n",
            "\n",
            "Global step: 3340,loss: 0.016053837\n",
            "\n",
            "Global step: 3341,loss: 0.016613899\n",
            "\n",
            "Global step: 3342,loss: 0.016962318\n",
            "\n",
            "Global step: 3343,loss: 0.016725905\n",
            "\n",
            "Global step: 3344,loss: 0.016335314\n",
            "\n",
            "Global step: 3345,loss: 0.015543875\n",
            "\n",
            "Global step: 3346,loss: 0.016888881\n",
            "\n",
            "Global step: 3347,loss: 0.016176509\n",
            "\n",
            "Global step: 3348,loss: 0.016790679\n",
            "\n",
            "Global step: 3349,loss: 0.016359424\n",
            "\n",
            "Global step: 3350,loss: 0.015516984\n",
            "\n",
            "Global step: 3351,loss: 0.017493777\n",
            "\n",
            "Global step: 3352,loss: 0.016985076\n",
            "\n",
            "Global step: 3353,loss: 0.015845696\n",
            "\n",
            "Global step: 3354,loss: 0.016257692\n",
            "\n",
            "Global step: 3355,loss: 0.017307427\n",
            "\n",
            "Global step: 3356,loss: 0.016755044\n",
            "\n",
            "Global step: 3357,loss: 0.016341634\n",
            "\n",
            "Global step: 3358,loss: 0.016405724\n",
            "\n",
            "Global step: 3359,loss: 0.016618893\n",
            "\n",
            "Global step: 3360,loss: 0.015603669\n",
            "\n",
            "Global step: 3361,loss: 0.016524384\n",
            "\n",
            "Global step: 3362,loss: 0.016522987\n",
            "\n",
            "Global step: 3363,loss: 0.016113628\n",
            "\n",
            "Global step: 3364,loss: 0.016363658\n",
            "\n",
            "Global step: 3365,loss: 0.017216397\n",
            "\n",
            "Global step: 3366,loss: 0.016143017\n",
            "\n",
            "Global step: 3367,loss: 0.016245045\n",
            "\n",
            "Global step: 3368,loss: 0.017078836\n",
            "\n",
            "Global step: 3369,loss: 0.016344188\n",
            "\n",
            "Global step: 3370,loss: 0.01631209\n",
            "\n",
            "Global step: 3371,loss: 0.016493028\n",
            "\n",
            "Global step: 3372,loss: 0.01660682\n",
            "\n",
            "Global step: 3373,loss: 0.016358493\n",
            "\n",
            "Global step: 3374,loss: 0.015946237\n",
            "\n",
            "Global step: 3375,loss: 0.016108923\n",
            "\n",
            "Global step: 3376,loss: 0.015433804\n",
            "\n",
            "Global step: 3377,loss: 0.016514676\n",
            "\n",
            "Global step: 3378,loss: 0.015301569\n",
            "\n",
            "Global step: 3379,loss: 0.01836335\n",
            "\n",
            "Global step: 3380,loss: 0.017191608\n",
            "\n",
            "Global step: 3381,loss: 0.016443867\n",
            "\n",
            "Global step: 3382,loss: 0.016891517\n",
            "\n",
            "Global step: 3383,loss: 0.016591078\n",
            "\n",
            "Global step: 3384,loss: 0.01706604\n",
            "\n",
            "Global step: 3385,loss: 0.016014073\n",
            "\n",
            "Global step: 3386,loss: 0.016504256\n",
            "\n",
            "Global step: 3387,loss: 0.01664786\n",
            "\n",
            "Global step: 3388,loss: 0.016740607\n",
            "\n",
            "Global step: 3389,loss: 0.016523816\n",
            "\n",
            "Global step: 3390,loss: 0.016747257\n",
            "\n",
            "Global step: 3391,loss: 0.01670427\n",
            "\n",
            "Global step: 3392,loss: 0.016916806\n",
            "\n",
            "Global step: 3393,loss: 0.016142495\n",
            "\n",
            "Global step: 3394,loss: 0.016003765\n",
            "\n",
            "Global step: 3395,loss: 0.016526703\n",
            "\n",
            "Global step: 3396,loss: 0.016615316\n",
            "\n",
            "Global step: 3397,loss: 0.016253158\n",
            "\n",
            "Global step: 3398,loss: 0.016636446\n",
            "\n",
            "Global step: 3399,loss: 0.017226404\n",
            "\n",
            "Global step: 3400,loss: 0.01625518\n",
            "\n",
            "Global step: 3401,loss: 0.016532367\n",
            "\n",
            "Global step: 3402,loss: 0.016639816\n",
            "\n",
            "Global step: 3403,loss: 0.016477227\n",
            "\n",
            "Global step: 3404,loss: 0.017065907\n",
            "\n",
            "Global step: 3405,loss: 0.016762957\n",
            "\n",
            "Global step: 3406,loss: 0.016493574\n",
            "\n",
            "Global step: 3407,loss: 0.01728667\n",
            "\n",
            "Global step: 3408,loss: 0.016032547\n",
            "\n",
            "Global step: 3409,loss: 0.016793368\n",
            "\n",
            "Global step: 3410,loss: 0.0165885\n",
            "\n",
            "Global step: 3411,loss: 0.017321933\n",
            "\n",
            "Global step: 3412,loss: 0.016415041\n",
            "\n",
            "Global step: 3413,loss: 0.01728869\n",
            "\n",
            "Global step: 3414,loss: 0.017142372\n",
            "\n",
            "Global step: 3415,loss: 0.015863974\n",
            "\n",
            "Global step: 3416,loss: 0.01619193\n",
            "\n",
            "Global step: 3417,loss: 0.016603282\n",
            "\n",
            "Global step: 3418,loss: 0.016575905\n",
            "\n",
            "Global step: 3419,loss: 0.016182913\n",
            "\n",
            "Global step: 3420,loss: 0.016209789\n",
            "\n",
            "Global step: 3421,loss: 0.015897283\n",
            "\n",
            "Global step: 3422,loss: 0.016421579\n",
            "\n",
            "Global step: 3423,loss: 0.016371924\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3423,Val_Loss: 0.02047500749559779,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:41:03.506716 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 17/50:\n",
            "Global step: 3424,loss: 0.015678065\n",
            "\n",
            "Global step: 3425,loss: 0.017460443\n",
            "\n",
            "Global step: 3426,loss: 0.016432002\n",
            "\n",
            "Global step: 3427,loss: 0.015953323\n",
            "\n",
            "Global step: 3428,loss: 0.016681954\n",
            "\n",
            "Global step: 3429,loss: 0.01631084\n",
            "\n",
            "Global step: 3430,loss: 0.016369648\n",
            "\n",
            "Global step: 3431,loss: 0.016483156\n",
            "\n",
            "Global step: 3432,loss: 0.016153824\n",
            "\n",
            "Global step: 3433,loss: 0.016756605\n",
            "\n",
            "Global step: 3434,loss: 0.016296567\n",
            "\n",
            "Global step: 3435,loss: 0.016797096\n",
            "\n",
            "Global step: 3436,loss: 0.016440202\n",
            "\n",
            "Global step: 3437,loss: 0.016123664\n",
            "\n",
            "Global step: 3438,loss: 0.01641787\n",
            "\n",
            "Global step: 3439,loss: 0.016126186\n",
            "\n",
            "Global step: 3440,loss: 0.017138578\n",
            "\n",
            "Global step: 3441,loss: 0.016574834\n",
            "\n",
            "Global step: 3442,loss: 0.016646506\n",
            "\n",
            "Global step: 3443,loss: 0.01669956\n",
            "\n",
            "Global step: 3444,loss: 0.016001472\n",
            "\n",
            "Global step: 3445,loss: 0.016203132\n",
            "\n",
            "Global step: 3446,loss: 0.016428882\n",
            "\n",
            "Global step: 3447,loss: 0.015545182\n",
            "\n",
            "Global step: 3448,loss: 0.014610872\n",
            "\n",
            "Global step: 3449,loss: 0.017285459\n",
            "\n",
            "Global step: 3450,loss: 0.015625626\n",
            "\n",
            "Global step: 3451,loss: 0.01578393\n",
            "\n",
            "Global step: 3452,loss: 0.016679015\n",
            "\n",
            "Global step: 3453,loss: 0.015975812\n",
            "\n",
            "Global step: 3454,loss: 0.016192256\n",
            "\n",
            "Global step: 3455,loss: 0.016658118\n",
            "\n",
            "Global step: 3456,loss: 0.016506264\n",
            "\n",
            "Global step: 3457,loss: 0.017030627\n",
            "\n",
            "Global step: 3458,loss: 0.015362409\n",
            "\n",
            "Global step: 3459,loss: 0.016454067\n",
            "\n",
            "Global step: 3460,loss: 0.016692745\n",
            "\n",
            "Global step: 3461,loss: 0.015754752\n",
            "\n",
            "Global step: 3462,loss: 0.016086515\n",
            "\n",
            "Global step: 3463,loss: 0.016398495\n",
            "\n",
            "Global step: 3464,loss: 0.017126381\n",
            "\n",
            "Global step: 3465,loss: 0.0160714\n",
            "\n",
            "Global step: 3466,loss: 0.015588513\n",
            "\n",
            "Global step: 3467,loss: 0.016677873\n",
            "\n",
            "Global step: 3468,loss: 0.015564208\n",
            "\n",
            "Global step: 3469,loss: 0.018118683\n",
            "\n",
            "Global step: 3470,loss: 0.016587429\n",
            "\n",
            "Global step: 3471,loss: 0.01652238\n",
            "\n",
            "Global step: 3472,loss: 0.01605424\n",
            "\n",
            "Global step: 3473,loss: 0.016430663\n",
            "\n",
            "Global step: 3474,loss: 0.01606317\n",
            "\n",
            "Global step: 3475,loss: 0.016267117\n",
            "\n",
            "Global step: 3476,loss: 0.015957354\n",
            "\n",
            "Global step: 3477,loss: 0.016276803\n",
            "\n",
            "Global step: 3478,loss: 0.01636987\n",
            "\n",
            "Global step: 3479,loss: 0.0161251\n",
            "\n",
            "Global step: 3480,loss: 0.015370058\n",
            "\n",
            "Global step: 3481,loss: 0.015741903\n",
            "\n",
            "Global step: 3482,loss: 0.015937004\n",
            "\n",
            "Global step: 3483,loss: 0.015789786\n",
            "\n",
            "Global step: 3484,loss: 0.016803635\n",
            "\n",
            "Global step: 3485,loss: 0.01743682\n",
            "\n",
            "Global step: 3486,loss: 0.01649751\n",
            "\n",
            "Global step: 3487,loss: 0.015986854\n",
            "\n",
            "Global step: 3488,loss: 0.017427703\n",
            "\n",
            "Global step: 3489,loss: 0.016377263\n",
            "\n",
            "Global step: 3490,loss: 0.015298961\n",
            "\n",
            "Global step: 3491,loss: 0.016185565\n",
            "\n",
            "Global step: 3492,loss: 0.015581688\n",
            "\n",
            "Global step: 3493,loss: 0.016133519\n",
            "\n",
            "Global step: 3494,loss: 0.01663981\n",
            "\n",
            "Global step: 3495,loss: 0.016879365\n",
            "\n",
            "Global step: 3496,loss: 0.01593729\n",
            "\n",
            "Global step: 3497,loss: 0.016592529\n",
            "\n",
            "Global step: 3498,loss: 0.016620733\n",
            "\n",
            "Global step: 3499,loss: 0.016067293\n",
            "\n",
            "Global step: 3500,loss: 0.015680948\n",
            "\n",
            "Global step: 3501,loss: 0.017660316\n",
            "\n",
            "Global step: 3502,loss: 0.01563065\n",
            "\n",
            "Global step: 3503,loss: 0.016671043\n",
            "\n",
            "Global step: 3504,loss: 0.015407849\n",
            "\n",
            "Global step: 3505,loss: 0.016126165\n",
            "\n",
            "Global step: 3506,loss: 0.016185395\n",
            "\n",
            "Global step: 3507,loss: 0.016139561\n",
            "\n",
            "Global step: 3508,loss: 0.0179895\n",
            "\n",
            "Global step: 3509,loss: 0.017354483\n",
            "\n",
            "Global step: 3510,loss: 0.016714182\n",
            "\n",
            "Global step: 3511,loss: 0.016349856\n",
            "\n",
            "Global step: 3512,loss: 0.016014429\n",
            "\n",
            "Global step: 3513,loss: 0.016403569\n",
            "\n",
            "Global step: 3514,loss: 0.016260387\n",
            "\n",
            "Global step: 3515,loss: 0.016122796\n",
            "\n",
            "Global step: 3516,loss: 0.015793743\n",
            "\n",
            "Global step: 3517,loss: 0.01524975\n",
            "\n",
            "Global step: 3518,loss: 0.015859868\n",
            "\n",
            "Global step: 3519,loss: 0.017137473\n",
            "\n",
            "Global step: 3520,loss: 0.015781684\n",
            "\n",
            "Global step: 3521,loss: 0.016385142\n",
            "\n",
            "Global step: 3522,loss: 0.0155719705\n",
            "\n",
            "Global step: 3523,loss: 0.017041741\n",
            "\n",
            "Global step: 3524,loss: 0.016463941\n",
            "\n",
            "Global step: 3525,loss: 0.01537909\n",
            "\n",
            "Global step: 3526,loss: 0.01639384\n",
            "\n",
            "Global step: 3527,loss: 0.016081236\n",
            "\n",
            "Global step: 3528,loss: 0.015857533\n",
            "\n",
            "Global step: 3529,loss: 0.01696733\n",
            "\n",
            "Global step: 3530,loss: 0.016551938\n",
            "\n",
            "Global step: 3531,loss: 0.016465886\n",
            "\n",
            "Global step: 3532,loss: 0.01645228\n",
            "\n",
            "Global step: 3533,loss: 0.015922224\n",
            "\n",
            "Global step: 3534,loss: 0.01597679\n",
            "\n",
            "Global step: 3535,loss: 0.01631221\n",
            "\n",
            "Global step: 3536,loss: 0.01553139\n",
            "\n",
            "Global step: 3537,loss: 0.016191281\n",
            "\n",
            "Global step: 3538,loss: 0.015426774\n",
            "\n",
            "Global step: 3539,loss: 0.016018135\n",
            "\n",
            "Global step: 3540,loss: 0.015255771\n",
            "\n",
            "Global step: 3541,loss: 0.015650645\n",
            "\n",
            "Global step: 3542,loss: 0.016886672\n",
            "\n",
            "Global step: 3543,loss: 0.016421208\n",
            "\n",
            "Global step: 3544,loss: 0.01668239\n",
            "\n",
            "Global step: 3545,loss: 0.015439762\n",
            "\n",
            "Global step: 3546,loss: 0.015421008\n",
            "\n",
            "Global step: 3547,loss: 0.015413104\n",
            "\n",
            "Global step: 3548,loss: 0.016362837\n",
            "\n",
            "Global step: 3549,loss: 0.017159175\n",
            "\n",
            "Global step: 3550,loss: 0.015797764\n",
            "\n",
            "Global step: 3551,loss: 0.01643692\n",
            "\n",
            "Global step: 3552,loss: 0.016216123\n",
            "\n",
            "Global step: 3553,loss: 0.018408613\n",
            "\n",
            "Global step: 3554,loss: 0.016317448\n",
            "\n",
            "Global step: 3555,loss: 0.015912984\n",
            "\n",
            "Global step: 3556,loss: 0.015692573\n",
            "\n",
            "Global step: 3557,loss: 0.016182015\n",
            "\n",
            "Global step: 3558,loss: 0.015503162\n",
            "\n",
            "Global step: 3559,loss: 0.015715227\n",
            "\n",
            "Global step: 3560,loss: 0.015292931\n",
            "\n",
            "Global step: 3561,loss: 0.0171819\n",
            "\n",
            "Global step: 3562,loss: 0.015475638\n",
            "\n",
            "Global step: 3563,loss: 0.015769115\n",
            "\n",
            "Global step: 3564,loss: 0.015779102\n",
            "\n",
            "Global step: 3565,loss: 0.015936494\n",
            "\n",
            "Global step: 3566,loss: 0.015220641\n",
            "\n",
            "Global step: 3567,loss: 0.015771862\n",
            "\n",
            "Global step: 3568,loss: 0.015575806\n",
            "\n",
            "Global step: 3569,loss: 0.016261717\n",
            "\n",
            "Global step: 3570,loss: 0.015906084\n",
            "\n",
            "Global step: 3571,loss: 0.01606053\n",
            "\n",
            "Global step: 3572,loss: 0.015491641\n",
            "\n",
            "Global step: 3573,loss: 0.015773682\n",
            "\n",
            "Global step: 3574,loss: 0.015272851\n",
            "\n",
            "Global step: 3575,loss: 0.016656946\n",
            "\n",
            "Global step: 3576,loss: 0.016230814\n",
            "\n",
            "Global step: 3577,loss: 0.015124531\n",
            "\n",
            "Global step: 3578,loss: 0.016939258\n",
            "\n",
            "Global step: 3579,loss: 0.015056672\n",
            "\n",
            "Global step: 3580,loss: 0.01637275\n",
            "\n",
            "Global step: 3581,loss: 0.01599028\n",
            "\n",
            "Global step: 3582,loss: 0.015518241\n",
            "\n",
            "Global step: 3583,loss: 0.01655323\n",
            "\n",
            "Global step: 3584,loss: 0.016071808\n",
            "\n",
            "Global step: 3585,loss: 0.015933774\n",
            "\n",
            "Global step: 3586,loss: 0.015996857\n",
            "\n",
            "Global step: 3587,loss: 0.017033443\n",
            "\n",
            "Global step: 3588,loss: 0.015580298\n",
            "\n",
            "Global step: 3589,loss: 0.015516235\n",
            "\n",
            "Global step: 3590,loss: 0.015785078\n",
            "\n",
            "Global step: 3591,loss: 0.01585251\n",
            "\n",
            "Global step: 3592,loss: 0.016750306\n",
            "\n",
            "Global step: 3593,loss: 0.01631473\n",
            "\n",
            "Global step: 3594,loss: 0.015885841\n",
            "\n",
            "Global step: 3595,loss: 0.015672367\n",
            "\n",
            "Global step: 3596,loss: 0.016359882\n",
            "\n",
            "Global step: 3597,loss: 0.016014485\n",
            "\n",
            "Global step: 3598,loss: 0.015398663\n",
            "\n",
            "Global step: 3599,loss: 0.01629619\n",
            "\n",
            "Global step: 3600,loss: 0.015580138\n",
            "\n",
            "Global step: 3601,loss: 0.015682755\n",
            "\n",
            "Global step: 3602,loss: 0.015796384\n",
            "\n",
            "Global step: 3603,loss: 0.01603279\n",
            "\n",
            "Global step: 3604,loss: 0.015439108\n",
            "\n",
            "Global step: 3605,loss: 0.016109847\n",
            "\n",
            "Global step: 3606,loss: 0.015647734\n",
            "\n",
            "Global step: 3607,loss: 0.015564022\n",
            "\n",
            "Global step: 3608,loss: 0.015288396\n",
            "\n",
            "Global step: 3609,loss: 0.0148203755\n",
            "\n",
            "Global step: 3610,loss: 0.015713593\n",
            "\n",
            "Global step: 3611,loss: 0.015123732\n",
            "\n",
            "Global step: 3612,loss: 0.01678248\n",
            "\n",
            "Global step: 3613,loss: 0.016493307\n",
            "\n",
            "Global step: 3614,loss: 0.01576743\n",
            "\n",
            "Global step: 3615,loss: 0.015913159\n",
            "\n",
            "Global step: 3616,loss: 0.016882766\n",
            "\n",
            "Global step: 3617,loss: 0.015145362\n",
            "\n",
            "Global step: 3618,loss: 0.015401593\n",
            "\n",
            "Global step: 3619,loss: 0.016302573\n",
            "\n",
            "Global step: 3620,loss: 0.01549502\n",
            "\n",
            "Global step: 3621,loss: 0.015387817\n",
            "\n",
            "Global step: 3622,loss: 0.016283443\n",
            "\n",
            "Global step: 3623,loss: 0.015446912\n",
            "\n",
            "Global step: 3624,loss: 0.0153977685\n",
            "\n",
            "Global step: 3625,loss: 0.016248077\n",
            "\n",
            "Global step: 3626,loss: 0.016414143\n",
            "\n",
            "Global step: 3627,loss: 0.015494736\n",
            "\n",
            "Global step: 3628,loss: 0.015276858\n",
            "\n",
            "Global step: 3629,loss: 0.016228925\n",
            "\n",
            "Global step: 3630,loss: 0.015087609\n",
            "\n",
            "Global step: 3631,loss: 0.015531629\n",
            "\n",
            "Global step: 3632,loss: 0.015490989\n",
            "\n",
            "Global step: 3633,loss: 0.016525658\n",
            "\n",
            "Global step: 3634,loss: 0.01637072\n",
            "\n",
            "Global step: 3635,loss: 0.014896774\n",
            "\n",
            "Global step: 3636,loss: 0.015245001\n",
            "\n",
            "Global step: 3637,loss: 0.016353972\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3637,Val_Loss: 0.019739168941190367,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:41:57.527303 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 18/50:\n",
            "Global step: 3638,loss: 0.014623658\n",
            "\n",
            "Global step: 3639,loss: 0.015466461\n",
            "\n",
            "Global step: 3640,loss: 0.015679078\n",
            "\n",
            "Global step: 3641,loss: 0.015070814\n",
            "\n",
            "Global step: 3642,loss: 0.015963214\n",
            "\n",
            "Global step: 3643,loss: 0.015481428\n",
            "\n",
            "Global step: 3644,loss: 0.015662959\n",
            "\n",
            "Global step: 3645,loss: 0.015282106\n",
            "\n",
            "Global step: 3646,loss: 0.015575773\n",
            "\n",
            "Global step: 3647,loss: 0.014900861\n",
            "\n",
            "Global step: 3648,loss: 0.015181645\n",
            "\n",
            "Global step: 3649,loss: 0.015851079\n",
            "\n",
            "Global step: 3650,loss: 0.015446166\n",
            "\n",
            "Global step: 3651,loss: 0.016063021\n",
            "\n",
            "Global step: 3652,loss: 0.015446689\n",
            "\n",
            "Global step: 3653,loss: 0.015872844\n",
            "\n",
            "Global step: 3654,loss: 0.015328868\n",
            "\n",
            "Global step: 3655,loss: 0.016484521\n",
            "\n",
            "Global step: 3656,loss: 0.016479723\n",
            "\n",
            "Global step: 3657,loss: 0.013909196\n",
            "\n",
            "Global step: 3658,loss: 0.015734905\n",
            "\n",
            "Global step: 3659,loss: 0.015523909\n",
            "\n",
            "Global step: 3660,loss: 0.015707377\n",
            "\n",
            "Global step: 3661,loss: 0.015550894\n",
            "\n",
            "Global step: 3662,loss: 0.01498508\n",
            "\n",
            "Global step: 3663,loss: 0.015663618\n",
            "\n",
            "Global step: 3664,loss: 0.015376692\n",
            "\n",
            "Global step: 3665,loss: 0.016481163\n",
            "\n",
            "Global step: 3666,loss: 0.015294746\n",
            "\n",
            "Global step: 3667,loss: 0.015215529\n",
            "\n",
            "Global step: 3668,loss: 0.01585239\n",
            "\n",
            "Global step: 3669,loss: 0.014817983\n",
            "\n",
            "Global step: 3670,loss: 0.016356327\n",
            "\n",
            "Global step: 3671,loss: 0.014367898\n",
            "\n",
            "Global step: 3672,loss: 0.015302722\n",
            "\n",
            "Global step: 3673,loss: 0.015695931\n",
            "\n",
            "Global step: 3674,loss: 0.015999336\n",
            "\n",
            "Global step: 3675,loss: 0.016065573\n",
            "\n",
            "Global step: 3676,loss: 0.0153061105\n",
            "\n",
            "Global step: 3677,loss: 0.015627451\n",
            "\n",
            "Global step: 3678,loss: 0.015348182\n",
            "\n",
            "Global step: 3679,loss: 0.016180374\n",
            "\n",
            "Global step: 3680,loss: 0.015957903\n",
            "\n",
            "Global step: 3681,loss: 0.015977234\n",
            "\n",
            "Global step: 3682,loss: 0.015515662\n",
            "\n",
            "Global step: 3683,loss: 0.015243585\n",
            "\n",
            "Global step: 3684,loss: 0.015347032\n",
            "\n",
            "Global step: 3685,loss: 0.01580169\n",
            "\n",
            "Global step: 3686,loss: 0.015005992\n",
            "\n",
            "Global step: 3687,loss: 0.014977075\n",
            "\n",
            "Global step: 3688,loss: 0.015883198\n",
            "\n",
            "Global step: 3689,loss: 0.015406173\n",
            "\n",
            "Global step: 3690,loss: 0.015893731\n",
            "\n",
            "Global step: 3691,loss: 0.015995814\n",
            "\n",
            "Global step: 3692,loss: 0.015850252\n",
            "\n",
            "Global step: 3693,loss: 0.015461574\n",
            "\n",
            "Global step: 3694,loss: 0.01593299\n",
            "\n",
            "Global step: 3695,loss: 0.015077927\n",
            "\n",
            "Global step: 3696,loss: 0.015839709\n",
            "\n",
            "Global step: 3697,loss: 0.015809797\n",
            "\n",
            "Global step: 3698,loss: 0.015448884\n",
            "\n",
            "Global step: 3699,loss: 0.015966883\n",
            "\n",
            "Global step: 3700,loss: 0.01581917\n",
            "\n",
            "Global step: 3701,loss: 0.015441572\n",
            "\n",
            "Global step: 3702,loss: 0.0151582975\n",
            "\n",
            "Global step: 3703,loss: 0.015404048\n",
            "\n",
            "Global step: 3704,loss: 0.015273204\n",
            "\n",
            "Global step: 3705,loss: 0.016404636\n",
            "\n",
            "Global step: 3706,loss: 0.015032734\n",
            "\n",
            "Global step: 3707,loss: 0.015104696\n",
            "\n",
            "Global step: 3708,loss: 0.015439787\n",
            "\n",
            "Global step: 3709,loss: 0.015808266\n",
            "\n",
            "Global step: 3710,loss: 0.015138354\n",
            "\n",
            "Global step: 3711,loss: 0.01493413\n",
            "\n",
            "Global step: 3712,loss: 0.016152939\n",
            "\n",
            "Global step: 3713,loss: 0.015098551\n",
            "\n",
            "Global step: 3714,loss: 0.014441326\n",
            "\n",
            "Global step: 3715,loss: 0.01567124\n",
            "\n",
            "Global step: 3716,loss: 0.015258161\n",
            "\n",
            "Global step: 3717,loss: 0.016104152\n",
            "\n",
            "Global step: 3718,loss: 0.015552799\n",
            "\n",
            "Global step: 3719,loss: 0.014652216\n",
            "\n",
            "Global step: 3720,loss: 0.014449289\n",
            "\n",
            "Global step: 3721,loss: 0.015260309\n",
            "\n",
            "Global step: 3722,loss: 0.01449556\n",
            "\n",
            "Global step: 3723,loss: 0.01582677\n",
            "\n",
            "Global step: 3724,loss: 0.015067619\n",
            "\n",
            "Global step: 3725,loss: 0.016235726\n",
            "\n",
            "Global step: 3726,loss: 0.015801998\n",
            "\n",
            "Global step: 3727,loss: 0.015881293\n",
            "\n",
            "Global step: 3728,loss: 0.015157235\n",
            "\n",
            "Global step: 3729,loss: 0.015873808\n",
            "\n",
            "Global step: 3730,loss: 0.015849018\n",
            "\n",
            "Global step: 3731,loss: 0.014857526\n",
            "\n",
            "Global step: 3732,loss: 0.014995552\n",
            "\n",
            "Global step: 3733,loss: 0.01497314\n",
            "\n",
            "Global step: 3734,loss: 0.014810832\n",
            "\n",
            "Global step: 3735,loss: 0.015854953\n",
            "\n",
            "Global step: 3736,loss: 0.015081281\n",
            "\n",
            "Global step: 3737,loss: 0.01536031\n",
            "\n",
            "Global step: 3738,loss: 0.015085955\n",
            "\n",
            "Global step: 3739,loss: 0.015664639\n",
            "\n",
            "Global step: 3740,loss: 0.015460804\n",
            "\n",
            "Global step: 3741,loss: 0.015687624\n",
            "\n",
            "Global step: 3742,loss: 0.015577954\n",
            "\n",
            "Global step: 3743,loss: 0.015175738\n",
            "\n",
            "Global step: 3744,loss: 0.015338975\n",
            "\n",
            "Global step: 3745,loss: 0.015593986\n",
            "\n",
            "Global step: 3746,loss: 0.016096275\n",
            "\n",
            "Global step: 3747,loss: 0.015960699\n",
            "\n",
            "Global step: 3748,loss: 0.0155164385\n",
            "\n",
            "Global step: 3749,loss: 0.01583082\n",
            "\n",
            "Global step: 3750,loss: 0.014696449\n",
            "\n",
            "Global step: 3751,loss: 0.015562419\n",
            "\n",
            "Global step: 3752,loss: 0.015766209\n",
            "\n",
            "Global step: 3753,loss: 0.0153595405\n",
            "\n",
            "Global step: 3754,loss: 0.016042607\n",
            "\n",
            "Global step: 3755,loss: 0.0151434885\n",
            "\n",
            "Global step: 3756,loss: 0.01617554\n",
            "\n",
            "Global step: 3757,loss: 0.014734398\n",
            "\n",
            "Global step: 3758,loss: 0.015638439\n",
            "\n",
            "Global step: 3759,loss: 0.015542045\n",
            "\n",
            "Global step: 3760,loss: 0.014317899\n",
            "\n",
            "Global step: 3761,loss: 0.015126129\n",
            "\n",
            "Global step: 3762,loss: 0.01536359\n",
            "\n",
            "Global step: 3763,loss: 0.015574536\n",
            "\n",
            "Global step: 3764,loss: 0.014611746\n",
            "\n",
            "Global step: 3765,loss: 0.015213096\n",
            "\n",
            "Global step: 3766,loss: 0.015210135\n",
            "\n",
            "Global step: 3767,loss: 0.015980598\n",
            "\n",
            "Global step: 3768,loss: 0.015181021\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 3.98251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:42:29.388630 140166520051456 supervisor.py:1099] global_step/sec: 3.98251\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 3769.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:42:29.389821 140167885084416 supervisor.py:1050] Recording summary at step 3769.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3769,loss: 0.0148634985\n",
            "\n",
            "Global step: 3770,loss: 0.0150465295\n",
            "\n",
            "Global step: 3771,loss: 0.016015518\n",
            "\n",
            "Global step: 3772,loss: 0.01556821\n",
            "\n",
            "Global step: 3773,loss: 0.016071247\n",
            "\n",
            "Global step: 3774,loss: 0.016007446\n",
            "\n",
            "Global step: 3775,loss: 0.015330008\n",
            "\n",
            "Global step: 3776,loss: 0.015339171\n",
            "\n",
            "Global step: 3777,loss: 0.014996144\n",
            "\n",
            "Global step: 3778,loss: 0.016331771\n",
            "\n",
            "Global step: 3779,loss: 0.015044242\n",
            "\n",
            "Global step: 3780,loss: 0.015853444\n",
            "\n",
            "Global step: 3781,loss: 0.015462172\n",
            "\n",
            "Global step: 3782,loss: 0.015862841\n",
            "\n",
            "Global step: 3783,loss: 0.0156452\n",
            "\n",
            "Global step: 3784,loss: 0.0154153025\n",
            "\n",
            "Global step: 3785,loss: 0.015261892\n",
            "\n",
            "Global step: 3786,loss: 0.015435822\n",
            "\n",
            "Global step: 3787,loss: 0.014809696\n",
            "\n",
            "Global step: 3788,loss: 0.015196001\n",
            "\n",
            "Global step: 3789,loss: 0.016289154\n",
            "\n",
            "Global step: 3790,loss: 0.015204636\n",
            "\n",
            "Global step: 3791,loss: 0.016737355\n",
            "\n",
            "Global step: 3792,loss: 0.014892024\n",
            "\n",
            "Global step: 3793,loss: 0.015398895\n",
            "\n",
            "Global step: 3794,loss: 0.015652042\n",
            "\n",
            "Global step: 3795,loss: 0.016726382\n",
            "\n",
            "Global step: 3796,loss: 0.015980259\n",
            "\n",
            "Global step: 3797,loss: 0.01448333\n",
            "\n",
            "Global step: 3798,loss: 0.015664056\n",
            "\n",
            "Global step: 3799,loss: 0.015658336\n",
            "\n",
            "Global step: 3800,loss: 0.015028819\n",
            "\n",
            "Global step: 3801,loss: 0.015217934\n",
            "\n",
            "Global step: 3802,loss: 0.015418739\n",
            "\n",
            "Global step: 3803,loss: 0.015047056\n",
            "\n",
            "Global step: 3804,loss: 0.0152768465\n",
            "\n",
            "Global step: 3805,loss: 0.015178971\n",
            "\n",
            "Global step: 3806,loss: 0.016129386\n",
            "\n",
            "Global step: 3807,loss: 0.015029486\n",
            "\n",
            "Global step: 3808,loss: 0.015857667\n",
            "\n",
            "Global step: 3809,loss: 0.014988715\n",
            "\n",
            "Global step: 3810,loss: 0.015473828\n",
            "\n",
            "Global step: 3811,loss: 0.015731504\n",
            "\n",
            "Global step: 3812,loss: 0.015573147\n",
            "\n",
            "Global step: 3813,loss: 0.015450054\n",
            "\n",
            "Global step: 3814,loss: 0.015190449\n",
            "\n",
            "Global step: 3815,loss: 0.015163676\n",
            "\n",
            "Global step: 3816,loss: 0.017150026\n",
            "\n",
            "Global step: 3817,loss: 0.015294505\n",
            "\n",
            "Global step: 3818,loss: 0.014273723\n",
            "\n",
            "Global step: 3819,loss: 0.0151403975\n",
            "\n",
            "Global step: 3820,loss: 0.015257702\n",
            "\n",
            "Global step: 3821,loss: 0.015492006\n",
            "\n",
            "Global step: 3822,loss: 0.014379242\n",
            "\n",
            "Global step: 3823,loss: 0.0152901765\n",
            "\n",
            "Global step: 3824,loss: 0.014659398\n",
            "\n",
            "Global step: 3825,loss: 0.016276266\n",
            "\n",
            "Global step: 3826,loss: 0.015062071\n",
            "\n",
            "Global step: 3827,loss: 0.015730632\n",
            "\n",
            "Global step: 3828,loss: 0.015829513\n",
            "\n",
            "Global step: 3829,loss: 0.016390521\n",
            "\n",
            "Global step: 3830,loss: 0.015601476\n",
            "\n",
            "Global step: 3831,loss: 0.015073979\n",
            "\n",
            "Global step: 3832,loss: 0.01645468\n",
            "\n",
            "Global step: 3833,loss: 0.015396714\n",
            "\n",
            "Global step: 3834,loss: 0.014334266\n",
            "\n",
            "Global step: 3835,loss: 0.015165723\n",
            "\n",
            "Global step: 3836,loss: 0.0153429145\n",
            "\n",
            "Global step: 3837,loss: 0.014927463\n",
            "\n",
            "Global step: 3838,loss: 0.015286577\n",
            "\n",
            "Global step: 3839,loss: 0.015275613\n",
            "\n",
            "Global step: 3840,loss: 0.015720544\n",
            "\n",
            "Global step: 3841,loss: 0.015307812\n",
            "\n",
            "Global step: 3842,loss: 0.015191442\n",
            "\n",
            "Global step: 3843,loss: 0.014495\n",
            "\n",
            "Global step: 3844,loss: 0.015459658\n",
            "\n",
            "Global step: 3845,loss: 0.01555302\n",
            "\n",
            "Global step: 3846,loss: 0.014832859\n",
            "\n",
            "Global step: 3847,loss: 0.01531658\n",
            "\n",
            "Global step: 3848,loss: 0.014299967\n",
            "\n",
            "Global step: 3849,loss: 0.015335523\n",
            "\n",
            "Global step: 3850,loss: 0.015975133\n",
            "\n",
            "Global step: 3851,loss: 0.01559948\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3851,Val_Loss: 0.019586483408745966,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:42:51.649537 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 19/50:\n",
            "Global step: 3852,loss: 0.015190902\n",
            "\n",
            "Global step: 3853,loss: 0.01563049\n",
            "\n",
            "Global step: 3854,loss: 0.014638188\n",
            "\n",
            "Global step: 3855,loss: 0.015626593\n",
            "\n",
            "Global step: 3856,loss: 0.015946472\n",
            "\n",
            "Global step: 3857,loss: 0.014726927\n",
            "\n",
            "Global step: 3858,loss: 0.014702586\n",
            "\n",
            "Global step: 3859,loss: 0.014636868\n",
            "\n",
            "Global step: 3860,loss: 0.01464038\n",
            "\n",
            "Global step: 3861,loss: 0.0149931125\n",
            "\n",
            "Global step: 3862,loss: 0.015158908\n",
            "\n",
            "Global step: 3863,loss: 0.015364542\n",
            "\n",
            "Global step: 3864,loss: 0.014661126\n",
            "\n",
            "Global step: 3865,loss: 0.014777215\n",
            "\n",
            "Global step: 3866,loss: 0.015180241\n",
            "\n",
            "Global step: 3867,loss: 0.015055694\n",
            "\n",
            "Global step: 3868,loss: 0.015117162\n",
            "\n",
            "Global step: 3869,loss: 0.015433171\n",
            "\n",
            "Global step: 3870,loss: 0.015189917\n",
            "\n",
            "Global step: 3871,loss: 0.014507209\n",
            "\n",
            "Global step: 3872,loss: 0.014652815\n",
            "\n",
            "Global step: 3873,loss: 0.014792679\n",
            "\n",
            "Global step: 3874,loss: 0.014775869\n",
            "\n",
            "Global step: 3875,loss: 0.015596059\n",
            "\n",
            "Global step: 3876,loss: 0.014719436\n",
            "\n",
            "Global step: 3877,loss: 0.014240921\n",
            "\n",
            "Global step: 3878,loss: 0.014762024\n",
            "\n",
            "Global step: 3879,loss: 0.015427039\n",
            "\n",
            "Global step: 3880,loss: 0.015288209\n",
            "\n",
            "Global step: 3881,loss: 0.015135953\n",
            "\n",
            "Global step: 3882,loss: 0.015353791\n",
            "\n",
            "Global step: 3883,loss: 0.01435577\n",
            "\n",
            "Global step: 3884,loss: 0.015592525\n",
            "\n",
            "Global step: 3885,loss: 0.014504947\n",
            "\n",
            "Global step: 3886,loss: 0.015685208\n",
            "\n",
            "Global step: 3887,loss: 0.015379352\n",
            "\n",
            "Global step: 3888,loss: 0.01570199\n",
            "\n",
            "Global step: 3889,loss: 0.014988368\n",
            "\n",
            "Global step: 3890,loss: 0.014998281\n",
            "\n",
            "Global step: 3891,loss: 0.015161773\n",
            "\n",
            "Global step: 3892,loss: 0.015623449\n",
            "\n",
            "Global step: 3893,loss: 0.015611836\n",
            "\n",
            "Global step: 3894,loss: 0.015816145\n",
            "\n",
            "Global step: 3895,loss: 0.014384227\n",
            "\n",
            "Global step: 3896,loss: 0.015395934\n",
            "\n",
            "Global step: 3897,loss: 0.015151485\n",
            "\n",
            "Global step: 3898,loss: 0.014247658\n",
            "\n",
            "Global step: 3899,loss: 0.01544722\n",
            "\n",
            "Global step: 3900,loss: 0.014761946\n",
            "\n",
            "Global step: 3901,loss: 0.015459602\n",
            "\n",
            "Global step: 3902,loss: 0.014669201\n",
            "\n",
            "Global step: 3903,loss: 0.014811276\n",
            "\n",
            "Global step: 3904,loss: 0.015457497\n",
            "\n",
            "Global step: 3905,loss: 0.015353289\n",
            "\n",
            "Global step: 3906,loss: 0.01542117\n",
            "\n",
            "Global step: 3907,loss: 0.015216493\n",
            "\n",
            "Global step: 3908,loss: 0.015835004\n",
            "\n",
            "Global step: 3909,loss: 0.015174672\n",
            "\n",
            "Global step: 3910,loss: 0.015041244\n",
            "\n",
            "Global step: 3911,loss: 0.01481852\n",
            "\n",
            "Global step: 3912,loss: 0.015210972\n",
            "\n",
            "Global step: 3913,loss: 0.01575467\n",
            "\n",
            "Global step: 3914,loss: 0.0147916535\n",
            "\n",
            "Global step: 3915,loss: 0.0153697105\n",
            "\n",
            "Global step: 3916,loss: 0.01563056\n",
            "\n",
            "Global step: 3917,loss: 0.014890549\n",
            "\n",
            "Global step: 3918,loss: 0.015197622\n",
            "\n",
            "Global step: 3919,loss: 0.016065888\n",
            "\n",
            "Global step: 3920,loss: 0.015038002\n",
            "\n",
            "Global step: 3921,loss: 0.014121057\n",
            "\n",
            "Global step: 3922,loss: 0.014532714\n",
            "\n",
            "Global step: 3923,loss: 0.01523982\n",
            "\n",
            "Global step: 3924,loss: 0.014695078\n",
            "\n",
            "Global step: 3925,loss: 0.014905037\n",
            "\n",
            "Global step: 3926,loss: 0.014841244\n",
            "\n",
            "Global step: 3927,loss: 0.015298411\n",
            "\n",
            "Global step: 3928,loss: 0.015068003\n",
            "\n",
            "Global step: 3929,loss: 0.013998368\n",
            "\n",
            "Global step: 3930,loss: 0.014475392\n",
            "\n",
            "Global step: 3931,loss: 0.015720228\n",
            "\n",
            "Global step: 3932,loss: 0.01410176\n",
            "\n",
            "Global step: 3933,loss: 0.015739111\n",
            "\n",
            "Global step: 3934,loss: 0.015179711\n",
            "\n",
            "Global step: 3935,loss: 0.014914051\n",
            "\n",
            "Global step: 3936,loss: 0.014415673\n",
            "\n",
            "Global step: 3937,loss: 0.015699204\n",
            "\n",
            "Global step: 3938,loss: 0.014690956\n",
            "\n",
            "Global step: 3939,loss: 0.014805699\n",
            "\n",
            "Global step: 3940,loss: 0.014702538\n",
            "\n",
            "Global step: 3941,loss: 0.014801425\n",
            "\n",
            "Global step: 3942,loss: 0.014317341\n",
            "\n",
            "Global step: 3943,loss: 0.014481993\n",
            "\n",
            "Global step: 3944,loss: 0.014549746\n",
            "\n",
            "Global step: 3945,loss: 0.015050449\n",
            "\n",
            "Global step: 3946,loss: 0.014968873\n",
            "\n",
            "Global step: 3947,loss: 0.014602894\n",
            "\n",
            "Global step: 3948,loss: 0.014552573\n",
            "\n",
            "Global step: 3949,loss: 0.0143895475\n",
            "\n",
            "Global step: 3950,loss: 0.015221068\n",
            "\n",
            "Global step: 3951,loss: 0.014054619\n",
            "\n",
            "Global step: 3952,loss: 0.014862936\n",
            "\n",
            "Global step: 3953,loss: 0.015015942\n",
            "\n",
            "Global step: 3954,loss: 0.015392632\n",
            "\n",
            "Global step: 3955,loss: 0.014700503\n",
            "\n",
            "Global step: 3956,loss: 0.015030953\n",
            "\n",
            "Global step: 3957,loss: 0.015038581\n",
            "\n",
            "Global step: 3958,loss: 0.014699335\n",
            "\n",
            "Global step: 3959,loss: 0.015084801\n",
            "\n",
            "Global step: 3960,loss: 0.014329369\n",
            "\n",
            "Global step: 3961,loss: 0.014838098\n",
            "\n",
            "Global step: 3962,loss: 0.015033767\n",
            "\n",
            "Global step: 3963,loss: 0.014769092\n",
            "\n",
            "Global step: 3964,loss: 0.014846636\n",
            "\n",
            "Global step: 3965,loss: 0.015222735\n",
            "\n",
            "Global step: 3966,loss: 0.015334743\n",
            "\n",
            "Global step: 3967,loss: 0.014331274\n",
            "\n",
            "Global step: 3968,loss: 0.014832102\n",
            "\n",
            "Global step: 3969,loss: 0.015723975\n",
            "\n",
            "Global step: 3970,loss: 0.014703453\n",
            "\n",
            "Global step: 3971,loss: 0.014908142\n",
            "\n",
            "Global step: 3972,loss: 0.014977281\n",
            "\n",
            "Global step: 3973,loss: 0.015201822\n",
            "\n",
            "Global step: 3974,loss: 0.014025427\n",
            "\n",
            "Global step: 3975,loss: 0.014548904\n",
            "\n",
            "Global step: 3976,loss: 0.016026638\n",
            "\n",
            "Global step: 3977,loss: 0.01494174\n",
            "\n",
            "Global step: 3978,loss: 0.01456905\n",
            "\n",
            "Global step: 3979,loss: 0.014650677\n",
            "\n",
            "Global step: 3980,loss: 0.01528457\n",
            "\n",
            "Global step: 3981,loss: 0.015034322\n",
            "\n",
            "Global step: 3982,loss: 0.014690811\n",
            "\n",
            "Global step: 3983,loss: 0.015543124\n",
            "\n",
            "Global step: 3984,loss: 0.014744071\n",
            "\n",
            "Global step: 3985,loss: 0.014723373\n",
            "\n",
            "Global step: 3986,loss: 0.015967345\n",
            "\n",
            "Global step: 3987,loss: 0.0141565595\n",
            "\n",
            "Global step: 3988,loss: 0.014609891\n",
            "\n",
            "Global step: 3989,loss: 0.014524276\n",
            "\n",
            "Global step: 3990,loss: 0.0155733265\n",
            "\n",
            "Global step: 3991,loss: 0.0150938025\n",
            "\n",
            "Global step: 3992,loss: 0.015642826\n",
            "\n",
            "Global step: 3993,loss: 0.014483631\n",
            "\n",
            "Global step: 3994,loss: 0.014741561\n",
            "\n",
            "Global step: 3995,loss: 0.014186368\n",
            "\n",
            "Global step: 3996,loss: 0.013992164\n",
            "\n",
            "Global step: 3997,loss: 0.01601795\n",
            "\n",
            "Global step: 3998,loss: 0.015728923\n",
            "\n",
            "Global step: 3999,loss: 0.015328998\n",
            "\n",
            "Global step: 4000,loss: 0.015120524\n",
            "\n",
            "Global step: 4001,loss: 0.014925471\n",
            "\n",
            "Global step: 4002,loss: 0.014886069\n",
            "\n",
            "Global step: 4003,loss: 0.014709203\n",
            "\n",
            "Global step: 4004,loss: 0.014272007\n",
            "\n",
            "Global step: 4005,loss: 0.01540353\n",
            "\n",
            "Global step: 4006,loss: 0.01539401\n",
            "\n",
            "Global step: 4007,loss: 0.014751534\n",
            "\n",
            "Global step: 4008,loss: 0.014998261\n",
            "\n",
            "Global step: 4009,loss: 0.015719937\n",
            "\n",
            "Global step: 4010,loss: 0.014572488\n",
            "\n",
            "Global step: 4011,loss: 0.015518895\n",
            "\n",
            "Global step: 4012,loss: 0.015516684\n",
            "\n",
            "Global step: 4013,loss: 0.015219209\n",
            "\n",
            "Global step: 4014,loss: 0.014912162\n",
            "\n",
            "Global step: 4015,loss: 0.014481795\n",
            "\n",
            "Global step: 4016,loss: 0.014650225\n",
            "\n",
            "Global step: 4017,loss: 0.014191401\n",
            "\n",
            "Global step: 4018,loss: 0.016747603\n",
            "\n",
            "Global step: 4019,loss: 0.015137982\n",
            "\n",
            "Global step: 4020,loss: 0.01407818\n",
            "\n",
            "Global step: 4021,loss: 0.0150553435\n",
            "\n",
            "Global step: 4022,loss: 0.015097548\n",
            "\n",
            "Global step: 4023,loss: 0.015515253\n",
            "\n",
            "Global step: 4024,loss: 0.015047082\n",
            "\n",
            "Global step: 4025,loss: 0.015122082\n",
            "\n",
            "Global step: 4026,loss: 0.014667571\n",
            "\n",
            "Global step: 4027,loss: 0.014866899\n",
            "\n",
            "Global step: 4028,loss: 0.014427892\n",
            "\n",
            "Global step: 4029,loss: 0.015556137\n",
            "\n",
            "Global step: 4030,loss: 0.014934806\n",
            "\n",
            "Global step: 4031,loss: 0.014989366\n",
            "\n",
            "Global step: 4032,loss: 0.014759251\n",
            "\n",
            "Global step: 4033,loss: 0.015269231\n",
            "\n",
            "Global step: 4034,loss: 0.013956649\n",
            "\n",
            "Global step: 4035,loss: 0.014852922\n",
            "\n",
            "Global step: 4036,loss: 0.014795206\n",
            "\n",
            "Global step: 4037,loss: 0.0142077785\n",
            "\n",
            "Global step: 4038,loss: 0.015581004\n",
            "\n",
            "Global step: 4039,loss: 0.016148383\n",
            "\n",
            "Global step: 4040,loss: 0.01440155\n",
            "\n",
            "Global step: 4041,loss: 0.014679409\n",
            "\n",
            "Global step: 4042,loss: 0.015042176\n",
            "\n",
            "Global step: 4043,loss: 0.015373988\n",
            "\n",
            "Global step: 4044,loss: 0.0145385405\n",
            "\n",
            "Global step: 4045,loss: 0.015133684\n",
            "\n",
            "Global step: 4046,loss: 0.014561235\n",
            "\n",
            "Global step: 4047,loss: 0.014680526\n",
            "\n",
            "Global step: 4048,loss: 0.015071228\n",
            "\n",
            "Global step: 4049,loss: 0.0140407365\n",
            "\n",
            "Global step: 4050,loss: 0.014767302\n",
            "\n",
            "Global step: 4051,loss: 0.01511936\n",
            "\n",
            "Global step: 4052,loss: 0.014423995\n",
            "\n",
            "Global step: 4053,loss: 0.014733936\n",
            "\n",
            "Global step: 4054,loss: 0.015295627\n",
            "\n",
            "Global step: 4055,loss: 0.015205763\n",
            "\n",
            "Global step: 4056,loss: 0.014947751\n",
            "\n",
            "Global step: 4057,loss: 0.014955589\n",
            "\n",
            "Global step: 4058,loss: 0.014252784\n",
            "\n",
            "Global step: 4059,loss: 0.014600164\n",
            "\n",
            "Global step: 4060,loss: 0.015407548\n",
            "\n",
            "Global step: 4061,loss: 0.01424336\n",
            "\n",
            "Global step: 4062,loss: 0.014960114\n",
            "\n",
            "Global step: 4063,loss: 0.014789559\n",
            "\n",
            "Global step: 4064,loss: 0.014128594\n",
            "\n",
            "Global step: 4065,loss: 0.0149509385\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4065,Val_Loss: 0.01890178588464072,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:43:45.766377 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 20/50:\n",
            "Global step: 4066,loss: 0.014264775\n",
            "\n",
            "Global step: 4067,loss: 0.014216132\n",
            "\n",
            "Global step: 4068,loss: 0.014785702\n",
            "\n",
            "Global step: 4069,loss: 0.014096412\n",
            "\n",
            "Global step: 4070,loss: 0.0156189455\n",
            "\n",
            "Global step: 4071,loss: 0.014831663\n",
            "\n",
            "Global step: 4072,loss: 0.014733027\n",
            "\n",
            "Global step: 4073,loss: 0.014854286\n",
            "\n",
            "Global step: 4074,loss: 0.014209017\n",
            "\n",
            "Global step: 4075,loss: 0.01524726\n",
            "\n",
            "Global step: 4076,loss: 0.015627448\n",
            "\n",
            "Global step: 4077,loss: 0.014691887\n",
            "\n",
            "Global step: 4078,loss: 0.014629908\n",
            "\n",
            "Global step: 4079,loss: 0.015438982\n",
            "\n",
            "Global step: 4080,loss: 0.014509739\n",
            "\n",
            "Global step: 4081,loss: 0.01430407\n",
            "\n",
            "Global step: 4082,loss: 0.01475065\n",
            "\n",
            "Global step: 4083,loss: 0.014683694\n",
            "\n",
            "Global step: 4084,loss: 0.015259664\n",
            "\n",
            "Global step: 4085,loss: 0.01492377\n",
            "\n",
            "Global step: 4086,loss: 0.014026901\n",
            "\n",
            "Global step: 4087,loss: 0.016158117\n",
            "\n",
            "Global step: 4088,loss: 0.013784232\n",
            "\n",
            "Global step: 4089,loss: 0.014549453\n",
            "\n",
            "Global step: 4090,loss: 0.014518855\n",
            "\n",
            "Global step: 4091,loss: 0.014248684\n",
            "\n",
            "Global step: 4092,loss: 0.014903195\n",
            "\n",
            "Global step: 4093,loss: 0.015015645\n",
            "\n",
            "Global step: 4094,loss: 0.014122362\n",
            "\n",
            "Global step: 4095,loss: 0.015061261\n",
            "\n",
            "Global step: 4096,loss: 0.014703306\n",
            "\n",
            "Global step: 4097,loss: 0.014932851\n",
            "\n",
            "Global step: 4098,loss: 0.014396365\n",
            "\n",
            "Global step: 4099,loss: 0.0145245185\n",
            "\n",
            "Global step: 4100,loss: 0.015104361\n",
            "\n",
            "Global step: 4101,loss: 0.015419108\n",
            "\n",
            "Global step: 4102,loss: 0.013594282\n",
            "\n",
            "Global step: 4103,loss: 0.014665146\n",
            "\n",
            "Global step: 4104,loss: 0.014952849\n",
            "\n",
            "Global step: 4105,loss: 0.015141794\n",
            "\n",
            "Global step: 4106,loss: 0.014088501\n",
            "\n",
            "Global step: 4107,loss: 0.013981993\n",
            "\n",
            "Global step: 4108,loss: 0.014251304\n",
            "\n",
            "Global step: 4109,loss: 0.014256487\n",
            "\n",
            "Global step: 4110,loss: 0.014426067\n",
            "\n",
            "Global step: 4111,loss: 0.014233776\n",
            "\n",
            "Global step: 4112,loss: 0.014787602\n",
            "\n",
            "Global step: 4113,loss: 0.0152539415\n",
            "\n",
            "Global step: 4114,loss: 0.014606023\n",
            "\n",
            "Global step: 4115,loss: 0.014756979\n",
            "\n",
            "Global step: 4116,loss: 0.014716991\n",
            "\n",
            "Global step: 4117,loss: 0.014074874\n",
            "\n",
            "Global step: 4118,loss: 0.015371484\n",
            "\n",
            "Global step: 4119,loss: 0.014138303\n",
            "\n",
            "Global step: 4120,loss: 0.014994347\n",
            "\n",
            "Global step: 4121,loss: 0.015061474\n",
            "\n",
            "Global step: 4122,loss: 0.014444735\n",
            "\n",
            "Global step: 4123,loss: 0.014293535\n",
            "\n",
            "Global step: 4124,loss: 0.01442811\n",
            "\n",
            "Global step: 4125,loss: 0.014189273\n",
            "\n",
            "Global step: 4126,loss: 0.015084788\n",
            "\n",
            "Global step: 4127,loss: 0.014360774\n",
            "\n",
            "Global step: 4128,loss: 0.014438226\n",
            "\n",
            "Global step: 4129,loss: 0.013517214\n",
            "\n",
            "Global step: 4130,loss: 0.013945986\n",
            "\n",
            "Global step: 4131,loss: 0.01440011\n",
            "\n",
            "Global step: 4132,loss: 0.01423088\n",
            "\n",
            "Global step: 4133,loss: 0.014522742\n",
            "\n",
            "Global step: 4134,loss: 0.014555613\n",
            "\n",
            "Global step: 4135,loss: 0.013751635\n",
            "\n",
            "Global step: 4136,loss: 0.014623764\n",
            "\n",
            "Global step: 4137,loss: 0.014299871\n",
            "\n",
            "Global step: 4138,loss: 0.014578032\n",
            "\n",
            "Global step: 4139,loss: 0.014000109\n",
            "\n",
            "Global step: 4140,loss: 0.013564832\n",
            "\n",
            "Global step: 4141,loss: 0.01458466\n",
            "\n",
            "Global step: 4142,loss: 0.014300368\n",
            "\n",
            "Global step: 4143,loss: 0.014423953\n",
            "\n",
            "Global step: 4144,loss: 0.014541292\n",
            "\n",
            "Global step: 4145,loss: 0.015005343\n",
            "\n",
            "Global step: 4146,loss: 0.014406575\n",
            "\n",
            "Global step: 4147,loss: 0.014593387\n",
            "\n",
            "Global step: 4148,loss: 0.014364316\n",
            "\n",
            "Global step: 4149,loss: 0.014229417\n",
            "\n",
            "Global step: 4150,loss: 0.013978956\n",
            "\n",
            "Global step: 4151,loss: 0.0140591245\n",
            "\n",
            "Global step: 4152,loss: 0.015040502\n",
            "\n",
            "Global step: 4153,loss: 0.014200714\n",
            "\n",
            "Global step: 4154,loss: 0.014762395\n",
            "\n",
            "Global step: 4155,loss: 0.014846216\n",
            "\n",
            "Global step: 4156,loss: 0.014403154\n",
            "\n",
            "Global step: 4157,loss: 0.014102573\n",
            "\n",
            "Global step: 4158,loss: 0.01430835\n",
            "\n",
            "Global step: 4159,loss: 0.014854691\n",
            "\n",
            "Global step: 4160,loss: 0.014275822\n",
            "\n",
            "Global step: 4161,loss: 0.015419324\n",
            "\n",
            "Global step: 4162,loss: 0.015063039\n",
            "\n",
            "Global step: 4163,loss: 0.015086983\n",
            "\n",
            "Global step: 4164,loss: 0.014742218\n",
            "\n",
            "Global step: 4165,loss: 0.014618387\n",
            "\n",
            "Global step: 4166,loss: 0.014376313\n",
            "\n",
            "Global step: 4167,loss: 0.01371473\n",
            "\n",
            "Global step: 4168,loss: 0.014009491\n",
            "\n",
            "Global step: 4169,loss: 0.01411015\n",
            "\n",
            "Global step: 4170,loss: 0.013815951\n",
            "\n",
            "Global step: 4171,loss: 0.014405804\n",
            "\n",
            "Global step: 4172,loss: 0.014195759\n",
            "\n",
            "Global step: 4173,loss: 0.014817986\n",
            "\n",
            "Global step: 4174,loss: 0.014512507\n",
            "\n",
            "Global step: 4175,loss: 0.013981897\n",
            "\n",
            "Global step: 4176,loss: 0.014814375\n",
            "\n",
            "Global step: 4177,loss: 0.014573687\n",
            "\n",
            "Global step: 4178,loss: 0.014897753\n",
            "\n",
            "Global step: 4179,loss: 0.014907613\n",
            "\n",
            "Global step: 4180,loss: 0.014257876\n",
            "\n",
            "Global step: 4181,loss: 0.014588474\n",
            "\n",
            "Global step: 4182,loss: 0.014197581\n",
            "\n",
            "Global step: 4183,loss: 0.014373992\n",
            "\n",
            "Global step: 4184,loss: 0.0153138675\n",
            "\n",
            "Global step: 4185,loss: 0.015077118\n",
            "\n",
            "Global step: 4186,loss: 0.015069297\n",
            "\n",
            "Global step: 4187,loss: 0.013475672\n",
            "\n",
            "Global step: 4188,loss: 0.014186896\n",
            "\n",
            "Global step: 4189,loss: 0.014879132\n",
            "\n",
            "Global step: 4190,loss: 0.013849198\n",
            "\n",
            "Global step: 4191,loss: 0.0145974755\n",
            "\n",
            "Global step: 4192,loss: 0.01480717\n",
            "\n",
            "Global step: 4193,loss: 0.013940601\n",
            "\n",
            "Global step: 4194,loss: 0.014742498\n",
            "\n",
            "Global step: 4195,loss: 0.014846193\n",
            "\n",
            "Global step: 4196,loss: 0.01441191\n",
            "\n",
            "Global step: 4197,loss: 0.013861203\n",
            "\n",
            "Global step: 4198,loss: 0.014004873\n",
            "\n",
            "Global step: 4199,loss: 0.013606935\n",
            "\n",
            "Global step: 4200,loss: 0.01403455\n",
            "\n",
            "Global step: 4201,loss: 0.014617167\n",
            "\n",
            "Global step: 4202,loss: 0.01437466\n",
            "\n",
            "Global step: 4203,loss: 0.014502497\n",
            "\n",
            "Global step: 4204,loss: 0.013948198\n",
            "\n",
            "Global step: 4205,loss: 0.014760217\n",
            "\n",
            "Global step: 4206,loss: 0.014359397\n",
            "\n",
            "Global step: 4207,loss: 0.014398844\n",
            "\n",
            "Global step: 4208,loss: 0.014281801\n",
            "\n",
            "Global step: 4209,loss: 0.013727034\n",
            "\n",
            "Global step: 4210,loss: 0.014271556\n",
            "\n",
            "Global step: 4211,loss: 0.014669884\n",
            "\n",
            "Global step: 4212,loss: 0.014504697\n",
            "\n",
            "Global step: 4213,loss: 0.014797923\n",
            "\n",
            "Global step: 4214,loss: 0.014790146\n",
            "\n",
            "Global step: 4215,loss: 0.013764136\n",
            "\n",
            "Global step: 4216,loss: 0.014735612\n",
            "\n",
            "Global step: 4217,loss: 0.0144146\n",
            "\n",
            "Global step: 4218,loss: 0.014292859\n",
            "\n",
            "Global step: 4219,loss: 0.014230965\n",
            "\n",
            "Global step: 4220,loss: 0.014907135\n",
            "\n",
            "Global step: 4221,loss: 0.01554632\n",
            "\n",
            "Global step: 4222,loss: 0.01481799\n",
            "\n",
            "Global step: 4223,loss: 0.014330617\n",
            "\n",
            "Global step: 4224,loss: 0.014065109\n",
            "\n",
            "Global step: 4225,loss: 0.014657045\n",
            "\n",
            "Global step: 4226,loss: 0.014653178\n",
            "\n",
            "Global step: 4227,loss: 0.014258703\n",
            "\n",
            "Global step: 4228,loss: 0.014423494\n",
            "\n",
            "Global step: 4229,loss: 0.014572536\n",
            "\n",
            "Global step: 4230,loss: 0.01444593\n",
            "\n",
            "Global step: 4231,loss: 0.014173626\n",
            "\n",
            "Global step: 4232,loss: 0.0147824185\n",
            "\n",
            "Global step: 4233,loss: 0.014780703\n",
            "\n",
            "Global step: 4234,loss: 0.015622701\n",
            "\n",
            "Global step: 4235,loss: 0.014385708\n",
            "\n",
            "Global step: 4236,loss: 0.014434092\n",
            "\n",
            "Global step: 4237,loss: 0.014929357\n",
            "\n",
            "Global step: 4238,loss: 0.014360264\n",
            "\n",
            "Global step: 4239,loss: 0.013496396\n",
            "\n",
            "Global step: 4240,loss: 0.014303503\n",
            "\n",
            "Global step: 4241,loss: 0.01443963\n",
            "\n",
            "Global step: 4242,loss: 0.013760505\n",
            "\n",
            "Global step: 4243,loss: 0.01418566\n",
            "\n",
            "Global step: 4244,loss: 0.014674162\n",
            "\n",
            "Global step: 4245,loss: 0.014490364\n",
            "\n",
            "Global step: 4246,loss: 0.013725574\n",
            "\n",
            "Global step: 4247,loss: 0.01400853\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 4248.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:44:29.352247 140167885084416 supervisor.py:1050] Recording summary at step 4248.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.99269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:44:29.357752 140166520051456 supervisor.py:1099] global_step/sec: 3.99269\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4248,loss: 0.0142921535\n",
            "\n",
            "Global step: 4249,loss: 0.014341446\n",
            "\n",
            "Global step: 4250,loss: 0.015219018\n",
            "\n",
            "Global step: 4251,loss: 0.014045295\n",
            "\n",
            "Global step: 4252,loss: 0.014577332\n",
            "\n",
            "Global step: 4253,loss: 0.014964066\n",
            "\n",
            "Global step: 4254,loss: 0.014088447\n",
            "\n",
            "Global step: 4255,loss: 0.014267714\n",
            "\n",
            "Global step: 4256,loss: 0.014355746\n",
            "\n",
            "Global step: 4257,loss: 0.0140454145\n",
            "\n",
            "Global step: 4258,loss: 0.014497407\n",
            "\n",
            "Global step: 4259,loss: 0.014513641\n",
            "\n",
            "Global step: 4260,loss: 0.014562196\n",
            "\n",
            "Global step: 4261,loss: 0.014799445\n",
            "\n",
            "Global step: 4262,loss: 0.014137065\n",
            "\n",
            "Global step: 4263,loss: 0.014422715\n",
            "\n",
            "Global step: 4264,loss: 0.014521642\n",
            "\n",
            "Global step: 4265,loss: 0.014025359\n",
            "\n",
            "Global step: 4266,loss: 0.014506424\n",
            "\n",
            "Global step: 4267,loss: 0.01505896\n",
            "\n",
            "Global step: 4268,loss: 0.013914519\n",
            "\n",
            "Global step: 4269,loss: 0.014265443\n",
            "\n",
            "Global step: 4270,loss: 0.014282339\n",
            "\n",
            "Global step: 4271,loss: 0.014829367\n",
            "\n",
            "Global step: 4272,loss: 0.014135721\n",
            "\n",
            "Global step: 4273,loss: 0.014407201\n",
            "\n",
            "Global step: 4274,loss: 0.013929425\n",
            "\n",
            "Global step: 4275,loss: 0.014438376\n",
            "\n",
            "Global step: 4276,loss: 0.014398126\n",
            "\n",
            "Global step: 4277,loss: 0.014335819\n",
            "\n",
            "Global step: 4278,loss: 0.014174308\n",
            "\n",
            "Global step: 4279,loss: 0.013736579\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4279,Val_Loss: 0.01862159386081131,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:44:39.627050 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 21/50:\n",
            "Global step: 4280,loss: 0.013723289\n",
            "\n",
            "Global step: 4281,loss: 0.014242177\n",
            "\n",
            "Global step: 4282,loss: 0.013988526\n",
            "\n",
            "Global step: 4283,loss: 0.014379108\n",
            "\n",
            "Global step: 4284,loss: 0.013140149\n",
            "\n",
            "Global step: 4285,loss: 0.014470462\n",
            "\n",
            "Global step: 4286,loss: 0.013716325\n",
            "\n",
            "Global step: 4287,loss: 0.013779828\n",
            "\n",
            "Global step: 4288,loss: 0.014655944\n",
            "\n",
            "Global step: 4289,loss: 0.013939595\n",
            "\n",
            "Global step: 4290,loss: 0.014209182\n",
            "\n",
            "Global step: 4291,loss: 0.014013478\n",
            "\n",
            "Global step: 4292,loss: 0.0140777845\n",
            "\n",
            "Global step: 4293,loss: 0.015091011\n",
            "\n",
            "Global step: 4294,loss: 0.014168642\n",
            "\n",
            "Global step: 4295,loss: 0.014435467\n",
            "\n",
            "Global step: 4296,loss: 0.014310285\n",
            "\n",
            "Global step: 4297,loss: 0.014107842\n",
            "\n",
            "Global step: 4298,loss: 0.014433275\n",
            "\n",
            "Global step: 4299,loss: 0.015192346\n",
            "\n",
            "Global step: 4300,loss: 0.014731195\n",
            "\n",
            "Global step: 4301,loss: 0.013273814\n",
            "\n",
            "Global step: 4302,loss: 0.014594643\n",
            "\n",
            "Global step: 4303,loss: 0.01495648\n",
            "\n",
            "Global step: 4304,loss: 0.015466466\n",
            "\n",
            "Global step: 4305,loss: 0.013253349\n",
            "\n",
            "Global step: 4306,loss: 0.014522056\n",
            "\n",
            "Global step: 4307,loss: 0.0137220025\n",
            "\n",
            "Global step: 4308,loss: 0.014256431\n",
            "\n",
            "Global step: 4309,loss: 0.014505591\n",
            "\n",
            "Global step: 4310,loss: 0.013934057\n",
            "\n",
            "Global step: 4311,loss: 0.014013506\n",
            "\n",
            "Global step: 4312,loss: 0.014055554\n",
            "\n",
            "Global step: 4313,loss: 0.014232312\n",
            "\n",
            "Global step: 4314,loss: 0.014639521\n",
            "\n",
            "Global step: 4315,loss: 0.01452332\n",
            "\n",
            "Global step: 4316,loss: 0.014454972\n",
            "\n",
            "Global step: 4317,loss: 0.013810904\n",
            "\n",
            "Global step: 4318,loss: 0.0133582335\n",
            "\n",
            "Global step: 4319,loss: 0.013891253\n",
            "\n",
            "Global step: 4320,loss: 0.014309308\n",
            "\n",
            "Global step: 4321,loss: 0.014784813\n",
            "\n",
            "Global step: 4322,loss: 0.014373472\n",
            "\n",
            "Global step: 4323,loss: 0.013450119\n",
            "\n",
            "Global step: 4324,loss: 0.014409008\n",
            "\n",
            "Global step: 4325,loss: 0.013835005\n",
            "\n",
            "Global step: 4326,loss: 0.01475774\n",
            "\n",
            "Global step: 4327,loss: 0.013255888\n",
            "\n",
            "Global step: 4328,loss: 0.014537391\n",
            "\n",
            "Global step: 4329,loss: 0.013955996\n",
            "\n",
            "Global step: 4330,loss: 0.014151605\n",
            "\n",
            "Global step: 4331,loss: 0.013317231\n",
            "\n",
            "Global step: 4332,loss: 0.01442369\n",
            "\n",
            "Global step: 4333,loss: 0.014593366\n",
            "\n",
            "Global step: 4334,loss: 0.013596845\n",
            "\n",
            "Global step: 4335,loss: 0.013432336\n",
            "\n",
            "Global step: 4336,loss: 0.014357345\n",
            "\n",
            "Global step: 4337,loss: 0.013670554\n",
            "\n",
            "Global step: 4338,loss: 0.01434148\n",
            "\n",
            "Global step: 4339,loss: 0.01350791\n",
            "\n",
            "Global step: 4340,loss: 0.013781483\n",
            "\n",
            "Global step: 4341,loss: 0.013633957\n",
            "\n",
            "Global step: 4342,loss: 0.014196339\n",
            "\n",
            "Global step: 4343,loss: 0.013050526\n",
            "\n",
            "Global step: 4344,loss: 0.014184228\n",
            "\n",
            "Global step: 4345,loss: 0.013442917\n",
            "\n",
            "Global step: 4346,loss: 0.013782132\n",
            "\n",
            "Global step: 4347,loss: 0.013349871\n",
            "\n",
            "Global step: 4348,loss: 0.013727921\n",
            "\n",
            "Global step: 4349,loss: 0.013999052\n",
            "\n",
            "Global step: 4350,loss: 0.0143566225\n",
            "\n",
            "Global step: 4351,loss: 0.015301354\n",
            "\n",
            "Global step: 4352,loss: 0.012972846\n",
            "\n",
            "Global step: 4353,loss: 0.014026131\n",
            "\n",
            "Global step: 4354,loss: 0.015029202\n",
            "\n",
            "Global step: 4355,loss: 0.01415059\n",
            "\n",
            "Global step: 4356,loss: 0.015395954\n",
            "\n",
            "Global step: 4357,loss: 0.013487525\n",
            "\n",
            "Global step: 4358,loss: 0.0142570855\n",
            "\n",
            "Global step: 4359,loss: 0.014634322\n",
            "\n",
            "Global step: 4360,loss: 0.014626688\n",
            "\n",
            "Global step: 4361,loss: 0.0133743305\n",
            "\n",
            "Global step: 4362,loss: 0.0131073585\n",
            "\n",
            "Global step: 4363,loss: 0.015085262\n",
            "\n",
            "Global step: 4364,loss: 0.014664255\n",
            "\n",
            "Global step: 4365,loss: 0.013827793\n",
            "\n",
            "Global step: 4366,loss: 0.014193183\n",
            "\n",
            "Global step: 4367,loss: 0.014616875\n",
            "\n",
            "Global step: 4368,loss: 0.01465084\n",
            "\n",
            "Global step: 4369,loss: 0.01461649\n",
            "\n",
            "Global step: 4370,loss: 0.0138867935\n",
            "\n",
            "Global step: 4371,loss: 0.013767893\n",
            "\n",
            "Global step: 4372,loss: 0.014110409\n",
            "\n",
            "Global step: 4373,loss: 0.01437047\n",
            "\n",
            "Global step: 4374,loss: 0.014012869\n",
            "\n",
            "Global step: 4375,loss: 0.013375648\n",
            "\n",
            "Global step: 4376,loss: 0.014094446\n",
            "\n",
            "Global step: 4377,loss: 0.01375388\n",
            "\n",
            "Global step: 4378,loss: 0.013997232\n",
            "\n",
            "Global step: 4379,loss: 0.014585866\n",
            "\n",
            "Global step: 4380,loss: 0.014200045\n",
            "\n",
            "Global step: 4381,loss: 0.013586152\n",
            "\n",
            "Global step: 4382,loss: 0.014120753\n",
            "\n",
            "Global step: 4383,loss: 0.014046238\n",
            "\n",
            "Global step: 4384,loss: 0.013922529\n",
            "\n",
            "Global step: 4385,loss: 0.014045268\n",
            "\n",
            "Global step: 4386,loss: 0.013997124\n",
            "\n",
            "Global step: 4387,loss: 0.014261484\n",
            "\n",
            "Global step: 4388,loss: 0.014058357\n",
            "\n",
            "Global step: 4389,loss: 0.013788962\n",
            "\n",
            "Global step: 4390,loss: 0.014635386\n",
            "\n",
            "Global step: 4391,loss: 0.014257523\n",
            "\n",
            "Global step: 4392,loss: 0.014567417\n",
            "\n",
            "Global step: 4393,loss: 0.012981594\n",
            "\n",
            "Global step: 4394,loss: 0.013612573\n",
            "\n",
            "Global step: 4395,loss: 0.012859126\n",
            "\n",
            "Global step: 4396,loss: 0.01444113\n",
            "\n",
            "Global step: 4397,loss: 0.013891208\n",
            "\n",
            "Global step: 4398,loss: 0.013998286\n",
            "\n",
            "Global step: 4399,loss: 0.013843153\n",
            "\n",
            "Global step: 4400,loss: 0.014084609\n",
            "\n",
            "Global step: 4401,loss: 0.013754346\n",
            "\n",
            "Global step: 4402,loss: 0.014680049\n",
            "\n",
            "Global step: 4403,loss: 0.013603342\n",
            "\n",
            "Global step: 4404,loss: 0.013866802\n",
            "\n",
            "Global step: 4405,loss: 0.013466645\n",
            "\n",
            "Global step: 4406,loss: 0.014179768\n",
            "\n",
            "Global step: 4407,loss: 0.013576954\n",
            "\n",
            "Global step: 4408,loss: 0.013947441\n",
            "\n",
            "Global step: 4409,loss: 0.01443792\n",
            "\n",
            "Global step: 4410,loss: 0.013994114\n",
            "\n",
            "Global step: 4411,loss: 0.013748697\n",
            "\n",
            "Global step: 4412,loss: 0.013922031\n",
            "\n",
            "Global step: 4413,loss: 0.01334317\n",
            "\n",
            "Global step: 4414,loss: 0.013956613\n",
            "\n",
            "Global step: 4415,loss: 0.0141110625\n",
            "\n",
            "Global step: 4416,loss: 0.013646032\n",
            "\n",
            "Global step: 4417,loss: 0.014157297\n",
            "\n",
            "Global step: 4418,loss: 0.013980338\n",
            "\n",
            "Global step: 4419,loss: 0.014648553\n",
            "\n",
            "Global step: 4420,loss: 0.0132136475\n",
            "\n",
            "Global step: 4421,loss: 0.014985836\n",
            "\n",
            "Global step: 4422,loss: 0.013741848\n",
            "\n",
            "Global step: 4423,loss: 0.014684704\n",
            "\n",
            "Global step: 4424,loss: 0.013796466\n",
            "\n",
            "Global step: 4425,loss: 0.013751012\n",
            "\n",
            "Global step: 4426,loss: 0.013781003\n",
            "\n",
            "Global step: 4427,loss: 0.013471302\n",
            "\n",
            "Global step: 4428,loss: 0.01383039\n",
            "\n",
            "Global step: 4429,loss: 0.013188418\n",
            "\n",
            "Global step: 4430,loss: 0.0138244415\n",
            "\n",
            "Global step: 4431,loss: 0.014021372\n",
            "\n",
            "Global step: 4432,loss: 0.014399686\n",
            "\n",
            "Global step: 4433,loss: 0.0142831085\n",
            "\n",
            "Global step: 4434,loss: 0.013730861\n",
            "\n",
            "Global step: 4435,loss: 0.014079069\n",
            "\n",
            "Global step: 4436,loss: 0.013855432\n",
            "\n",
            "Global step: 4437,loss: 0.014090379\n",
            "\n",
            "Global step: 4438,loss: 0.014100731\n",
            "\n",
            "Global step: 4439,loss: 0.013656976\n",
            "\n",
            "Global step: 4440,loss: 0.014080672\n",
            "\n",
            "Global step: 4441,loss: 0.013483308\n",
            "\n",
            "Global step: 4442,loss: 0.014054449\n",
            "\n",
            "Global step: 4443,loss: 0.014352911\n",
            "\n",
            "Global step: 4444,loss: 0.014169579\n",
            "\n",
            "Global step: 4445,loss: 0.014216242\n",
            "\n",
            "Global step: 4446,loss: 0.01401759\n",
            "\n",
            "Global step: 4447,loss: 0.013407036\n",
            "\n",
            "Global step: 4448,loss: 0.014344948\n",
            "\n",
            "Global step: 4449,loss: 0.013854971\n",
            "\n",
            "Global step: 4450,loss: 0.014005453\n",
            "\n",
            "Global step: 4451,loss: 0.014317849\n",
            "\n",
            "Global step: 4452,loss: 0.014587389\n",
            "\n",
            "Global step: 4453,loss: 0.0148625\n",
            "\n",
            "Global step: 4454,loss: 0.015292251\n",
            "\n",
            "Global step: 4455,loss: 0.0136591\n",
            "\n",
            "Global step: 4456,loss: 0.014747197\n",
            "\n",
            "Global step: 4457,loss: 0.013054373\n",
            "\n",
            "Global step: 4458,loss: 0.014491508\n",
            "\n",
            "Global step: 4459,loss: 0.013569998\n",
            "\n",
            "Global step: 4460,loss: 0.015069958\n",
            "\n",
            "Global step: 4461,loss: 0.014093377\n",
            "\n",
            "Global step: 4462,loss: 0.014390391\n",
            "\n",
            "Global step: 4463,loss: 0.013867983\n",
            "\n",
            "Global step: 4464,loss: 0.014184279\n",
            "\n",
            "Global step: 4465,loss: 0.013763852\n",
            "\n",
            "Global step: 4466,loss: 0.0151092075\n",
            "\n",
            "Global step: 4467,loss: 0.0148598\n",
            "\n",
            "Global step: 4468,loss: 0.013701764\n",
            "\n",
            "Global step: 4469,loss: 0.014177255\n",
            "\n",
            "Global step: 4470,loss: 0.013219952\n",
            "\n",
            "Global step: 4471,loss: 0.01482524\n",
            "\n",
            "Global step: 4472,loss: 0.013556441\n",
            "\n",
            "Global step: 4473,loss: 0.013700438\n",
            "\n",
            "Global step: 4474,loss: 0.01462795\n",
            "\n",
            "Global step: 4475,loss: 0.0147444075\n",
            "\n",
            "Global step: 4476,loss: 0.014386659\n",
            "\n",
            "Global step: 4477,loss: 0.013912551\n",
            "\n",
            "Global step: 4478,loss: 0.013882113\n",
            "\n",
            "Global step: 4479,loss: 0.014357674\n",
            "\n",
            "Global step: 4480,loss: 0.014108231\n",
            "\n",
            "Global step: 4481,loss: 0.013891435\n",
            "\n",
            "Global step: 4482,loss: 0.013770614\n",
            "\n",
            "Global step: 4483,loss: 0.013886063\n",
            "\n",
            "Global step: 4484,loss: 0.014156873\n",
            "\n",
            "Global step: 4485,loss: 0.013544647\n",
            "\n",
            "Global step: 4486,loss: 0.013961067\n",
            "\n",
            "Global step: 4487,loss: 0.013435247\n",
            "\n",
            "Global step: 4488,loss: 0.01357172\n",
            "\n",
            "Global step: 4489,loss: 0.013935721\n",
            "\n",
            "Global step: 4490,loss: 0.013656435\n",
            "\n",
            "Global step: 4491,loss: 0.013775897\n",
            "\n",
            "Global step: 4492,loss: 0.014368032\n",
            "\n",
            "Global step: 4493,loss: 0.014125157\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4493,Val_Loss: 0.018181963872752692,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:45:33.682286 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 22/50:\n",
            "Global step: 4494,loss: 0.013751955\n",
            "\n",
            "Global step: 4495,loss: 0.01407793\n",
            "\n",
            "Global step: 4496,loss: 0.014214095\n",
            "\n",
            "Global step: 4497,loss: 0.013992263\n",
            "\n",
            "Global step: 4498,loss: 0.013724128\n",
            "\n",
            "Global step: 4499,loss: 0.0142498445\n",
            "\n",
            "Global step: 4500,loss: 0.0141240405\n",
            "\n",
            "Global step: 4501,loss: 0.013951402\n",
            "\n",
            "Global step: 4502,loss: 0.014019935\n",
            "\n",
            "Global step: 4503,loss: 0.013406174\n",
            "\n",
            "Global step: 4504,loss: 0.01423285\n",
            "\n",
            "Global step: 4505,loss: 0.013620452\n",
            "\n",
            "Global step: 4506,loss: 0.013854813\n",
            "\n",
            "Global step: 4507,loss: 0.014040499\n",
            "\n",
            "Global step: 4508,loss: 0.0141287\n",
            "\n",
            "Global step: 4509,loss: 0.013520137\n",
            "\n",
            "Global step: 4510,loss: 0.013426205\n",
            "\n",
            "Global step: 4511,loss: 0.014024554\n",
            "\n",
            "Global step: 4512,loss: 0.013667392\n",
            "\n",
            "Global step: 4513,loss: 0.014233161\n",
            "\n",
            "Global step: 4514,loss: 0.013050511\n",
            "\n",
            "Global step: 4515,loss: 0.014079272\n",
            "\n",
            "Global step: 4516,loss: 0.014365171\n",
            "\n",
            "Global step: 4517,loss: 0.013606377\n",
            "\n",
            "Global step: 4518,loss: 0.013499186\n",
            "\n",
            "Global step: 4519,loss: 0.013434049\n",
            "\n",
            "Global step: 4520,loss: 0.013685133\n",
            "\n",
            "Global step: 4521,loss: 0.013582289\n",
            "\n",
            "Global step: 4522,loss: 0.014483892\n",
            "\n",
            "Global step: 4523,loss: 0.0148921115\n",
            "\n",
            "Global step: 4524,loss: 0.013965321\n",
            "\n",
            "Global step: 4525,loss: 0.013485654\n",
            "\n",
            "Global step: 4526,loss: 0.013842585\n",
            "\n",
            "Global step: 4527,loss: 0.013789984\n",
            "\n",
            "Global step: 4528,loss: 0.013560564\n",
            "\n",
            "Global step: 4529,loss: 0.013969124\n",
            "\n",
            "Global step: 4530,loss: 0.013233389\n",
            "\n",
            "Global step: 4531,loss: 0.013346146\n",
            "\n",
            "Global step: 4532,loss: 0.013186896\n",
            "\n",
            "Global step: 4533,loss: 0.01351608\n",
            "\n",
            "Global step: 4534,loss: 0.013612187\n",
            "\n",
            "Global step: 4535,loss: 0.013052547\n",
            "\n",
            "Global step: 4536,loss: 0.013381051\n",
            "\n",
            "Global step: 4537,loss: 0.013948186\n",
            "\n",
            "Global step: 4538,loss: 0.013794275\n",
            "\n",
            "Global step: 4539,loss: 0.014214753\n",
            "\n",
            "Global step: 4540,loss: 0.013449174\n",
            "\n",
            "Global step: 4541,loss: 0.013932747\n",
            "\n",
            "Global step: 4542,loss: 0.0144195\n",
            "\n",
            "Global step: 4543,loss: 0.013018865\n",
            "\n",
            "Global step: 4544,loss: 0.013843055\n",
            "\n",
            "Global step: 4545,loss: 0.01371492\n",
            "\n",
            "Global step: 4546,loss: 0.013913798\n",
            "\n",
            "Global step: 4547,loss: 0.013738463\n",
            "\n",
            "Global step: 4548,loss: 0.014466679\n",
            "\n",
            "Global step: 4549,loss: 0.013391753\n",
            "\n",
            "Global step: 4550,loss: 0.014370892\n",
            "\n",
            "Global step: 4551,loss: 0.013465438\n",
            "\n",
            "Global step: 4552,loss: 0.014074155\n",
            "\n",
            "Global step: 4553,loss: 0.01413012\n",
            "\n",
            "Global step: 4554,loss: 0.013708691\n",
            "\n",
            "Global step: 4555,loss: 0.014057858\n",
            "\n",
            "Global step: 4556,loss: 0.014164446\n",
            "\n",
            "Global step: 4557,loss: 0.0141498605\n",
            "\n",
            "Global step: 4558,loss: 0.013844224\n",
            "\n",
            "Global step: 4559,loss: 0.013709295\n",
            "\n",
            "Global step: 4560,loss: 0.013695582\n",
            "\n",
            "Global step: 4561,loss: 0.013745666\n",
            "\n",
            "Global step: 4562,loss: 0.013858725\n",
            "\n",
            "Global step: 4563,loss: 0.012893665\n",
            "\n",
            "Global step: 4564,loss: 0.013384203\n",
            "\n",
            "Global step: 4565,loss: 0.013568817\n",
            "\n",
            "Global step: 4566,loss: 0.013460477\n",
            "\n",
            "Global step: 4567,loss: 0.013478904\n",
            "\n",
            "Global step: 4568,loss: 0.013282555\n",
            "\n",
            "Global step: 4569,loss: 0.013896169\n",
            "\n",
            "Global step: 4570,loss: 0.013930532\n",
            "\n",
            "Global step: 4571,loss: 0.013986063\n",
            "\n",
            "Global step: 4572,loss: 0.01329457\n",
            "\n",
            "Global step: 4573,loss: 0.013124633\n",
            "\n",
            "Global step: 4574,loss: 0.014246652\n",
            "\n",
            "Global step: 4575,loss: 0.013659949\n",
            "\n",
            "Global step: 4576,loss: 0.013408348\n",
            "\n",
            "Global step: 4577,loss: 0.013506199\n",
            "\n",
            "Global step: 4578,loss: 0.012934381\n",
            "\n",
            "Global step: 4579,loss: 0.013593003\n",
            "\n",
            "Global step: 4580,loss: 0.013736289\n",
            "\n",
            "Global step: 4581,loss: 0.01327518\n",
            "\n",
            "Global step: 4582,loss: 0.013003311\n",
            "\n",
            "Global step: 4583,loss: 0.014345186\n",
            "\n",
            "Global step: 4584,loss: 0.013284931\n",
            "\n",
            "Global step: 4585,loss: 0.013562436\n",
            "\n",
            "Global step: 4586,loss: 0.013141618\n",
            "\n",
            "Global step: 4587,loss: 0.013917163\n",
            "\n",
            "Global step: 4588,loss: 0.01372529\n",
            "\n",
            "Global step: 4589,loss: 0.01378418\n",
            "\n",
            "Global step: 4590,loss: 0.013982776\n",
            "\n",
            "Global step: 4591,loss: 0.013588038\n",
            "\n",
            "Global step: 4592,loss: 0.014055092\n",
            "\n",
            "Global step: 4593,loss: 0.013648985\n",
            "\n",
            "Global step: 4594,loss: 0.013070502\n",
            "\n",
            "Global step: 4595,loss: 0.013720155\n",
            "\n",
            "Global step: 4596,loss: 0.013285247\n",
            "\n",
            "Global step: 4597,loss: 0.0133581385\n",
            "\n",
            "Global step: 4598,loss: 0.013136317\n",
            "\n",
            "Global step: 4599,loss: 0.013834074\n",
            "\n",
            "Global step: 4600,loss: 0.013225923\n",
            "\n",
            "Global step: 4601,loss: 0.012868815\n",
            "\n",
            "Global step: 4602,loss: 0.012837951\n",
            "\n",
            "Global step: 4603,loss: 0.012785087\n",
            "\n",
            "Global step: 4604,loss: 0.013137956\n",
            "\n",
            "Global step: 4605,loss: 0.013985006\n",
            "\n",
            "Global step: 4606,loss: 0.013800973\n",
            "\n",
            "Global step: 4607,loss: 0.013772347\n",
            "\n",
            "Global step: 4608,loss: 0.013435971\n",
            "\n",
            "Global step: 4609,loss: 0.0142016\n",
            "\n",
            "Global step: 4610,loss: 0.013656265\n",
            "\n",
            "Global step: 4611,loss: 0.013368981\n",
            "\n",
            "Global step: 4612,loss: 0.013781158\n",
            "\n",
            "Global step: 4613,loss: 0.013992824\n",
            "\n",
            "Global step: 4614,loss: 0.0139562525\n",
            "\n",
            "Global step: 4615,loss: 0.014389217\n",
            "\n",
            "Global step: 4616,loss: 0.013311508\n",
            "\n",
            "Global step: 4617,loss: 0.013576311\n",
            "\n",
            "Global step: 4618,loss: 0.014553263\n",
            "\n",
            "Global step: 4619,loss: 0.012807688\n",
            "\n",
            "Global step: 4620,loss: 0.013746977\n",
            "\n",
            "Global step: 4621,loss: 0.013977132\n",
            "\n",
            "Global step: 4622,loss: 0.0137396995\n",
            "\n",
            "Global step: 4623,loss: 0.013839925\n",
            "\n",
            "Global step: 4624,loss: 0.013837979\n",
            "\n",
            "Global step: 4625,loss: 0.013483949\n",
            "\n",
            "Global step: 4626,loss: 0.013514644\n",
            "\n",
            "Global step: 4627,loss: 0.013854878\n",
            "\n",
            "Global step: 4628,loss: 0.014402845\n",
            "\n",
            "Global step: 4629,loss: 0.013676737\n",
            "\n",
            "Global step: 4630,loss: 0.013654237\n",
            "\n",
            "Global step: 4631,loss: 0.014090047\n",
            "\n",
            "Global step: 4632,loss: 0.013233106\n",
            "\n",
            "Global step: 4633,loss: 0.01324135\n",
            "\n",
            "Global step: 4634,loss: 0.013346171\n",
            "\n",
            "Global step: 4635,loss: 0.013981236\n",
            "\n",
            "Global step: 4636,loss: 0.013151144\n",
            "\n",
            "Global step: 4637,loss: 0.014236329\n",
            "\n",
            "Global step: 4638,loss: 0.01336892\n",
            "\n",
            "Global step: 4639,loss: 0.013539528\n",
            "\n",
            "Global step: 4640,loss: 0.013457419\n",
            "\n",
            "Global step: 4641,loss: 0.013919602\n",
            "\n",
            "Global step: 4642,loss: 0.0137285665\n",
            "\n",
            "Global step: 4643,loss: 0.013021148\n",
            "\n",
            "Global step: 4644,loss: 0.014430221\n",
            "\n",
            "Global step: 4645,loss: 0.014084245\n",
            "\n",
            "Global step: 4646,loss: 0.013804355\n",
            "\n",
            "Global step: 4647,loss: 0.013236067\n",
            "\n",
            "Global step: 4648,loss: 0.013044533\n",
            "\n",
            "Global step: 4649,loss: 0.01387591\n",
            "\n",
            "Global step: 4650,loss: 0.013212234\n",
            "\n",
            "Global step: 4651,loss: 0.014319492\n",
            "\n",
            "Global step: 4652,loss: 0.014145974\n",
            "\n",
            "Global step: 4653,loss: 0.013673491\n",
            "\n",
            "Global step: 4654,loss: 0.014205387\n",
            "\n",
            "Global step: 4655,loss: 0.014250383\n",
            "\n",
            "Global step: 4656,loss: 0.013323047\n",
            "\n",
            "Global step: 4657,loss: 0.013278791\n",
            "\n",
            "Global step: 4658,loss: 0.0136276595\n",
            "\n",
            "Global step: 4659,loss: 0.014189041\n",
            "\n",
            "Global step: 4660,loss: 0.013869192\n",
            "\n",
            "Global step: 4661,loss: 0.013794525\n",
            "\n",
            "Global step: 4662,loss: 0.013193415\n",
            "\n",
            "Global step: 4663,loss: 0.014089957\n",
            "\n",
            "Global step: 4664,loss: 0.013359817\n",
            "\n",
            "Global step: 4665,loss: 0.013186669\n",
            "\n",
            "Global step: 4666,loss: 0.01400164\n",
            "\n",
            "Global step: 4667,loss: 0.012740289\n",
            "\n",
            "Global step: 4668,loss: 0.013676747\n",
            "\n",
            "Global step: 4669,loss: 0.013679962\n",
            "\n",
            "Global step: 4670,loss: 0.01387548\n",
            "\n",
            "Global step: 4671,loss: 0.013382824\n",
            "\n",
            "Global step: 4672,loss: 0.013389389\n",
            "\n",
            "Global step: 4673,loss: 0.0137206055\n",
            "\n",
            "Global step: 4674,loss: 0.013902807\n",
            "\n",
            "Global step: 4675,loss: 0.01317133\n",
            "\n",
            "Global step: 4676,loss: 0.013359312\n",
            "\n",
            "Global step: 4677,loss: 0.013531802\n",
            "\n",
            "Global step: 4678,loss: 0.01346761\n",
            "\n",
            "Global step: 4679,loss: 0.013389998\n",
            "\n",
            "Global step: 4680,loss: 0.014160911\n",
            "\n",
            "Global step: 4681,loss: 0.0142125245\n",
            "\n",
            "Global step: 4682,loss: 0.013358034\n",
            "\n",
            "Global step: 4683,loss: 0.01314294\n",
            "\n",
            "Global step: 4684,loss: 0.013204603\n",
            "\n",
            "Global step: 4685,loss: 0.012829342\n",
            "\n",
            "Global step: 4686,loss: 0.012961085\n",
            "\n",
            "Global step: 4687,loss: 0.013799609\n",
            "\n",
            "Global step: 4688,loss: 0.014089143\n",
            "\n",
            "Global step: 4689,loss: 0.01306829\n",
            "\n",
            "Global step: 4690,loss: 0.013918601\n",
            "\n",
            "Global step: 4691,loss: 0.013532388\n",
            "\n",
            "Global step: 4692,loss: 0.013844241\n",
            "\n",
            "Global step: 4693,loss: 0.013988491\n",
            "\n",
            "Global step: 4694,loss: 0.014291125\n",
            "\n",
            "Global step: 4695,loss: 0.012970252\n",
            "\n",
            "Global step: 4696,loss: 0.013621436\n",
            "\n",
            "Global step: 4697,loss: 0.013948114\n",
            "\n",
            "Global step: 4698,loss: 0.0134545695\n",
            "\n",
            "Global step: 4699,loss: 0.013611424\n",
            "\n",
            "Global step: 4700,loss: 0.01316466\n",
            "\n",
            "Global step: 4701,loss: 0.013762797\n",
            "\n",
            "Global step: 4702,loss: 0.013126204\n",
            "\n",
            "Global step: 4703,loss: 0.013118728\n",
            "\n",
            "Global step: 4704,loss: 0.013283771\n",
            "\n",
            "Global step: 4705,loss: 0.013423878\n",
            "\n",
            "Global step: 4706,loss: 0.012546727\n",
            "\n",
            "Global step: 4707,loss: 0.013310673\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4707,Val_Loss: 0.017810381409761152,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:46:27.504742 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 23/50:\n",
            "Global step: 4708,loss: 0.013557677\n",
            "\n",
            "Global step: 4709,loss: 0.012854795\n",
            "\n",
            "Global step: 4710,loss: 0.0135375345\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 4711.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:46:29.346547 140167885084416 supervisor.py:1050] Recording summary at step 4711.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4711,loss: 0.0132837\n",
            "\n",
            "Global step: 4712,loss: 0.013298847\n",
            "\n",
            "Global step: 4713,loss: 0.014062281\n",
            "\n",
            "Global step: 4714,loss: 0.013605495\n",
            "\n",
            "Global step: 4715,loss: 0.013219862\n",
            "\n",
            "Global step: 4716,loss: 0.013142795\n",
            "\n",
            "Global step: 4717,loss: 0.012911974\n",
            "\n",
            "Global step: 4718,loss: 0.013098834\n",
            "\n",
            "Global step: 4719,loss: 0.012707145\n",
            "\n",
            "Global step: 4720,loss: 0.013666278\n",
            "\n",
            "Global step: 4721,loss: 0.014022525\n",
            "\n",
            "Global step: 4722,loss: 0.013267826\n",
            "\n",
            "Global step: 4723,loss: 0.0129648745\n",
            "\n",
            "Global step: 4724,loss: 0.013159387\n",
            "\n",
            "Global step: 4725,loss: 0.013639979\n",
            "\n",
            "Global step: 4726,loss: 0.013907295\n",
            "\n",
            "Global step: 4727,loss: 0.01392654\n",
            "\n",
            "Global step: 4728,loss: 0.014029711\n",
            "\n",
            "Global step: 4729,loss: 0.013600628\n",
            "\n",
            "Global step: 4730,loss: 0.013637532\n",
            "\n",
            "Global step: 4731,loss: 0.013212277\n",
            "\n",
            "Global step: 4732,loss: 0.013202196\n",
            "\n",
            "Global step: 4733,loss: 0.01279926\n",
            "\n",
            "Global step: 4734,loss: 0.012902562\n",
            "\n",
            "Global step: 4735,loss: 0.013994178\n",
            "\n",
            "Global step: 4736,loss: 0.013364082\n",
            "\n",
            "Global step: 4737,loss: 0.013810588\n",
            "\n",
            "Global step: 4738,loss: 0.012981187\n",
            "\n",
            "Global step: 4739,loss: 0.014038153\n",
            "\n",
            "Global step: 4740,loss: 0.013170439\n",
            "\n",
            "Global step: 4741,loss: 0.013007977\n",
            "\n",
            "Global step: 4742,loss: 0.012464929\n",
            "\n",
            "Global step: 4743,loss: 0.01371624\n",
            "\n",
            "Global step: 4744,loss: 0.013943319\n",
            "\n",
            "Global step: 4745,loss: 0.013578945\n",
            "\n",
            "Global step: 4746,loss: 0.013805863\n",
            "\n",
            "Global step: 4747,loss: 0.013577929\n",
            "\n",
            "Global step: 4748,loss: 0.013223488\n",
            "\n",
            "Global step: 4749,loss: 0.0133859515\n",
            "\n",
            "Global step: 4750,loss: 0.012837454\n",
            "\n",
            "Global step: 4751,loss: 0.013022843\n",
            "\n",
            "Global step: 4752,loss: 0.013816388\n",
            "\n",
            "Global step: 4753,loss: 0.012846686\n",
            "\n",
            "Global step: 4754,loss: 0.013340433\n",
            "\n",
            "Global step: 4755,loss: 0.012599211\n",
            "\n",
            "Global step: 4756,loss: 0.013144997\n",
            "\n",
            "Global step: 4757,loss: 0.013546029\n",
            "\n",
            "Global step: 4758,loss: 0.013351547\n",
            "\n",
            "Global step: 4759,loss: 0.013476391\n",
            "\n",
            "Global step: 4760,loss: 0.013975385\n",
            "\n",
            "Global step: 4761,loss: 0.013627186\n",
            "\n",
            "Global step: 4762,loss: 0.013478643\n",
            "\n",
            "Global step: 4763,loss: 0.01314589\n",
            "\n",
            "Global step: 4764,loss: 0.012895305\n",
            "\n",
            "Global step: 4765,loss: 0.013246323\n",
            "\n",
            "Global step: 4766,loss: 0.012898879\n",
            "\n",
            "Global step: 4767,loss: 0.013225404\n",
            "\n",
            "Global step: 4768,loss: 0.013396167\n",
            "\n",
            "Global step: 4769,loss: 0.013054911\n",
            "\n",
            "Global step: 4770,loss: 0.01278102\n",
            "\n",
            "Global step: 4771,loss: 0.01333408\n",
            "\n",
            "Global step: 4772,loss: 0.013151576\n",
            "\n",
            "Global step: 4773,loss: 0.014300565\n",
            "\n",
            "Global step: 4774,loss: 0.013524137\n",
            "\n",
            "Global step: 4775,loss: 0.013472198\n",
            "\n",
            "Global step: 4776,loss: 0.013403168\n",
            "\n",
            "Global step: 4777,loss: 0.013414522\n",
            "\n",
            "Global step: 4778,loss: 0.013058067\n",
            "\n",
            "Global step: 4779,loss: 0.0138607025\n",
            "\n",
            "Global step: 4780,loss: 0.013232524\n",
            "\n",
            "Global step: 4781,loss: 0.01382575\n",
            "\n",
            "Global step: 4782,loss: 0.012713928\n",
            "\n",
            "Global step: 4783,loss: 0.0138364\n",
            "\n",
            "Global step: 4784,loss: 0.013194941\n",
            "\n",
            "Global step: 4785,loss: 0.01303626\n",
            "\n",
            "Global step: 4786,loss: 0.012509658\n",
            "\n",
            "Global step: 4787,loss: 0.01321728\n",
            "\n",
            "Global step: 4788,loss: 0.013180173\n",
            "\n",
            "Global step: 4789,loss: 0.012795259\n",
            "\n",
            "Global step: 4790,loss: 0.013501986\n",
            "\n",
            "Global step: 4791,loss: 0.013578685\n",
            "\n",
            "Global step: 4792,loss: 0.012760872\n",
            "\n",
            "Global step: 4793,loss: 0.01457105\n",
            "\n",
            "Global step: 4794,loss: 0.012727979\n",
            "\n",
            "Global step: 4795,loss: 0.013487286\n",
            "\n",
            "Global step: 4796,loss: 0.013115029\n",
            "\n",
            "Global step: 4797,loss: 0.01276482\n",
            "\n",
            "Global step: 4798,loss: 0.013752746\n",
            "\n",
            "Global step: 4799,loss: 0.012897105\n",
            "\n",
            "Global step: 4800,loss: 0.012482305\n",
            "\n",
            "Global step: 4801,loss: 0.012759017\n",
            "\n",
            "Global step: 4802,loss: 0.013190922\n",
            "\n",
            "Global step: 4803,loss: 0.012763245\n",
            "\n",
            "Global step: 4804,loss: 0.0139591005\n",
            "\n",
            "Global step: 4805,loss: 0.012884629\n",
            "\n",
            "Global step: 4806,loss: 0.012608997\n",
            "\n",
            "Global step: 4807,loss: 0.0134867\n",
            "\n",
            "Global step: 4808,loss: 0.012813284\n",
            "\n",
            "Global step: 4809,loss: 0.013550653\n",
            "\n",
            "Global step: 4810,loss: 0.012751641\n",
            "\n",
            "Global step: 4811,loss: 0.012914743\n",
            "\n",
            "Global step: 4812,loss: 0.013413112\n",
            "\n",
            "Global step: 4813,loss: 0.013473299\n",
            "\n",
            "Global step: 4814,loss: 0.013603883\n",
            "\n",
            "Global step: 4815,loss: 0.013558766\n",
            "\n",
            "Global step: 4816,loss: 0.013455357\n",
            "\n",
            "Global step: 4817,loss: 0.012995023\n",
            "\n",
            "Global step: 4818,loss: 0.013645102\n",
            "\n",
            "Global step: 4819,loss: 0.012977797\n",
            "\n",
            "Global step: 4820,loss: 0.012833273\n",
            "\n",
            "Global step: 4821,loss: 0.014497653\n",
            "\n",
            "Global step: 4822,loss: 0.013570521\n",
            "\n",
            "Global step: 4823,loss: 0.013244925\n",
            "\n",
            "Global step: 4824,loss: 0.01339342\n",
            "\n",
            "Global step: 4825,loss: 0.012853188\n",
            "\n",
            "Global step: 4826,loss: 0.013490681\n",
            "\n",
            "Global step: 4827,loss: 0.0137683945\n",
            "\n",
            "Global step: 4828,loss: 0.013412695\n",
            "\n",
            "Global step: 4829,loss: 0.013300889\n",
            "\n",
            "Global step: 4830,loss: 0.0137413945\n",
            "\n",
            "Global step: 4831,loss: 0.013464898\n",
            "\n",
            "Global step: 4832,loss: 0.01295338\n",
            "\n",
            "Global step: 4833,loss: 0.013068866\n",
            "\n",
            "Global step: 4834,loss: 0.013671312\n",
            "\n",
            "Global step: 4835,loss: 0.012300671\n",
            "\n",
            "Global step: 4836,loss: 0.013229578\n",
            "\n",
            "Global step: 4837,loss: 0.012175523\n",
            "\n",
            "Global step: 4838,loss: 0.013198052\n",
            "\n",
            "Global step: 4839,loss: 0.013517982\n",
            "\n",
            "Global step: 4840,loss: 0.013866463\n",
            "\n",
            "Global step: 4841,loss: 0.013481852\n",
            "\n",
            "Global step: 4842,loss: 0.013067895\n",
            "\n",
            "Global step: 4843,loss: 0.01299987\n",
            "\n",
            "Global step: 4844,loss: 0.013291967\n",
            "\n",
            "Global step: 4845,loss: 0.013749627\n",
            "\n",
            "Global step: 4846,loss: 0.012808383\n",
            "\n",
            "Global step: 4847,loss: 0.01354668\n",
            "\n",
            "Global step: 4848,loss: 0.013147017\n",
            "\n",
            "Global step: 4849,loss: 0.012952074\n",
            "\n",
            "Global step: 4850,loss: 0.013073671\n",
            "\n",
            "Global step: 4851,loss: 0.012166141\n",
            "\n",
            "Global step: 4852,loss: 0.013163686\n",
            "\n",
            "Global step: 4853,loss: 0.013926942\n",
            "\n",
            "Global step: 4854,loss: 0.013761171\n",
            "\n",
            "Global step: 4855,loss: 0.013264181\n",
            "\n",
            "Global step: 4856,loss: 0.012496942\n",
            "\n",
            "Global step: 4857,loss: 0.013676499\n",
            "\n",
            "Global step: 4858,loss: 0.01363749\n",
            "\n",
            "Global step: 4859,loss: 0.0128562\n",
            "\n",
            "Global step: 4860,loss: 0.012624078\n",
            "\n",
            "Global step: 4861,loss: 0.013648465\n",
            "\n",
            "Global step: 4862,loss: 0.013513237\n",
            "\n",
            "Global step: 4863,loss: 0.013587052\n",
            "\n",
            "Global step: 4864,loss: 0.013890334\n",
            "\n",
            "Global step: 4865,loss: 0.013595282\n",
            "\n",
            "Global step: 4866,loss: 0.01260411\n",
            "\n",
            "Global step: 4867,loss: 0.012637596\n",
            "\n",
            "Global step: 4868,loss: 0.013276369\n",
            "\n",
            "Global step: 4869,loss: 0.013596499\n",
            "\n",
            "Global step: 4870,loss: 0.012885019\n",
            "\n",
            "Global step: 4871,loss: 0.01402947\n",
            "\n",
            "Global step: 4872,loss: 0.014360968\n",
            "\n",
            "Global step: 4873,loss: 0.0130813345\n",
            "\n",
            "Global step: 4874,loss: 0.014850379\n",
            "\n",
            "Global step: 4875,loss: 0.01329783\n",
            "\n",
            "Global step: 4876,loss: 0.013688697\n",
            "\n",
            "Global step: 4877,loss: 0.01286099\n",
            "\n",
            "Global step: 4878,loss: 0.0134752095\n",
            "\n",
            "Global step: 4879,loss: 0.013931124\n",
            "\n",
            "Global step: 4880,loss: 0.0133760385\n",
            "\n",
            "Global step: 4881,loss: 0.012865205\n",
            "\n",
            "Global step: 4882,loss: 0.013050788\n",
            "\n",
            "Global step: 4883,loss: 0.0132263955\n",
            "\n",
            "Global step: 4884,loss: 0.0140127195\n",
            "\n",
            "Global step: 4885,loss: 0.012855017\n",
            "\n",
            "Global step: 4886,loss: 0.014027409\n",
            "\n",
            "Global step: 4887,loss: 0.01350679\n",
            "\n",
            "Global step: 4888,loss: 0.013594791\n",
            "\n",
            "Global step: 4889,loss: 0.013543283\n",
            "\n",
            "Global step: 4890,loss: 0.013729723\n",
            "\n",
            "Global step: 4891,loss: 0.0133567825\n",
            "\n",
            "Global step: 4892,loss: 0.012707686\n",
            "\n",
            "Global step: 4893,loss: 0.013212898\n",
            "\n",
            "Global step: 4894,loss: 0.012664045\n",
            "\n",
            "Global step: 4895,loss: 0.012676203\n",
            "\n",
            "Global step: 4896,loss: 0.0129261445\n",
            "\n",
            "Global step: 4897,loss: 0.013726492\n",
            "\n",
            "Global step: 4898,loss: 0.014679197\n",
            "\n",
            "Global step: 4899,loss: 0.013428026\n",
            "\n",
            "Global step: 4900,loss: 0.013672661\n",
            "\n",
            "Global step: 4901,loss: 0.013335674\n",
            "\n",
            "Global step: 4902,loss: 0.012650423\n",
            "\n",
            "Global step: 4903,loss: 0.0137763815\n",
            "\n",
            "Global step: 4904,loss: 0.013056401\n",
            "\n",
            "Global step: 4905,loss: 0.012658388\n",
            "\n",
            "Global step: 4906,loss: 0.012854657\n",
            "\n",
            "Global step: 4907,loss: 0.013393381\n",
            "\n",
            "Global step: 4908,loss: 0.012944992\n",
            "\n",
            "Global step: 4909,loss: 0.012572937\n",
            "\n",
            "Global step: 4910,loss: 0.012718234\n",
            "\n",
            "Global step: 4911,loss: 0.012959399\n",
            "\n",
            "Global step: 4912,loss: 0.012689155\n",
            "\n",
            "Global step: 4913,loss: 0.013077729\n",
            "\n",
            "Global step: 4914,loss: 0.013170072\n",
            "\n",
            "Global step: 4915,loss: 0.012967529\n",
            "\n",
            "Global step: 4916,loss: 0.013145222\n",
            "\n",
            "Global step: 4917,loss: 0.01324989\n",
            "\n",
            "Global step: 4918,loss: 0.01278547\n",
            "\n",
            "Global step: 4919,loss: 0.012574289\n",
            "\n",
            "Global step: 4920,loss: 0.014151053\n",
            "\n",
            "Global step: 4921,loss: 0.013462102\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4921,Val_Loss: 0.01741842195195587,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:47:21.511731 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 24/50:\n",
            "Global step: 4922,loss: 0.014056539\n",
            "\n",
            "Global step: 4923,loss: 0.0128396675\n",
            "\n",
            "Global step: 4924,loss: 0.013478035\n",
            "\n",
            "Global step: 4925,loss: 0.013373672\n",
            "\n",
            "Global step: 4926,loss: 0.012211124\n",
            "\n",
            "Global step: 4927,loss: 0.0133216325\n",
            "\n",
            "Global step: 4928,loss: 0.012537445\n",
            "\n",
            "Global step: 4929,loss: 0.013632222\n",
            "\n",
            "Global step: 4930,loss: 0.012580485\n",
            "\n",
            "Global step: 4931,loss: 0.012791096\n",
            "\n",
            "Global step: 4932,loss: 0.012688044\n",
            "\n",
            "Global step: 4933,loss: 0.0128429895\n",
            "\n",
            "Global step: 4934,loss: 0.01319873\n",
            "\n",
            "Global step: 4935,loss: 0.012368739\n",
            "\n",
            "Global step: 4936,loss: 0.01348527\n",
            "\n",
            "Global step: 4937,loss: 0.013281754\n",
            "\n",
            "Global step: 4938,loss: 0.01231637\n",
            "\n",
            "Global step: 4939,loss: 0.012697161\n",
            "\n",
            "Global step: 4940,loss: 0.013012245\n",
            "\n",
            "Global step: 4941,loss: 0.013680247\n",
            "\n",
            "Global step: 4942,loss: 0.012962115\n",
            "\n",
            "Global step: 4943,loss: 0.012818393\n",
            "\n",
            "Global step: 4944,loss: 0.012443731\n",
            "\n",
            "Global step: 4945,loss: 0.01351323\n",
            "\n",
            "Global step: 4946,loss: 0.013130954\n",
            "\n",
            "Global step: 4947,loss: 0.012923721\n",
            "\n",
            "Global step: 4948,loss: 0.012642217\n",
            "\n",
            "Global step: 4949,loss: 0.012717224\n",
            "\n",
            "Global step: 4950,loss: 0.0132649895\n",
            "\n",
            "Global step: 4951,loss: 0.012208067\n",
            "\n",
            "Global step: 4952,loss: 0.012823061\n",
            "\n",
            "Global step: 4953,loss: 0.013287668\n",
            "\n",
            "Global step: 4954,loss: 0.013296451\n",
            "\n",
            "Global step: 4955,loss: 0.012879482\n",
            "\n",
            "Global step: 4956,loss: 0.013290578\n",
            "\n",
            "Global step: 4957,loss: 0.012430243\n",
            "\n",
            "Global step: 4958,loss: 0.012528836\n",
            "\n",
            "Global step: 4959,loss: 0.012923417\n",
            "\n",
            "Global step: 4960,loss: 0.013767807\n",
            "\n",
            "Global step: 4961,loss: 0.012645963\n",
            "\n",
            "Global step: 4962,loss: 0.012887371\n",
            "\n",
            "Global step: 4963,loss: 0.013858645\n",
            "\n",
            "Global step: 4964,loss: 0.013070665\n",
            "\n",
            "Global step: 4965,loss: 0.012722444\n",
            "\n",
            "Global step: 4966,loss: 0.013602799\n",
            "\n",
            "Global step: 4967,loss: 0.012439918\n",
            "\n",
            "Global step: 4968,loss: 0.013149619\n",
            "\n",
            "Global step: 4969,loss: 0.012393034\n",
            "\n",
            "Global step: 4970,loss: 0.012897102\n",
            "\n",
            "Global step: 4971,loss: 0.0129071195\n",
            "\n",
            "Global step: 4972,loss: 0.013131062\n",
            "\n",
            "Global step: 4973,loss: 0.0132569475\n",
            "\n",
            "Global step: 4974,loss: 0.012732895\n",
            "\n",
            "Global step: 4975,loss: 0.012864667\n",
            "\n",
            "Global step: 4976,loss: 0.013147771\n",
            "\n",
            "Global step: 4977,loss: 0.013033814\n",
            "\n",
            "Global step: 4978,loss: 0.012800353\n",
            "\n",
            "Global step: 4979,loss: 0.013220531\n",
            "\n",
            "Global step: 4980,loss: 0.013279009\n",
            "\n",
            "Global step: 4981,loss: 0.013307715\n",
            "\n",
            "Global step: 4982,loss: 0.013794156\n",
            "\n",
            "Global step: 4983,loss: 0.013798619\n",
            "\n",
            "Global step: 4984,loss: 0.012663573\n",
            "\n",
            "Global step: 4985,loss: 0.012773286\n",
            "\n",
            "Global step: 4986,loss: 0.01314968\n",
            "\n",
            "Global step: 4987,loss: 0.012553357\n",
            "\n",
            "Global step: 4988,loss: 0.013330565\n",
            "\n",
            "Global step: 4989,loss: 0.012726846\n",
            "\n",
            "Global step: 4990,loss: 0.01282443\n",
            "\n",
            "Global step: 4991,loss: 0.012945958\n",
            "\n",
            "Global step: 4992,loss: 0.013155864\n",
            "\n",
            "Global step: 4993,loss: 0.013312081\n",
            "\n",
            "Global step: 4994,loss: 0.012461389\n",
            "\n",
            "Global step: 4995,loss: 0.012769571\n",
            "\n",
            "Global step: 4996,loss: 0.0138752945\n",
            "\n",
            "Global step: 4997,loss: 0.013959218\n",
            "\n",
            "Global step: 4998,loss: 0.013078928\n",
            "\n",
            "Global step: 4999,loss: 0.01238608\n",
            "\n",
            "Global step: 5000,loss: 0.01360708\n",
            "\n",
            "Global step: 5001,loss: 0.011970906\n",
            "\n",
            "Global step: 5002,loss: 0.012786447\n",
            "\n",
            "Global step: 5003,loss: 0.012957798\n",
            "\n",
            "Global step: 5004,loss: 0.012041308\n",
            "\n",
            "Global step: 5005,loss: 0.0125796925\n",
            "\n",
            "Global step: 5006,loss: 0.013937994\n",
            "\n",
            "Global step: 5007,loss: 0.012412402\n",
            "\n",
            "Global step: 5008,loss: 0.013563587\n",
            "\n",
            "Global step: 5009,loss: 0.013255649\n",
            "\n",
            "Global step: 5010,loss: 0.012630161\n",
            "\n",
            "Global step: 5011,loss: 0.013185596\n",
            "\n",
            "Global step: 5012,loss: 0.01319921\n",
            "\n",
            "Global step: 5013,loss: 0.012734375\n",
            "\n",
            "Global step: 5014,loss: 0.012991689\n",
            "\n",
            "Global step: 5015,loss: 0.01267574\n",
            "\n",
            "Global step: 5016,loss: 0.013101065\n",
            "\n",
            "Global step: 5017,loss: 0.012909642\n",
            "\n",
            "Global step: 5018,loss: 0.013249843\n",
            "\n",
            "Global step: 5019,loss: 0.0133241\n",
            "\n",
            "Global step: 5020,loss: 0.013175189\n",
            "\n",
            "Global step: 5021,loss: 0.013322789\n",
            "\n",
            "Global step: 5022,loss: 0.0127148\n",
            "\n",
            "Global step: 5023,loss: 0.012112361\n",
            "\n",
            "Global step: 5024,loss: 0.012401635\n",
            "\n",
            "Global step: 5025,loss: 0.012941813\n",
            "\n",
            "Global step: 5026,loss: 0.012502129\n",
            "\n",
            "Global step: 5027,loss: 0.0126980385\n",
            "\n",
            "Global step: 5028,loss: 0.013144669\n",
            "\n",
            "Global step: 5029,loss: 0.012760324\n",
            "\n",
            "Global step: 5030,loss: 0.012696857\n",
            "\n",
            "Global step: 5031,loss: 0.012796641\n",
            "\n",
            "Global step: 5032,loss: 0.013437234\n",
            "\n",
            "Global step: 5033,loss: 0.01258619\n",
            "\n",
            "Global step: 5034,loss: 0.01254351\n",
            "\n",
            "Global step: 5035,loss: 0.013363617\n",
            "\n",
            "Global step: 5036,loss: 0.013103071\n",
            "\n",
            "Global step: 5037,loss: 0.012777699\n",
            "\n",
            "Global step: 5038,loss: 0.013241879\n",
            "\n",
            "Global step: 5039,loss: 0.013200433\n",
            "\n",
            "Global step: 5040,loss: 0.013178996\n",
            "\n",
            "Global step: 5041,loss: 0.013176015\n",
            "\n",
            "Global step: 5042,loss: 0.013388881\n",
            "\n",
            "Global step: 5043,loss: 0.012882241\n",
            "\n",
            "Global step: 5044,loss: 0.013337055\n",
            "\n",
            "Global step: 5045,loss: 0.013307111\n",
            "\n",
            "Global step: 5046,loss: 0.012811957\n",
            "\n",
            "Global step: 5047,loss: 0.012225922\n",
            "\n",
            "Global step: 5048,loss: 0.01367117\n",
            "\n",
            "Global step: 5049,loss: 0.012783139\n",
            "\n",
            "Global step: 5050,loss: 0.013609789\n",
            "\n",
            "Global step: 5051,loss: 0.0130469445\n",
            "\n",
            "Global step: 5052,loss: 0.013542433\n",
            "\n",
            "Global step: 5053,loss: 0.0135159865\n",
            "\n",
            "Global step: 5054,loss: 0.012786719\n",
            "\n",
            "Global step: 5055,loss: 0.013103651\n",
            "\n",
            "Global step: 5056,loss: 0.012866542\n",
            "\n",
            "Global step: 5057,loss: 0.012809404\n",
            "\n",
            "Global step: 5058,loss: 0.013051937\n",
            "\n",
            "Global step: 5059,loss: 0.012948822\n",
            "\n",
            "Global step: 5060,loss: 0.013228461\n",
            "\n",
            "Global step: 5061,loss: 0.01258342\n",
            "\n",
            "Global step: 5062,loss: 0.012612851\n",
            "\n",
            "Global step: 5063,loss: 0.013109388\n",
            "\n",
            "Global step: 5064,loss: 0.012373262\n",
            "\n",
            "Global step: 5065,loss: 0.0129022235\n",
            "\n",
            "Global step: 5066,loss: 0.013242499\n",
            "\n",
            "Global step: 5067,loss: 0.012802521\n",
            "\n",
            "Global step: 5068,loss: 0.013385673\n",
            "\n",
            "Global step: 5069,loss: 0.012534447\n",
            "\n",
            "Global step: 5070,loss: 0.012976962\n",
            "\n",
            "Global step: 5071,loss: 0.01353493\n",
            "\n",
            "Global step: 5072,loss: 0.012587785\n",
            "\n",
            "Global step: 5073,loss: 0.013848772\n",
            "\n",
            "Global step: 5074,loss: 0.013420112\n",
            "\n",
            "Global step: 5075,loss: 0.012515631\n",
            "\n",
            "Global step: 5076,loss: 0.0130625665\n",
            "\n",
            "Global step: 5077,loss: 0.012914262\n",
            "\n",
            "Global step: 5078,loss: 0.013013907\n",
            "\n",
            "Global step: 5079,loss: 0.012749849\n",
            "\n",
            "Global step: 5080,loss: 0.013597567\n",
            "\n",
            "Global step: 5081,loss: 0.013459155\n",
            "\n",
            "Global step: 5082,loss: 0.013568703\n",
            "\n",
            "Global step: 5083,loss: 0.013326211\n",
            "\n",
            "Global step: 5084,loss: 0.012701699\n",
            "\n",
            "Global step: 5085,loss: 0.0135034835\n",
            "\n",
            "Global step: 5086,loss: 0.0131901195\n",
            "\n",
            "Global step: 5087,loss: 0.013618519\n",
            "\n",
            "Global step: 5088,loss: 0.0131674595\n",
            "\n",
            "Global step: 5089,loss: 0.01307155\n",
            "\n",
            "Global step: 5090,loss: 0.012870361\n",
            "\n",
            "Global step: 5091,loss: 0.013419123\n",
            "\n",
            "Global step: 5092,loss: 0.013343293\n",
            "\n",
            "Global step: 5093,loss: 0.012797825\n",
            "\n",
            "Global step: 5094,loss: 0.012894002\n",
            "\n",
            "Global step: 5095,loss: 0.013186691\n",
            "\n",
            "Global step: 5096,loss: 0.012867959\n",
            "\n",
            "Global step: 5097,loss: 0.013481228\n",
            "\n",
            "Global step: 5098,loss: 0.013108934\n",
            "\n",
            "Global step: 5099,loss: 0.013526369\n",
            "\n",
            "Global step: 5100,loss: 0.012531912\n",
            "\n",
            "Global step: 5101,loss: 0.012748208\n",
            "\n",
            "Global step: 5102,loss: 0.01290662\n",
            "\n",
            "Global step: 5103,loss: 0.013444013\n",
            "\n",
            "Global step: 5104,loss: 0.012887017\n",
            "\n",
            "Global step: 5105,loss: 0.013118453\n",
            "\n",
            "Global step: 5106,loss: 0.012944855\n",
            "\n",
            "Global step: 5107,loss: 0.012182255\n",
            "\n",
            "Global step: 5108,loss: 0.013443401\n",
            "\n",
            "Global step: 5109,loss: 0.012736484\n",
            "\n",
            "Global step: 5110,loss: 0.012753389\n",
            "\n",
            "Global step: 5111,loss: 0.012951866\n",
            "\n",
            "Global step: 5112,loss: 0.013383753\n",
            "\n",
            "Global step: 5113,loss: 0.013366287\n",
            "\n",
            "Global step: 5114,loss: 0.013223098\n",
            "\n",
            "Global step: 5115,loss: 0.013616521\n",
            "\n",
            "Global step: 5116,loss: 0.013485964\n",
            "\n",
            "Global step: 5117,loss: 0.013026296\n",
            "\n",
            "Global step: 5118,loss: 0.013543033\n",
            "\n",
            "Global step: 5119,loss: 0.01184731\n",
            "\n",
            "Global step: 5120,loss: 0.012092261\n",
            "\n",
            "Global step: 5121,loss: 0.01275862\n",
            "\n",
            "Global step: 5122,loss: 0.012628024\n",
            "\n",
            "Global step: 5123,loss: 0.012833477\n",
            "\n",
            "Global step: 5124,loss: 0.0126825785\n",
            "\n",
            "Global step: 5125,loss: 0.0122653525\n",
            "\n",
            "Global step: 5126,loss: 0.012538312\n",
            "\n",
            "Global step: 5127,loss: 0.01261108\n",
            "\n",
            "Global step: 5128,loss: 0.0122751715\n",
            "\n",
            "Global step: 5129,loss: 0.012897102\n",
            "\n",
            "Global step: 5130,loss: 0.01306987\n",
            "\n",
            "Global step: 5131,loss: 0.012797913\n",
            "\n",
            "Global step: 5132,loss: 0.013236337\n",
            "\n",
            "Global step: 5133,loss: 0.012201567\n",
            "\n",
            "Global step: 5134,loss: 0.012267614\n",
            "\n",
            "Global step: 5135,loss: 0.012337738\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5135,Val_Loss: 0.01708100363612175,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:48:15.238521 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 25/50:\n",
            "Global step: 5136,loss: 0.012718625\n",
            "\n",
            "Global step: 5137,loss: 0.011600194\n",
            "\n",
            "Global step: 5138,loss: 0.012634548\n",
            "\n",
            "Global step: 5139,loss: 0.013932158\n",
            "\n",
            "Global step: 5140,loss: 0.013672041\n",
            "\n",
            "Global step: 5141,loss: 0.012990401\n",
            "\n",
            "Global step: 5142,loss: 0.012799756\n",
            "\n",
            "Global step: 5143,loss: 0.012240512\n",
            "\n",
            "Global step: 5144,loss: 0.012399382\n",
            "\n",
            "Global step: 5145,loss: 0.012622582\n",
            "\n",
            "Global step: 5146,loss: 0.01226015\n",
            "\n",
            "Global step: 5147,loss: 0.012895739\n",
            "\n",
            "Global step: 5148,loss: 0.012773137\n",
            "\n",
            "Global step: 5149,loss: 0.012407444\n",
            "\n",
            "Global step: 5150,loss: 0.012662706\n",
            "\n",
            "Global step: 5151,loss: 0.0128351245\n",
            "\n",
            "Global step: 5152,loss: 0.012924481\n",
            "\n",
            "Global step: 5153,loss: 0.012456499\n",
            "\n",
            "Global step: 5154,loss: 0.012776532\n",
            "\n",
            "Global step: 5155,loss: 0.01289601\n",
            "\n",
            "Global step: 5156,loss: 0.01284985\n",
            "\n",
            "Global step: 5157,loss: 0.013008636\n",
            "\n",
            "Global step: 5158,loss: 0.012757511\n",
            "\n",
            "Global step: 5159,loss: 0.012489014\n",
            "\n",
            "Global step: 5160,loss: 0.013688811\n",
            "\n",
            "Global step: 5161,loss: 0.012826778\n",
            "\n",
            "Global step: 5162,loss: 0.013261471\n",
            "\n",
            "Global step: 5163,loss: 0.012926828\n",
            "\n",
            "Global step: 5164,loss: 0.012119287\n",
            "\n",
            "Global step: 5165,loss: 0.011943843\n",
            "\n",
            "Global step: 5166,loss: 0.012870228\n",
            "\n",
            "Global step: 5167,loss: 0.01258984\n",
            "\n",
            "Global step: 5168,loss: 0.012225817\n",
            "\n",
            "Global step: 5169,loss: 0.012233462\n",
            "\n",
            "Global step: 5170,loss: 0.012880149\n",
            "\n",
            "Global step: 5171,loss: 0.01258643\n",
            "\n",
            "Global step: 5172,loss: 0.01335759\n",
            "\n",
            "Global step: 5173,loss: 0.012547174\n",
            "\n",
            "Global step: 5174,loss: 0.012435194\n",
            "\n",
            "Global step: 5175,loss: 0.012116642\n",
            "\n",
            "Global step: 5176,loss: 0.012498663\n",
            "\n",
            "Global step: 5177,loss: 0.01187573\n",
            "\n",
            "Global step: 5178,loss: 0.012829116\n",
            "\n",
            "Global step: 5179,loss: 0.012948752\n",
            "\n",
            "Global step: 5180,loss: 0.0119110895\n",
            "\n",
            "Global step: 5181,loss: 0.012928683\n",
            "\n",
            "Global step: 5182,loss: 0.012976539\n",
            "\n",
            "Global step: 5183,loss: 0.012351096\n",
            "\n",
            "Global step: 5184,loss: 0.013006464\n",
            "\n",
            "Global step: 5185,loss: 0.012673377\n",
            "\n",
            "Global step: 5186,loss: 0.012386513\n",
            "\n",
            "Global step: 5187,loss: 0.013503692\n",
            "\n",
            "Global step: 5188,loss: 0.012774596\n",
            "\n",
            "Global step: 5189,loss: 0.0122929225\n",
            "\n",
            "Global step: 5190,loss: 0.012641822\n",
            "\n",
            "Global step: 5191,loss: 0.013528896\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 5192.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:48:29.316129 140167885084416 supervisor.py:1050] Recording summary at step 5192.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5192,loss: 0.013124038\n",
            "\n",
            "Global step: 5193,loss: 0.012135831\n",
            "\n",
            "Global step: 5194,loss: 0.011792156\n",
            "\n",
            "Global step: 5195,loss: 0.012577493\n",
            "\n",
            "Global step: 5196,loss: 0.014166275\n",
            "\n",
            "Global step: 5197,loss: 0.013624558\n",
            "\n",
            "Global step: 5198,loss: 0.011811659\n",
            "\n",
            "Global step: 5199,loss: 0.012042947\n",
            "\n",
            "Global step: 5200,loss: 0.012875859\n",
            "\n",
            "Global step: 5201,loss: 0.01219708\n",
            "\n",
            "Global step: 5202,loss: 0.013021793\n",
            "\n",
            "Global step: 5203,loss: 0.0122094\n",
            "\n",
            "Global step: 5204,loss: 0.013516762\n",
            "\n",
            "Global step: 5205,loss: 0.0127896145\n",
            "\n",
            "Global step: 5206,loss: 0.012953606\n",
            "\n",
            "Global step: 5207,loss: 0.0127357785\n",
            "\n",
            "Global step: 5208,loss: 0.012306449\n",
            "\n",
            "Global step: 5209,loss: 0.01235763\n",
            "\n",
            "Global step: 5210,loss: 0.012938503\n",
            "\n",
            "Global step: 5211,loss: 0.012716784\n",
            "\n",
            "Global step: 5212,loss: 0.012527772\n",
            "\n",
            "Global step: 5213,loss: 0.01235842\n",
            "\n",
            "Global step: 5214,loss: 0.012457969\n",
            "\n",
            "Global step: 5215,loss: 0.013060354\n",
            "\n",
            "Global step: 5216,loss: 0.012330582\n",
            "\n",
            "Global step: 5217,loss: 0.012514976\n",
            "\n",
            "Global step: 5218,loss: 0.012670496\n",
            "\n",
            "Global step: 5219,loss: 0.013782772\n",
            "\n",
            "Global step: 5220,loss: 0.012886397\n",
            "\n",
            "Global step: 5221,loss: 0.011988604\n",
            "\n",
            "Global step: 5222,loss: 0.013096537\n",
            "\n",
            "Global step: 5223,loss: 0.012174517\n",
            "\n",
            "Global step: 5224,loss: 0.011553872\n",
            "\n",
            "Global step: 5225,loss: 0.012690972\n",
            "\n",
            "Global step: 5226,loss: 0.012812424\n",
            "\n",
            "Global step: 5227,loss: 0.013012354\n",
            "\n",
            "Global step: 5228,loss: 0.012802907\n",
            "\n",
            "Global step: 5229,loss: 0.012903303\n",
            "\n",
            "Global step: 5230,loss: 0.012520123\n",
            "\n",
            "Global step: 5231,loss: 0.012653697\n",
            "\n",
            "Global step: 5232,loss: 0.012802684\n",
            "\n",
            "Global step: 5233,loss: 0.012074196\n",
            "\n",
            "Global step: 5234,loss: 0.012694575\n",
            "\n",
            "Global step: 5235,loss: 0.012568855\n",
            "\n",
            "Global step: 5236,loss: 0.012542944\n",
            "\n",
            "Global step: 5237,loss: 0.012588143\n",
            "\n",
            "Global step: 5238,loss: 0.01259245\n",
            "\n",
            "Global step: 5239,loss: 0.012854098\n",
            "\n",
            "Global step: 5240,loss: 0.012501003\n",
            "\n",
            "Global step: 5241,loss: 0.012918398\n",
            "\n",
            "Global step: 5242,loss: 0.01216709\n",
            "\n",
            "Global step: 5243,loss: 0.012176665\n",
            "\n",
            "Global step: 5244,loss: 0.013124357\n",
            "\n",
            "Global step: 5245,loss: 0.011793653\n",
            "\n",
            "Global step: 5246,loss: 0.013445877\n",
            "\n",
            "Global step: 5247,loss: 0.01323622\n",
            "\n",
            "Global step: 5248,loss: 0.012552509\n",
            "\n",
            "Global step: 5249,loss: 0.013713106\n",
            "\n",
            "Global step: 5250,loss: 0.012470148\n",
            "\n",
            "Global step: 5251,loss: 0.012883619\n",
            "\n",
            "Global step: 5252,loss: 0.013593857\n",
            "\n",
            "Global step: 5253,loss: 0.012921227\n",
            "\n",
            "Global step: 5254,loss: 0.013218966\n",
            "\n",
            "Global step: 5255,loss: 0.012468514\n",
            "\n",
            "Global step: 5256,loss: 0.012122311\n",
            "\n",
            "Global step: 5257,loss: 0.012979677\n",
            "\n",
            "Global step: 5258,loss: 0.012742405\n",
            "\n",
            "Global step: 5259,loss: 0.013025672\n",
            "\n",
            "Global step: 5260,loss: 0.012713806\n",
            "\n",
            "Global step: 5261,loss: 0.013230146\n",
            "\n",
            "Global step: 5262,loss: 0.013212025\n",
            "\n",
            "Global step: 5263,loss: 0.012876954\n",
            "\n",
            "Global step: 5264,loss: 0.011853319\n",
            "\n",
            "Global step: 5265,loss: 0.012611364\n",
            "\n",
            "Global step: 5266,loss: 0.012416195\n",
            "\n",
            "Global step: 5267,loss: 0.01270549\n",
            "\n",
            "Global step: 5268,loss: 0.012675496\n",
            "\n",
            "Global step: 5269,loss: 0.01250525\n",
            "\n",
            "Global step: 5270,loss: 0.013363479\n",
            "\n",
            "Global step: 5271,loss: 0.012585808\n",
            "\n",
            "Global step: 5272,loss: 0.01272804\n",
            "\n",
            "Global step: 5273,loss: 0.012553043\n",
            "\n",
            "Global step: 5274,loss: 0.012316508\n",
            "\n",
            "Global step: 5275,loss: 0.012258676\n",
            "\n",
            "Global step: 5276,loss: 0.011924892\n",
            "\n",
            "Global step: 5277,loss: 0.012290082\n",
            "\n",
            "Global step: 5278,loss: 0.01260044\n",
            "\n",
            "Global step: 5279,loss: 0.012556828\n",
            "\n",
            "Global step: 5280,loss: 0.011718378\n",
            "\n",
            "Global step: 5281,loss: 0.012612549\n",
            "\n",
            "Global step: 5282,loss: 0.012335125\n",
            "\n",
            "Global step: 5283,loss: 0.012561754\n",
            "\n",
            "Global step: 5284,loss: 0.013047232\n",
            "\n",
            "Global step: 5285,loss: 0.013099959\n",
            "\n",
            "Global step: 5286,loss: 0.01264064\n",
            "\n",
            "Global step: 5287,loss: 0.012604704\n",
            "\n",
            "Global step: 5288,loss: 0.013436796\n",
            "\n",
            "Global step: 5289,loss: 0.011699689\n",
            "\n",
            "Global step: 5290,loss: 0.012814143\n",
            "\n",
            "Global step: 5291,loss: 0.012479784\n",
            "\n",
            "Global step: 5292,loss: 0.012586104\n",
            "\n",
            "Global step: 5293,loss: 0.012213149\n",
            "\n",
            "Global step: 5294,loss: 0.012057927\n",
            "\n",
            "Global step: 5295,loss: 0.012969914\n",
            "\n",
            "Global step: 5296,loss: 0.013792558\n",
            "\n",
            "Global step: 5297,loss: 0.012626536\n",
            "\n",
            "Global step: 5298,loss: 0.013275116\n",
            "\n",
            "Global step: 5299,loss: 0.01285601\n",
            "\n",
            "Global step: 5300,loss: 0.012717812\n",
            "\n",
            "Global step: 5301,loss: 0.012559289\n",
            "\n",
            "Global step: 5302,loss: 0.012879268\n",
            "\n",
            "Global step: 5303,loss: 0.012703208\n",
            "\n",
            "Global step: 5304,loss: 0.012493577\n",
            "\n",
            "Global step: 5305,loss: 0.012294715\n",
            "\n",
            "Global step: 5306,loss: 0.012564667\n",
            "\n",
            "Global step: 5307,loss: 0.012609379\n",
            "\n",
            "Global step: 5308,loss: 0.012959073\n",
            "\n",
            "Global step: 5309,loss: 0.012719992\n",
            "\n",
            "Global step: 5310,loss: 0.012451846\n",
            "\n",
            "Global step: 5311,loss: 0.012124672\n",
            "\n",
            "Global step: 5312,loss: 0.01263431\n",
            "\n",
            "Global step: 5313,loss: 0.013119295\n",
            "\n",
            "Global step: 5314,loss: 0.012030697\n",
            "\n",
            "Global step: 5315,loss: 0.013670692\n",
            "\n",
            "Global step: 5316,loss: 0.012945265\n",
            "\n",
            "Global step: 5317,loss: 0.012623029\n",
            "\n",
            "Global step: 5318,loss: 0.012682975\n",
            "\n",
            "Global step: 5319,loss: 0.013283251\n",
            "\n",
            "Global step: 5320,loss: 0.012701101\n",
            "\n",
            "Global step: 5321,loss: 0.012185566\n",
            "\n",
            "Global step: 5322,loss: 0.012086617\n",
            "\n",
            "Global step: 5323,loss: 0.012515231\n",
            "\n",
            "Global step: 5324,loss: 0.013624775\n",
            "\n",
            "Global step: 5325,loss: 0.012261202\n",
            "\n",
            "Global step: 5326,loss: 0.012396223\n",
            "\n",
            "Global step: 5327,loss: 0.012345464\n",
            "\n",
            "Global step: 5328,loss: 0.012557607\n",
            "\n",
            "Global step: 5329,loss: 0.01248713\n",
            "\n",
            "Global step: 5330,loss: 0.012961942\n",
            "\n",
            "Global step: 5331,loss: 0.012616478\n",
            "\n",
            "Global step: 5332,loss: 0.013177742\n",
            "\n",
            "Global step: 5333,loss: 0.012242866\n",
            "\n",
            "Global step: 5334,loss: 0.0134347165\n",
            "\n",
            "Global step: 5335,loss: 0.012051407\n",
            "\n",
            "Global step: 5336,loss: 0.012235729\n",
            "\n",
            "Global step: 5337,loss: 0.012450105\n",
            "\n",
            "Global step: 5338,loss: 0.012451292\n",
            "\n",
            "Global step: 5339,loss: 0.012401836\n",
            "\n",
            "Global step: 5340,loss: 0.012533649\n",
            "\n",
            "Global step: 5341,loss: 0.013193655\n",
            "\n",
            "Global step: 5342,loss: 0.011732223\n",
            "\n",
            "Global step: 5343,loss: 0.012354998\n",
            "\n",
            "Global step: 5344,loss: 0.0123588275\n",
            "\n",
            "Global step: 5345,loss: 0.012010302\n",
            "\n",
            "Global step: 5346,loss: 0.012425841\n",
            "\n",
            "Global step: 5347,loss: 0.012595165\n",
            "\n",
            "Global step: 5348,loss: 0.012795559\n",
            "\n",
            "Global step: 5349,loss: 0.011688586\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5349,Val_Loss: 0.016861822652189357,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:49:09.421952 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 26/50:\n",
            "Global step: 5350,loss: 0.012159629\n",
            "\n",
            "Global step: 5351,loss: 0.012495577\n",
            "\n",
            "Global step: 5352,loss: 0.012839587\n",
            "\n",
            "Global step: 5353,loss: 0.012652683\n",
            "\n",
            "Global step: 5354,loss: 0.012223404\n",
            "\n",
            "Global step: 5355,loss: 0.0125831645\n",
            "\n",
            "Global step: 5356,loss: 0.012987811\n",
            "\n",
            "Global step: 5357,loss: 0.012274295\n",
            "\n",
            "Global step: 5358,loss: 0.012367152\n",
            "\n",
            "Global step: 5359,loss: 0.012334675\n",
            "\n",
            "Global step: 5360,loss: 0.012327932\n",
            "\n",
            "Global step: 5361,loss: 0.013297138\n",
            "\n",
            "Global step: 5362,loss: 0.01287339\n",
            "\n",
            "Global step: 5363,loss: 0.0127216345\n",
            "\n",
            "Global step: 5364,loss: 0.012279304\n",
            "\n",
            "Global step: 5365,loss: 0.012216247\n",
            "\n",
            "Global step: 5366,loss: 0.012575761\n",
            "\n",
            "Global step: 5367,loss: 0.012585943\n",
            "\n",
            "Global step: 5368,loss: 0.012651008\n",
            "\n",
            "Global step: 5369,loss: 0.012485683\n",
            "\n",
            "Global step: 5370,loss: 0.012830533\n",
            "\n",
            "Global step: 5371,loss: 0.012455004\n",
            "\n",
            "Global step: 5372,loss: 0.013470513\n",
            "\n",
            "Global step: 5373,loss: 0.012456855\n",
            "\n",
            "Global step: 5374,loss: 0.0117485905\n",
            "\n",
            "Global step: 5375,loss: 0.013315016\n",
            "\n",
            "Global step: 5376,loss: 0.012464206\n",
            "\n",
            "Global step: 5377,loss: 0.012241495\n",
            "\n",
            "Global step: 5378,loss: 0.012515827\n",
            "\n",
            "Global step: 5379,loss: 0.012373574\n",
            "\n",
            "Global step: 5380,loss: 0.012526698\n",
            "\n",
            "Global step: 5381,loss: 0.012232323\n",
            "\n",
            "Global step: 5382,loss: 0.012673388\n",
            "\n",
            "Global step: 5383,loss: 0.0133046005\n",
            "\n",
            "Global step: 5384,loss: 0.012393168\n",
            "\n",
            "Global step: 5385,loss: 0.012650381\n",
            "\n",
            "Global step: 5386,loss: 0.012182984\n",
            "\n",
            "Global step: 5387,loss: 0.012533951\n",
            "\n",
            "Global step: 5388,loss: 0.012349138\n",
            "\n",
            "Global step: 5389,loss: 0.012039627\n",
            "\n",
            "Global step: 5390,loss: 0.01184907\n",
            "\n",
            "Global step: 5391,loss: 0.011870482\n",
            "\n",
            "Global step: 5392,loss: 0.012495434\n",
            "\n",
            "Global step: 5393,loss: 0.0129990615\n",
            "\n",
            "Global step: 5394,loss: 0.011812857\n",
            "\n",
            "Global step: 5395,loss: 0.012572334\n",
            "\n",
            "Global step: 5396,loss: 0.011844693\n",
            "\n",
            "Global step: 5397,loss: 0.0124298865\n",
            "\n",
            "Global step: 5398,loss: 0.01237079\n",
            "\n",
            "Global step: 5399,loss: 0.013458158\n",
            "\n",
            "Global step: 5400,loss: 0.012342068\n",
            "\n",
            "Global step: 5401,loss: 0.012675071\n",
            "\n",
            "Global step: 5402,loss: 0.012804145\n",
            "\n",
            "Global step: 5403,loss: 0.012131709\n",
            "\n",
            "Global step: 5404,loss: 0.01231052\n",
            "\n",
            "Global step: 5405,loss: 0.012589081\n",
            "\n",
            "Global step: 5406,loss: 0.012493551\n",
            "\n",
            "Global step: 5407,loss: 0.012788438\n",
            "\n",
            "Global step: 5408,loss: 0.012313428\n",
            "\n",
            "Global step: 5409,loss: 0.012934476\n",
            "\n",
            "Global step: 5410,loss: 0.012088863\n",
            "\n",
            "Global step: 5411,loss: 0.013439243\n",
            "\n",
            "Global step: 5412,loss: 0.0122701395\n",
            "\n",
            "Global step: 5413,loss: 0.0120684365\n",
            "\n",
            "Global step: 5414,loss: 0.01240698\n",
            "\n",
            "Global step: 5415,loss: 0.012241075\n",
            "\n",
            "Global step: 5416,loss: 0.012082273\n",
            "\n",
            "Global step: 5417,loss: 0.012200269\n",
            "\n",
            "Global step: 5418,loss: 0.012059646\n",
            "\n",
            "Global step: 5419,loss: 0.01234718\n",
            "\n",
            "Global step: 5420,loss: 0.012650277\n",
            "\n",
            "Global step: 5421,loss: 0.012341831\n",
            "\n",
            "Global step: 5422,loss: 0.012632747\n",
            "\n",
            "Global step: 5423,loss: 0.012795898\n",
            "\n",
            "Global step: 5424,loss: 0.012915845\n",
            "\n",
            "Global step: 5425,loss: 0.012295076\n",
            "\n",
            "Global step: 5426,loss: 0.01222346\n",
            "\n",
            "Global step: 5427,loss: 0.012336482\n",
            "\n",
            "Global step: 5428,loss: 0.012574144\n",
            "\n",
            "Global step: 5429,loss: 0.011792773\n",
            "\n",
            "Global step: 5430,loss: 0.011981179\n",
            "\n",
            "Global step: 5431,loss: 0.012340477\n",
            "\n",
            "Global step: 5432,loss: 0.012175793\n",
            "\n",
            "Global step: 5433,loss: 0.0125622405\n",
            "\n",
            "Global step: 5434,loss: 0.012266519\n",
            "\n",
            "Global step: 5435,loss: 0.013401161\n",
            "\n",
            "Global step: 5436,loss: 0.011676543\n",
            "\n",
            "Global step: 5437,loss: 0.012198386\n",
            "\n",
            "Global step: 5438,loss: 0.012627763\n",
            "\n",
            "Global step: 5439,loss: 0.011913327\n",
            "\n",
            "Global step: 5440,loss: 0.012894966\n",
            "\n",
            "Global step: 5441,loss: 0.012693436\n",
            "\n",
            "Global step: 5442,loss: 0.012099264\n",
            "\n",
            "Global step: 5443,loss: 0.012558992\n",
            "\n",
            "Global step: 5444,loss: 0.013217008\n",
            "\n",
            "Global step: 5445,loss: 0.011836092\n",
            "\n",
            "Global step: 5446,loss: 0.012346192\n",
            "\n",
            "Global step: 5447,loss: 0.013442206\n",
            "\n",
            "Global step: 5448,loss: 0.0126488665\n",
            "\n",
            "Global step: 5449,loss: 0.011880452\n",
            "\n",
            "Global step: 5450,loss: 0.013188376\n",
            "\n",
            "Global step: 5451,loss: 0.012393351\n",
            "\n",
            "Global step: 5452,loss: 0.012725371\n",
            "\n",
            "Global step: 5453,loss: 0.012425832\n",
            "\n",
            "Global step: 5454,loss: 0.01209101\n",
            "\n",
            "Global step: 5455,loss: 0.011877011\n",
            "\n",
            "Global step: 5456,loss: 0.012743301\n",
            "\n",
            "Global step: 5457,loss: 0.012157231\n",
            "\n",
            "Global step: 5458,loss: 0.012736635\n",
            "\n",
            "Global step: 5459,loss: 0.011982109\n",
            "\n",
            "Global step: 5460,loss: 0.012585741\n",
            "\n",
            "Global step: 5461,loss: 0.0117655555\n",
            "\n",
            "Global step: 5462,loss: 0.012482099\n",
            "\n",
            "Global step: 5463,loss: 0.012311385\n",
            "\n",
            "Global step: 5464,loss: 0.012303058\n",
            "\n",
            "Global step: 5465,loss: 0.012536263\n",
            "\n",
            "Global step: 5466,loss: 0.012982478\n",
            "\n",
            "Global step: 5467,loss: 0.012078024\n",
            "\n",
            "Global step: 5468,loss: 0.012900354\n",
            "\n",
            "Global step: 5469,loss: 0.012144537\n",
            "\n",
            "Global step: 5470,loss: 0.012777582\n",
            "\n",
            "Global step: 5471,loss: 0.011751737\n",
            "\n",
            "Global step: 5472,loss: 0.011962643\n",
            "\n",
            "Global step: 5473,loss: 0.013063239\n",
            "\n",
            "Global step: 5474,loss: 0.012359598\n",
            "\n",
            "Global step: 5475,loss: 0.01234703\n",
            "\n",
            "Global step: 5476,loss: 0.012711879\n",
            "\n",
            "Global step: 5477,loss: 0.012776227\n",
            "\n",
            "Global step: 5478,loss: 0.012304088\n",
            "\n",
            "Global step: 5479,loss: 0.011846939\n",
            "\n",
            "Global step: 5480,loss: 0.013072565\n",
            "\n",
            "Global step: 5481,loss: 0.0118004605\n",
            "\n",
            "Global step: 5482,loss: 0.012515362\n",
            "\n",
            "Global step: 5483,loss: 0.013013609\n",
            "\n",
            "Global step: 5484,loss: 0.012184913\n",
            "\n",
            "Global step: 5485,loss: 0.01218933\n",
            "\n",
            "Global step: 5486,loss: 0.012403953\n",
            "\n",
            "Global step: 5487,loss: 0.012270401\n",
            "\n",
            "Global step: 5488,loss: 0.012277165\n",
            "\n",
            "Global step: 5489,loss: 0.012652171\n",
            "\n",
            "Global step: 5490,loss: 0.012340199\n",
            "\n",
            "Global step: 5491,loss: 0.012157457\n",
            "\n",
            "Global step: 5492,loss: 0.012463154\n",
            "\n",
            "Global step: 5493,loss: 0.013144864\n",
            "\n",
            "Global step: 5494,loss: 0.012164462\n",
            "\n",
            "Global step: 5495,loss: 0.012153277\n",
            "\n",
            "Global step: 5496,loss: 0.012089376\n",
            "\n",
            "Global step: 5497,loss: 0.012337001\n",
            "\n",
            "Global step: 5498,loss: 0.012328378\n",
            "\n",
            "Global step: 5499,loss: 0.012060795\n",
            "\n",
            "Global step: 5500,loss: 0.011758167\n",
            "\n",
            "Global step: 5501,loss: 0.012846109\n",
            "\n",
            "Global step: 5502,loss: 0.012976956\n",
            "\n",
            "Global step: 5503,loss: 0.011901214\n",
            "\n",
            "Global step: 5504,loss: 0.01218062\n",
            "\n",
            "Global step: 5505,loss: 0.0116500445\n",
            "\n",
            "Global step: 5506,loss: 0.012384637\n",
            "\n",
            "Global step: 5507,loss: 0.011773702\n",
            "\n",
            "Global step: 5508,loss: 0.012041921\n",
            "\n",
            "Global step: 5509,loss: 0.012659004\n",
            "\n",
            "Global step: 5510,loss: 0.012390804\n",
            "\n",
            "Global step: 5511,loss: 0.013503716\n",
            "\n",
            "Global step: 5512,loss: 0.012969067\n",
            "\n",
            "Global step: 5513,loss: 0.012134892\n",
            "\n",
            "Global step: 5514,loss: 0.011571352\n",
            "\n",
            "Global step: 5515,loss: 0.012149569\n",
            "\n",
            "Global step: 5516,loss: 0.012590878\n",
            "\n",
            "Global step: 5517,loss: 0.012343406\n",
            "\n",
            "Global step: 5518,loss: 0.012180253\n",
            "\n",
            "Global step: 5519,loss: 0.012434264\n",
            "\n",
            "Global step: 5520,loss: 0.012209703\n",
            "\n",
            "Global step: 5521,loss: 0.01262567\n",
            "\n",
            "Global step: 5522,loss: 0.012151148\n",
            "\n",
            "Global step: 5523,loss: 0.012619886\n",
            "\n",
            "Global step: 5524,loss: 0.013447787\n",
            "\n",
            "Global step: 5525,loss: 0.013233347\n",
            "\n",
            "Global step: 5526,loss: 0.0129058\n",
            "\n",
            "Global step: 5527,loss: 0.012679207\n",
            "\n",
            "Global step: 5528,loss: 0.011991435\n",
            "\n",
            "Global step: 5529,loss: 0.011656327\n",
            "\n",
            "Global step: 5530,loss: 0.01190023\n",
            "\n",
            "Global step: 5531,loss: 0.01294753\n",
            "\n",
            "Global step: 5532,loss: 0.012623769\n",
            "\n",
            "Global step: 5533,loss: 0.012206169\n",
            "\n",
            "Global step: 5534,loss: 0.01229678\n",
            "\n",
            "Global step: 5535,loss: 0.012019336\n",
            "\n",
            "Global step: 5536,loss: 0.012603538\n",
            "\n",
            "Global step: 5537,loss: 0.012150595\n",
            "\n",
            "Global step: 5538,loss: 0.0130751785\n",
            "\n",
            "Global step: 5539,loss: 0.012146389\n",
            "\n",
            "Global step: 5540,loss: 0.012860203\n",
            "\n",
            "Global step: 5541,loss: 0.012359726\n",
            "\n",
            "Global step: 5542,loss: 0.012205517\n",
            "\n",
            "Global step: 5543,loss: 0.012292995\n",
            "\n",
            "Global step: 5544,loss: 0.012132574\n",
            "\n",
            "Global step: 5545,loss: 0.011955069\n",
            "\n",
            "Global step: 5546,loss: 0.012349123\n",
            "\n",
            "Global step: 5547,loss: 0.012534151\n",
            "\n",
            "Global step: 5548,loss: 0.011489787\n",
            "\n",
            "Global step: 5549,loss: 0.012008066\n",
            "\n",
            "Global step: 5550,loss: 0.012345501\n",
            "\n",
            "Global step: 5551,loss: 0.012530216\n",
            "\n",
            "Global step: 5552,loss: 0.012168004\n",
            "\n",
            "Global step: 5553,loss: 0.011912291\n",
            "\n",
            "Global step: 5554,loss: 0.012482457\n",
            "\n",
            "Global step: 5555,loss: 0.012062828\n",
            "\n",
            "Global step: 5556,loss: 0.012648104\n",
            "\n",
            "Global step: 5557,loss: 0.011884119\n",
            "\n",
            "Global step: 5558,loss: 0.0120005915\n",
            "\n",
            "Global step: 5559,loss: 0.012119186\n",
            "\n",
            "Global step: 5560,loss: 0.0120733995\n",
            "\n",
            "Global step: 5561,loss: 0.01228229\n",
            "\n",
            "Global step: 5562,loss: 0.012690038\n",
            "\n",
            "Global step: 5563,loss: 0.012032129\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5563,Val_Loss: 0.016497712435298843,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:50:03.336969 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 27/50:\n",
            "Global step: 5564,loss: 0.012725964\n",
            "\n",
            "Global step: 5565,loss: 0.012505525\n",
            "\n",
            "Global step: 5566,loss: 0.0123976655\n",
            "\n",
            "Global step: 5567,loss: 0.012511946\n",
            "\n",
            "Global step: 5568,loss: 0.012651858\n",
            "\n",
            "Global step: 5569,loss: 0.012060946\n",
            "\n",
            "Global step: 5570,loss: 0.012594597\n",
            "\n",
            "Global step: 5571,loss: 0.0119637465\n",
            "\n",
            "Global step: 5572,loss: 0.01184722\n",
            "\n",
            "Global step: 5573,loss: 0.012353889\n",
            "\n",
            "Global step: 5574,loss: 0.012657641\n",
            "\n",
            "Global step: 5575,loss: 0.011811366\n",
            "\n",
            "Global step: 5576,loss: 0.012204497\n",
            "\n",
            "Global step: 5577,loss: 0.012110974\n",
            "\n",
            "Global step: 5578,loss: 0.012198618\n",
            "\n",
            "Global step: 5579,loss: 0.012226194\n",
            "\n",
            "Global step: 5580,loss: 0.013010736\n",
            "\n",
            "Global step: 5581,loss: 0.012152532\n",
            "\n",
            "Global step: 5582,loss: 0.0122873485\n",
            "\n",
            "Global step: 5583,loss: 0.013338063\n",
            "\n",
            "Global step: 5584,loss: 0.011827069\n",
            "\n",
            "Global step: 5585,loss: 0.012442242\n",
            "\n",
            "Global step: 5586,loss: 0.011877336\n",
            "\n",
            "Global step: 5587,loss: 0.012291993\n",
            "\n",
            "Global step: 5588,loss: 0.011706052\n",
            "\n",
            "Global step: 5589,loss: 0.012485519\n",
            "\n",
            "Global step: 5590,loss: 0.0116467755\n",
            "\n",
            "Global step: 5591,loss: 0.01169797\n",
            "\n",
            "Global step: 5592,loss: 0.012508415\n",
            "\n",
            "Global step: 5593,loss: 0.01318066\n",
            "\n",
            "Global step: 5594,loss: 0.011920281\n",
            "\n",
            "Global step: 5595,loss: 0.012171236\n",
            "\n",
            "Global step: 5596,loss: 0.011542325\n",
            "\n",
            "Global step: 5597,loss: 0.012333171\n",
            "\n",
            "Global step: 5598,loss: 0.011772511\n",
            "\n",
            "Global step: 5599,loss: 0.011895741\n",
            "\n",
            "Global step: 5600,loss: 0.012345677\n",
            "\n",
            "Global step: 5601,loss: 0.01181715\n",
            "\n",
            "Global step: 5602,loss: 0.011727477\n",
            "\n",
            "Global step: 5603,loss: 0.011574904\n",
            "\n",
            "Global step: 5604,loss: 0.011743788\n",
            "\n",
            "Global step: 5605,loss: 0.011821484\n",
            "\n",
            "Global step: 5606,loss: 0.0120395515\n",
            "\n",
            "Global step: 5607,loss: 0.012195519\n",
            "\n",
            "Global step: 5608,loss: 0.012693265\n",
            "\n",
            "Global step: 5609,loss: 0.012566426\n",
            "\n",
            "Global step: 5610,loss: 0.011737642\n",
            "\n",
            "Global step: 5611,loss: 0.011873561\n",
            "\n",
            "Global step: 5612,loss: 0.012669507\n",
            "\n",
            "Global step: 5613,loss: 0.012076119\n",
            "\n",
            "Global step: 5614,loss: 0.011613828\n",
            "\n",
            "Global step: 5615,loss: 0.011814188\n",
            "\n",
            "Global step: 5616,loss: 0.012020556\n",
            "\n",
            "Global step: 5617,loss: 0.011941264\n",
            "\n",
            "Global step: 5618,loss: 0.011698982\n",
            "\n",
            "Global step: 5619,loss: 0.012800019\n",
            "\n",
            "Global step: 5620,loss: 0.011207455\n",
            "\n",
            "Global step: 5621,loss: 0.0118259005\n",
            "\n",
            "Global step: 5622,loss: 0.01217359\n",
            "\n",
            "Global step: 5623,loss: 0.012188803\n",
            "\n",
            "Global step: 5624,loss: 0.012536751\n",
            "\n",
            "Global step: 5625,loss: 0.013080107\n",
            "\n",
            "Global step: 5626,loss: 0.012914241\n",
            "\n",
            "Global step: 5627,loss: 0.011983238\n",
            "\n",
            "Global step: 5628,loss: 0.0118253175\n",
            "\n",
            "Global step: 5629,loss: 0.01230862\n",
            "\n",
            "Global step: 5630,loss: 0.012718491\n",
            "\n",
            "Global step: 5631,loss: 0.012392551\n",
            "\n",
            "Global step: 5632,loss: 0.011589888\n",
            "\n",
            "Global step: 5633,loss: 0.012885543\n",
            "\n",
            "Global step: 5634,loss: 0.01204337\n",
            "\n",
            "Global step: 5635,loss: 0.01218422\n",
            "\n",
            "Global step: 5636,loss: 0.012042811\n",
            "\n",
            "Global step: 5637,loss: 0.012074574\n",
            "\n",
            "Global step: 5638,loss: 0.012090372\n",
            "\n",
            "Global step: 5639,loss: 0.011971373\n",
            "\n",
            "Global step: 5640,loss: 0.012285007\n",
            "\n",
            "Global step: 5641,loss: 0.01237464\n",
            "\n",
            "Global step: 5642,loss: 0.012364946\n",
            "\n",
            "Global step: 5643,loss: 0.011230492\n",
            "\n",
            "Global step: 5644,loss: 0.012260174\n",
            "\n",
            "Global step: 5645,loss: 0.011542821\n",
            "\n",
            "Global step: 5646,loss: 0.0120184375\n",
            "\n",
            "Global step: 5647,loss: 0.011883966\n",
            "\n",
            "Global step: 5648,loss: 0.012207902\n",
            "\n",
            "Global step: 5649,loss: 0.011946725\n",
            "\n",
            "Global step: 5650,loss: 0.012560087\n",
            "\n",
            "Global step: 5651,loss: 0.012177969\n",
            "\n",
            "Global step: 5652,loss: 0.012549376\n",
            "\n",
            "Global step: 5653,loss: 0.0125417635\n",
            "\n",
            "Global step: 5654,loss: 0.012733188\n",
            "\n",
            "Global step: 5655,loss: 0.0116951335\n",
            "\n",
            "Global step: 5656,loss: 0.012454909\n",
            "\n",
            "Global step: 5657,loss: 0.012636057\n",
            "\n",
            "Global step: 5658,loss: 0.011658941\n",
            "\n",
            "Global step: 5659,loss: 0.013240128\n",
            "\n",
            "Global step: 5660,loss: 0.013061334\n",
            "\n",
            "Global step: 5661,loss: 0.012376301\n",
            "\n",
            "Global step: 5662,loss: 0.012717696\n",
            "\n",
            "Global step: 5663,loss: 0.012602717\n",
            "\n",
            "Global step: 5664,loss: 0.01238972\n",
            "\n",
            "Global step: 5665,loss: 0.01255023\n",
            "\n",
            "Global step: 5666,loss: 0.013119407\n",
            "\n",
            "Global step: 5667,loss: 0.011715112\n",
            "\n",
            "Global step: 5668,loss: 0.012416659\n",
            "\n",
            "Global step: 5669,loss: 0.011543632\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 5670.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:50:29.227255 140167885084416 supervisor.py:1050] Recording summary at step 5670.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5670,loss: 0.012225201\n",
            "\n",
            "Global step: 5671,loss: 0.012271704\n",
            "\n",
            "Global step: 5672,loss: 0.012114294\n",
            "\n",
            "Global step: 5673,loss: 0.012268763\n",
            "\n",
            "Global step: 5674,loss: 0.011947982\n",
            "\n",
            "Global step: 5675,loss: 0.012048504\n",
            "\n",
            "Global step: 5676,loss: 0.011924379\n",
            "\n",
            "Global step: 5677,loss: 0.012280804\n",
            "\n",
            "Global step: 5678,loss: 0.012512689\n",
            "\n",
            "Global step: 5679,loss: 0.01232519\n",
            "\n",
            "Global step: 5680,loss: 0.012409815\n",
            "\n",
            "Global step: 5681,loss: 0.0118985595\n",
            "\n",
            "Global step: 5682,loss: 0.011823782\n",
            "\n",
            "Global step: 5683,loss: 0.012481553\n",
            "\n",
            "Global step: 5684,loss: 0.012294224\n",
            "\n",
            "Global step: 5685,loss: 0.012016353\n",
            "\n",
            "Global step: 5686,loss: 0.01242643\n",
            "\n",
            "Global step: 5687,loss: 0.01209787\n",
            "\n",
            "Global step: 5688,loss: 0.01256925\n",
            "\n",
            "Global step: 5689,loss: 0.012029497\n",
            "\n",
            "Global step: 5690,loss: 0.012978784\n",
            "\n",
            "Global step: 5691,loss: 0.011481457\n",
            "\n",
            "Global step: 5692,loss: 0.011993883\n",
            "\n",
            "Global step: 5693,loss: 0.011453714\n",
            "\n",
            "Global step: 5694,loss: 0.011985047\n",
            "\n",
            "Global step: 5695,loss: 0.011844944\n",
            "\n",
            "Global step: 5696,loss: 0.012678308\n",
            "\n",
            "Global step: 5697,loss: 0.012228977\n",
            "\n",
            "Global step: 5698,loss: 0.012856621\n",
            "\n",
            "Global step: 5699,loss: 0.011778652\n",
            "\n",
            "Global step: 5700,loss: 0.011271913\n",
            "\n",
            "Global step: 5701,loss: 0.011695407\n",
            "\n",
            "Global step: 5702,loss: 0.012283033\n",
            "\n",
            "Global step: 5703,loss: 0.011904176\n",
            "\n",
            "Global step: 5704,loss: 0.011245443\n",
            "\n",
            "Global step: 5705,loss: 0.012144499\n",
            "\n",
            "Global step: 5706,loss: 0.0123259965\n",
            "\n",
            "Global step: 5707,loss: 0.011607674\n",
            "\n",
            "Global step: 5708,loss: 0.012031158\n",
            "\n",
            "Global step: 5709,loss: 0.01160818\n",
            "\n",
            "Global step: 5710,loss: 0.01221363\n",
            "\n",
            "Global step: 5711,loss: 0.011604115\n",
            "\n",
            "Global step: 5712,loss: 0.012018398\n",
            "\n",
            "Global step: 5713,loss: 0.011921639\n",
            "\n",
            "Global step: 5714,loss: 0.012407095\n",
            "\n",
            "Global step: 5715,loss: 0.01167978\n",
            "\n",
            "Global step: 5716,loss: 0.012132338\n",
            "\n",
            "Global step: 5717,loss: 0.012632369\n",
            "\n",
            "Global step: 5718,loss: 0.011973878\n",
            "\n",
            "Global step: 5719,loss: 0.0122253215\n",
            "\n",
            "Global step: 5720,loss: 0.012290852\n",
            "\n",
            "Global step: 5721,loss: 0.011800834\n",
            "\n",
            "Global step: 5722,loss: 0.011773746\n",
            "\n",
            "Global step: 5723,loss: 0.011468951\n",
            "\n",
            "Global step: 5724,loss: 0.012457957\n",
            "\n",
            "Global step: 5725,loss: 0.012820035\n",
            "\n",
            "Global step: 5726,loss: 0.012335112\n",
            "\n",
            "Global step: 5727,loss: 0.011759765\n",
            "\n",
            "Global step: 5728,loss: 0.012242311\n",
            "\n",
            "Global step: 5729,loss: 0.011927152\n",
            "\n",
            "Global step: 5730,loss: 0.011389379\n",
            "\n",
            "Global step: 5731,loss: 0.011936352\n",
            "\n",
            "Global step: 5732,loss: 0.011651408\n",
            "\n",
            "Global step: 5733,loss: 0.012052881\n",
            "\n",
            "Global step: 5734,loss: 0.011921302\n",
            "\n",
            "Global step: 5735,loss: 0.011504662\n",
            "\n",
            "Global step: 5736,loss: 0.011664544\n",
            "\n",
            "Global step: 5737,loss: 0.011319953\n",
            "\n",
            "Global step: 5738,loss: 0.011872729\n",
            "\n",
            "Global step: 5739,loss: 0.011700889\n",
            "\n",
            "Global step: 5740,loss: 0.012072347\n",
            "\n",
            "Global step: 5741,loss: 0.012499375\n",
            "\n",
            "Global step: 5742,loss: 0.012178678\n",
            "\n",
            "Global step: 5743,loss: 0.012436225\n",
            "\n",
            "Global step: 5744,loss: 0.012388592\n",
            "\n",
            "Global step: 5745,loss: 0.012150638\n",
            "\n",
            "Global step: 5746,loss: 0.012193304\n",
            "\n",
            "Global step: 5747,loss: 0.011816511\n",
            "\n",
            "Global step: 5748,loss: 0.011511603\n",
            "\n",
            "Global step: 5749,loss: 0.01240144\n",
            "\n",
            "Global step: 5750,loss: 0.012082584\n",
            "\n",
            "Global step: 5751,loss: 0.012002113\n",
            "\n",
            "Global step: 5752,loss: 0.012088066\n",
            "\n",
            "Global step: 5753,loss: 0.012253798\n",
            "\n",
            "Global step: 5754,loss: 0.012451387\n",
            "\n",
            "Global step: 5755,loss: 0.0116758365\n",
            "\n",
            "Global step: 5756,loss: 0.011426405\n",
            "\n",
            "Global step: 5757,loss: 0.012348301\n",
            "\n",
            "Global step: 5758,loss: 0.012131425\n",
            "\n",
            "Global step: 5759,loss: 0.012580005\n",
            "\n",
            "Global step: 5760,loss: 0.01238343\n",
            "\n",
            "Global step: 5761,loss: 0.011371661\n",
            "\n",
            "Global step: 5762,loss: 0.012378403\n",
            "\n",
            "Global step: 5763,loss: 0.012722999\n",
            "\n",
            "Global step: 5764,loss: 0.012072645\n",
            "\n",
            "Global step: 5765,loss: 0.011795727\n",
            "\n",
            "Global step: 5766,loss: 0.012150065\n",
            "\n",
            "Global step: 5767,loss: 0.012136108\n",
            "\n",
            "Global step: 5768,loss: 0.0114204995\n",
            "\n",
            "Global step: 5769,loss: 0.01258962\n",
            "\n",
            "Global step: 5770,loss: 0.012417455\n",
            "\n",
            "Global step: 5771,loss: 0.012145466\n",
            "\n",
            "Global step: 5772,loss: 0.012096254\n",
            "\n",
            "Global step: 5773,loss: 0.011741229\n",
            "\n",
            "Global step: 5774,loss: 0.011437601\n",
            "\n",
            "Global step: 5775,loss: 0.01168446\n",
            "\n",
            "Global step: 5776,loss: 0.01168791\n",
            "\n",
            "Global step: 5777,loss: 0.012306893\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5777,Val_Loss: 0.016230900673881957,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:50:57.414653 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 28/50:\n",
            "Global step: 5778,loss: 0.012048927\n",
            "\n",
            "Global step: 5779,loss: 0.01227511\n",
            "\n",
            "Global step: 5780,loss: 0.011610532\n",
            "\n",
            "Global step: 5781,loss: 0.011584962\n",
            "\n",
            "Global step: 5782,loss: 0.011888732\n",
            "\n",
            "Global step: 5783,loss: 0.011289921\n",
            "\n",
            "Global step: 5784,loss: 0.011551628\n",
            "\n",
            "Global step: 5785,loss: 0.01260076\n",
            "\n",
            "Global step: 5786,loss: 0.011730993\n",
            "\n",
            "Global step: 5787,loss: 0.011234024\n",
            "\n",
            "Global step: 5788,loss: 0.011521242\n",
            "\n",
            "Global step: 5789,loss: 0.011474775\n",
            "\n",
            "Global step: 5790,loss: 0.011914987\n",
            "\n",
            "Global step: 5791,loss: 0.012327854\n",
            "\n",
            "Global step: 5792,loss: 0.011652655\n",
            "\n",
            "Global step: 5793,loss: 0.012273854\n",
            "\n",
            "Global step: 5794,loss: 0.011549912\n",
            "\n",
            "Global step: 5795,loss: 0.012108577\n",
            "\n",
            "Global step: 5796,loss: 0.01170284\n",
            "\n",
            "Global step: 5797,loss: 0.011659039\n",
            "\n",
            "Global step: 5798,loss: 0.011902\n",
            "\n",
            "Global step: 5799,loss: 0.0114281075\n",
            "\n",
            "Global step: 5800,loss: 0.011469516\n",
            "\n",
            "Global step: 5801,loss: 0.012630792\n",
            "\n",
            "Global step: 5802,loss: 0.012067377\n",
            "\n",
            "Global step: 5803,loss: 0.011323749\n",
            "\n",
            "Global step: 5804,loss: 0.012103036\n",
            "\n",
            "Global step: 5805,loss: 0.012446037\n",
            "\n",
            "Global step: 5806,loss: 0.010994681\n",
            "\n",
            "Global step: 5807,loss: 0.012002841\n",
            "\n",
            "Global step: 5808,loss: 0.012374502\n",
            "\n",
            "Global step: 5809,loss: 0.011874816\n",
            "\n",
            "Global step: 5810,loss: 0.01230519\n",
            "\n",
            "Global step: 5811,loss: 0.011854444\n",
            "\n",
            "Global step: 5812,loss: 0.012539734\n",
            "\n",
            "Global step: 5813,loss: 0.012027862\n",
            "\n",
            "Global step: 5814,loss: 0.011956318\n",
            "\n",
            "Global step: 5815,loss: 0.011741092\n",
            "\n",
            "Global step: 5816,loss: 0.011698618\n",
            "\n",
            "Global step: 5817,loss: 0.011940698\n",
            "\n",
            "Global step: 5818,loss: 0.012674205\n",
            "\n",
            "Global step: 5819,loss: 0.012013301\n",
            "\n",
            "Global step: 5820,loss: 0.012273248\n",
            "\n",
            "Global step: 5821,loss: 0.012325993\n",
            "\n",
            "Global step: 5822,loss: 0.011912798\n",
            "\n",
            "Global step: 5823,loss: 0.012065266\n",
            "\n",
            "Global step: 5824,loss: 0.011412116\n",
            "\n",
            "Global step: 5825,loss: 0.011530854\n",
            "\n",
            "Global step: 5826,loss: 0.0116693145\n",
            "\n",
            "Global step: 5827,loss: 0.011509255\n",
            "\n",
            "Global step: 5828,loss: 0.011778401\n",
            "\n",
            "Global step: 5829,loss: 0.01194681\n",
            "\n",
            "Global step: 5830,loss: 0.011414959\n",
            "\n",
            "Global step: 5831,loss: 0.012012194\n",
            "\n",
            "Global step: 5832,loss: 0.012159083\n",
            "\n",
            "Global step: 5833,loss: 0.011832059\n",
            "\n",
            "Global step: 5834,loss: 0.012269416\n",
            "\n",
            "Global step: 5835,loss: 0.010793417\n",
            "\n",
            "Global step: 5836,loss: 0.012016462\n",
            "\n",
            "Global step: 5837,loss: 0.011988963\n",
            "\n",
            "Global step: 5838,loss: 0.011980639\n",
            "\n",
            "Global step: 5839,loss: 0.012553673\n",
            "\n",
            "Global step: 5840,loss: 0.011869587\n",
            "\n",
            "Global step: 5841,loss: 0.011723299\n",
            "\n",
            "Global step: 5842,loss: 0.011929368\n",
            "\n",
            "Global step: 5843,loss: 0.011820931\n",
            "\n",
            "Global step: 5844,loss: 0.011298719\n",
            "\n",
            "Global step: 5845,loss: 0.011533194\n",
            "\n",
            "Global step: 5846,loss: 0.011228075\n",
            "\n",
            "Global step: 5847,loss: 0.011850043\n",
            "\n",
            "Global step: 5848,loss: 0.011605868\n",
            "\n",
            "Global step: 5849,loss: 0.011803302\n",
            "\n",
            "Global step: 5850,loss: 0.011829525\n",
            "\n",
            "Global step: 5851,loss: 0.011345277\n",
            "\n",
            "Global step: 5852,loss: 0.011711239\n",
            "\n",
            "Global step: 5853,loss: 0.01210179\n",
            "\n",
            "Global step: 5854,loss: 0.011607581\n",
            "\n",
            "Global step: 5855,loss: 0.011762976\n",
            "\n",
            "Global step: 5856,loss: 0.011923084\n",
            "\n",
            "Global step: 5857,loss: 0.0121368\n",
            "\n",
            "Global step: 5858,loss: 0.011234388\n",
            "\n",
            "Global step: 5859,loss: 0.011313717\n",
            "\n",
            "Global step: 5860,loss: 0.012060801\n",
            "\n",
            "Global step: 5861,loss: 0.011777409\n",
            "\n",
            "Global step: 5862,loss: 0.011742734\n",
            "\n",
            "Global step: 5863,loss: 0.012004277\n",
            "\n",
            "Global step: 5864,loss: 0.011898082\n",
            "\n",
            "Global step: 5865,loss: 0.011000525\n",
            "\n",
            "Global step: 5866,loss: 0.011779448\n",
            "\n",
            "Global step: 5867,loss: 0.01211428\n",
            "\n",
            "Global step: 5868,loss: 0.011719648\n",
            "\n",
            "Global step: 5869,loss: 0.012242876\n",
            "\n",
            "Global step: 5870,loss: 0.012159526\n",
            "\n",
            "Global step: 5871,loss: 0.011431756\n",
            "\n",
            "Global step: 5872,loss: 0.011894214\n",
            "\n",
            "Global step: 5873,loss: 0.012213543\n",
            "\n",
            "Global step: 5874,loss: 0.011542441\n",
            "\n",
            "Global step: 5875,loss: 0.012166264\n",
            "\n",
            "Global step: 5876,loss: 0.012582284\n",
            "\n",
            "Global step: 5877,loss: 0.01175292\n",
            "\n",
            "Global step: 5878,loss: 0.011854384\n",
            "\n",
            "Global step: 5879,loss: 0.011841101\n",
            "\n",
            "Global step: 5880,loss: 0.012175778\n",
            "\n",
            "Global step: 5881,loss: 0.012167073\n",
            "\n",
            "Global step: 5882,loss: 0.011587434\n",
            "\n",
            "Global step: 5883,loss: 0.0121949855\n",
            "\n",
            "Global step: 5884,loss: 0.011657919\n",
            "\n",
            "Global step: 5885,loss: 0.011869123\n",
            "\n",
            "Global step: 5886,loss: 0.011372395\n",
            "\n",
            "Global step: 5887,loss: 0.012137856\n",
            "\n",
            "Global step: 5888,loss: 0.011759568\n",
            "\n",
            "Global step: 5889,loss: 0.011697508\n",
            "\n",
            "Global step: 5890,loss: 0.012379922\n",
            "\n",
            "Global step: 5891,loss: 0.011915215\n",
            "\n",
            "Global step: 5892,loss: 0.011737664\n",
            "\n",
            "Global step: 5893,loss: 0.012556473\n",
            "\n",
            "Global step: 5894,loss: 0.012066358\n",
            "\n",
            "Global step: 5895,loss: 0.012103917\n",
            "\n",
            "Global step: 5896,loss: 0.012578657\n",
            "\n",
            "Global step: 5897,loss: 0.012184339\n",
            "\n",
            "Global step: 5898,loss: 0.01206375\n",
            "\n",
            "Global step: 5899,loss: 0.011906601\n",
            "\n",
            "Global step: 5900,loss: 0.011921459\n",
            "\n",
            "Global step: 5901,loss: 0.011693598\n",
            "\n",
            "Global step: 5902,loss: 0.011692395\n",
            "\n",
            "Global step: 5903,loss: 0.012163155\n",
            "\n",
            "Global step: 5904,loss: 0.012613514\n",
            "\n",
            "Global step: 5905,loss: 0.0126527315\n",
            "\n",
            "Global step: 5906,loss: 0.011856021\n",
            "\n",
            "Global step: 5907,loss: 0.011301872\n",
            "\n",
            "Global step: 5908,loss: 0.012262797\n",
            "\n",
            "Global step: 5909,loss: 0.012257349\n",
            "\n",
            "Global step: 5910,loss: 0.011946011\n",
            "\n",
            "Global step: 5911,loss: 0.012135797\n",
            "\n",
            "Global step: 5912,loss: 0.011549958\n",
            "\n",
            "Global step: 5913,loss: 0.011954594\n",
            "\n",
            "Global step: 5914,loss: 0.011866183\n",
            "\n",
            "Global step: 5915,loss: 0.012508494\n",
            "\n",
            "Global step: 5916,loss: 0.011658924\n",
            "\n",
            "Global step: 5917,loss: 0.011620302\n",
            "\n",
            "Global step: 5918,loss: 0.011829779\n",
            "\n",
            "Global step: 5919,loss: 0.012115434\n",
            "\n",
            "Global step: 5920,loss: 0.012014615\n",
            "\n",
            "Global step: 5921,loss: 0.012116653\n",
            "\n",
            "Global step: 5922,loss: 0.011398051\n",
            "\n",
            "Global step: 5923,loss: 0.01214766\n",
            "\n",
            "Global step: 5924,loss: 0.011654472\n",
            "\n",
            "Global step: 5925,loss: 0.0123455785\n",
            "\n",
            "Global step: 5926,loss: 0.011268967\n",
            "\n",
            "Global step: 5927,loss: 0.012333608\n",
            "\n",
            "Global step: 5928,loss: 0.011930916\n",
            "\n",
            "Global step: 5929,loss: 0.011779698\n",
            "\n",
            "Global step: 5930,loss: 0.01264686\n",
            "\n",
            "Global step: 5931,loss: 0.012676484\n",
            "\n",
            "Global step: 5932,loss: 0.011523338\n",
            "\n",
            "Global step: 5933,loss: 0.01191517\n",
            "\n",
            "Global step: 5934,loss: 0.011866674\n",
            "\n",
            "Global step: 5935,loss: 0.012624687\n",
            "\n",
            "Global step: 5936,loss: 0.012298945\n",
            "\n",
            "Global step: 5937,loss: 0.011565718\n",
            "\n",
            "Global step: 5938,loss: 0.0120509155\n",
            "\n",
            "Global step: 5939,loss: 0.012306994\n",
            "\n",
            "Global step: 5940,loss: 0.011637682\n",
            "\n",
            "Global step: 5941,loss: 0.011569737\n",
            "\n",
            "Global step: 5942,loss: 0.011938789\n",
            "\n",
            "Global step: 5943,loss: 0.011877739\n",
            "\n",
            "Global step: 5944,loss: 0.010985569\n",
            "\n",
            "Global step: 5945,loss: 0.011798571\n",
            "\n",
            "Global step: 5946,loss: 0.0122774765\n",
            "\n",
            "Global step: 5947,loss: 0.011595647\n",
            "\n",
            "Global step: 5948,loss: 0.011834925\n",
            "\n",
            "Global step: 5949,loss: 0.011977722\n",
            "\n",
            "Global step: 5950,loss: 0.012763685\n",
            "\n",
            "Global step: 5951,loss: 0.011433583\n",
            "\n",
            "Global step: 5952,loss: 0.011831515\n",
            "\n",
            "Global step: 5953,loss: 0.011619849\n",
            "\n",
            "Global step: 5954,loss: 0.011593091\n",
            "\n",
            "Global step: 5955,loss: 0.011482038\n",
            "\n",
            "Global step: 5956,loss: 0.0120871235\n",
            "\n",
            "Global step: 5957,loss: 0.0120888\n",
            "\n",
            "Global step: 5958,loss: 0.0118187815\n",
            "\n",
            "Global step: 5959,loss: 0.011668713\n",
            "\n",
            "Global step: 5960,loss: 0.011785992\n",
            "\n",
            "Global step: 5961,loss: 0.01148906\n",
            "\n",
            "Global step: 5962,loss: 0.011454256\n",
            "\n",
            "Global step: 5963,loss: 0.012097266\n",
            "\n",
            "Global step: 5964,loss: 0.0127889225\n",
            "\n",
            "Global step: 5965,loss: 0.011683443\n",
            "\n",
            "Global step: 5966,loss: 0.011746177\n",
            "\n",
            "Global step: 5967,loss: 0.011189023\n",
            "\n",
            "Global step: 5968,loss: 0.011785458\n",
            "\n",
            "Global step: 5969,loss: 0.011388906\n",
            "\n",
            "Global step: 5970,loss: 0.0122483615\n",
            "\n",
            "Global step: 5971,loss: 0.011709717\n",
            "\n",
            "Global step: 5972,loss: 0.011937994\n",
            "\n",
            "Global step: 5973,loss: 0.011985386\n",
            "\n",
            "Global step: 5974,loss: 0.011219444\n",
            "\n",
            "Global step: 5975,loss: 0.011596188\n",
            "\n",
            "Global step: 5976,loss: 0.011835295\n",
            "\n",
            "Global step: 5977,loss: 0.0118405195\n",
            "\n",
            "Global step: 5978,loss: 0.012272436\n",
            "\n",
            "Global step: 5979,loss: 0.012334037\n",
            "\n",
            "Global step: 5980,loss: 0.011281821\n",
            "\n",
            "Global step: 5981,loss: 0.011060838\n",
            "\n",
            "Global step: 5982,loss: 0.011234371\n",
            "\n",
            "Global step: 5983,loss: 0.0118500795\n",
            "\n",
            "Global step: 5984,loss: 0.012445541\n",
            "\n",
            "Global step: 5985,loss: 0.012294868\n",
            "\n",
            "Global step: 5986,loss: 0.011294801\n",
            "\n",
            "Global step: 5987,loss: 0.011649433\n",
            "\n",
            "Global step: 5988,loss: 0.011989084\n",
            "\n",
            "Global step: 5989,loss: 0.011895173\n",
            "\n",
            "Global step: 5990,loss: 0.011199097\n",
            "\n",
            "Global step: 5991,loss: 0.011799928\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5991,Val_Loss: 0.01600607391446829,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:51:51.389812 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 29/50:\n",
            "Global step: 5992,loss: 0.012508715\n",
            "\n",
            "Global step: 5993,loss: 0.011168949\n",
            "\n",
            "Global step: 5994,loss: 0.012350249\n",
            "\n",
            "Global step: 5995,loss: 0.011011583\n",
            "\n",
            "Global step: 5996,loss: 0.011272839\n",
            "\n",
            "Global step: 5997,loss: 0.011541245\n",
            "\n",
            "Global step: 5998,loss: 0.011730173\n",
            "\n",
            "Global step: 5999,loss: 0.011824182\n",
            "\n",
            "Global step: 6000,loss: 0.012002446\n",
            "\n",
            "Global step: 6001,loss: 0.011606227\n",
            "\n",
            "Global step: 6002,loss: 0.011532911\n",
            "\n",
            "Global step: 6003,loss: 0.011864786\n",
            "\n",
            "Global step: 6004,loss: 0.011072469\n",
            "\n",
            "Global step: 6005,loss: 0.011830765\n",
            "\n",
            "Global step: 6006,loss: 0.011628563\n",
            "\n",
            "Global step: 6007,loss: 0.010942461\n",
            "\n",
            "Global step: 6008,loss: 0.011723388\n",
            "\n",
            "Global step: 6009,loss: 0.012069834\n",
            "\n",
            "Global step: 6010,loss: 0.011449444\n",
            "\n",
            "Global step: 6011,loss: 0.011770546\n",
            "\n",
            "Global step: 6012,loss: 0.0113630295\n",
            "\n",
            "Global step: 6013,loss: 0.011822227\n",
            "\n",
            "Global step: 6014,loss: 0.011901486\n",
            "\n",
            "Global step: 6015,loss: 0.011810318\n",
            "\n",
            "Global step: 6016,loss: 0.0113311\n",
            "\n",
            "Global step: 6017,loss: 0.011607962\n",
            "\n",
            "Global step: 6018,loss: 0.011927561\n",
            "\n",
            "Global step: 6019,loss: 0.01141975\n",
            "\n",
            "Global step: 6020,loss: 0.011653189\n",
            "\n",
            "Global step: 6021,loss: 0.011691747\n",
            "\n",
            "Global step: 6022,loss: 0.011771911\n",
            "\n",
            "Global step: 6023,loss: 0.011997788\n",
            "\n",
            "Global step: 6024,loss: 0.011220226\n",
            "\n",
            "Global step: 6025,loss: 0.011422649\n",
            "\n",
            "Global step: 6026,loss: 0.011724144\n",
            "\n",
            "Global step: 6027,loss: 0.011764572\n",
            "\n",
            "Global step: 6028,loss: 0.011028332\n",
            "\n",
            "Global step: 6029,loss: 0.011972078\n",
            "\n",
            "Global step: 6030,loss: 0.011996864\n",
            "\n",
            "Global step: 6031,loss: 0.011555871\n",
            "\n",
            "Global step: 6032,loss: 0.012311239\n",
            "\n",
            "Global step: 6033,loss: 0.01138261\n",
            "\n",
            "Global step: 6034,loss: 0.011759958\n",
            "\n",
            "Global step: 6035,loss: 0.011994583\n",
            "\n",
            "Global step: 6036,loss: 0.012373435\n",
            "\n",
            "Global step: 6037,loss: 0.011269645\n",
            "\n",
            "Global step: 6038,loss: 0.012231672\n",
            "\n",
            "Global step: 6039,loss: 0.011076736\n",
            "\n",
            "Global step: 6040,loss: 0.012079999\n",
            "\n",
            "Global step: 6041,loss: 0.011513756\n",
            "\n",
            "Global step: 6042,loss: 0.011160844\n",
            "\n",
            "Global step: 6043,loss: 0.012568117\n",
            "\n",
            "Global step: 6044,loss: 0.011523692\n",
            "\n",
            "Global step: 6045,loss: 0.011804803\n",
            "\n",
            "Global step: 6046,loss: 0.012373014\n",
            "\n",
            "Global step: 6047,loss: 0.0118615385\n",
            "\n",
            "Global step: 6048,loss: 0.011750068\n",
            "\n",
            "Global step: 6049,loss: 0.012001736\n",
            "\n",
            "Global step: 6050,loss: 0.012105798\n",
            "\n",
            "Global step: 6051,loss: 0.011806076\n",
            "\n",
            "Global step: 6052,loss: 0.010756475\n",
            "\n",
            "Global step: 6053,loss: 0.011530921\n",
            "\n",
            "Global step: 6054,loss: 0.01159622\n",
            "\n",
            "Global step: 6055,loss: 0.011539575\n",
            "\n",
            "Global step: 6056,loss: 0.011306554\n",
            "\n",
            "Global step: 6057,loss: 0.011279941\n",
            "\n",
            "Global step: 6058,loss: 0.010877194\n",
            "\n",
            "Global step: 6059,loss: 0.011744631\n",
            "\n",
            "Global step: 6060,loss: 0.012420227\n",
            "\n",
            "Global step: 6061,loss: 0.012028374\n",
            "\n",
            "Global step: 6062,loss: 0.010943307\n",
            "\n",
            "Global step: 6063,loss: 0.011163063\n",
            "\n",
            "Global step: 6064,loss: 0.012017291\n",
            "\n",
            "Global step: 6065,loss: 0.012132645\n",
            "\n",
            "Global step: 6066,loss: 0.012074772\n",
            "\n",
            "Global step: 6067,loss: 0.012045849\n",
            "\n",
            "Global step: 6068,loss: 0.011501189\n",
            "\n",
            "Global step: 6069,loss: 0.01215334\n",
            "\n",
            "Global step: 6070,loss: 0.012074002\n",
            "\n",
            "Global step: 6071,loss: 0.011390858\n",
            "\n",
            "Global step: 6072,loss: 0.0113004455\n",
            "\n",
            "Global step: 6073,loss: 0.01162766\n",
            "\n",
            "Global step: 6074,loss: 0.011771514\n",
            "\n",
            "Global step: 6075,loss: 0.01185167\n",
            "\n",
            "Global step: 6076,loss: 0.011426948\n",
            "\n",
            "Global step: 6077,loss: 0.011782862\n",
            "\n",
            "Global step: 6078,loss: 0.011201263\n",
            "\n",
            "Global step: 6079,loss: 0.012212275\n",
            "\n",
            "Global step: 6080,loss: 0.012141062\n",
            "\n",
            "Global step: 6081,loss: 0.011578661\n",
            "\n",
            "Global step: 6082,loss: 0.011749107\n",
            "\n",
            "Global step: 6083,loss: 0.012113741\n",
            "\n",
            "Global step: 6084,loss: 0.011547836\n",
            "\n",
            "Global step: 6085,loss: 0.011832844\n",
            "\n",
            "Global step: 6086,loss: 0.01140242\n",
            "\n",
            "Global step: 6087,loss: 0.011359394\n",
            "\n",
            "Global step: 6088,loss: 0.012317739\n",
            "\n",
            "Global step: 6089,loss: 0.01116341\n",
            "\n",
            "Global step: 6090,loss: 0.011918604\n",
            "\n",
            "Global step: 6091,loss: 0.011664922\n",
            "\n",
            "Global step: 6092,loss: 0.011941678\n",
            "\n",
            "Global step: 6093,loss: 0.01173039\n",
            "\n",
            "Global step: 6094,loss: 0.011657261\n",
            "\n",
            "Global step: 6095,loss: 0.012807048\n",
            "\n",
            "Global step: 6096,loss: 0.011161866\n",
            "\n",
            "Global step: 6097,loss: 0.012649968\n",
            "\n",
            "Global step: 6098,loss: 0.0108256955\n",
            "\n",
            "Global step: 6099,loss: 0.011500885\n",
            "\n",
            "Global step: 6100,loss: 0.011649367\n",
            "\n",
            "Global step: 6101,loss: 0.011671595\n",
            "\n",
            "Global step: 6102,loss: 0.011686981\n",
            "\n",
            "Global step: 6103,loss: 0.011245571\n",
            "\n",
            "Global step: 6104,loss: 0.01138802\n",
            "\n",
            "Global step: 6105,loss: 0.01189053\n",
            "\n",
            "Global step: 6106,loss: 0.012659536\n",
            "\n",
            "Global step: 6107,loss: 0.011219814\n",
            "\n",
            "Global step: 6108,loss: 0.011616788\n",
            "\n",
            "Global step: 6109,loss: 0.011349424\n",
            "\n",
            "Global step: 6110,loss: 0.011552579\n",
            "\n",
            "Global step: 6111,loss: 0.011849184\n",
            "\n",
            "Global step: 6112,loss: 0.012045378\n",
            "\n",
            "Global step: 6113,loss: 0.0113979345\n",
            "\n",
            "Global step: 6114,loss: 0.012231138\n",
            "\n",
            "Global step: 6115,loss: 0.011456185\n",
            "\n",
            "Global step: 6116,loss: 0.011630785\n",
            "\n",
            "Global step: 6117,loss: 0.010974544\n",
            "\n",
            "Global step: 6118,loss: 0.0119469\n",
            "\n",
            "Global step: 6119,loss: 0.011995274\n",
            "\n",
            "Global step: 6120,loss: 0.011448797\n",
            "\n",
            "Global step: 6121,loss: 0.011565391\n",
            "\n",
            "Global step: 6122,loss: 0.011476781\n",
            "\n",
            "Global step: 6123,loss: 0.011395407\n",
            "\n",
            "Global step: 6124,loss: 0.010707393\n",
            "\n",
            "Global step: 6125,loss: 0.011817576\n",
            "\n",
            "Global step: 6126,loss: 0.011990965\n",
            "\n",
            "Global step: 6127,loss: 0.011876176\n",
            "\n",
            "Global step: 6128,loss: 0.01194257\n",
            "\n",
            "Global step: 6129,loss: 0.011532674\n",
            "\n",
            "Global step: 6130,loss: 0.011377328\n",
            "\n",
            "Global step: 6131,loss: 0.012285876\n",
            "\n",
            "Global step: 6132,loss: 0.012311274\n",
            "\n",
            "Global step: 6133,loss: 0.011844899\n",
            "\n",
            "Global step: 6134,loss: 0.011516994\n",
            "\n",
            "Global step: 6135,loss: 0.011624865\n",
            "\n",
            "Global step: 6136,loss: 0.011850629\n",
            "\n",
            "Global step: 6137,loss: 0.011660292\n",
            "\n",
            "Global step: 6138,loss: 0.011254722\n",
            "\n",
            "Global step: 6139,loss: 0.011760261\n",
            "\n",
            "Global step: 6140,loss: 0.011874296\n",
            "\n",
            "Global step: 6141,loss: 0.012303599\n",
            "\n",
            "Global step: 6142,loss: 0.011731678\n",
            "\n",
            "Global step: 6143,loss: 0.011562809\n",
            "\n",
            "Global step: 6144,loss: 0.012033481\n",
            "\n",
            "Global step: 6145,loss: 0.011456923\n",
            "\n",
            "Global step: 6146,loss: 0.011703375\n",
            "\n",
            "Global step: 6147,loss: 0.012225877\n",
            "\n",
            "Global step: 6148,loss: 0.012079941\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 6149.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:52:29.285145 140167885084416 supervisor.py:1050] Recording summary at step 6149.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 6149,loss: 0.01167264\n",
            "\n",
            "Global step: 6150,loss: 0.011240345\n",
            "\n",
            "Global step: 6151,loss: 0.011940053\n",
            "\n",
            "Global step: 6152,loss: 0.011398993\n",
            "\n",
            "Global step: 6153,loss: 0.011391665\n",
            "\n",
            "Global step: 6154,loss: 0.012010777\n",
            "\n",
            "Global step: 6155,loss: 0.011550341\n",
            "\n",
            "Global step: 6156,loss: 0.01111542\n",
            "\n",
            "Global step: 6157,loss: 0.011523684\n",
            "\n",
            "Global step: 6158,loss: 0.012313083\n",
            "\n",
            "Global step: 6159,loss: 0.011654309\n",
            "\n",
            "Global step: 6160,loss: 0.011773624\n",
            "\n",
            "Global step: 6161,loss: 0.011257939\n",
            "\n",
            "Global step: 6162,loss: 0.011204484\n",
            "\n",
            "Global step: 6163,loss: 0.011275373\n",
            "\n",
            "Global step: 6164,loss: 0.011937748\n",
            "\n",
            "Global step: 6165,loss: 0.011719984\n",
            "\n",
            "Global step: 6166,loss: 0.01191935\n",
            "\n",
            "Global step: 6167,loss: 0.011781873\n",
            "\n",
            "Global step: 6168,loss: 0.011781399\n",
            "\n",
            "Global step: 6169,loss: 0.011741473\n",
            "\n",
            "Global step: 6170,loss: 0.012021315\n",
            "\n",
            "Global step: 6171,loss: 0.012158925\n",
            "\n",
            "Global step: 6172,loss: 0.011567274\n",
            "\n",
            "Global step: 6173,loss: 0.011213265\n",
            "\n",
            "Global step: 6174,loss: 0.011156559\n",
            "\n",
            "Global step: 6175,loss: 0.011797822\n",
            "\n",
            "Global step: 6176,loss: 0.011747121\n",
            "\n",
            "Global step: 6177,loss: 0.011223535\n",
            "\n",
            "Global step: 6178,loss: 0.012013256\n",
            "\n",
            "Global step: 6179,loss: 0.010470685\n",
            "\n",
            "Global step: 6180,loss: 0.01105142\n",
            "\n",
            "Global step: 6181,loss: 0.011179449\n",
            "\n",
            "Global step: 6182,loss: 0.012451668\n",
            "\n",
            "Global step: 6183,loss: 0.011522483\n",
            "\n",
            "Global step: 6184,loss: 0.012718374\n",
            "\n",
            "Global step: 6185,loss: 0.011908464\n",
            "\n",
            "Global step: 6186,loss: 0.011839248\n",
            "\n",
            "Global step: 6187,loss: 0.012218054\n",
            "\n",
            "Global step: 6188,loss: 0.011727093\n",
            "\n",
            "Global step: 6189,loss: 0.011701884\n",
            "\n",
            "Global step: 6190,loss: 0.011722202\n",
            "\n",
            "Global step: 6191,loss: 0.011722707\n",
            "\n",
            "Global step: 6192,loss: 0.012483713\n",
            "\n",
            "Global step: 6193,loss: 0.012382169\n",
            "\n",
            "Global step: 6194,loss: 0.011627012\n",
            "\n",
            "Global step: 6195,loss: 0.011381767\n",
            "\n",
            "Global step: 6196,loss: 0.011312407\n",
            "\n",
            "Global step: 6197,loss: 0.010741861\n",
            "\n",
            "Global step: 6198,loss: 0.011740459\n",
            "\n",
            "Global step: 6199,loss: 0.011613151\n",
            "\n",
            "Global step: 6200,loss: 0.011582751\n",
            "\n",
            "Global step: 6201,loss: 0.011753109\n",
            "\n",
            "Global step: 6202,loss: 0.011384863\n",
            "\n",
            "Global step: 6203,loss: 0.011647426\n",
            "\n",
            "Global step: 6204,loss: 0.011399217\n",
            "\n",
            "Global step: 6205,loss: 0.011563637\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 6205,val_loss: 0.01604038143628522\n",
            "\n",
            "Training for epoch 30/50:\n",
            "Global step: 6206,loss: 0.011103279\n",
            "\n",
            "Global step: 6207,loss: 0.012082635\n",
            "\n",
            "Global step: 6208,loss: 0.011201678\n",
            "\n",
            "Global step: 6209,loss: 0.011946114\n",
            "\n",
            "Global step: 6210,loss: 0.011591334\n",
            "\n",
            "Global step: 6211,loss: 0.011419364\n",
            "\n",
            "Global step: 6212,loss: 0.011882324\n",
            "\n",
            "Global step: 6213,loss: 0.011696842\n",
            "\n",
            "Global step: 6214,loss: 0.011041741\n",
            "\n",
            "Global step: 6215,loss: 0.011773118\n",
            "\n",
            "Global step: 6216,loss: 0.011194137\n",
            "\n",
            "Global step: 6217,loss: 0.012033984\n",
            "\n",
            "Global step: 6218,loss: 0.011924968\n",
            "\n",
            "Global step: 6219,loss: 0.011882753\n",
            "\n",
            "Global step: 6220,loss: 0.01178431\n",
            "\n",
            "Global step: 6221,loss: 0.012128613\n",
            "\n",
            "Global step: 6222,loss: 0.012130059\n",
            "\n",
            "Global step: 6223,loss: 0.011470795\n",
            "\n",
            "Global step: 6224,loss: 0.0113362335\n",
            "\n",
            "Global step: 6225,loss: 0.011691893\n",
            "\n",
            "Global step: 6226,loss: 0.011586141\n",
            "\n",
            "Global step: 6227,loss: 0.011435996\n",
            "\n",
            "Global step: 6228,loss: 0.011829278\n",
            "\n",
            "Global step: 6229,loss: 0.010969694\n",
            "\n",
            "Global step: 6230,loss: 0.011390671\n",
            "\n",
            "Global step: 6231,loss: 0.011006772\n",
            "\n",
            "Global step: 6232,loss: 0.010964078\n",
            "\n",
            "Global step: 6233,loss: 0.0113482475\n",
            "\n",
            "Global step: 6234,loss: 0.012111152\n",
            "\n",
            "Global step: 6235,loss: 0.011634094\n",
            "\n",
            "Global step: 6236,loss: 0.01186401\n",
            "\n",
            "Global step: 6237,loss: 0.010505005\n",
            "\n",
            "Global step: 6238,loss: 0.011476295\n",
            "\n",
            "Global step: 6239,loss: 0.011547657\n",
            "\n",
            "Global step: 6240,loss: 0.011636591\n",
            "\n",
            "Global step: 6241,loss: 0.012084362\n",
            "\n",
            "Global step: 6242,loss: 0.011322184\n",
            "\n",
            "Global step: 6243,loss: 0.011681358\n",
            "\n",
            "Global step: 6244,loss: 0.012221617\n",
            "\n",
            "Global step: 6245,loss: 0.011603715\n",
            "\n",
            "Global step: 6246,loss: 0.01085117\n",
            "\n",
            "Global step: 6247,loss: 0.011889564\n",
            "\n",
            "Global step: 6248,loss: 0.011289872\n",
            "\n",
            "Global step: 6249,loss: 0.012055877\n",
            "\n",
            "Global step: 6250,loss: 0.011408927\n",
            "\n",
            "Global step: 6251,loss: 0.011192678\n",
            "\n",
            "Global step: 6252,loss: 0.01108334\n",
            "\n",
            "Global step: 6253,loss: 0.011653749\n",
            "\n",
            "Global step: 6254,loss: 0.011951601\n",
            "\n",
            "Global step: 6255,loss: 0.0119488\n",
            "\n",
            "Global step: 6256,loss: 0.011273233\n",
            "\n",
            "Global step: 6257,loss: 0.011485434\n",
            "\n",
            "Global step: 6258,loss: 0.012223156\n",
            "\n",
            "Global step: 6259,loss: 0.010880168\n",
            "\n",
            "Global step: 6260,loss: 0.0114891045\n",
            "\n",
            "Global step: 6261,loss: 0.011287095\n",
            "\n",
            "Global step: 6262,loss: 0.012013562\n",
            "\n",
            "Global step: 6263,loss: 0.0122177955\n",
            "\n",
            "Global step: 6264,loss: 0.011869805\n",
            "\n",
            "Global step: 6265,loss: 0.01067762\n",
            "\n",
            "Global step: 6266,loss: 0.011158941\n",
            "\n",
            "Global step: 6267,loss: 0.011793368\n",
            "\n",
            "Global step: 6268,loss: 0.011925263\n",
            "\n",
            "Global step: 6269,loss: 0.011329135\n",
            "\n",
            "Global step: 6270,loss: 0.011163969\n",
            "\n",
            "Global step: 6271,loss: 0.01167941\n",
            "\n",
            "Global step: 6272,loss: 0.011549601\n",
            "\n",
            "Global step: 6273,loss: 0.011992357\n",
            "\n",
            "Global step: 6274,loss: 0.010885285\n",
            "\n",
            "Global step: 6275,loss: 0.011626124\n",
            "\n",
            "Global step: 6276,loss: 0.010953422\n",
            "\n",
            "Global step: 6277,loss: 0.0111183915\n",
            "\n",
            "Global step: 6278,loss: 0.011208965\n",
            "\n",
            "Global step: 6279,loss: 0.011118774\n",
            "\n",
            "Global step: 6280,loss: 0.011279253\n",
            "\n",
            "Global step: 6281,loss: 0.011373477\n",
            "\n",
            "Global step: 6282,loss: 0.011185596\n",
            "\n",
            "Global step: 6283,loss: 0.011101236\n",
            "\n",
            "Global step: 6284,loss: 0.01097957\n",
            "\n",
            "Global step: 6285,loss: 0.01123157\n",
            "\n",
            "Global step: 6286,loss: 0.010988025\n",
            "\n",
            "Global step: 6287,loss: 0.011504112\n",
            "\n",
            "Global step: 6288,loss: 0.011471856\n",
            "\n",
            "Global step: 6289,loss: 0.011774238\n",
            "\n",
            "Global step: 6290,loss: 0.01155498\n",
            "\n",
            "Global step: 6291,loss: 0.010978172\n",
            "\n",
            "Global step: 6292,loss: 0.011640122\n",
            "\n",
            "Global step: 6293,loss: 0.011465219\n",
            "\n",
            "Global step: 6294,loss: 0.010972375\n",
            "\n",
            "Global step: 6295,loss: 0.011426048\n",
            "\n",
            "Global step: 6296,loss: 0.011358816\n",
            "\n",
            "Global step: 6297,loss: 0.011400779\n",
            "\n",
            "Global step: 6298,loss: 0.011390266\n",
            "\n",
            "Global step: 6299,loss: 0.01181567\n",
            "\n",
            "Global step: 6300,loss: 0.011518062\n",
            "\n",
            "Global step: 6301,loss: 0.012022246\n",
            "\n",
            "Global step: 6302,loss: 0.011615124\n",
            "\n",
            "Global step: 6303,loss: 0.012500919\n",
            "\n",
            "Global step: 6304,loss: 0.010763892\n",
            "\n",
            "Global step: 6305,loss: 0.010757933\n",
            "\n",
            "Global step: 6306,loss: 0.011647556\n",
            "\n",
            "Global step: 6307,loss: 0.012240221\n",
            "\n",
            "Global step: 6308,loss: 0.010584315\n",
            "\n",
            "Global step: 6309,loss: 0.011647053\n",
            "\n",
            "Global step: 6310,loss: 0.011615975\n",
            "\n",
            "Global step: 6311,loss: 0.011950151\n",
            "\n",
            "Global step: 6312,loss: 0.011105992\n",
            "\n",
            "Global step: 6313,loss: 0.012043837\n",
            "\n",
            "Global step: 6314,loss: 0.011456043\n",
            "\n",
            "Global step: 6315,loss: 0.011340584\n",
            "\n",
            "Global step: 6316,loss: 0.010609137\n",
            "\n",
            "Global step: 6317,loss: 0.012550523\n",
            "\n",
            "Global step: 6318,loss: 0.011578876\n",
            "\n",
            "Global step: 6319,loss: 0.011224245\n",
            "\n",
            "Global step: 6320,loss: 0.011342837\n",
            "\n",
            "Global step: 6321,loss: 0.01133557\n",
            "\n",
            "Global step: 6322,loss: 0.011347966\n",
            "\n",
            "Global step: 6323,loss: 0.011393573\n",
            "\n",
            "Global step: 6324,loss: 0.010959468\n",
            "\n",
            "Global step: 6325,loss: 0.011377959\n",
            "\n",
            "Global step: 6326,loss: 0.011654118\n",
            "\n",
            "Global step: 6327,loss: 0.012089203\n",
            "\n",
            "Global step: 6328,loss: 0.011114911\n",
            "\n",
            "Global step: 6329,loss: 0.011250081\n",
            "\n",
            "Global step: 6330,loss: 0.011680637\n",
            "\n",
            "Global step: 6331,loss: 0.01112157\n",
            "\n",
            "Global step: 6332,loss: 0.011893703\n",
            "\n",
            "Global step: 6333,loss: 0.010718724\n",
            "\n",
            "Global step: 6334,loss: 0.011599347\n",
            "\n",
            "Global step: 6335,loss: 0.011341922\n",
            "\n",
            "Global step: 6336,loss: 0.010552126\n",
            "\n",
            "Global step: 6337,loss: 0.011281424\n",
            "\n",
            "Global step: 6338,loss: 0.011429989\n",
            "\n",
            "Global step: 6339,loss: 0.011803888\n",
            "\n",
            "Global step: 6340,loss: 0.0112055885\n",
            "\n",
            "Global step: 6341,loss: 0.011501004\n",
            "\n",
            "Global step: 6342,loss: 0.011628169\n",
            "\n",
            "Global step: 6343,loss: 0.011538312\n",
            "\n",
            "Global step: 6344,loss: 0.011189908\n",
            "\n",
            "Global step: 6345,loss: 0.011431278\n",
            "\n",
            "Global step: 6346,loss: 0.011859827\n",
            "\n",
            "Global step: 6347,loss: 0.0110827815\n",
            "\n",
            "Global step: 6348,loss: 0.010778305\n",
            "\n",
            "Global step: 6349,loss: 0.011435908\n",
            "\n",
            "Global step: 6350,loss: 0.011240503\n",
            "\n",
            "Global step: 6351,loss: 0.01191051\n",
            "\n",
            "Global step: 6352,loss: 0.011032062\n",
            "\n",
            "Global step: 6353,loss: 0.011539596\n",
            "\n",
            "Global step: 6354,loss: 0.011017904\n",
            "\n",
            "Global step: 6355,loss: 0.01153118\n",
            "\n",
            "Global step: 6356,loss: 0.011032046\n",
            "\n",
            "Global step: 6357,loss: 0.011388541\n",
            "\n",
            "Global step: 6358,loss: 0.011294446\n",
            "\n",
            "Global step: 6359,loss: 0.011959842\n",
            "\n",
            "Global step: 6360,loss: 0.011811197\n",
            "\n",
            "Global step: 6361,loss: 0.010951971\n",
            "\n",
            "Global step: 6362,loss: 0.010773889\n",
            "\n",
            "Global step: 6363,loss: 0.011422689\n",
            "\n",
            "Global step: 6364,loss: 0.011692087\n",
            "\n",
            "Global step: 6365,loss: 0.0113118505\n",
            "\n",
            "Global step: 6366,loss: 0.0117797395\n",
            "\n",
            "Global step: 6367,loss: 0.011344527\n",
            "\n",
            "Global step: 6368,loss: 0.011566113\n",
            "\n",
            "Global step: 6369,loss: 0.011832273\n",
            "\n",
            "Global step: 6370,loss: 0.011135912\n",
            "\n",
            "Global step: 6371,loss: 0.010571953\n",
            "\n",
            "Global step: 6372,loss: 0.011084706\n",
            "\n",
            "Global step: 6373,loss: 0.011328255\n",
            "\n",
            "Global step: 6374,loss: 0.011584792\n",
            "\n",
            "Global step: 6375,loss: 0.010777805\n",
            "\n",
            "Global step: 6376,loss: 0.011225386\n",
            "\n",
            "Global step: 6377,loss: 0.011446884\n",
            "\n",
            "Global step: 6378,loss: 0.010891099\n",
            "\n",
            "Global step: 6379,loss: 0.011105779\n",
            "\n",
            "Global step: 6380,loss: 0.010835501\n",
            "\n",
            "Global step: 6381,loss: 0.010955353\n",
            "\n",
            "Global step: 6382,loss: 0.01147532\n",
            "\n",
            "Global step: 6383,loss: 0.011743622\n",
            "\n",
            "Global step: 6384,loss: 0.011723338\n",
            "\n",
            "Global step: 6385,loss: 0.011186437\n",
            "\n",
            "Global step: 6386,loss: 0.011284076\n",
            "\n",
            "Global step: 6387,loss: 0.011277716\n",
            "\n",
            "Global step: 6388,loss: 0.012718649\n",
            "\n",
            "Global step: 6389,loss: 0.011056865\n",
            "\n",
            "Global step: 6390,loss: 0.011709394\n",
            "\n",
            "Global step: 6391,loss: 0.011639326\n",
            "\n",
            "Global step: 6392,loss: 0.011148355\n",
            "\n",
            "Global step: 6393,loss: 0.011429079\n",
            "\n",
            "Global step: 6394,loss: 0.011771647\n",
            "\n",
            "Global step: 6395,loss: 0.011749516\n",
            "\n",
            "Global step: 6396,loss: 0.011356718\n",
            "\n",
            "Global step: 6397,loss: 0.010561832\n",
            "\n",
            "Global step: 6398,loss: 0.011528211\n",
            "\n",
            "Global step: 6399,loss: 0.011729726\n",
            "\n",
            "Global step: 6400,loss: 0.011262675\n",
            "\n",
            "Global step: 6401,loss: 0.011505955\n",
            "\n",
            "Global step: 6402,loss: 0.010767669\n",
            "\n",
            "Global step: 6403,loss: 0.01195211\n",
            "\n",
            "Global step: 6404,loss: 0.0117550455\n",
            "\n",
            "Global step: 6405,loss: 0.01117691\n",
            "\n",
            "Global step: 6406,loss: 0.0110435\n",
            "\n",
            "Global step: 6407,loss: 0.011923986\n",
            "\n",
            "Global step: 6408,loss: 0.011305164\n",
            "\n",
            "Global step: 6409,loss: 0.011444169\n",
            "\n",
            "Global step: 6410,loss: 0.011628981\n",
            "\n",
            "Global step: 6411,loss: 0.011296503\n",
            "\n",
            "Global step: 6412,loss: 0.011708232\n",
            "\n",
            "Global step: 6413,loss: 0.0123932855\n",
            "\n",
            "Global step: 6414,loss: 0.011613931\n",
            "\n",
            "Global step: 6415,loss: 0.012059648\n",
            "\n",
            "Global step: 6416,loss: 0.010858506\n",
            "\n",
            "Global step: 6417,loss: 0.011621769\n",
            "\n",
            "Global step: 6418,loss: 0.012249265\n",
            "\n",
            "Global step: 6419,loss: 0.011255829\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 6419,val_loss: 0.01603481212728902\n",
            "\n",
            "Training for epoch 31/50:\n",
            "Global step: 6420,loss: 0.01146718\n",
            "\n",
            "Global step: 6421,loss: 0.011228678\n",
            "\n",
            "Global step: 6422,loss: 0.011436341\n",
            "\n",
            "Global step: 6423,loss: 0.011372397\n",
            "\n",
            "Global step: 6424,loss: 0.011053659\n",
            "\n",
            "Global step: 6425,loss: 0.010908721\n",
            "\n",
            "Global step: 6426,loss: 0.011149087\n",
            "\n",
            "Global step: 6427,loss: 0.010854159\n",
            "\n",
            "Global step: 6428,loss: 0.011764662\n",
            "\n",
            "Global step: 6429,loss: 0.010931212\n",
            "\n",
            "Global step: 6430,loss: 0.011038587\n",
            "\n",
            "Global step: 6431,loss: 0.010254812\n",
            "\n",
            "Global step: 6432,loss: 0.011303896\n",
            "\n",
            "Global step: 6433,loss: 0.01153171\n",
            "\n",
            "Global step: 6434,loss: 0.012057885\n",
            "\n",
            "Global step: 6435,loss: 0.010587194\n",
            "\n",
            "Global step: 6436,loss: 0.011576067\n",
            "\n",
            "Global step: 6437,loss: 0.0120703215\n",
            "\n",
            "Global step: 6438,loss: 0.011581295\n",
            "\n",
            "Global step: 6439,loss: 0.011147547\n",
            "\n",
            "Global step: 6440,loss: 0.01087764\n",
            "\n",
            "Global step: 6441,loss: 0.011620198\n",
            "\n",
            "Global step: 6442,loss: 0.010661985\n",
            "\n",
            "Global step: 6443,loss: 0.011252947\n",
            "\n",
            "Global step: 6444,loss: 0.011512543\n",
            "\n",
            "Global step: 6445,loss: 0.011466781\n",
            "\n",
            "Global step: 6446,loss: 0.011431479\n",
            "\n",
            "Global step: 6447,loss: 0.010563332\n",
            "\n",
            "Global step: 6448,loss: 0.0111685665\n",
            "\n",
            "Global step: 6449,loss: 0.010588877\n",
            "\n",
            "Global step: 6450,loss: 0.011455072\n",
            "\n",
            "Global step: 6451,loss: 0.011425678\n",
            "\n",
            "Global step: 6452,loss: 0.010822747\n",
            "\n",
            "Global step: 6453,loss: 0.011286651\n",
            "\n",
            "Global step: 6454,loss: 0.0116055375\n",
            "\n",
            "Global step: 6455,loss: 0.010661313\n",
            "\n",
            "Global step: 6456,loss: 0.01125405\n",
            "\n",
            "Global step: 6457,loss: 0.010676887\n",
            "\n",
            "Global step: 6458,loss: 0.011285708\n",
            "\n",
            "Global step: 6459,loss: 0.010798174\n",
            "\n",
            "Global step: 6460,loss: 0.010791388\n",
            "\n",
            "Global step: 6461,loss: 0.011674225\n",
            "\n",
            "Global step: 6462,loss: 0.0111164935\n",
            "\n",
            "Global step: 6463,loss: 0.01167153\n",
            "\n",
            "Global step: 6464,loss: 0.011230986\n",
            "\n",
            "Global step: 6465,loss: 0.011434472\n",
            "\n",
            "Global step: 6466,loss: 0.0107371295\n",
            "\n",
            "Global step: 6467,loss: 0.011092276\n",
            "\n",
            "Global step: 6468,loss: 0.011241732\n",
            "\n",
            "Global step: 6469,loss: 0.010425097\n",
            "\n",
            "Global step: 6470,loss: 0.011404512\n",
            "\n",
            "Global step: 6471,loss: 0.011711351\n",
            "\n",
            "Global step: 6472,loss: 0.012177638\n",
            "\n",
            "Global step: 6473,loss: 0.011267659\n",
            "\n",
            "Global step: 6474,loss: 0.010662696\n",
            "\n",
            "Global step: 6475,loss: 0.011764661\n",
            "\n",
            "Global step: 6476,loss: 0.0117272185\n",
            "\n",
            "Global step: 6477,loss: 0.01114584\n",
            "\n",
            "Global step: 6478,loss: 0.0112157725\n",
            "\n",
            "Global step: 6479,loss: 0.0114911385\n",
            "\n",
            "Global step: 6480,loss: 0.010881965\n",
            "\n",
            "Global step: 6481,loss: 0.011602199\n",
            "\n",
            "Global step: 6482,loss: 0.011520858\n",
            "\n",
            "Global step: 6483,loss: 0.011337047\n",
            "\n",
            "Global step: 6484,loss: 0.012249728\n",
            "\n",
            "Global step: 6485,loss: 0.011557321\n",
            "\n",
            "Global step: 6486,loss: 0.011710904\n",
            "\n",
            "Global step: 6487,loss: 0.011420776\n",
            "\n",
            "Global step: 6488,loss: 0.010538513\n",
            "\n",
            "Global step: 6489,loss: 0.011006999\n",
            "\n",
            "Global step: 6490,loss: 0.011184007\n",
            "\n",
            "Global step: 6491,loss: 0.0111821005\n",
            "\n",
            "Global step: 6492,loss: 0.011577955\n",
            "\n",
            "Global step: 6493,loss: 0.011011552\n",
            "\n",
            "Global step: 6494,loss: 0.011109734\n",
            "\n",
            "Global step: 6495,loss: 0.011488458\n",
            "\n",
            "Global step: 6496,loss: 0.010725541\n",
            "\n",
            "Global step: 6497,loss: 0.010756195\n",
            "\n",
            "Global step: 6498,loss: 0.011870093\n",
            "\n",
            "Global step: 6499,loss: 0.010802933\n",
            "\n",
            "Global step: 6500,loss: 0.011579138\n",
            "\n",
            "Global step: 6501,loss: 0.010892935\n",
            "\n",
            "Global step: 6502,loss: 0.011209528\n",
            "\n",
            "Global step: 6503,loss: 0.0112922415\n",
            "\n",
            "Global step: 6504,loss: 0.010742184\n",
            "\n",
            "Global step: 6505,loss: 0.010981519\n",
            "\n",
            "Global step: 6506,loss: 0.011495309\n",
            "\n",
            "Global step: 6507,loss: 0.011548281\n",
            "\n",
            "Global step: 6508,loss: 0.011118578\n",
            "\n",
            "Global step: 6509,loss: 0.010465885\n",
            "\n",
            "Global step: 6510,loss: 0.010842789\n",
            "\n",
            "Global step: 6511,loss: 0.010888819\n",
            "\n",
            "Global step: 6512,loss: 0.011109005\n",
            "\n",
            "Global step: 6513,loss: 0.011029958\n",
            "\n",
            "Global step: 6514,loss: 0.011828206\n",
            "\n",
            "Global step: 6515,loss: 0.01192008\n",
            "\n",
            "Global step: 6516,loss: 0.0111821145\n",
            "\n",
            "Global step: 6517,loss: 0.01062723\n",
            "\n",
            "Global step: 6518,loss: 0.011341382\n",
            "\n",
            "Global step: 6519,loss: 0.011140944\n",
            "\n",
            "Global step: 6520,loss: 0.010781833\n",
            "\n",
            "Global step: 6521,loss: 0.011193012\n",
            "\n",
            "Global step: 6522,loss: 0.010924088\n",
            "\n",
            "Global step: 6523,loss: 0.010934907\n",
            "\n",
            "Global step: 6524,loss: 0.011255823\n",
            "\n",
            "Global step: 6525,loss: 0.010935208\n",
            "\n",
            "Global step: 6526,loss: 0.010727151\n",
            "\n",
            "Global step: 6527,loss: 0.011278741\n",
            "\n",
            "Global step: 6528,loss: 0.011305164\n",
            "\n",
            "Global step: 6529,loss: 0.011618584\n",
            "\n",
            "Global step: 6530,loss: 0.012198656\n",
            "\n",
            "Global step: 6531,loss: 0.011390124\n",
            "\n",
            "Global step: 6532,loss: 0.011386293\n",
            "\n",
            "Global step: 6533,loss: 0.012314999\n",
            "\n",
            "Global step: 6534,loss: 0.011109029\n",
            "\n",
            "Global step: 6535,loss: 0.011304754\n",
            "\n",
            "Global step: 6536,loss: 0.010737948\n",
            "\n",
            "Global step: 6537,loss: 0.011184148\n",
            "\n",
            "Global step: 6538,loss: 0.01129785\n",
            "\n",
            "Global step: 6539,loss: 0.010879046\n",
            "\n",
            "Global step: 6540,loss: 0.010795927\n",
            "\n",
            "Global step: 6541,loss: 0.011303795\n",
            "\n",
            "Global step: 6542,loss: 0.011171341\n",
            "\n",
            "Global step: 6543,loss: 0.011223898\n",
            "\n",
            "Global step: 6544,loss: 0.011611469\n",
            "\n",
            "Global step: 6545,loss: 0.01102158\n",
            "\n",
            "Global step: 6546,loss: 0.0115293525\n",
            "\n",
            "Global step: 6547,loss: 0.011589588\n",
            "\n",
            "Global step: 6548,loss: 0.011373149\n",
            "\n",
            "Global step: 6549,loss: 0.011478158\n",
            "\n",
            "Global step: 6550,loss: 0.011102313\n",
            "\n",
            "Global step: 6551,loss: 0.011829408\n",
            "\n",
            "Global step: 6552,loss: 0.0112823825\n",
            "\n",
            "Global step: 6553,loss: 0.011275985\n",
            "\n",
            "Global step: 6554,loss: 0.011434336\n",
            "\n",
            "Global step: 6555,loss: 0.011807243\n",
            "\n",
            "Global step: 6556,loss: 0.011011146\n",
            "\n",
            "Global step: 6557,loss: 0.010827485\n",
            "\n",
            "Global step: 6558,loss: 0.010939814\n",
            "\n",
            "Global step: 6559,loss: 0.010582603\n",
            "\n",
            "Global step: 6560,loss: 0.011445863\n",
            "\n",
            "Global step: 6561,loss: 0.011478638\n",
            "\n",
            "Global step: 6562,loss: 0.011008094\n",
            "\n",
            "Global step: 6563,loss: 0.011451694\n",
            "\n",
            "Global step: 6564,loss: 0.011405575\n",
            "\n",
            "Global step: 6565,loss: 0.0118460385\n",
            "\n",
            "Global step: 6566,loss: 0.011024095\n",
            "\n",
            "Global step: 6567,loss: 0.011186613\n",
            "\n",
            "Global step: 6568,loss: 0.011397496\n",
            "\n",
            "Global step: 6569,loss: 0.011035459\n",
            "\n",
            "Global step: 6570,loss: 0.011316334\n",
            "\n",
            "Global step: 6571,loss: 0.011601375\n",
            "\n",
            "Global step: 6572,loss: 0.011531474\n",
            "\n",
            "Global step: 6573,loss: 0.011475668\n",
            "\n",
            "Global step: 6574,loss: 0.01121079\n",
            "\n",
            "Global step: 6575,loss: 0.011438568\n",
            "\n",
            "Global step: 6576,loss: 0.011546628\n",
            "\n",
            "Global step: 6577,loss: 0.011564562\n",
            "\n",
            "Global step: 6578,loss: 0.011326472\n",
            "\n",
            "Global step: 6579,loss: 0.011367729\n",
            "\n",
            "Global step: 6580,loss: 0.010736939\n",
            "\n",
            "Global step: 6581,loss: 0.01169083\n",
            "\n",
            "Global step: 6582,loss: 0.011097376\n",
            "\n",
            "Global step: 6583,loss: 0.011491681\n",
            "\n",
            "Global step: 6584,loss: 0.011552605\n",
            "\n",
            "Global step: 6585,loss: 0.0112623\n",
            "\n",
            "Global step: 6586,loss: 0.011475999\n",
            "\n",
            "Global step: 6587,loss: 0.01085714\n",
            "\n",
            "Global step: 6588,loss: 0.010873407\n",
            "\n",
            "Global step: 6589,loss: 0.011316222\n",
            "\n",
            "Global step: 6590,loss: 0.01147834\n",
            "\n",
            "Global step: 6591,loss: 0.010915787\n",
            "\n",
            "Global step: 6592,loss: 0.012002518\n",
            "\n",
            "Global step: 6593,loss: 0.011230688\n",
            "\n",
            "Global step: 6594,loss: 0.011300165\n",
            "\n",
            "Global step: 6595,loss: 0.011978337\n",
            "\n",
            "Global step: 6596,loss: 0.011171206\n",
            "\n",
            "Global step: 6597,loss: 0.011194102\n",
            "\n",
            "Global step: 6598,loss: 0.011059061\n",
            "\n",
            "Global step: 6599,loss: 0.01180955\n",
            "\n",
            "Global step: 6600,loss: 0.011355849\n",
            "\n",
            "Global step: 6601,loss: 0.011057271\n",
            "\n",
            "Global step: 6602,loss: 0.011102946\n",
            "\n",
            "Global step: 6603,loss: 0.011279771\n",
            "\n",
            "Global step: 6604,loss: 0.011235751\n",
            "\n",
            "Global step: 6605,loss: 0.010744795\n",
            "\n",
            "Global step: 6606,loss: 0.011163302\n",
            "\n",
            "Global step: 6607,loss: 0.011059514\n",
            "\n",
            "Global step: 6608,loss: 0.010696176\n",
            "\n",
            "Global step: 6609,loss: 0.011107728\n",
            "\n",
            "Global step: 6610,loss: 0.01163636\n",
            "\n",
            "Global step: 6611,loss: 0.011230172\n",
            "\n",
            "Global step: 6612,loss: 0.011689297\n",
            "\n",
            "Global step: 6613,loss: 0.010959181\n",
            "\n",
            "Global step: 6614,loss: 0.010877053\n",
            "\n",
            "Global step: 6615,loss: 0.010816238\n",
            "\n",
            "Global step: 6616,loss: 0.011241472\n",
            "\n",
            "Global step: 6617,loss: 0.011183189\n",
            "\n",
            "Global step: 6618,loss: 0.011216906\n",
            "\n",
            "Global step: 6619,loss: 0.011052095\n",
            "\n",
            "Global step: 6620,loss: 0.010745569\n",
            "\n",
            "Global step: 6621,loss: 0.011441169\n",
            "\n",
            "Global step: 6622,loss: 0.011068676\n",
            "\n",
            "Global step: 6623,loss: 0.011780896\n",
            "\n",
            "Global step: 6624,loss: 0.010974492\n",
            "\n",
            "Global step: 6625,loss: 0.011058676\n",
            "\n",
            "Global step: 6626,loss: 0.011677871\n",
            "\n",
            "Global step: 6627,loss: 0.012076276\n",
            "\n",
            "Global step: 6628,loss: 0.010430959\n",
            "\n",
            "Global step: 6629,loss: 0.0110725565\n",
            "\n",
            "Global step: 6630,loss: 0.011265597\n",
            "\n",
            "Global step: 6631,loss: 0.010818807\n",
            "\n",
            "Global step: 6632,loss: 0.011284554\n",
            "\n",
            "Global step: 6633,loss: 0.010919888\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 6634.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:54:29.173671 140167885084416 supervisor.py:1050] Recording summary at step 6634.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 6633,Val_Loss: 0.015809436573794012,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:54:30.024478 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 32/50:\n",
            "Global step: 6634,loss: 0.010730246\n",
            "\n",
            "Global step: 6635,loss: 0.011242079\n",
            "\n",
            "Global step: 6636,loss: 0.0109980935\n",
            "\n",
            "Global step: 6637,loss: 0.011470528\n",
            "\n",
            "Global step: 6638,loss: 0.010912605\n",
            "\n",
            "Global step: 6639,loss: 0.011004113\n",
            "\n",
            "Global step: 6640,loss: 0.011154493\n",
            "\n",
            "Global step: 6641,loss: 0.011501565\n",
            "\n",
            "Global step: 6642,loss: 0.01072807\n",
            "\n",
            "Global step: 6643,loss: 0.010748433\n",
            "\n",
            "Global step: 6644,loss: 0.010634509\n",
            "\n",
            "Global step: 6645,loss: 0.011053427\n",
            "\n",
            "Global step: 6646,loss: 0.011363329\n",
            "\n",
            "Global step: 6647,loss: 0.011012706\n",
            "\n",
            "Global step: 6648,loss: 0.011245221\n",
            "\n",
            "Global step: 6649,loss: 0.011355087\n",
            "\n",
            "Global step: 6650,loss: 0.011344034\n",
            "\n",
            "Global step: 6651,loss: 0.011643985\n",
            "\n",
            "Global step: 6652,loss: 0.01112291\n",
            "\n",
            "Global step: 6653,loss: 0.011541045\n",
            "\n",
            "Global step: 6654,loss: 0.010967144\n",
            "\n",
            "Global step: 6655,loss: 0.011848258\n",
            "\n",
            "Global step: 6656,loss: 0.010861246\n",
            "\n",
            "Global step: 6657,loss: 0.011401049\n",
            "\n",
            "Global step: 6658,loss: 0.010301781\n",
            "\n",
            "Global step: 6659,loss: 0.011004674\n",
            "\n",
            "Global step: 6660,loss: 0.011802187\n",
            "\n",
            "Global step: 6661,loss: 0.01118906\n",
            "\n",
            "Global step: 6662,loss: 0.011673222\n",
            "\n",
            "Global step: 6663,loss: 0.010614027\n",
            "\n",
            "Global step: 6664,loss: 0.01135074\n",
            "\n",
            "Global step: 6665,loss: 0.011657392\n",
            "\n",
            "Global step: 6666,loss: 0.01097522\n",
            "\n",
            "Global step: 6667,loss: 0.011294641\n",
            "\n",
            "Global step: 6668,loss: 0.011272447\n",
            "\n",
            "Global step: 6669,loss: 0.011161451\n",
            "\n",
            "Global step: 6670,loss: 0.011027108\n",
            "\n",
            "Global step: 6671,loss: 0.0110855205\n",
            "\n",
            "Global step: 6672,loss: 0.010965505\n",
            "\n",
            "Global step: 6673,loss: 0.010636181\n",
            "\n",
            "Global step: 6674,loss: 0.011774359\n",
            "\n",
            "Global step: 6675,loss: 0.011455464\n",
            "\n",
            "Global step: 6676,loss: 0.012078621\n",
            "\n",
            "Global step: 6677,loss: 0.011226116\n",
            "\n",
            "Global step: 6678,loss: 0.017098408\n",
            "\n",
            "Global step: 6679,loss: 0.12688924\n",
            "\n",
            "Global step: 6680,loss: 0.09325132\n",
            "\n",
            "Global step: 6681,loss: 0.08122956\n",
            "\n",
            "Global step: 6682,loss: 0.050751626\n",
            "\n",
            "Global step: 6683,loss: 0.04707457\n",
            "\n",
            "Global step: 6684,loss: 0.041098814\n",
            "\n",
            "Global step: 6685,loss: 0.038114034\n",
            "\n",
            "Global step: 6686,loss: 0.04172423\n",
            "\n",
            "Global step: 6687,loss: 0.028909286\n",
            "\n",
            "Global step: 6688,loss: 0.034618802\n",
            "\n",
            "Global step: 6689,loss: 0.03428725\n",
            "\n",
            "Global step: 6690,loss: 0.033560798\n",
            "\n",
            "Global step: 6691,loss: 0.031027384\n",
            "\n",
            "Global step: 6692,loss: 0.026755754\n",
            "\n",
            "Global step: 6693,loss: 0.028816208\n",
            "\n",
            "Global step: 6694,loss: 0.028860152\n",
            "\n",
            "Global step: 6695,loss: 0.024097392\n",
            "\n",
            "Global step: 6696,loss: 0.027443765\n",
            "\n",
            "Global step: 6697,loss: 0.02493523\n",
            "\n",
            "Global step: 6698,loss: 0.027429005\n",
            "\n",
            "Global step: 6699,loss: 0.027112972\n",
            "\n",
            "Global step: 6700,loss: 0.02202778\n",
            "\n",
            "Global step: 6701,loss: 0.023035005\n",
            "\n",
            "Global step: 6702,loss: 0.025457572\n",
            "\n",
            "Global step: 6703,loss: 0.020455156\n",
            "\n",
            "Global step: 6704,loss: 0.02332389\n",
            "\n",
            "Global step: 6705,loss: 0.0260125\n",
            "\n",
            "Global step: 6706,loss: 0.018452004\n",
            "\n",
            "Global step: 6707,loss: 0.021728223\n",
            "\n",
            "Global step: 6708,loss: 0.018383717\n",
            "\n",
            "Global step: 6709,loss: 0.019733682\n",
            "\n",
            "Global step: 6710,loss: 0.019019254\n",
            "\n",
            "Global step: 6711,loss: 0.020611346\n",
            "\n",
            "Global step: 6712,loss: 0.018350221\n",
            "\n",
            "Global step: 6713,loss: 0.023349274\n",
            "\n",
            "Global step: 6714,loss: 0.020213157\n",
            "\n",
            "Global step: 6715,loss: 0.020890206\n",
            "\n",
            "Global step: 6716,loss: 0.020021036\n",
            "\n",
            "Global step: 6717,loss: 0.01990037\n",
            "\n",
            "Global step: 6718,loss: 0.019059733\n",
            "\n",
            "Global step: 6719,loss: 0.01822165\n",
            "\n",
            "Global step: 6720,loss: 0.019454435\n",
            "\n",
            "Global step: 6721,loss: 0.019299977\n",
            "\n",
            "Global step: 6722,loss: 0.015628332\n",
            "\n",
            "Global step: 6723,loss: 0.02013842\n",
            "\n",
            "Global step: 6724,loss: 0.020943912\n",
            "\n",
            "Global step: 6725,loss: 0.01843175\n",
            "\n",
            "Global step: 6726,loss: 0.017734308\n",
            "\n",
            "Global step: 6727,loss: 0.019898841\n",
            "\n",
            "Global step: 6728,loss: 0.017099086\n",
            "\n",
            "Global step: 6729,loss: 0.017912006\n",
            "\n",
            "Global step: 6730,loss: 0.017925294\n",
            "\n",
            "Global step: 6731,loss: 0.015951958\n",
            "\n",
            "Global step: 6732,loss: 0.019876156\n",
            "\n",
            "Global step: 6733,loss: 0.017351447\n",
            "\n",
            "Global step: 6734,loss: 0.01867022\n",
            "\n",
            "Global step: 6735,loss: 0.019600939\n",
            "\n",
            "Global step: 6736,loss: 0.016770551\n",
            "\n",
            "Global step: 6737,loss: 0.01583265\n",
            "\n",
            "Global step: 6738,loss: 0.020569067\n",
            "\n",
            "Global step: 6739,loss: 0.016934067\n",
            "\n",
            "Global step: 6740,loss: 0.023053765\n",
            "\n",
            "Global step: 6741,loss: 0.018021226\n",
            "\n",
            "Global step: 6742,loss: 0.017264016\n",
            "\n",
            "Global step: 6743,loss: 0.018828236\n",
            "\n",
            "Global step: 6744,loss: 0.015090406\n",
            "\n",
            "Global step: 6745,loss: 0.016682686\n",
            "\n",
            "Global step: 6746,loss: 0.015540462\n",
            "\n",
            "Global step: 6747,loss: 0.016722595\n",
            "\n",
            "Global step: 6748,loss: 0.016141454\n",
            "\n",
            "Global step: 6749,loss: 0.016349832\n",
            "\n",
            "Global step: 6750,loss: 0.018044239\n",
            "\n",
            "Global step: 6751,loss: 0.016988933\n",
            "\n",
            "Global step: 6752,loss: 0.01711015\n",
            "\n",
            "Global step: 6753,loss: 0.01794258\n",
            "\n",
            "Global step: 6754,loss: 0.018572003\n",
            "\n",
            "Global step: 6755,loss: 0.017850067\n",
            "\n",
            "Global step: 6756,loss: 0.018324371\n",
            "\n",
            "Global step: 6757,loss: 0.015984735\n",
            "\n",
            "Global step: 6758,loss: 0.016265672\n",
            "\n",
            "Global step: 6759,loss: 0.014784908\n",
            "\n",
            "Global step: 6760,loss: 0.01894647\n",
            "\n",
            "Global step: 6761,loss: 0.0161729\n",
            "\n",
            "Global step: 6762,loss: 0.014143141\n",
            "\n",
            "Global step: 6763,loss: 0.016098045\n",
            "\n",
            "Global step: 6764,loss: 0.015019232\n",
            "\n",
            "Global step: 6765,loss: 0.017108297\n",
            "\n",
            "Global step: 6766,loss: 0.016981041\n",
            "\n",
            "Global step: 6767,loss: 0.014763155\n",
            "\n",
            "Global step: 6768,loss: 0.016036583\n",
            "\n",
            "Global step: 6769,loss: 0.015489066\n",
            "\n",
            "Global step: 6770,loss: 0.015942622\n",
            "\n",
            "Global step: 6771,loss: 0.018127814\n",
            "\n",
            "Global step: 6772,loss: 0.015217569\n",
            "\n",
            "Global step: 6773,loss: 0.015745861\n",
            "\n",
            "Global step: 6774,loss: 0.017115057\n",
            "\n",
            "Global step: 6775,loss: 0.0153948665\n",
            "\n",
            "Global step: 6776,loss: 0.017530832\n",
            "\n",
            "Global step: 6777,loss: 0.015484838\n",
            "\n",
            "Global step: 6778,loss: 0.015906231\n",
            "\n",
            "Global step: 6779,loss: 0.014818523\n",
            "\n",
            "Global step: 6780,loss: 0.019209687\n",
            "\n",
            "Global step: 6781,loss: 0.014476737\n",
            "\n",
            "Global step: 6782,loss: 0.01842795\n",
            "\n",
            "Global step: 6783,loss: 0.014836779\n",
            "\n",
            "Global step: 6784,loss: 0.014321277\n",
            "\n",
            "Global step: 6785,loss: 0.015624985\n",
            "\n",
            "Global step: 6786,loss: 0.015276757\n",
            "\n",
            "Global step: 6787,loss: 0.014529079\n",
            "\n",
            "Global step: 6788,loss: 0.016238503\n",
            "\n",
            "Global step: 6789,loss: 0.014324104\n",
            "\n",
            "Global step: 6790,loss: 0.017727979\n",
            "\n",
            "Global step: 6791,loss: 0.015214242\n",
            "\n",
            "Global step: 6792,loss: 0.020347089\n",
            "\n",
            "Global step: 6793,loss: 0.014011211\n",
            "\n",
            "Global step: 6794,loss: 0.016170736\n",
            "\n",
            "Global step: 6795,loss: 0.015175842\n",
            "\n",
            "Global step: 6796,loss: 0.015457774\n",
            "\n",
            "Global step: 6797,loss: 0.01482762\n",
            "\n",
            "Global step: 6798,loss: 0.015441422\n",
            "\n",
            "Global step: 6799,loss: 0.014705093\n",
            "\n",
            "Global step: 6800,loss: 0.014290219\n",
            "\n",
            "Global step: 6801,loss: 0.014322326\n",
            "\n",
            "Global step: 6802,loss: 0.017127737\n",
            "\n",
            "Global step: 6803,loss: 0.014455105\n",
            "\n",
            "Global step: 6804,loss: 0.018386044\n",
            "\n",
            "Global step: 6805,loss: 0.013688144\n",
            "\n",
            "Global step: 6806,loss: 0.013786054\n",
            "\n",
            "Global step: 6807,loss: 0.017665382\n",
            "\n",
            "Global step: 6808,loss: 0.013180117\n",
            "\n",
            "Global step: 6809,loss: 0.015431347\n",
            "\n",
            "Global step: 6810,loss: 0.015171682\n",
            "\n",
            "Global step: 6811,loss: 0.01357289\n",
            "\n",
            "Global step: 6812,loss: 0.014211314\n",
            "\n",
            "Global step: 6813,loss: 0.014369493\n",
            "\n",
            "Global step: 6814,loss: 0.01537541\n",
            "\n",
            "Global step: 6815,loss: 0.015698163\n",
            "\n",
            "Global step: 6816,loss: 0.015604742\n",
            "\n",
            "Global step: 6817,loss: 0.014966581\n",
            "\n",
            "Global step: 6818,loss: 0.014977914\n",
            "\n",
            "Global step: 6819,loss: 0.0129758455\n",
            "\n",
            "Global step: 6820,loss: 0.014467558\n",
            "\n",
            "Global step: 6821,loss: 0.012611056\n",
            "\n",
            "Global step: 6822,loss: 0.016667804\n",
            "\n",
            "Global step: 6823,loss: 0.013602542\n",
            "\n",
            "Global step: 6824,loss: 0.015316315\n",
            "\n",
            "Global step: 6825,loss: 0.015643949\n",
            "\n",
            "Global step: 6826,loss: 0.018357255\n",
            "\n",
            "Global step: 6827,loss: 0.016977342\n",
            "\n",
            "Global step: 6828,loss: 0.014073801\n",
            "\n",
            "Global step: 6829,loss: 0.013875292\n",
            "\n",
            "Global step: 6830,loss: 0.020910317\n",
            "\n",
            "Global step: 6831,loss: 0.018432798\n",
            "\n",
            "Global step: 6832,loss: 0.016813328\n",
            "\n",
            "Global step: 6833,loss: 0.0149130775\n",
            "\n",
            "Global step: 6834,loss: 0.01758501\n",
            "\n",
            "Global step: 6835,loss: 0.013559078\n",
            "\n",
            "Global step: 6836,loss: 0.02019738\n",
            "\n",
            "Global step: 6837,loss: 0.0137443105\n",
            "\n",
            "Global step: 6838,loss: 0.018481135\n",
            "\n",
            "Global step: 6839,loss: 0.015281038\n",
            "\n",
            "Global step: 6840,loss: 0.013282154\n",
            "\n",
            "Global step: 6841,loss: 0.01707025\n",
            "\n",
            "Global step: 6842,loss: 0.014231035\n",
            "\n",
            "Global step: 6843,loss: 0.013775286\n",
            "\n",
            "Global step: 6844,loss: 0.013556776\n",
            "\n",
            "Global step: 6845,loss: 0.013664884\n",
            "\n",
            "Global step: 6846,loss: 0.014487856\n",
            "\n",
            "Global step: 6847,loss: 0.013215591\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 6847,val_loss: 0.018048607940344435\n",
            "\n",
            "Training for epoch 33/50:\n",
            "Global step: 6848,loss: 0.014339716\n",
            "\n",
            "Global step: 6849,loss: 0.013242687\n",
            "\n",
            "Global step: 6850,loss: 0.014728502\n",
            "\n",
            "Global step: 6851,loss: 0.015295325\n",
            "\n",
            "Global step: 6852,loss: 0.013255009\n",
            "\n",
            "Global step: 6853,loss: 0.013416403\n",
            "\n",
            "Global step: 6854,loss: 0.016600184\n",
            "\n",
            "Global step: 6855,loss: 0.014264806\n",
            "\n",
            "Global step: 6856,loss: 0.014710931\n",
            "\n",
            "Global step: 6857,loss: 0.013409316\n",
            "\n",
            "Global step: 6858,loss: 0.017685227\n",
            "\n",
            "Global step: 6859,loss: 0.013696651\n",
            "\n",
            "Global step: 6860,loss: 0.015821667\n",
            "\n",
            "Global step: 6861,loss: 0.015228222\n",
            "\n",
            "Global step: 6862,loss: 0.013765563\n",
            "\n",
            "Global step: 6863,loss: 0.015049068\n",
            "\n",
            "Global step: 6864,loss: 0.013557318\n",
            "\n",
            "Global step: 6865,loss: 0.012746672\n",
            "\n",
            "Global step: 6866,loss: 0.013770941\n",
            "\n",
            "Global step: 6867,loss: 0.012934799\n",
            "\n",
            "Global step: 6868,loss: 0.013055989\n",
            "\n",
            "Global step: 6869,loss: 0.013082474\n",
            "\n",
            "Global step: 6870,loss: 0.01353575\n",
            "\n",
            "Global step: 6871,loss: 0.014379844\n",
            "\n",
            "Global step: 6872,loss: 0.0131283095\n",
            "\n",
            "Global step: 6873,loss: 0.014986562\n",
            "\n",
            "Global step: 6874,loss: 0.013083124\n",
            "\n",
            "Global step: 6875,loss: 0.013676204\n",
            "\n",
            "Global step: 6876,loss: 0.013619846\n",
            "\n",
            "Global step: 6877,loss: 0.014866836\n",
            "\n",
            "Global step: 6878,loss: 0.013540671\n",
            "\n",
            "Global step: 6879,loss: 0.01357787\n",
            "\n",
            "Global step: 6880,loss: 0.013631376\n",
            "\n",
            "Global step: 6881,loss: 0.0141518125\n",
            "\n",
            "Global step: 6882,loss: 0.013175221\n",
            "\n",
            "Global step: 6883,loss: 0.012552449\n",
            "\n",
            "Global step: 6884,loss: 0.017094718\n",
            "\n",
            "Global step: 6885,loss: 0.012905575\n",
            "\n",
            "Global step: 6886,loss: 0.012668917\n",
            "\n",
            "Global step: 6887,loss: 0.013391386\n",
            "\n",
            "Global step: 6888,loss: 0.013022352\n",
            "\n",
            "Global step: 6889,loss: 0.013086105\n",
            "\n",
            "Global step: 6890,loss: 0.012114411\n",
            "\n",
            "Global step: 6891,loss: 0.012488757\n",
            "\n",
            "Global step: 6892,loss: 0.012422775\n",
            "\n",
            "Global step: 6893,loss: 0.013061916\n",
            "\n",
            "Global step: 6894,loss: 0.0121254325\n",
            "\n",
            "Global step: 6895,loss: 0.013735703\n",
            "\n",
            "Global step: 6896,loss: 0.01357021\n",
            "\n",
            "Global step: 6897,loss: 0.016071873\n",
            "\n",
            "Global step: 6898,loss: 0.0129472185\n",
            "\n",
            "Global step: 6899,loss: 0.014410851\n",
            "\n",
            "Global step: 6900,loss: 0.012966354\n",
            "\n",
            "Global step: 6901,loss: 0.012621411\n",
            "\n",
            "Global step: 6902,loss: 0.013004993\n",
            "\n",
            "Global step: 6903,loss: 0.012692464\n",
            "\n",
            "Global step: 6904,loss: 0.013996679\n",
            "\n",
            "Global step: 6905,loss: 0.012608364\n",
            "\n",
            "Global step: 6906,loss: 0.013592145\n",
            "\n",
            "Global step: 6907,loss: 0.013010195\n",
            "\n",
            "Global step: 6908,loss: 0.011632856\n",
            "\n",
            "Global step: 6909,loss: 0.013109332\n",
            "\n",
            "Global step: 6910,loss: 0.012023052\n",
            "\n",
            "Global step: 6911,loss: 0.012145425\n",
            "\n",
            "Global step: 6912,loss: 0.011837113\n",
            "\n",
            "Global step: 6913,loss: 0.014920059\n",
            "\n",
            "Global step: 6914,loss: 0.013119363\n",
            "\n",
            "Global step: 6915,loss: 0.013166808\n",
            "\n",
            "Global step: 6916,loss: 0.012968538\n",
            "\n",
            "Global step: 6917,loss: 0.014656104\n",
            "\n",
            "Global step: 6918,loss: 0.012392249\n",
            "\n",
            "Global step: 6919,loss: 0.012182731\n",
            "\n",
            "Global step: 6920,loss: 0.014013572\n",
            "\n",
            "Global step: 6921,loss: 0.012793578\n",
            "\n",
            "Global step: 6922,loss: 0.012532954\n",
            "\n",
            "Global step: 6923,loss: 0.012567017\n",
            "\n",
            "Global step: 6924,loss: 0.013031216\n",
            "\n",
            "Global step: 6925,loss: 0.013362781\n",
            "\n",
            "Global step: 6926,loss: 0.013398856\n",
            "\n",
            "Global step: 6927,loss: 0.012943228\n",
            "\n",
            "Global step: 6928,loss: 0.0138403205\n",
            "\n",
            "Global step: 6929,loss: 0.013487351\n",
            "\n",
            "Global step: 6930,loss: 0.012518631\n",
            "\n",
            "Global step: 6931,loss: 0.012294876\n",
            "\n",
            "Global step: 6932,loss: 0.012854598\n",
            "\n",
            "Global step: 6933,loss: 0.014924603\n",
            "\n",
            "Global step: 6934,loss: 0.012702628\n",
            "\n",
            "Global step: 6935,loss: 0.013014522\n",
            "\n",
            "Global step: 6936,loss: 0.012190288\n",
            "\n",
            "Global step: 6937,loss: 0.014153901\n",
            "\n",
            "Global step: 6938,loss: 0.012906198\n",
            "\n",
            "Global step: 6939,loss: 0.01337076\n",
            "\n",
            "Global step: 6940,loss: 0.012512427\n",
            "\n",
            "Global step: 6941,loss: 0.0119891465\n",
            "\n",
            "Global step: 6942,loss: 0.01303975\n",
            "\n",
            "Global step: 6943,loss: 0.013083608\n",
            "\n",
            "Global step: 6944,loss: 0.011873378\n",
            "\n",
            "Global step: 6945,loss: 0.012497744\n",
            "\n",
            "Global step: 6946,loss: 0.013259875\n",
            "\n",
            "Global step: 6947,loss: 0.01191448\n",
            "\n",
            "Global step: 6948,loss: 0.014137666\n",
            "\n",
            "Global step: 6949,loss: 0.013028756\n",
            "\n",
            "Global step: 6950,loss: 0.012936547\n",
            "\n",
            "Global step: 6951,loss: 0.012194306\n",
            "\n",
            "Global step: 6952,loss: 0.012115113\n",
            "\n",
            "Global step: 6953,loss: 0.013639773\n",
            "\n",
            "Global step: 6954,loss: 0.013402532\n",
            "\n",
            "Global step: 6955,loss: 0.013748491\n",
            "\n",
            "Global step: 6956,loss: 0.011476659\n",
            "\n",
            "Global step: 6957,loss: 0.012366622\n",
            "\n",
            "Global step: 6958,loss: 0.016038906\n",
            "\n",
            "Global step: 6959,loss: 0.012057313\n",
            "\n",
            "Global step: 6960,loss: 0.012652531\n",
            "\n",
            "Global step: 6961,loss: 0.012177172\n",
            "\n",
            "Global step: 6962,loss: 0.0122127505\n",
            "\n",
            "Global step: 6963,loss: 0.011807492\n",
            "\n",
            "Global step: 6964,loss: 0.013151293\n",
            "\n",
            "Global step: 6965,loss: 0.012658304\n",
            "\n",
            "Global step: 6966,loss: 0.013030691\n",
            "\n",
            "Global step: 6967,loss: 0.012514673\n",
            "\n",
            "Global step: 6968,loss: 0.014091415\n",
            "\n",
            "Global step: 6969,loss: 0.012946058\n",
            "\n",
            "Global step: 6970,loss: 0.01206529\n",
            "\n",
            "Global step: 6971,loss: 0.012856761\n",
            "\n",
            "Global step: 6972,loss: 0.011962612\n",
            "\n",
            "Global step: 6973,loss: 0.011961989\n",
            "\n",
            "Global step: 6974,loss: 0.012600569\n",
            "\n",
            "Global step: 6975,loss: 0.011966478\n",
            "\n",
            "Global step: 6976,loss: 0.013009117\n",
            "\n",
            "Global step: 6977,loss: 0.0122973\n",
            "\n",
            "Global step: 6978,loss: 0.015025429\n",
            "\n",
            "Global step: 6979,loss: 0.011695462\n",
            "\n",
            "Global step: 6980,loss: 0.011986733\n",
            "\n",
            "Global step: 6981,loss: 0.012697858\n",
            "\n",
            "Global step: 6982,loss: 0.012541225\n",
            "\n",
            "Global step: 6983,loss: 0.012877169\n",
            "\n",
            "Global step: 6984,loss: 0.012625439\n",
            "\n",
            "Global step: 6985,loss: 0.013559103\n",
            "\n",
            "Global step: 6986,loss: 0.011995195\n",
            "\n",
            "Global step: 6987,loss: 0.01166735\n",
            "\n",
            "Global step: 6988,loss: 0.012352657\n",
            "\n",
            "Global step: 6989,loss: 0.012159113\n",
            "\n",
            "Global step: 6990,loss: 0.012167678\n",
            "\n",
            "Global step: 6991,loss: 0.012136138\n",
            "\n",
            "Global step: 6992,loss: 0.012730584\n",
            "\n",
            "Global step: 6993,loss: 0.013330691\n",
            "\n",
            "Global step: 6994,loss: 0.012611769\n",
            "\n",
            "Global step: 6995,loss: 0.01215034\n",
            "\n",
            "Global step: 6996,loss: 0.013777258\n",
            "\n",
            "Global step: 6997,loss: 0.011922023\n",
            "\n",
            "Global step: 6998,loss: 0.012509774\n",
            "\n",
            "Global step: 6999,loss: 0.012872082\n",
            "\n",
            "Global step: 7000,loss: 0.012186939\n",
            "\n",
            "Global step: 7001,loss: 0.012662806\n",
            "\n",
            "Global step: 7002,loss: 0.012403795\n",
            "\n",
            "Global step: 7003,loss: 0.012334531\n",
            "\n",
            "Global step: 7004,loss: 0.012437969\n",
            "\n",
            "Global step: 7005,loss: 0.011849196\n",
            "\n",
            "Global step: 7006,loss: 0.011753224\n",
            "\n",
            "Global step: 7007,loss: 0.01471767\n",
            "\n",
            "Global step: 7008,loss: 0.012407783\n",
            "\n",
            "Global step: 7009,loss: 0.012399991\n",
            "\n",
            "Global step: 7010,loss: 0.011898431\n",
            "\n",
            "Global step: 7011,loss: 0.012777412\n",
            "\n",
            "Global step: 7012,loss: 0.012293711\n",
            "\n",
            "Global step: 7013,loss: 0.012770825\n",
            "\n",
            "Global step: 7014,loss: 0.012055061\n",
            "\n",
            "Global step: 7015,loss: 0.011980366\n",
            "\n",
            "Global step: 7016,loss: 0.012345723\n",
            "\n",
            "Global step: 7017,loss: 0.012079518\n",
            "\n",
            "Global step: 7018,loss: 0.012019096\n",
            "\n",
            "Global step: 7019,loss: 0.01204718\n",
            "\n",
            "Global step: 7020,loss: 0.011810828\n",
            "\n",
            "Global step: 7021,loss: 0.012285656\n",
            "\n",
            "Global step: 7022,loss: 0.011518248\n",
            "\n",
            "Global step: 7023,loss: 0.012304844\n",
            "\n",
            "Global step: 7024,loss: 0.013166353\n",
            "\n",
            "Global step: 7025,loss: 0.012478337\n",
            "\n",
            "Global step: 7026,loss: 0.012080252\n",
            "\n",
            "Global step: 7027,loss: 0.012140936\n",
            "\n",
            "Global step: 7028,loss: 0.012356147\n",
            "\n",
            "Global step: 7029,loss: 0.013339291\n",
            "\n",
            "Global step: 7030,loss: 0.012914081\n",
            "\n",
            "Global step: 7031,loss: 0.012225041\n",
            "\n",
            "Global step: 7032,loss: 0.011968119\n",
            "\n",
            "Global step: 7033,loss: 0.012716145\n",
            "\n",
            "Global step: 7034,loss: 0.012342236\n",
            "\n",
            "Global step: 7035,loss: 0.012390729\n",
            "\n",
            "Global step: 7036,loss: 0.012765545\n",
            "\n",
            "Global step: 7037,loss: 0.012852118\n",
            "\n",
            "Global step: 7038,loss: 0.011425269\n",
            "\n",
            "Global step: 7039,loss: 0.011828126\n",
            "\n",
            "Global step: 7040,loss: 0.012863566\n",
            "\n",
            "Global step: 7041,loss: 0.012122901\n",
            "\n",
            "Global step: 7042,loss: 0.011997223\n",
            "\n",
            "Global step: 7043,loss: 0.0118494425\n",
            "\n",
            "Global step: 7044,loss: 0.012731627\n",
            "\n",
            "Global step: 7045,loss: 0.012719917\n",
            "\n",
            "Global step: 7046,loss: 0.012569091\n",
            "\n",
            "Global step: 7047,loss: 0.011972166\n",
            "\n",
            "Global step: 7048,loss: 0.012054852\n",
            "\n",
            "Global step: 7049,loss: 0.012093023\n",
            "\n",
            "Global step: 7050,loss: 0.011996278\n",
            "\n",
            "Global step: 7051,loss: 0.012695568\n",
            "\n",
            "Global step: 7052,loss: 0.011970593\n",
            "\n",
            "Global step: 7053,loss: 0.0114504425\n",
            "\n",
            "Global step: 7054,loss: 0.012905858\n",
            "\n",
            "Global step: 7055,loss: 0.012354378\n",
            "\n",
            "Global step: 7056,loss: 0.011666493\n",
            "\n",
            "Global step: 7057,loss: 0.011952785\n",
            "\n",
            "Global step: 7058,loss: 0.012060007\n",
            "\n",
            "Global step: 7059,loss: 0.012157555\n",
            "\n",
            "Global step: 7060,loss: 0.011871977\n",
            "\n",
            "Global step: 7061,loss: 0.011966729\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 7061,val_loss: 0.016242266142446744\n",
            "\n",
            "Training for epoch 34/50:\n",
            "Global step: 7062,loss: 0.011371879\n",
            "\n",
            "Global step: 7063,loss: 0.011979686\n",
            "\n",
            "Global step: 7064,loss: 0.011772366\n",
            "\n",
            "Global step: 7065,loss: 0.0119005935\n",
            "\n",
            "Global step: 7066,loss: 0.011768233\n",
            "\n",
            "Global step: 7067,loss: 0.011249241\n",
            "\n",
            "Global step: 7068,loss: 0.012872949\n",
            "\n",
            "Global step: 7069,loss: 0.011452402\n",
            "\n",
            "Global step: 7070,loss: 0.012036165\n",
            "\n",
            "Global step: 7071,loss: 0.013488789\n",
            "\n",
            "Global step: 7072,loss: 0.012064882\n",
            "\n",
            "Global step: 7073,loss: 0.013012544\n",
            "\n",
            "Global step: 7074,loss: 0.012190272\n",
            "\n",
            "Global step: 7075,loss: 0.01150685\n",
            "\n",
            "Global step: 7076,loss: 0.012905864\n",
            "\n",
            "Global step: 7077,loss: 0.012119068\n",
            "\n",
            "Global step: 7078,loss: 0.011729239\n",
            "\n",
            "Global step: 7079,loss: 0.012222298\n",
            "\n",
            "Global step: 7080,loss: 0.011458431\n",
            "\n",
            "Global step: 7081,loss: 0.011311651\n",
            "\n",
            "Global step: 7082,loss: 0.010777589\n",
            "\n",
            "Global step: 7083,loss: 0.012741965\n",
            "\n",
            "Global step: 7084,loss: 0.012002193\n",
            "\n",
            "Global step: 7085,loss: 0.011851897\n",
            "\n",
            "Global step: 7086,loss: 0.012400171\n",
            "\n",
            "Global step: 7087,loss: 0.011557\n",
            "\n",
            "Global step: 7088,loss: 0.011557396\n",
            "\n",
            "Global step: 7089,loss: 0.011854517\n",
            "\n",
            "Global step: 7090,loss: 0.011942294\n",
            "\n",
            "Global step: 7091,loss: 0.012999352\n",
            "\n",
            "Global step: 7092,loss: 0.011724193\n",
            "\n",
            "Global step: 7093,loss: 0.011349035\n",
            "\n",
            "Global step: 7094,loss: 0.011896889\n",
            "\n",
            "Global step: 7095,loss: 0.011712914\n",
            "\n",
            "Global step: 7096,loss: 0.0122659225\n",
            "\n",
            "Global step: 7097,loss: 0.011643457\n",
            "\n",
            "Global step: 7098,loss: 0.011234655\n",
            "\n",
            "Global step: 7099,loss: 0.011505613\n",
            "\n",
            "Global step: 7100,loss: 0.011917673\n",
            "\n",
            "Global step: 7101,loss: 0.01188823\n",
            "\n",
            "Global step: 7102,loss: 0.011805393\n",
            "\n",
            "Global step: 7103,loss: 0.011477802\n",
            "\n",
            "Global step: 7104,loss: 0.011607211\n",
            "\n",
            "Global step: 7105,loss: 0.011906715\n",
            "\n",
            "Global step: 7106,loss: 0.01204157\n",
            "\n",
            "Global step: 7107,loss: 0.011871553\n",
            "\n",
            "Global step: 7108,loss: 0.011162661\n",
            "\n",
            "Global step: 7109,loss: 0.0115282545\n",
            "\n",
            "Global step: 7110,loss: 0.011556841\n",
            "\n",
            "Global step: 7111,loss: 0.011575765\n",
            "\n",
            "Global step: 7112,loss: 0.0119077545\n",
            "\n",
            "Global step: 7113,loss: 0.012026172\n",
            "\n",
            "Global step: 7114,loss: 0.012416745\n",
            "\n",
            "Global step: 7115,loss: 0.0117566455\n",
            "\n",
            "Global step: 7116,loss: 0.012014991\n",
            "\n",
            "Global step: 7117,loss: 0.0118734\n",
            "\n",
            "Global step: 7118,loss: 0.011815069\n",
            "\n",
            "Global step: 7119,loss: 0.011614122\n",
            "\n",
            "Global step: 7120,loss: 0.011919446\n",
            "\n",
            "Global step: 7121,loss: 0.012181532\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 7122.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:56:29.375487 140167885084416 supervisor.py:1050] Recording summary at step 7122.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 7122,loss: 0.011654032\n",
            "\n",
            "Global step: 7123,loss: 0.011476632\n",
            "\n",
            "Global step: 7124,loss: 0.011321205\n",
            "\n",
            "Global step: 7125,loss: 0.011814109\n",
            "\n",
            "Global step: 7126,loss: 0.011290974\n",
            "\n",
            "Global step: 7127,loss: 0.011630261\n",
            "\n",
            "Global step: 7128,loss: 0.011256083\n",
            "\n",
            "Global step: 7129,loss: 0.011597801\n",
            "\n",
            "Global step: 7130,loss: 0.012117898\n",
            "\n",
            "Global step: 7131,loss: 0.011490271\n",
            "\n",
            "Global step: 7132,loss: 0.010921892\n",
            "\n",
            "Global step: 7133,loss: 0.0112398155\n",
            "\n",
            "Global step: 7134,loss: 0.0112646\n",
            "\n",
            "Global step: 7135,loss: 0.011365184\n",
            "\n",
            "Global step: 7136,loss: 0.011884966\n",
            "\n",
            "Global step: 7137,loss: 0.01253025\n",
            "\n",
            "Global step: 7138,loss: 0.012084941\n",
            "\n",
            "Global step: 7139,loss: 0.0122981155\n",
            "\n",
            "Global step: 7140,loss: 0.011172686\n",
            "\n",
            "Global step: 7141,loss: 0.011895143\n",
            "\n",
            "Global step: 7142,loss: 0.011328333\n",
            "\n",
            "Global step: 7143,loss: 0.011960223\n",
            "\n",
            "Global step: 7144,loss: 0.011358193\n",
            "\n",
            "Global step: 7145,loss: 0.011530715\n",
            "\n",
            "Global step: 7146,loss: 0.0114530735\n",
            "\n",
            "Global step: 7147,loss: 0.011536867\n",
            "\n",
            "Global step: 7148,loss: 0.0114034675\n",
            "\n",
            "Global step: 7149,loss: 0.012495738\n",
            "\n",
            "Global step: 7150,loss: 0.011531685\n",
            "\n",
            "Global step: 7151,loss: 0.01181102\n",
            "\n",
            "Global step: 7152,loss: 0.0120073985\n",
            "\n",
            "Global step: 7153,loss: 0.01192596\n",
            "\n",
            "Global step: 7154,loss: 0.011493959\n",
            "\n",
            "Global step: 7155,loss: 0.011900518\n",
            "\n",
            "Global step: 7156,loss: 0.010422462\n",
            "\n",
            "Global step: 7157,loss: 0.011612578\n",
            "\n",
            "Global step: 7158,loss: 0.011713984\n",
            "\n",
            "Global step: 7159,loss: 0.011753362\n",
            "\n",
            "Global step: 7160,loss: 0.011508075\n",
            "\n",
            "Global step: 7161,loss: 0.011320372\n",
            "\n",
            "Global step: 7162,loss: 0.011787625\n",
            "\n",
            "Global step: 7163,loss: 0.011582865\n",
            "\n",
            "Global step: 7164,loss: 0.011592446\n",
            "\n",
            "Global step: 7165,loss: 0.011472883\n",
            "\n",
            "Global step: 7166,loss: 0.01159661\n",
            "\n",
            "Global step: 7167,loss: 0.010993566\n",
            "\n",
            "Global step: 7168,loss: 0.011463473\n",
            "\n",
            "Global step: 7169,loss: 0.011738025\n",
            "\n",
            "Global step: 7170,loss: 0.010737032\n",
            "\n",
            "Global step: 7171,loss: 0.011454648\n",
            "\n",
            "Global step: 7172,loss: 0.011114667\n",
            "\n",
            "Global step: 7173,loss: 0.011088488\n",
            "\n",
            "Global step: 7174,loss: 0.012091194\n",
            "\n",
            "Global step: 7175,loss: 0.011951185\n",
            "\n",
            "Global step: 7176,loss: 0.011221554\n",
            "\n",
            "Global step: 7177,loss: 0.011845842\n",
            "\n",
            "Global step: 7178,loss: 0.011653938\n",
            "\n",
            "Global step: 7179,loss: 0.011572205\n",
            "\n",
            "Global step: 7180,loss: 0.011441326\n",
            "\n",
            "Global step: 7181,loss: 0.011285762\n",
            "\n",
            "Global step: 7182,loss: 0.012185617\n",
            "\n",
            "Global step: 7183,loss: 0.012084662\n",
            "\n",
            "Global step: 7184,loss: 0.011013512\n",
            "\n",
            "Global step: 7185,loss: 0.011025998\n",
            "\n",
            "Global step: 7186,loss: 0.011094572\n",
            "\n",
            "Global step: 7187,loss: 0.011414318\n",
            "\n",
            "Global step: 7188,loss: 0.011518644\n",
            "\n",
            "Global step: 7189,loss: 0.01140169\n",
            "\n",
            "Global step: 7190,loss: 0.010767376\n",
            "\n",
            "Global step: 7191,loss: 0.010708873\n",
            "\n",
            "Global step: 7192,loss: 0.012271584\n",
            "\n",
            "Global step: 7193,loss: 0.011481539\n",
            "\n",
            "Global step: 7194,loss: 0.012077754\n",
            "\n",
            "Global step: 7195,loss: 0.012310587\n",
            "\n",
            "Global step: 7196,loss: 0.01108652\n",
            "\n",
            "Global step: 7197,loss: 0.01154693\n",
            "\n",
            "Global step: 7198,loss: 0.01130749\n",
            "\n",
            "Global step: 7199,loss: 0.010898395\n",
            "\n",
            "Global step: 7200,loss: 0.011577619\n",
            "\n",
            "Global step: 7201,loss: 0.011544456\n",
            "\n",
            "Global step: 7202,loss: 0.012028518\n",
            "\n",
            "Global step: 7203,loss: 0.011524899\n",
            "\n",
            "Global step: 7204,loss: 0.01172549\n",
            "\n",
            "Global step: 7205,loss: 0.012096617\n",
            "\n",
            "Global step: 7206,loss: 0.0112188505\n",
            "\n",
            "Global step: 7207,loss: 0.011593488\n",
            "\n",
            "Global step: 7208,loss: 0.010907118\n",
            "\n",
            "Global step: 7209,loss: 0.011413207\n",
            "\n",
            "Global step: 7210,loss: 0.012141209\n",
            "\n",
            "Global step: 7211,loss: 0.011450188\n",
            "\n",
            "Global step: 7212,loss: 0.012226311\n",
            "\n",
            "Global step: 7213,loss: 0.011584947\n",
            "\n",
            "Global step: 7214,loss: 0.010900141\n",
            "\n",
            "Global step: 7215,loss: 0.01201559\n",
            "\n",
            "Global step: 7216,loss: 0.011144618\n",
            "\n",
            "Global step: 7217,loss: 0.011787665\n",
            "\n",
            "Global step: 7218,loss: 0.011381408\n",
            "\n",
            "Global step: 7219,loss: 0.011499332\n",
            "\n",
            "Global step: 7220,loss: 0.011538726\n",
            "\n",
            "Global step: 7221,loss: 0.011955924\n",
            "\n",
            "Global step: 7222,loss: 0.01193226\n",
            "\n",
            "Global step: 7223,loss: 0.0115805585\n",
            "\n",
            "Global step: 7224,loss: 0.010947897\n",
            "\n",
            "Global step: 7225,loss: 0.012316499\n",
            "\n",
            "Global step: 7226,loss: 0.011382146\n",
            "\n",
            "Global step: 7227,loss: 0.0112024965\n",
            "\n",
            "Global step: 7228,loss: 0.010970002\n",
            "\n",
            "Global step: 7229,loss: 0.0116787385\n",
            "\n",
            "Global step: 7230,loss: 0.011787646\n",
            "\n",
            "Global step: 7231,loss: 0.011294127\n",
            "\n",
            "Global step: 7232,loss: 0.01132806\n",
            "\n",
            "Global step: 7233,loss: 0.011069015\n",
            "\n",
            "Global step: 7234,loss: 0.011245319\n",
            "\n",
            "Global step: 7235,loss: 0.011467604\n",
            "\n",
            "Global step: 7236,loss: 0.011618915\n",
            "\n",
            "Global step: 7237,loss: 0.01160444\n",
            "\n",
            "Global step: 7238,loss: 0.0111919\n",
            "\n",
            "Global step: 7239,loss: 0.012129497\n",
            "\n",
            "Global step: 7240,loss: 0.012046271\n",
            "\n",
            "Global step: 7241,loss: 0.011055468\n",
            "\n",
            "Global step: 7242,loss: 0.011886236\n",
            "\n",
            "Global step: 7243,loss: 0.011580411\n",
            "\n",
            "Global step: 7244,loss: 0.011495291\n",
            "\n",
            "Global step: 7245,loss: 0.011309442\n",
            "\n",
            "Global step: 7246,loss: 0.010912777\n",
            "\n",
            "Global step: 7247,loss: 0.011644369\n",
            "\n",
            "Global step: 7248,loss: 0.011262787\n",
            "\n",
            "Global step: 7249,loss: 0.012299946\n",
            "\n",
            "Global step: 7250,loss: 0.011665797\n",
            "\n",
            "Global step: 7251,loss: 0.011500844\n",
            "\n",
            "Global step: 7252,loss: 0.011452729\n",
            "\n",
            "Global step: 7253,loss: 0.011895172\n",
            "\n",
            "Global step: 7254,loss: 0.011221558\n",
            "\n",
            "Global step: 7255,loss: 0.011371502\n",
            "\n",
            "Global step: 7256,loss: 0.011637112\n",
            "\n",
            "Global step: 7257,loss: 0.011379754\n",
            "\n",
            "Global step: 7258,loss: 0.011894979\n",
            "\n",
            "Global step: 7259,loss: 0.011274469\n",
            "\n",
            "Global step: 7260,loss: 0.011605787\n",
            "\n",
            "Global step: 7261,loss: 0.0116626\n",
            "\n",
            "Global step: 7262,loss: 0.011366639\n",
            "\n",
            "Global step: 7263,loss: 0.011591552\n",
            "\n",
            "Global step: 7264,loss: 0.011823129\n",
            "\n",
            "Global step: 7265,loss: 0.0109493\n",
            "\n",
            "Global step: 7266,loss: 0.0113713285\n",
            "\n",
            "Global step: 7267,loss: 0.011121643\n",
            "\n",
            "Global step: 7268,loss: 0.011478983\n",
            "\n",
            "Global step: 7269,loss: 0.011114488\n",
            "\n",
            "Global step: 7270,loss: 0.011826474\n",
            "\n",
            "Global step: 7271,loss: 0.011265362\n",
            "\n",
            "Global step: 7272,loss: 0.011293249\n",
            "\n",
            "Global step: 7273,loss: 0.011929834\n",
            "\n",
            "Global step: 7274,loss: 0.011404762\n",
            "\n",
            "Global step: 7275,loss: 0.011622314\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 7275,Val_Loss: 0.015505713851828324,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:57:08.361552 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 35/50:\n",
            "Global step: 7276,loss: 0.011530287\n",
            "\n",
            "Global step: 7277,loss: 0.011108454\n",
            "\n",
            "Global step: 7278,loss: 0.010804193\n",
            "\n",
            "Global step: 7279,loss: 0.011322142\n",
            "\n",
            "Global step: 7280,loss: 0.011483741\n",
            "\n",
            "Global step: 7281,loss: 0.011291169\n",
            "\n",
            "Global step: 7282,loss: 0.010763131\n",
            "\n",
            "Global step: 7283,loss: 0.01196749\n",
            "\n",
            "Global step: 7284,loss: 0.011374323\n",
            "\n",
            "Global step: 7285,loss: 0.0110123325\n",
            "\n",
            "Global step: 7286,loss: 0.011257267\n",
            "\n",
            "Global step: 7287,loss: 0.01115928\n",
            "\n",
            "Global step: 7288,loss: 0.0117035\n",
            "\n",
            "Global step: 7289,loss: 0.011514489\n",
            "\n",
            "Global step: 7290,loss: 0.011778819\n",
            "\n",
            "Global step: 7291,loss: 0.011848115\n",
            "\n",
            "Global step: 7292,loss: 0.01224115\n",
            "\n",
            "Global step: 7293,loss: 0.011009026\n",
            "\n",
            "Global step: 7294,loss: 0.011244491\n",
            "\n",
            "Global step: 7295,loss: 0.01187464\n",
            "\n",
            "Global step: 7296,loss: 0.010881525\n",
            "\n",
            "Global step: 7297,loss: 0.011726857\n",
            "\n",
            "Global step: 7298,loss: 0.011069544\n",
            "\n",
            "Global step: 7299,loss: 0.012029813\n",
            "\n",
            "Global step: 7300,loss: 0.010689087\n",
            "\n",
            "Global step: 7301,loss: 0.010967738\n",
            "\n",
            "Global step: 7302,loss: 0.011103784\n",
            "\n",
            "Global step: 7303,loss: 0.010728107\n",
            "\n",
            "Global step: 7304,loss: 0.011400854\n",
            "\n",
            "Global step: 7305,loss: 0.011115844\n",
            "\n",
            "Global step: 7306,loss: 0.011297698\n",
            "\n",
            "Global step: 7307,loss: 0.013520055\n",
            "\n",
            "Global step: 7308,loss: 0.010713906\n",
            "\n",
            "Global step: 7309,loss: 0.01129477\n",
            "\n",
            "Global step: 7310,loss: 0.010566074\n",
            "\n",
            "Global step: 7311,loss: 0.010692047\n",
            "\n",
            "Global step: 7312,loss: 0.011933901\n",
            "\n",
            "Global step: 7313,loss: 0.011013437\n",
            "\n",
            "Global step: 7314,loss: 0.011135839\n",
            "\n",
            "Global step: 7315,loss: 0.011586127\n",
            "\n",
            "Global step: 7316,loss: 0.011287303\n",
            "\n",
            "Global step: 7317,loss: 0.011006778\n",
            "\n",
            "Global step: 7318,loss: 0.010887807\n",
            "\n",
            "Global step: 7319,loss: 0.011121255\n",
            "\n",
            "Global step: 7320,loss: 0.010666678\n",
            "\n",
            "Global step: 7321,loss: 0.011461164\n",
            "\n",
            "Global step: 7322,loss: 0.0112379445\n",
            "\n",
            "Global step: 7323,loss: 0.010657341\n",
            "\n",
            "Global step: 7324,loss: 0.01140443\n",
            "\n",
            "Global step: 7325,loss: 0.010932047\n",
            "\n",
            "Global step: 7326,loss: 0.01153468\n",
            "\n",
            "Global step: 7327,loss: 0.0113672465\n",
            "\n",
            "Global step: 7328,loss: 0.010370597\n",
            "\n",
            "Global step: 7329,loss: 0.010292072\n",
            "\n",
            "Global step: 7330,loss: 0.0105439965\n",
            "\n",
            "Global step: 7331,loss: 0.01137719\n",
            "\n",
            "Global step: 7332,loss: 0.011587242\n",
            "\n",
            "Global step: 7333,loss: 0.010696918\n",
            "\n",
            "Global step: 7334,loss: 0.010875899\n",
            "\n",
            "Global step: 7335,loss: 0.011538166\n",
            "\n",
            "Global step: 7336,loss: 0.011623319\n",
            "\n",
            "Global step: 7337,loss: 0.011132566\n",
            "\n",
            "Global step: 7338,loss: 0.010558923\n",
            "\n",
            "Global step: 7339,loss: 0.011553249\n",
            "\n",
            "Global step: 7340,loss: 0.010641741\n",
            "\n",
            "Global step: 7341,loss: 0.011326615\n",
            "\n",
            "Global step: 7342,loss: 0.011641146\n",
            "\n",
            "Global step: 7343,loss: 0.011311445\n",
            "\n",
            "Global step: 7344,loss: 0.010986952\n",
            "\n",
            "Global step: 7345,loss: 0.011200768\n",
            "\n",
            "Global step: 7346,loss: 0.011461701\n",
            "\n",
            "Global step: 7347,loss: 0.011552768\n",
            "\n",
            "Global step: 7348,loss: 0.010227608\n",
            "\n",
            "Global step: 7349,loss: 0.01104115\n",
            "\n",
            "Global step: 7350,loss: 0.010494371\n",
            "\n",
            "Global step: 7351,loss: 0.011253225\n",
            "\n",
            "Global step: 7352,loss: 0.010991952\n",
            "\n",
            "Global step: 7353,loss: 0.010870451\n",
            "\n",
            "Global step: 7354,loss: 0.010647717\n",
            "\n",
            "Global step: 7355,loss: 0.011255027\n",
            "\n",
            "Global step: 7356,loss: 0.01044416\n",
            "\n",
            "Global step: 7357,loss: 0.010806495\n",
            "\n",
            "Global step: 7358,loss: 0.011228909\n",
            "\n",
            "Global step: 7359,loss: 0.011695788\n",
            "\n",
            "Global step: 7360,loss: 0.01089435\n",
            "\n",
            "Global step: 7361,loss: 0.010966909\n",
            "\n",
            "Global step: 7362,loss: 0.011092761\n",
            "\n",
            "Global step: 7363,loss: 0.010799351\n",
            "\n",
            "Global step: 7364,loss: 0.011018044\n",
            "\n",
            "Global step: 7365,loss: 0.010959302\n",
            "\n",
            "Global step: 7366,loss: 0.011354094\n",
            "\n",
            "Global step: 7367,loss: 0.010668642\n",
            "\n",
            "Global step: 7368,loss: 0.011174216\n",
            "\n",
            "Global step: 7369,loss: 0.010954087\n",
            "\n",
            "Global step: 7370,loss: 0.011085543\n",
            "\n",
            "Global step: 7371,loss: 0.011354958\n",
            "\n",
            "Global step: 7372,loss: 0.010787523\n",
            "\n",
            "Global step: 7373,loss: 0.010575214\n",
            "\n",
            "Global step: 7374,loss: 0.010445975\n",
            "\n",
            "Global step: 7375,loss: 0.010815981\n",
            "\n",
            "Global step: 7376,loss: 0.010882966\n",
            "\n",
            "Global step: 7377,loss: 0.011607875\n",
            "\n",
            "Global step: 7378,loss: 0.011603768\n",
            "\n",
            "Global step: 7379,loss: 0.0105732195\n",
            "\n",
            "Global step: 7380,loss: 0.01071933\n",
            "\n",
            "Global step: 7381,loss: 0.010840188\n",
            "\n",
            "Global step: 7382,loss: 0.011395217\n",
            "\n",
            "Global step: 7383,loss: 0.011171186\n",
            "\n",
            "Global step: 7384,loss: 0.011303044\n",
            "\n",
            "Global step: 7385,loss: 0.011224816\n",
            "\n",
            "Global step: 7386,loss: 0.01145504\n",
            "\n",
            "Global step: 7387,loss: 0.010784344\n",
            "\n",
            "Global step: 7388,loss: 0.011362484\n",
            "\n",
            "Global step: 7389,loss: 0.0118206525\n",
            "\n",
            "Global step: 7390,loss: 0.011460428\n",
            "\n",
            "Global step: 7391,loss: 0.010638923\n",
            "\n",
            "Global step: 7392,loss: 0.011165254\n",
            "\n",
            "Global step: 7393,loss: 0.011267675\n",
            "\n",
            "Global step: 7394,loss: 0.012066176\n",
            "\n",
            "Global step: 7395,loss: 0.0111050615\n",
            "\n",
            "Global step: 7396,loss: 0.011346332\n",
            "\n",
            "Global step: 7397,loss: 0.010820311\n",
            "\n",
            "Global step: 7398,loss: 0.011919236\n",
            "\n",
            "Global step: 7399,loss: 0.011153072\n",
            "\n",
            "Global step: 7400,loss: 0.011324146\n",
            "\n",
            "Global step: 7401,loss: 0.01139905\n",
            "\n",
            "Global step: 7402,loss: 0.01168824\n",
            "\n",
            "Global step: 7403,loss: 0.010717739\n",
            "\n",
            "Global step: 7404,loss: 0.01166163\n",
            "\n",
            "Global step: 7405,loss: 0.0114466455\n",
            "\n",
            "Global step: 7406,loss: 0.011455899\n",
            "\n",
            "Global step: 7407,loss: 0.011187559\n",
            "\n",
            "Global step: 7408,loss: 0.0105102165\n",
            "\n",
            "Global step: 7409,loss: 0.012648549\n",
            "\n",
            "Global step: 7410,loss: 0.010810232\n",
            "\n",
            "Global step: 7411,loss: 0.010339936\n",
            "\n",
            "Global step: 7412,loss: 0.010873552\n",
            "\n",
            "Global step: 7413,loss: 0.01173661\n",
            "\n",
            "Global step: 7414,loss: 0.011256297\n",
            "\n",
            "Global step: 7415,loss: 0.010500914\n",
            "\n",
            "Global step: 7416,loss: 0.011193686\n",
            "\n",
            "Global step: 7417,loss: 0.011049038\n",
            "\n",
            "Global step: 7418,loss: 0.011590323\n",
            "\n",
            "Global step: 7419,loss: 0.011575478\n",
            "\n",
            "Global step: 7420,loss: 0.01046115\n",
            "\n",
            "Global step: 7421,loss: 0.010447665\n",
            "\n",
            "Global step: 7422,loss: 0.011170818\n",
            "\n",
            "Global step: 7423,loss: 0.011436098\n",
            "\n",
            "Global step: 7424,loss: 0.0111834\n",
            "\n",
            "Global step: 7425,loss: 0.011843683\n",
            "\n",
            "Global step: 7426,loss: 0.010808456\n",
            "\n",
            "Global step: 7427,loss: 0.01087949\n",
            "\n",
            "Global step: 7428,loss: 0.011483818\n",
            "\n",
            "Global step: 7429,loss: 0.011141725\n",
            "\n",
            "Global step: 7430,loss: 0.01033118\n",
            "\n",
            "Global step: 7431,loss: 0.01140386\n",
            "\n",
            "Global step: 7432,loss: 0.0107497955\n",
            "\n",
            "Global step: 7433,loss: 0.010052535\n",
            "\n",
            "Global step: 7434,loss: 0.0114588365\n",
            "\n",
            "Global step: 7435,loss: 0.011376277\n",
            "\n",
            "Global step: 7436,loss: 0.0114695225\n",
            "\n",
            "Global step: 7437,loss: 0.010882598\n",
            "\n",
            "Global step: 7438,loss: 0.010909261\n",
            "\n",
            "Global step: 7439,loss: 0.010998342\n",
            "\n",
            "Global step: 7440,loss: 0.010541063\n",
            "\n",
            "Global step: 7441,loss: 0.01089209\n",
            "\n",
            "Global step: 7442,loss: 0.010835668\n",
            "\n",
            "Global step: 7443,loss: 0.011523147\n",
            "\n",
            "Global step: 7444,loss: 0.010632517\n",
            "\n",
            "Global step: 7445,loss: 0.01045151\n",
            "\n",
            "Global step: 7446,loss: 0.010602864\n",
            "\n",
            "Global step: 7447,loss: 0.011604493\n",
            "\n",
            "Global step: 7448,loss: 0.011066986\n",
            "\n",
            "Global step: 7449,loss: 0.011565707\n",
            "\n",
            "Global step: 7450,loss: 0.010863423\n",
            "\n",
            "Global step: 7451,loss: 0.011105948\n",
            "\n",
            "Global step: 7452,loss: 0.011435029\n",
            "\n",
            "Global step: 7453,loss: 0.010935147\n",
            "\n",
            "Global step: 7454,loss: 0.010719852\n",
            "\n",
            "Global step: 7455,loss: 0.012083409\n",
            "\n",
            "Global step: 7456,loss: 0.011054232\n",
            "\n",
            "Global step: 7457,loss: 0.010430925\n",
            "\n",
            "Global step: 7458,loss: 0.011173026\n",
            "\n",
            "Global step: 7459,loss: 0.011192137\n",
            "\n",
            "Global step: 7460,loss: 0.011801834\n",
            "\n",
            "Global step: 7461,loss: 0.0117167225\n",
            "\n",
            "Global step: 7462,loss: 0.011634391\n",
            "\n",
            "Global step: 7463,loss: 0.011061864\n",
            "\n",
            "Global step: 7464,loss: 0.011102201\n",
            "\n",
            "Global step: 7465,loss: 0.011232164\n",
            "\n",
            "Global step: 7466,loss: 0.011429978\n",
            "\n",
            "Global step: 7467,loss: 0.01127861\n",
            "\n",
            "Global step: 7468,loss: 0.011928128\n",
            "\n",
            "Global step: 7469,loss: 0.011115852\n",
            "\n",
            "Global step: 7470,loss: 0.01011785\n",
            "\n",
            "Global step: 7471,loss: 0.010550582\n",
            "\n",
            "Global step: 7472,loss: 0.0112178875\n",
            "\n",
            "Global step: 7473,loss: 0.011190514\n",
            "\n",
            "Global step: 7474,loss: 0.01099285\n",
            "\n",
            "Global step: 7475,loss: 0.010912623\n",
            "\n",
            "Global step: 7476,loss: 0.011453552\n",
            "\n",
            "Global step: 7477,loss: 0.010772661\n",
            "\n",
            "Global step: 7478,loss: 0.010263922\n",
            "\n",
            "Global step: 7479,loss: 0.0109522715\n",
            "\n",
            "Global step: 7480,loss: 0.01243083\n",
            "\n",
            "Global step: 7481,loss: 0.01127129\n",
            "\n",
            "Global step: 7482,loss: 0.011103018\n",
            "\n",
            "Global step: 7483,loss: 0.010523877\n",
            "\n",
            "Global step: 7484,loss: 0.010525193\n",
            "\n",
            "Global step: 7485,loss: 0.011290603\n",
            "\n",
            "Global step: 7486,loss: 0.010589295\n",
            "\n",
            "Global step: 7487,loss: 0.010867029\n",
            "\n",
            "Global step: 7488,loss: 0.01062656\n",
            "\n",
            "Global step: 7489,loss: 0.010643507\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 7489,Val_Loss: 0.015315850559425982,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:58:01.927416 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 36/50:\n",
            "Global step: 7490,loss: 0.010883868\n",
            "\n",
            "Global step: 7491,loss: 0.01101487\n",
            "\n",
            "Global step: 7492,loss: 0.011413755\n",
            "\n",
            "Global step: 7493,loss: 0.0105762575\n",
            "\n",
            "Global step: 7494,loss: 0.010402341\n",
            "\n",
            "Global step: 7495,loss: 0.011394354\n",
            "\n",
            "Global step: 7496,loss: 0.010642745\n",
            "\n",
            "Global step: 7497,loss: 0.010955262\n",
            "\n",
            "Global step: 7498,loss: 0.010856244\n",
            "\n",
            "Global step: 7499,loss: 0.0110665895\n",
            "\n",
            "Global step: 7500,loss: 0.011084391\n",
            "\n",
            "Global step: 7501,loss: 0.010912932\n",
            "\n",
            "Global step: 7502,loss: 0.011572379\n",
            "\n",
            "Global step: 7503,loss: 0.010887986\n",
            "\n",
            "Global step: 7504,loss: 0.011488059\n",
            "\n",
            "Global step: 7505,loss: 0.010618149\n",
            "\n",
            "Global step: 7506,loss: 0.011307657\n",
            "\n",
            "Global step: 7507,loss: 0.011017259\n",
            "\n",
            "Global step: 7508,loss: 0.010914609\n",
            "\n",
            "Global step: 7509,loss: 0.011059449\n",
            "\n",
            "Global step: 7510,loss: 0.01079341\n",
            "\n",
            "Global step: 7511,loss: 0.010996328\n",
            "\n",
            "Global step: 7512,loss: 0.011190618\n",
            "\n",
            "Global step: 7513,loss: 0.011840402\n",
            "\n",
            "Global step: 7514,loss: 0.0115839755\n",
            "\n",
            "Global step: 7515,loss: 0.011048918\n",
            "\n",
            "Global step: 7516,loss: 0.011310228\n",
            "\n",
            "Global step: 7517,loss: 0.010327951\n",
            "\n",
            "Global step: 7518,loss: 0.0109972395\n",
            "\n",
            "Global step: 7519,loss: 0.011271492\n",
            "\n",
            "Global step: 7520,loss: 0.010811898\n",
            "\n",
            "Global step: 7521,loss: 0.010295069\n",
            "\n",
            "Global step: 7522,loss: 0.010929775\n",
            "\n",
            "Global step: 7523,loss: 0.011213527\n",
            "\n",
            "Global step: 7524,loss: 0.010634202\n",
            "\n",
            "Global step: 7525,loss: 0.010562625\n",
            "\n",
            "Global step: 7526,loss: 0.010453773\n",
            "\n",
            "Global step: 7527,loss: 0.011037423\n",
            "\n",
            "Global step: 7528,loss: 0.010708692\n",
            "\n",
            "Global step: 7529,loss: 0.010989975\n",
            "\n",
            "Global step: 7530,loss: 0.011098362\n",
            "\n",
            "Global step: 7531,loss: 0.011460087\n",
            "\n",
            "Global step: 7532,loss: 0.010958805\n",
            "\n",
            "Global step: 7533,loss: 0.010674464\n",
            "\n",
            "Global step: 7534,loss: 0.010776693\n",
            "\n",
            "Global step: 7535,loss: 0.010848967\n",
            "\n",
            "Global step: 7536,loss: 0.01116307\n",
            "\n",
            "Global step: 7537,loss: 0.010419325\n",
            "\n",
            "Global step: 7538,loss: 0.010711315\n",
            "\n",
            "Global step: 7539,loss: 0.010919423\n",
            "\n",
            "Global step: 7540,loss: 0.011591034\n",
            "\n",
            "Global step: 7541,loss: 0.010922213\n",
            "\n",
            "Global step: 7542,loss: 0.010781312\n",
            "\n",
            "Global step: 7543,loss: 0.011049353\n",
            "\n",
            "Global step: 7544,loss: 0.0106618265\n",
            "\n",
            "Global step: 7545,loss: 0.010977827\n",
            "\n",
            "Global step: 7546,loss: 0.011345489\n",
            "\n",
            "Global step: 7547,loss: 0.010698676\n",
            "\n",
            "Global step: 7548,loss: 0.010592691\n",
            "\n",
            "Global step: 7549,loss: 0.010998551\n",
            "\n",
            "Global step: 7550,loss: 0.010389742\n",
            "\n",
            "Global step: 7551,loss: 0.010735564\n",
            "\n",
            "Global step: 7552,loss: 0.010638315\n",
            "\n",
            "Global step: 7553,loss: 0.010771691\n",
            "\n",
            "Global step: 7554,loss: 0.010719451\n",
            "\n",
            "Global step: 7555,loss: 0.011037694\n",
            "\n",
            "Global step: 7556,loss: 0.0104152905\n",
            "\n",
            "Global step: 7557,loss: 0.010945886\n",
            "\n",
            "Global step: 7558,loss: 0.011504689\n",
            "\n",
            "Global step: 7559,loss: 0.010734297\n",
            "\n",
            "Global step: 7560,loss: 0.011021555\n",
            "\n",
            "Global step: 7561,loss: 0.010370258\n",
            "\n",
            "Global step: 7562,loss: 0.011167228\n",
            "\n",
            "Global step: 7563,loss: 0.01067975\n",
            "\n",
            "Global step: 7564,loss: 0.010712721\n",
            "\n",
            "Global step: 7565,loss: 0.011166288\n",
            "\n",
            "Global step: 7566,loss: 0.01087735\n",
            "\n",
            "Global step: 7567,loss: 0.011240738\n",
            "\n",
            "Global step: 7568,loss: 0.010870395\n",
            "\n",
            "Global step: 7569,loss: 0.010502041\n",
            "\n",
            "Global step: 7570,loss: 0.010570889\n",
            "\n",
            "Global step: 7571,loss: 0.010875614\n",
            "\n",
            "Global step: 7572,loss: 0.010168464\n",
            "\n",
            "Global step: 7573,loss: 0.011797053\n",
            "\n",
            "Global step: 7574,loss: 0.01123608\n",
            "\n",
            "Global step: 7575,loss: 0.011015148\n",
            "\n",
            "Global step: 7576,loss: 0.01117626\n",
            "\n",
            "Global step: 7577,loss: 0.010917132\n",
            "\n",
            "Global step: 7578,loss: 0.010759033\n",
            "\n",
            "Global step: 7579,loss: 0.010921693\n",
            "\n",
            "Global step: 7580,loss: 0.011492183\n",
            "\n",
            "Global step: 7581,loss: 0.010786058\n",
            "\n",
            "Global step: 7582,loss: 0.011167974\n",
            "\n",
            "Global step: 7583,loss: 0.011041698\n",
            "\n",
            "Global step: 7584,loss: 0.011207725\n",
            "\n",
            "Global step: 7585,loss: 0.010130607\n",
            "\n",
            "Global step: 7586,loss: 0.010439549\n",
            "\n",
            "Global step: 7587,loss: 0.011159184\n",
            "\n",
            "Global step: 7588,loss: 0.010696472\n",
            "\n",
            "Global step: 7589,loss: 0.01090595\n",
            "\n",
            "Global step: 7590,loss: 0.010862058\n",
            "\n",
            "Global step: 7591,loss: 0.010916255\n",
            "\n",
            "Global step: 7592,loss: 0.010408862\n",
            "\n",
            "Global step: 7593,loss: 0.010606243\n",
            "\n",
            "Global step: 7594,loss: 0.010490665\n",
            "\n",
            "Global step: 7595,loss: 0.010417794\n",
            "\n",
            "Global step: 7596,loss: 0.010842978\n",
            "\n",
            "Global step: 7597,loss: 0.011527181\n",
            "\n",
            "Global step: 7598,loss: 0.011155918\n",
            "\n",
            "Global step: 7599,loss: 0.011260584\n",
            "\n",
            "Global step: 7600,loss: 0.010821352\n",
            "\n",
            "Global step: 7601,loss: 0.010872675\n",
            "\n",
            "Global step: 7602,loss: 0.011050517\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 7603.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 22:58:29.352398 140167885084416 supervisor.py:1050] Recording summary at step 7603.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 7603,loss: 0.010752894\n",
            "\n",
            "Global step: 7604,loss: 0.010973156\n",
            "\n",
            "Global step: 7605,loss: 0.010549398\n",
            "\n",
            "Global step: 7606,loss: 0.010882401\n",
            "\n",
            "Global step: 7607,loss: 0.0105901975\n",
            "\n",
            "Global step: 7608,loss: 0.01045175\n",
            "\n",
            "Global step: 7609,loss: 0.01086839\n",
            "\n",
            "Global step: 7610,loss: 0.01128723\n",
            "\n",
            "Global step: 7611,loss: 0.011239271\n",
            "\n",
            "Global step: 7612,loss: 0.011200102\n",
            "\n",
            "Global step: 7613,loss: 0.011189392\n",
            "\n",
            "Global step: 7614,loss: 0.011237147\n",
            "\n",
            "Global step: 7615,loss: 0.010872619\n",
            "\n",
            "Global step: 7616,loss: 0.0106705725\n",
            "\n",
            "Global step: 7617,loss: 0.01078959\n",
            "\n",
            "Global step: 7618,loss: 0.011171054\n",
            "\n",
            "Global step: 7619,loss: 0.0112534985\n",
            "\n",
            "Global step: 7620,loss: 0.010817439\n",
            "\n",
            "Global step: 7621,loss: 0.010540759\n",
            "\n",
            "Global step: 7622,loss: 0.011364764\n",
            "\n",
            "Global step: 7623,loss: 0.010497747\n",
            "\n",
            "Global step: 7624,loss: 0.010779054\n",
            "\n",
            "Global step: 7625,loss: 0.01169445\n",
            "\n",
            "Global step: 7626,loss: 0.010815329\n",
            "\n",
            "Global step: 7627,loss: 0.010681006\n",
            "\n",
            "Global step: 7628,loss: 0.010661361\n",
            "\n",
            "Global step: 7629,loss: 0.010878924\n",
            "\n",
            "Global step: 7630,loss: 0.011148566\n",
            "\n",
            "Global step: 7631,loss: 0.010636154\n",
            "\n",
            "Global step: 7632,loss: 0.011376104\n",
            "\n",
            "Global step: 7633,loss: 0.010968323\n",
            "\n",
            "Global step: 7634,loss: 0.011338787\n",
            "\n",
            "Global step: 7635,loss: 0.0108495625\n",
            "\n",
            "Global step: 7636,loss: 0.010601846\n",
            "\n",
            "Global step: 7637,loss: 0.010877944\n",
            "\n",
            "Global step: 7638,loss: 0.011317602\n",
            "\n",
            "Global step: 7639,loss: 0.01041376\n",
            "\n",
            "Global step: 7640,loss: 0.0103759775\n",
            "\n",
            "Global step: 7641,loss: 0.01066831\n",
            "\n",
            "Global step: 7642,loss: 0.010838505\n",
            "\n",
            "Global step: 7643,loss: 0.011031518\n",
            "\n",
            "Global step: 7644,loss: 0.010991048\n",
            "\n",
            "Global step: 7645,loss: 0.010509483\n",
            "\n",
            "Global step: 7646,loss: 0.010939635\n",
            "\n",
            "Global step: 7647,loss: 0.010701767\n",
            "\n",
            "Global step: 7648,loss: 0.010545362\n",
            "\n",
            "Global step: 7649,loss: 0.01030335\n",
            "\n",
            "Global step: 7650,loss: 0.010522751\n",
            "\n",
            "Global step: 7651,loss: 0.0110052\n",
            "\n",
            "Global step: 7652,loss: 0.011144589\n",
            "\n",
            "Global step: 7653,loss: 0.011343102\n",
            "\n",
            "Global step: 7654,loss: 0.01116315\n",
            "\n",
            "Global step: 7655,loss: 0.010993993\n",
            "\n",
            "Global step: 7656,loss: 0.010354924\n",
            "\n",
            "Global step: 7657,loss: 0.01067935\n",
            "\n",
            "Global step: 7658,loss: 0.010695858\n",
            "\n",
            "Global step: 7659,loss: 0.011279058\n",
            "\n",
            "Global step: 7660,loss: 0.01103779\n",
            "\n",
            "Global step: 7661,loss: 0.011141089\n",
            "\n",
            "Global step: 7662,loss: 0.011192031\n",
            "\n",
            "Global step: 7663,loss: 0.011166858\n",
            "\n",
            "Global step: 7664,loss: 0.010367456\n",
            "\n",
            "Global step: 7665,loss: 0.010363078\n",
            "\n",
            "Global step: 7666,loss: 0.011337495\n",
            "\n",
            "Global step: 7667,loss: 0.010719467\n",
            "\n",
            "Global step: 7668,loss: 0.011240283\n",
            "\n",
            "Global step: 7669,loss: 0.010736181\n",
            "\n",
            "Global step: 7670,loss: 0.0105977\n",
            "\n",
            "Global step: 7671,loss: 0.01061302\n",
            "\n",
            "Global step: 7672,loss: 0.011201562\n",
            "\n",
            "Global step: 7673,loss: 0.01137701\n",
            "\n",
            "Global step: 7674,loss: 0.010931482\n",
            "\n",
            "Global step: 7675,loss: 0.010843742\n",
            "\n",
            "Global step: 7676,loss: 0.011853187\n",
            "\n",
            "Global step: 7677,loss: 0.010568636\n",
            "\n",
            "Global step: 7678,loss: 0.010994452\n",
            "\n",
            "Global step: 7679,loss: 0.010833618\n",
            "\n",
            "Global step: 7680,loss: 0.01079781\n",
            "\n",
            "Global step: 7681,loss: 0.011528025\n",
            "\n",
            "Global step: 7682,loss: 0.011252834\n",
            "\n",
            "Global step: 7683,loss: 0.010193054\n",
            "\n",
            "Global step: 7684,loss: 0.011907932\n",
            "\n",
            "Global step: 7685,loss: 0.011243201\n",
            "\n",
            "Global step: 7686,loss: 0.010651601\n",
            "\n",
            "Global step: 7687,loss: 0.011089378\n",
            "\n",
            "Global step: 7688,loss: 0.011076936\n",
            "\n",
            "Global step: 7689,loss: 0.01173285\n",
            "\n",
            "Global step: 7690,loss: 0.011461307\n",
            "\n",
            "Global step: 7691,loss: 0.009917398\n",
            "\n",
            "Global step: 7692,loss: 0.010748397\n",
            "\n",
            "Global step: 7693,loss: 0.010731146\n",
            "\n",
            "Global step: 7694,loss: 0.010469155\n",
            "\n",
            "Global step: 7695,loss: 0.011671171\n",
            "\n",
            "Global step: 7696,loss: 0.010521994\n",
            "\n",
            "Global step: 7697,loss: 0.010838554\n",
            "\n",
            "Global step: 7698,loss: 0.01089745\n",
            "\n",
            "Global step: 7699,loss: 0.010993485\n",
            "\n",
            "Global step: 7700,loss: 0.011618933\n",
            "\n",
            "Global step: 7701,loss: 0.011004485\n",
            "\n",
            "Global step: 7702,loss: 0.010254819\n",
            "\n",
            "Global step: 7703,loss: 0.010682081\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 7703,Val_Loss: 0.015185839104417124,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:58:55.671504 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 37/50:\n",
            "Global step: 7704,loss: 0.011261448\n",
            "\n",
            "Global step: 7705,loss: 0.010378344\n",
            "\n",
            "Global step: 7706,loss: 0.011063891\n",
            "\n",
            "Global step: 7707,loss: 0.01063781\n",
            "\n",
            "Global step: 7708,loss: 0.010139436\n",
            "\n",
            "Global step: 7709,loss: 0.010593157\n",
            "\n",
            "Global step: 7710,loss: 0.011026876\n",
            "\n",
            "Global step: 7711,loss: 0.0110184755\n",
            "\n",
            "Global step: 7712,loss: 0.011015301\n",
            "\n",
            "Global step: 7713,loss: 0.010464152\n",
            "\n",
            "Global step: 7714,loss: 0.010261553\n",
            "\n",
            "Global step: 7715,loss: 0.01028296\n",
            "\n",
            "Global step: 7716,loss: 0.010651419\n",
            "\n",
            "Global step: 7717,loss: 0.010898324\n",
            "\n",
            "Global step: 7718,loss: 0.010604267\n",
            "\n",
            "Global step: 7719,loss: 0.010856714\n",
            "\n",
            "Global step: 7720,loss: 0.010572049\n",
            "\n",
            "Global step: 7721,loss: 0.011333044\n",
            "\n",
            "Global step: 7722,loss: 0.010508138\n",
            "\n",
            "Global step: 7723,loss: 0.010983378\n",
            "\n",
            "Global step: 7724,loss: 0.011166661\n",
            "\n",
            "Global step: 7725,loss: 0.010469255\n",
            "\n",
            "Global step: 7726,loss: 0.010358969\n",
            "\n",
            "Global step: 7727,loss: 0.009938108\n",
            "\n",
            "Global step: 7728,loss: 0.010105359\n",
            "\n",
            "Global step: 7729,loss: 0.011151762\n",
            "\n",
            "Global step: 7730,loss: 0.011172459\n",
            "\n",
            "Global step: 7731,loss: 0.010728486\n",
            "\n",
            "Global step: 7732,loss: 0.010023915\n",
            "\n",
            "Global step: 7733,loss: 0.010743365\n",
            "\n",
            "Global step: 7734,loss: 0.01113346\n",
            "\n",
            "Global step: 7735,loss: 0.01064403\n",
            "\n",
            "Global step: 7736,loss: 0.010590011\n",
            "\n",
            "Global step: 7737,loss: 0.0106263235\n",
            "\n",
            "Global step: 7738,loss: 0.010574535\n",
            "\n",
            "Global step: 7739,loss: 0.011041398\n",
            "\n",
            "Global step: 7740,loss: 0.010059962\n",
            "\n",
            "Global step: 7741,loss: 0.010535771\n",
            "\n",
            "Global step: 7742,loss: 0.010691246\n",
            "\n",
            "Global step: 7743,loss: 0.011149768\n",
            "\n",
            "Global step: 7744,loss: 0.011074705\n",
            "\n",
            "Global step: 7745,loss: 0.01089521\n",
            "\n",
            "Global step: 7746,loss: 0.010860113\n",
            "\n",
            "Global step: 7747,loss: 0.010870604\n",
            "\n",
            "Global step: 7748,loss: 0.010725439\n",
            "\n",
            "Global step: 7749,loss: 0.011428603\n",
            "\n",
            "Global step: 7750,loss: 0.010916308\n",
            "\n",
            "Global step: 7751,loss: 0.010236856\n",
            "\n",
            "Global step: 7752,loss: 0.01111686\n",
            "\n",
            "Global step: 7753,loss: 0.009922224\n",
            "\n",
            "Global step: 7754,loss: 0.010955173\n",
            "\n",
            "Global step: 7755,loss: 0.010544476\n",
            "\n",
            "Global step: 7756,loss: 0.010376168\n",
            "\n",
            "Global step: 7757,loss: 0.010469964\n",
            "\n",
            "Global step: 7758,loss: 0.010208616\n",
            "\n",
            "Global step: 7759,loss: 0.010360491\n",
            "\n",
            "Global step: 7760,loss: 0.010930354\n",
            "\n",
            "Global step: 7761,loss: 0.011090234\n",
            "\n",
            "Global step: 7762,loss: 0.011182117\n",
            "\n",
            "Global step: 7763,loss: 0.010663606\n",
            "\n",
            "Global step: 7764,loss: 0.010776435\n",
            "\n",
            "Global step: 7765,loss: 0.010945875\n",
            "\n",
            "Global step: 7766,loss: 0.0108856475\n",
            "\n",
            "Global step: 7767,loss: 0.010620478\n",
            "\n",
            "Global step: 7768,loss: 0.010338115\n",
            "\n",
            "Global step: 7769,loss: 0.010318142\n",
            "\n",
            "Global step: 7770,loss: 0.0097089\n",
            "\n",
            "Global step: 7771,loss: 0.010727232\n",
            "\n",
            "Global step: 7772,loss: 0.009716286\n",
            "\n",
            "Global step: 7773,loss: 0.01045545\n",
            "\n",
            "Global step: 7774,loss: 0.010531007\n",
            "\n",
            "Global step: 7775,loss: 0.0104184\n",
            "\n",
            "Global step: 7776,loss: 0.010125626\n",
            "\n",
            "Global step: 7777,loss: 0.01033712\n",
            "\n",
            "Global step: 7778,loss: 0.010321094\n",
            "\n",
            "Global step: 7779,loss: 0.010356536\n",
            "\n",
            "Global step: 7780,loss: 0.010783446\n",
            "\n",
            "Global step: 7781,loss: 0.011433842\n",
            "\n",
            "Global step: 7782,loss: 0.010855211\n",
            "\n",
            "Global step: 7783,loss: 0.0107192965\n",
            "\n",
            "Global step: 7784,loss: 0.010525728\n",
            "\n",
            "Global step: 7785,loss: 0.011165729\n",
            "\n",
            "Global step: 7786,loss: 0.010496726\n",
            "\n",
            "Global step: 7787,loss: 0.010868735\n",
            "\n",
            "Global step: 7788,loss: 0.011203663\n",
            "\n",
            "Global step: 7789,loss: 0.0111640645\n",
            "\n",
            "Global step: 7790,loss: 0.0103385225\n",
            "\n",
            "Global step: 7791,loss: 0.010967878\n",
            "\n",
            "Global step: 7792,loss: 0.010756229\n",
            "\n",
            "Global step: 7793,loss: 0.009995951\n",
            "\n",
            "Global step: 7794,loss: 0.010376118\n",
            "\n",
            "Global step: 7795,loss: 0.010288382\n",
            "\n",
            "Global step: 7796,loss: 0.00986011\n",
            "\n",
            "Global step: 7797,loss: 0.010615947\n",
            "\n",
            "Global step: 7798,loss: 0.01088219\n",
            "\n",
            "Global step: 7799,loss: 0.01046666\n",
            "\n",
            "Global step: 7800,loss: 0.010849345\n",
            "\n",
            "Global step: 7801,loss: 0.010375634\n",
            "\n",
            "Global step: 7802,loss: 0.010472127\n",
            "\n",
            "Global step: 7803,loss: 0.010667397\n",
            "\n",
            "Global step: 7804,loss: 0.011102575\n",
            "\n",
            "Global step: 7805,loss: 0.010703128\n",
            "\n",
            "Global step: 7806,loss: 0.010446418\n",
            "\n",
            "Global step: 7807,loss: 0.010778203\n",
            "\n",
            "Global step: 7808,loss: 0.010920092\n",
            "\n",
            "Global step: 7809,loss: 0.00975827\n",
            "\n",
            "Global step: 7810,loss: 0.011237572\n",
            "\n",
            "Global step: 7811,loss: 0.010857632\n",
            "\n",
            "Global step: 7812,loss: 0.010616174\n",
            "\n",
            "Global step: 7813,loss: 0.010949515\n",
            "\n",
            "Global step: 7814,loss: 0.011013643\n",
            "\n",
            "Global step: 7815,loss: 0.010176938\n",
            "\n",
            "Global step: 7816,loss: 0.011308908\n",
            "\n",
            "Global step: 7817,loss: 0.010549843\n",
            "\n",
            "Global step: 7818,loss: 0.010469058\n",
            "\n",
            "Global step: 7819,loss: 0.010932648\n",
            "\n",
            "Global step: 7820,loss: 0.010998256\n",
            "\n",
            "Global step: 7821,loss: 0.010185249\n",
            "\n",
            "Global step: 7822,loss: 0.010998109\n",
            "\n",
            "Global step: 7823,loss: 0.011213702\n",
            "\n",
            "Global step: 7824,loss: 0.010471274\n",
            "\n",
            "Global step: 7825,loss: 0.010622504\n",
            "\n",
            "Global step: 7826,loss: 0.010649899\n",
            "\n",
            "Global step: 7827,loss: 0.0105351955\n",
            "\n",
            "Global step: 7828,loss: 0.010527536\n",
            "\n",
            "Global step: 7829,loss: 0.01099074\n",
            "\n",
            "Global step: 7830,loss: 0.010527813\n",
            "\n",
            "Global step: 7831,loss: 0.010775785\n",
            "\n",
            "Global step: 7832,loss: 0.010572517\n",
            "\n",
            "Global step: 7833,loss: 0.009971677\n",
            "\n",
            "Global step: 7834,loss: 0.010001434\n",
            "\n",
            "Global step: 7835,loss: 0.010283817\n",
            "\n",
            "Global step: 7836,loss: 0.011336114\n",
            "\n",
            "Global step: 7837,loss: 0.010607376\n",
            "\n",
            "Global step: 7838,loss: 0.010367423\n",
            "\n",
            "Global step: 7839,loss: 0.011096023\n",
            "\n",
            "Global step: 7840,loss: 0.010766384\n",
            "\n",
            "Global step: 7841,loss: 0.01079422\n",
            "\n",
            "Global step: 7842,loss: 0.010626285\n",
            "\n",
            "Global step: 7843,loss: 0.010861441\n",
            "\n",
            "Global step: 7844,loss: 0.010405008\n",
            "\n",
            "Global step: 7845,loss: 0.010079137\n",
            "\n",
            "Global step: 7846,loss: 0.010435271\n",
            "\n",
            "Global step: 7847,loss: 0.01061005\n",
            "\n",
            "Global step: 7848,loss: 0.010878391\n",
            "\n",
            "Global step: 7849,loss: 0.010646451\n",
            "\n",
            "Global step: 7850,loss: 0.011066901\n",
            "\n",
            "Global step: 7851,loss: 0.01106973\n",
            "\n",
            "Global step: 7852,loss: 0.01011445\n",
            "\n",
            "Global step: 7853,loss: 0.010606538\n",
            "\n",
            "Global step: 7854,loss: 0.010605802\n",
            "\n",
            "Global step: 7855,loss: 0.011153596\n",
            "\n",
            "Global step: 7856,loss: 0.010482818\n",
            "\n",
            "Global step: 7857,loss: 0.010406235\n",
            "\n",
            "Global step: 7858,loss: 0.010805754\n",
            "\n",
            "Global step: 7859,loss: 0.0109786615\n",
            "\n",
            "Global step: 7860,loss: 0.0101253465\n",
            "\n",
            "Global step: 7861,loss: 0.011451573\n",
            "\n",
            "Global step: 7862,loss: 0.010530122\n",
            "\n",
            "Global step: 7863,loss: 0.010602711\n",
            "\n",
            "Global step: 7864,loss: 0.010006815\n",
            "\n",
            "Global step: 7865,loss: 0.010786085\n",
            "\n",
            "Global step: 7866,loss: 0.011010836\n",
            "\n",
            "Global step: 7867,loss: 0.010539148\n",
            "\n",
            "Global step: 7868,loss: 0.010717032\n",
            "\n",
            "Global step: 7869,loss: 0.010655376\n",
            "\n",
            "Global step: 7870,loss: 0.011413462\n",
            "\n",
            "Global step: 7871,loss: 0.009953247\n",
            "\n",
            "Global step: 7872,loss: 0.0107857855\n",
            "\n",
            "Global step: 7873,loss: 0.0112519115\n",
            "\n",
            "Global step: 7874,loss: 0.01089942\n",
            "\n",
            "Global step: 7875,loss: 0.011262399\n",
            "\n",
            "Global step: 7876,loss: 0.010180494\n",
            "\n",
            "Global step: 7877,loss: 0.010321722\n",
            "\n",
            "Global step: 7878,loss: 0.009815155\n",
            "\n",
            "Global step: 7879,loss: 0.010074018\n",
            "\n",
            "Global step: 7880,loss: 0.010667763\n",
            "\n",
            "Global step: 7881,loss: 0.0102916295\n",
            "\n",
            "Global step: 7882,loss: 0.010715084\n",
            "\n",
            "Global step: 7883,loss: 0.011027269\n",
            "\n",
            "Global step: 7884,loss: 0.011090447\n",
            "\n",
            "Global step: 7885,loss: 0.011125819\n",
            "\n",
            "Global step: 7886,loss: 0.0108187115\n",
            "\n",
            "Global step: 7887,loss: 0.011595326\n",
            "\n",
            "Global step: 7888,loss: 0.011702354\n",
            "\n",
            "Global step: 7889,loss: 0.010701565\n",
            "\n",
            "Global step: 7890,loss: 0.010834821\n",
            "\n",
            "Global step: 7891,loss: 0.011227676\n",
            "\n",
            "Global step: 7892,loss: 0.011051112\n",
            "\n",
            "Global step: 7893,loss: 0.011099272\n",
            "\n",
            "Global step: 7894,loss: 0.010666943\n",
            "\n",
            "Global step: 7895,loss: 0.010859407\n",
            "\n",
            "Global step: 7896,loss: 0.01098829\n",
            "\n",
            "Global step: 7897,loss: 0.010713062\n",
            "\n",
            "Global step: 7898,loss: 0.010513111\n",
            "\n",
            "Global step: 7899,loss: 0.011021208\n",
            "\n",
            "Global step: 7900,loss: 0.0114506\n",
            "\n",
            "Global step: 7901,loss: 0.010939417\n",
            "\n",
            "Global step: 7902,loss: 0.010980715\n",
            "\n",
            "Global step: 7903,loss: 0.011016501\n",
            "\n",
            "Global step: 7904,loss: 0.010476237\n",
            "\n",
            "Global step: 7905,loss: 0.010508759\n",
            "\n",
            "Global step: 7906,loss: 0.010967297\n",
            "\n",
            "Global step: 7907,loss: 0.011207255\n",
            "\n",
            "Global step: 7908,loss: 0.01080269\n",
            "\n",
            "Global step: 7909,loss: 0.011084673\n",
            "\n",
            "Global step: 7910,loss: 0.011057835\n",
            "\n",
            "Global step: 7911,loss: 0.011203515\n",
            "\n",
            "Global step: 7912,loss: 0.010898055\n",
            "\n",
            "Global step: 7913,loss: 0.010365685\n",
            "\n",
            "Global step: 7914,loss: 0.010468185\n",
            "\n",
            "Global step: 7915,loss: 0.010513698\n",
            "\n",
            "Global step: 7916,loss: 0.01047735\n",
            "\n",
            "Global step: 7917,loss: 0.010658771\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 7917,Val_Loss: 0.015004856953103291,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 22:59:49.284941 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 38/50:\n",
            "Global step: 7918,loss: 0.010555921\n",
            "\n",
            "Global step: 7919,loss: 0.0105378525\n",
            "\n",
            "Global step: 7920,loss: 0.010682934\n",
            "\n",
            "Global step: 7921,loss: 0.010260159\n",
            "\n",
            "Global step: 7922,loss: 0.010601709\n",
            "\n",
            "Global step: 7923,loss: 0.010817106\n",
            "\n",
            "Global step: 7924,loss: 0.0109458575\n",
            "\n",
            "Global step: 7925,loss: 0.010756979\n",
            "\n",
            "Global step: 7926,loss: 0.010587241\n",
            "\n",
            "Global step: 7927,loss: 0.009824724\n",
            "\n",
            "Global step: 7928,loss: 0.010874762\n",
            "\n",
            "Global step: 7929,loss: 0.011116128\n",
            "\n",
            "Global step: 7930,loss: 0.010647184\n",
            "\n",
            "Global step: 7931,loss: 0.010525896\n",
            "\n",
            "Global step: 7932,loss: 0.010455607\n",
            "\n",
            "Global step: 7933,loss: 0.01044401\n",
            "\n",
            "Global step: 7934,loss: 0.010455029\n",
            "\n",
            "Global step: 7935,loss: 0.010224542\n",
            "\n",
            "Global step: 7936,loss: 0.010822801\n",
            "\n",
            "Global step: 7937,loss: 0.010664754\n",
            "\n",
            "Global step: 7938,loss: 0.010595952\n",
            "\n",
            "Global step: 7939,loss: 0.010759936\n",
            "\n",
            "Global step: 7940,loss: 0.010306727\n",
            "\n",
            "Global step: 7941,loss: 0.010709693\n",
            "\n",
            "Global step: 7942,loss: 0.010472147\n",
            "\n",
            "Global step: 7943,loss: 0.010555421\n",
            "\n",
            "Global step: 7944,loss: 0.010655082\n",
            "\n",
            "Global step: 7945,loss: 0.010111339\n",
            "\n",
            "Global step: 7946,loss: 0.010965601\n",
            "\n",
            "Global step: 7947,loss: 0.010484916\n",
            "\n",
            "Global step: 7948,loss: 0.010576442\n",
            "\n",
            "Global step: 7949,loss: 0.010465011\n",
            "\n",
            "Global step: 7950,loss: 0.010563942\n",
            "\n",
            "Global step: 7951,loss: 0.010822019\n",
            "\n",
            "Global step: 7952,loss: 0.009959656\n",
            "\n",
            "Global step: 7953,loss: 0.011131835\n",
            "\n",
            "Global step: 7954,loss: 0.010269656\n",
            "\n",
            "Global step: 7955,loss: 0.010478585\n",
            "\n",
            "Global step: 7956,loss: 0.010600111\n",
            "\n",
            "Global step: 7957,loss: 0.0098075485\n",
            "\n",
            "Global step: 7958,loss: 0.010084781\n",
            "\n",
            "Global step: 7959,loss: 0.010689726\n",
            "\n",
            "Global step: 7960,loss: 0.010043784\n",
            "\n",
            "Global step: 7961,loss: 0.010751279\n",
            "\n",
            "Global step: 7962,loss: 0.010220856\n",
            "\n",
            "Global step: 7963,loss: 0.009909177\n",
            "\n",
            "Global step: 7964,loss: 0.010766933\n",
            "\n",
            "Global step: 7965,loss: 0.010568159\n",
            "\n",
            "Global step: 7966,loss: 0.010843919\n",
            "\n",
            "Global step: 7967,loss: 0.010523052\n",
            "\n",
            "Global step: 7968,loss: 0.010557085\n",
            "\n",
            "Global step: 7969,loss: 0.01081078\n",
            "\n",
            "Global step: 7970,loss: 0.010995186\n",
            "\n",
            "Global step: 7971,loss: 0.010461275\n",
            "\n",
            "Global step: 7972,loss: 0.011111114\n",
            "\n",
            "Global step: 7973,loss: 0.010573465\n",
            "\n",
            "Global step: 7974,loss: 0.010535954\n",
            "\n",
            "Global step: 7975,loss: 0.009548465\n",
            "\n",
            "Global step: 7976,loss: 0.010848915\n",
            "\n",
            "Global step: 7977,loss: 0.010763879\n",
            "\n",
            "Global step: 7978,loss: 0.01018073\n",
            "\n",
            "Global step: 7979,loss: 0.010682331\n",
            "\n",
            "Global step: 7980,loss: 0.010651132\n",
            "\n",
            "Global step: 7981,loss: 0.009982978\n",
            "\n",
            "Global step: 7982,loss: 0.010325504\n",
            "\n",
            "Global step: 7983,loss: 0.010778782\n",
            "\n",
            "Global step: 7984,loss: 0.010192372\n",
            "\n",
            "Global step: 7985,loss: 0.01087798\n",
            "\n",
            "Global step: 7986,loss: 0.010058834\n",
            "\n",
            "Global step: 7987,loss: 0.010490043\n",
            "\n",
            "Global step: 7988,loss: 0.010739146\n",
            "\n",
            "Global step: 7989,loss: 0.010018371\n",
            "\n",
            "Global step: 7990,loss: 0.010549942\n",
            "\n",
            "Global step: 7991,loss: 0.01000006\n",
            "\n",
            "Global step: 7992,loss: 0.011005706\n",
            "\n",
            "Global step: 7993,loss: 0.010321095\n",
            "\n",
            "Global step: 7994,loss: 0.010519088\n",
            "\n",
            "Global step: 7995,loss: 0.010398444\n",
            "\n",
            "Global step: 7996,loss: 0.011040101\n",
            "\n",
            "Global step: 7997,loss: 0.011206059\n",
            "\n",
            "Global step: 7998,loss: 0.011391962\n",
            "\n",
            "Global step: 7999,loss: 0.010492693\n",
            "\n",
            "Global step: 8000,loss: 0.010227014\n",
            "\n",
            "Global step: 8001,loss: 0.010268124\n",
            "\n",
            "Global step: 8002,loss: 0.01097453\n",
            "\n",
            "Global step: 8003,loss: 0.011011233\n",
            "\n",
            "Global step: 8004,loss: 0.010810789\n",
            "\n",
            "Global step: 8005,loss: 0.010465776\n",
            "\n",
            "Global step: 8006,loss: 0.011037761\n",
            "\n",
            "Global step: 8007,loss: 0.011004337\n",
            "\n",
            "Global step: 8008,loss: 0.010022379\n",
            "\n",
            "Global step: 8009,loss: 0.0103706345\n",
            "\n",
            "Global step: 8010,loss: 0.01067873\n",
            "\n",
            "Global step: 8011,loss: 0.010286668\n",
            "\n",
            "Global step: 8012,loss: 0.010183083\n",
            "\n",
            "Global step: 8013,loss: 0.010446447\n",
            "\n",
            "Global step: 8014,loss: 0.010393657\n",
            "\n",
            "Global step: 8015,loss: 0.009852099\n",
            "\n",
            "Global step: 8016,loss: 0.010592113\n",
            "\n",
            "Global step: 8017,loss: 0.010430824\n",
            "\n",
            "Global step: 8018,loss: 0.01072856\n",
            "\n",
            "Global step: 8019,loss: 0.0103716785\n",
            "\n",
            "Global step: 8020,loss: 0.010636807\n",
            "\n",
            "Global step: 8021,loss: 0.010306018\n",
            "\n",
            "Global step: 8022,loss: 0.011003844\n",
            "\n",
            "Global step: 8023,loss: 0.010175586\n",
            "\n",
            "Global step: 8024,loss: 0.010914385\n",
            "\n",
            "Global step: 8025,loss: 0.010744197\n",
            "\n",
            "Global step: 8026,loss: 0.010222944\n",
            "\n",
            "Global step: 8027,loss: 0.010484858\n",
            "\n",
            "Global step: 8028,loss: 0.010347546\n",
            "\n",
            "Global step: 8029,loss: 0.010111655\n",
            "\n",
            "Global step: 8030,loss: 0.010594863\n",
            "\n",
            "Global step: 8031,loss: 0.010650847\n",
            "\n",
            "Global step: 8032,loss: 0.010768243\n",
            "\n",
            "Global step: 8033,loss: 0.010611418\n",
            "\n",
            "Global step: 8034,loss: 0.010815714\n",
            "\n",
            "Global step: 8035,loss: 0.010684526\n",
            "\n",
            "Global step: 8036,loss: 0.010657376\n",
            "\n",
            "Global step: 8037,loss: 0.0099747395\n",
            "\n",
            "Global step: 8038,loss: 0.010349982\n",
            "\n",
            "Global step: 8039,loss: 0.010583799\n",
            "\n",
            "Global step: 8040,loss: 0.011465417\n",
            "\n",
            "Global step: 8041,loss: 0.010513514\n",
            "\n",
            "Global step: 8042,loss: 0.009783733\n",
            "\n",
            "Global step: 8043,loss: 0.010801817\n",
            "\n",
            "Global step: 8044,loss: 0.010608845\n",
            "\n",
            "Global step: 8045,loss: 0.010879545\n",
            "\n",
            "Global step: 8046,loss: 0.0107018445\n",
            "\n",
            "Global step: 8047,loss: 0.010590696\n",
            "\n",
            "Global step: 8048,loss: 0.01028581\n",
            "\n",
            "Global step: 8049,loss: 0.009933705\n",
            "\n",
            "Global step: 8050,loss: 0.010877924\n",
            "\n",
            "Global step: 8051,loss: 0.011723478\n",
            "\n",
            "Global step: 8052,loss: 0.01085028\n",
            "\n",
            "Global step: 8053,loss: 0.010227682\n",
            "\n",
            "Global step: 8054,loss: 0.01014843\n",
            "\n",
            "Global step: 8055,loss: 0.010485618\n",
            "\n",
            "Global step: 8056,loss: 0.0099683255\n",
            "\n",
            "Global step: 8057,loss: 0.0105060395\n",
            "\n",
            "Global step: 8058,loss: 0.01028484\n",
            "\n",
            "Global step: 8059,loss: 0.010075875\n",
            "\n",
            "Global step: 8060,loss: 0.010243599\n",
            "\n",
            "Global step: 8061,loss: 0.010458982\n",
            "\n",
            "Global step: 8062,loss: 0.010573008\n",
            "\n",
            "Global step: 8063,loss: 0.011021798\n",
            "\n",
            "Global step: 8064,loss: 0.009873335\n",
            "\n",
            "Global step: 8065,loss: 0.010463975\n",
            "\n",
            "Global step: 8066,loss: 0.01072885\n",
            "\n",
            "Global step: 8067,loss: 0.010355565\n",
            "\n",
            "Global step: 8068,loss: 0.010026274\n",
            "\n",
            "Global step: 8069,loss: 0.00973636\n",
            "\n",
            "Global step: 8070,loss: 0.011092571\n",
            "\n",
            "Global step: 8071,loss: 0.01070454\n",
            "\n",
            "Global step: 8072,loss: 0.010228809\n",
            "\n",
            "Global step: 8073,loss: 0.010209489\n",
            "\n",
            "Global step: 8074,loss: 0.010387966\n",
            "\n",
            "Global step: 8075,loss: 0.010629776\n",
            "\n",
            "Global step: 8076,loss: 0.010503104\n",
            "\n",
            "Global step: 8077,loss: 0.010004314\n",
            "\n",
            "Global step: 8078,loss: 0.010518236\n",
            "\n",
            "Global step: 8079,loss: 0.010155695\n",
            "\n",
            "Global step: 8080,loss: 0.009997635\n",
            "\n",
            "Global step: 8081,loss: 0.010263693\n",
            "\n",
            "Global step: 8082,loss: 0.010089888\n",
            "\n",
            "Global step: 8083,loss: 0.010355981\n",
            "\n",
            "Global step: 8084,loss: 0.010866847\n",
            "\n",
            "Global step: 8085,loss: 0.010723156\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 8086.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:00:29.396873 140167885084416 supervisor.py:1050] Recording summary at step 8086.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 8086,loss: 0.010492097\n",
            "\n",
            "Global step: 8087,loss: 0.010321995\n",
            "\n",
            "Global step: 8088,loss: 0.010308135\n",
            "\n",
            "Global step: 8089,loss: 0.010937003\n",
            "\n",
            "Global step: 8090,loss: 0.010034523\n",
            "\n",
            "Global step: 8091,loss: 0.010181268\n",
            "\n",
            "Global step: 8092,loss: 0.010452042\n",
            "\n",
            "Global step: 8093,loss: 0.010818354\n",
            "\n",
            "Global step: 8094,loss: 0.011090767\n",
            "\n",
            "Global step: 8095,loss: 0.0101977745\n",
            "\n",
            "Global step: 8096,loss: 0.011344666\n",
            "\n",
            "Global step: 8097,loss: 0.011320937\n",
            "\n",
            "Global step: 8098,loss: 0.010236084\n",
            "\n",
            "Global step: 8099,loss: 0.010478236\n",
            "\n",
            "Global step: 8100,loss: 0.010124223\n",
            "\n",
            "Global step: 8101,loss: 0.010794731\n",
            "\n",
            "Global step: 8102,loss: 0.010825325\n",
            "\n",
            "Global step: 8103,loss: 0.010332593\n",
            "\n",
            "Global step: 8104,loss: 0.010704945\n",
            "\n",
            "Global step: 8105,loss: 0.010768527\n",
            "\n",
            "Global step: 8106,loss: 0.0105084665\n",
            "\n",
            "Global step: 8107,loss: 0.011410237\n",
            "\n",
            "Global step: 8108,loss: 0.010466422\n",
            "\n",
            "Global step: 8109,loss: 0.0107073095\n",
            "\n",
            "Global step: 8110,loss: 0.0111634405\n",
            "\n",
            "Global step: 8111,loss: 0.01028561\n",
            "\n",
            "Global step: 8112,loss: 0.010594778\n",
            "\n",
            "Global step: 8113,loss: 0.010739424\n",
            "\n",
            "Global step: 8114,loss: 0.010388846\n",
            "\n",
            "Global step: 8115,loss: 0.01136224\n",
            "\n",
            "Global step: 8116,loss: 0.010334357\n",
            "\n",
            "Global step: 8117,loss: 0.010263522\n",
            "\n",
            "Global step: 8118,loss: 0.010865819\n",
            "\n",
            "Global step: 8119,loss: 0.009954913\n",
            "\n",
            "Global step: 8120,loss: 0.01041872\n",
            "\n",
            "Global step: 8121,loss: 0.010434712\n",
            "\n",
            "Global step: 8122,loss: 0.010680856\n",
            "\n",
            "Global step: 8123,loss: 0.010761393\n",
            "\n",
            "Global step: 8124,loss: 0.010461006\n",
            "\n",
            "Global step: 8125,loss: 0.010570151\n",
            "\n",
            "Global step: 8126,loss: 0.0104141785\n",
            "\n",
            "Global step: 8127,loss: 0.010546703\n",
            "\n",
            "Global step: 8128,loss: 0.010195392\n",
            "\n",
            "Global step: 8129,loss: 0.010974287\n",
            "\n",
            "Global step: 8130,loss: 0.010326349\n",
            "\n",
            "Global step: 8131,loss: 0.01127479\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 8131,Val_Loss: 0.014965704936338099,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:00:42.750741 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 39/50:\n",
            "Global step: 8132,loss: 0.010873231\n",
            "\n",
            "Global step: 8133,loss: 0.009903246\n",
            "\n",
            "Global step: 8134,loss: 0.010657492\n",
            "\n",
            "Global step: 8135,loss: 0.010252965\n",
            "\n",
            "Global step: 8136,loss: 0.010764401\n",
            "\n",
            "Global step: 8137,loss: 0.010497788\n",
            "\n",
            "Global step: 8138,loss: 0.011010822\n",
            "\n",
            "Global step: 8139,loss: 0.009920823\n",
            "\n",
            "Global step: 8140,loss: 0.010923115\n",
            "\n",
            "Global step: 8141,loss: 0.009628698\n",
            "\n",
            "Global step: 8142,loss: 0.010357174\n",
            "\n",
            "Global step: 8143,loss: 0.010691711\n",
            "\n",
            "Global step: 8144,loss: 0.0105112605\n",
            "\n",
            "Global step: 8145,loss: 0.010540868\n",
            "\n",
            "Global step: 8146,loss: 0.010570858\n",
            "\n",
            "Global step: 8147,loss: 0.010722423\n",
            "\n",
            "Global step: 8148,loss: 0.011065186\n",
            "\n",
            "Global step: 8149,loss: 0.010322794\n",
            "\n",
            "Global step: 8150,loss: 0.009929621\n",
            "\n",
            "Global step: 8151,loss: 0.0103116045\n",
            "\n",
            "Global step: 8152,loss: 0.010335204\n",
            "\n",
            "Global step: 8153,loss: 0.010479977\n",
            "\n",
            "Global step: 8154,loss: 0.010497825\n",
            "\n",
            "Global step: 8155,loss: 0.01043846\n",
            "\n",
            "Global step: 8156,loss: 0.010484747\n",
            "\n",
            "Global step: 8157,loss: 0.010020482\n",
            "\n",
            "Global step: 8158,loss: 0.010136975\n",
            "\n",
            "Global step: 8159,loss: 0.010480631\n",
            "\n",
            "Global step: 8160,loss: 0.010520288\n",
            "\n",
            "Global step: 8161,loss: 0.010240593\n",
            "\n",
            "Global step: 8162,loss: 0.010725374\n",
            "\n",
            "Global step: 8163,loss: 0.010317214\n",
            "\n",
            "Global step: 8164,loss: 0.010056503\n",
            "\n",
            "Global step: 8165,loss: 0.010527319\n",
            "\n",
            "Global step: 8166,loss: 0.010704339\n",
            "\n",
            "Global step: 8167,loss: 0.011021184\n",
            "\n",
            "Global step: 8168,loss: 0.010130301\n",
            "\n",
            "Global step: 8169,loss: 0.010179016\n",
            "\n",
            "Global step: 8170,loss: 0.0108093405\n",
            "\n",
            "Global step: 8171,loss: 0.011323592\n",
            "\n",
            "Global step: 8172,loss: 0.010155171\n",
            "\n",
            "Global step: 8173,loss: 0.010659199\n",
            "\n",
            "Global step: 8174,loss: 0.010169448\n",
            "\n",
            "Global step: 8175,loss: 0.0102608055\n",
            "\n",
            "Global step: 8176,loss: 0.010314672\n",
            "\n",
            "Global step: 8177,loss: 0.010238177\n",
            "\n",
            "Global step: 8178,loss: 0.010779856\n",
            "\n",
            "Global step: 8179,loss: 0.00986528\n",
            "\n",
            "Global step: 8180,loss: 0.010555228\n",
            "\n",
            "Global step: 8181,loss: 0.010192256\n",
            "\n",
            "Global step: 8182,loss: 0.010162855\n",
            "\n",
            "Global step: 8183,loss: 0.010667573\n",
            "\n",
            "Global step: 8184,loss: 0.010614966\n",
            "\n",
            "Global step: 8185,loss: 0.010144736\n",
            "\n",
            "Global step: 8186,loss: 0.010333521\n",
            "\n",
            "Global step: 8187,loss: 0.010821362\n",
            "\n",
            "Global step: 8188,loss: 0.010121294\n",
            "\n",
            "Global step: 8189,loss: 0.010343197\n",
            "\n",
            "Global step: 8190,loss: 0.010427282\n",
            "\n",
            "Global step: 8191,loss: 0.0099715525\n",
            "\n",
            "Global step: 8192,loss: 0.010851804\n",
            "\n",
            "Global step: 8193,loss: 0.010837356\n",
            "\n",
            "Global step: 8194,loss: 0.011158912\n",
            "\n",
            "Global step: 8195,loss: 0.0104048485\n",
            "\n",
            "Global step: 8196,loss: 0.010420005\n",
            "\n",
            "Global step: 8197,loss: 0.0105535155\n",
            "\n",
            "Global step: 8198,loss: 0.010293137\n",
            "\n",
            "Global step: 8199,loss: 0.010011053\n",
            "\n",
            "Global step: 8200,loss: 0.011201139\n",
            "\n",
            "Global step: 8201,loss: 0.01031143\n",
            "\n",
            "Global step: 8202,loss: 0.010873142\n",
            "\n",
            "Global step: 8203,loss: 0.009904836\n",
            "\n",
            "Global step: 8204,loss: 0.010451074\n",
            "\n",
            "Global step: 8205,loss: 0.010305342\n",
            "\n",
            "Global step: 8206,loss: 0.010478435\n",
            "\n",
            "Global step: 8207,loss: 0.01060718\n",
            "\n",
            "Global step: 8208,loss: 0.010638914\n",
            "\n",
            "Global step: 8209,loss: 0.010070834\n",
            "\n",
            "Global step: 8210,loss: 0.010087384\n",
            "\n",
            "Global step: 8211,loss: 0.010256425\n",
            "\n",
            "Global step: 8212,loss: 0.009976366\n",
            "\n",
            "Global step: 8213,loss: 0.01018717\n",
            "\n",
            "Global step: 8214,loss: 0.010660791\n",
            "\n",
            "Global step: 8215,loss: 0.009687564\n",
            "\n",
            "Global step: 8216,loss: 0.010515722\n",
            "\n",
            "Global step: 8217,loss: 0.010662839\n",
            "\n",
            "Global step: 8218,loss: 0.01028491\n",
            "\n",
            "Global step: 8219,loss: 0.010746335\n",
            "\n",
            "Global step: 8220,loss: 0.01030859\n",
            "\n",
            "Global step: 8221,loss: 0.010914493\n",
            "\n",
            "Global step: 8222,loss: 0.010093007\n",
            "\n",
            "Global step: 8223,loss: 0.010583022\n",
            "\n",
            "Global step: 8224,loss: 0.011148038\n",
            "\n",
            "Global step: 8225,loss: 0.0103063695\n",
            "\n",
            "Global step: 8226,loss: 0.010658486\n",
            "\n",
            "Global step: 8227,loss: 0.010280472\n",
            "\n",
            "Global step: 8228,loss: 0.010301768\n",
            "\n",
            "Global step: 8229,loss: 0.011089929\n",
            "\n",
            "Global step: 8230,loss: 0.010081021\n",
            "\n",
            "Global step: 8231,loss: 0.010704724\n",
            "\n",
            "Global step: 8232,loss: 0.010192344\n",
            "\n",
            "Global step: 8233,loss: 0.010127796\n",
            "\n",
            "Global step: 8234,loss: 0.010398139\n",
            "\n",
            "Global step: 8235,loss: 0.010345968\n",
            "\n",
            "Global step: 8236,loss: 0.01036737\n",
            "\n",
            "Global step: 8237,loss: 0.010091388\n",
            "\n",
            "Global step: 8238,loss: 0.010387011\n",
            "\n",
            "Global step: 8239,loss: 0.010824774\n",
            "\n",
            "Global step: 8240,loss: 0.010281764\n",
            "\n",
            "Global step: 8241,loss: 0.0099462615\n",
            "\n",
            "Global step: 8242,loss: 0.0100176465\n",
            "\n",
            "Global step: 8243,loss: 0.01046824\n",
            "\n",
            "Global step: 8244,loss: 0.010456112\n",
            "\n",
            "Global step: 8245,loss: 0.00972246\n",
            "\n",
            "Global step: 8246,loss: 0.010634563\n",
            "\n",
            "Global step: 8247,loss: 0.009868956\n",
            "\n",
            "Global step: 8248,loss: 0.0096803345\n",
            "\n",
            "Global step: 8249,loss: 0.010175811\n",
            "\n",
            "Global step: 8250,loss: 0.010985931\n",
            "\n",
            "Global step: 8251,loss: 0.010215204\n",
            "\n",
            "Global step: 8252,loss: 0.010019492\n",
            "\n",
            "Global step: 8253,loss: 0.009907945\n",
            "\n",
            "Global step: 8254,loss: 0.010081117\n",
            "\n",
            "Global step: 8255,loss: 0.009942051\n",
            "\n",
            "Global step: 8256,loss: 0.0102296695\n",
            "\n",
            "Global step: 8257,loss: 0.009916691\n",
            "\n",
            "Global step: 8258,loss: 0.010552794\n",
            "\n",
            "Global step: 8259,loss: 0.010766672\n",
            "\n",
            "Global step: 8260,loss: 0.010283333\n",
            "\n",
            "Global step: 8261,loss: 0.010476652\n",
            "\n",
            "Global step: 8262,loss: 0.00974408\n",
            "\n",
            "Global step: 8263,loss: 0.009897465\n",
            "\n",
            "Global step: 8264,loss: 0.009712925\n",
            "\n",
            "Global step: 8265,loss: 0.010257083\n",
            "\n",
            "Global step: 8266,loss: 0.010834704\n",
            "\n",
            "Global step: 8267,loss: 0.010466029\n",
            "\n",
            "Global step: 8268,loss: 0.010348767\n",
            "\n",
            "Global step: 8269,loss: 0.01011064\n",
            "\n",
            "Global step: 8270,loss: 0.010388408\n",
            "\n",
            "Global step: 8271,loss: 0.009366323\n",
            "\n",
            "Global step: 8272,loss: 0.010663224\n",
            "\n",
            "Global step: 8273,loss: 0.010593941\n",
            "\n",
            "Global step: 8274,loss: 0.009915382\n",
            "\n",
            "Global step: 8275,loss: 0.010057383\n",
            "\n",
            "Global step: 8276,loss: 0.0103898\n",
            "\n",
            "Global step: 8277,loss: 0.010561838\n",
            "\n",
            "Global step: 8278,loss: 0.01024124\n",
            "\n",
            "Global step: 8279,loss: 0.010523244\n",
            "\n",
            "Global step: 8280,loss: 0.010663578\n",
            "\n",
            "Global step: 8281,loss: 0.010776281\n",
            "\n",
            "Global step: 8282,loss: 0.010771983\n",
            "\n",
            "Global step: 8283,loss: 0.010186809\n",
            "\n",
            "Global step: 8284,loss: 0.010674816\n",
            "\n",
            "Global step: 8285,loss: 0.010125236\n",
            "\n",
            "Global step: 8286,loss: 0.01005961\n",
            "\n",
            "Global step: 8287,loss: 0.0103831915\n",
            "\n",
            "Global step: 8288,loss: 0.010688363\n",
            "\n",
            "Global step: 8289,loss: 0.010232565\n",
            "\n",
            "Global step: 8290,loss: 0.01085061\n",
            "\n",
            "Global step: 8291,loss: 0.010353224\n",
            "\n",
            "Global step: 8292,loss: 0.010034635\n",
            "\n",
            "Global step: 8293,loss: 0.010428567\n",
            "\n",
            "Global step: 8294,loss: 0.009744892\n",
            "\n",
            "Global step: 8295,loss: 0.010020624\n",
            "\n",
            "Global step: 8296,loss: 0.010825045\n",
            "\n",
            "Global step: 8297,loss: 0.01070437\n",
            "\n",
            "Global step: 8298,loss: 0.009899199\n",
            "\n",
            "Global step: 8299,loss: 0.010037711\n",
            "\n",
            "Global step: 8300,loss: 0.010203822\n",
            "\n",
            "Global step: 8301,loss: 0.010817279\n",
            "\n",
            "Global step: 8302,loss: 0.010356941\n",
            "\n",
            "Global step: 8303,loss: 0.01041578\n",
            "\n",
            "Global step: 8304,loss: 0.009955931\n",
            "\n",
            "Global step: 8305,loss: 0.010416241\n",
            "\n",
            "Global step: 8306,loss: 0.010109822\n",
            "\n",
            "Global step: 8307,loss: 0.010330668\n",
            "\n",
            "Global step: 8308,loss: 0.009690258\n",
            "\n",
            "Global step: 8309,loss: 0.010226974\n",
            "\n",
            "Global step: 8310,loss: 0.010334367\n",
            "\n",
            "Global step: 8311,loss: 0.010535147\n",
            "\n",
            "Global step: 8312,loss: 0.010759171\n",
            "\n",
            "Global step: 8313,loss: 0.010230458\n",
            "\n",
            "Global step: 8314,loss: 0.010587954\n",
            "\n",
            "Global step: 8315,loss: 0.010274585\n",
            "\n",
            "Global step: 8316,loss: 0.0103558665\n",
            "\n",
            "Global step: 8317,loss: 0.010270712\n",
            "\n",
            "Global step: 8318,loss: 0.010311674\n",
            "\n",
            "Global step: 8319,loss: 0.010219312\n",
            "\n",
            "Global step: 8320,loss: 0.011225578\n",
            "\n",
            "Global step: 8321,loss: 0.0105930865\n",
            "\n",
            "Global step: 8322,loss: 0.010524075\n",
            "\n",
            "Global step: 8323,loss: 0.010417475\n",
            "\n",
            "Global step: 8324,loss: 0.01022765\n",
            "\n",
            "Global step: 8325,loss: 0.010319568\n",
            "\n",
            "Global step: 8326,loss: 0.011197043\n",
            "\n",
            "Global step: 8327,loss: 0.010735592\n",
            "\n",
            "Global step: 8328,loss: 0.009814247\n",
            "\n",
            "Global step: 8329,loss: 0.010678239\n",
            "\n",
            "Global step: 8330,loss: 0.009927936\n",
            "\n",
            "Global step: 8331,loss: 0.010832727\n",
            "\n",
            "Global step: 8332,loss: 0.0105112055\n",
            "\n",
            "Global step: 8333,loss: 0.0101214135\n",
            "\n",
            "Global step: 8334,loss: 0.010524658\n",
            "\n",
            "Global step: 8335,loss: 0.010367886\n",
            "\n",
            "Global step: 8336,loss: 0.010312138\n",
            "\n",
            "Global step: 8337,loss: 0.010691608\n",
            "\n",
            "Global step: 8338,loss: 0.010882917\n",
            "\n",
            "Global step: 8339,loss: 0.010714126\n",
            "\n",
            "Global step: 8340,loss: 0.010309243\n",
            "\n",
            "Global step: 8341,loss: 0.010423304\n",
            "\n",
            "Global step: 8342,loss: 0.010412489\n",
            "\n",
            "Global step: 8343,loss: 0.010619777\n",
            "\n",
            "Global step: 8344,loss: 0.010599071\n",
            "\n",
            "Global step: 8345,loss: 0.010362644\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 8345,Val_Loss: 0.01489701675937364,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:01:36.493921 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 40/50:\n",
            "Global step: 8346,loss: 0.010275554\n",
            "\n",
            "Global step: 8347,loss: 0.010185173\n",
            "\n",
            "Global step: 8348,loss: 0.010347058\n",
            "\n",
            "Global step: 8349,loss: 0.010389575\n",
            "\n",
            "Global step: 8350,loss: 0.011006867\n",
            "\n",
            "Global step: 8351,loss: 0.009558752\n",
            "\n",
            "Global step: 8352,loss: 0.010754567\n",
            "\n",
            "Global step: 8353,loss: 0.010321334\n",
            "\n",
            "Global step: 8354,loss: 0.010485672\n",
            "\n",
            "Global step: 8355,loss: 0.010185664\n",
            "\n",
            "Global step: 8356,loss: 0.010348867\n",
            "\n",
            "Global step: 8357,loss: 0.010764625\n",
            "\n",
            "Global step: 8358,loss: 0.010218735\n",
            "\n",
            "Global step: 8359,loss: 0.01071691\n",
            "\n",
            "Global step: 8360,loss: 0.010741317\n",
            "\n",
            "Global step: 8361,loss: 0.009765115\n",
            "\n",
            "Global step: 8362,loss: 0.01077043\n",
            "\n",
            "Global step: 8363,loss: 0.010829555\n",
            "\n",
            "Global step: 8364,loss: 0.010340558\n",
            "\n",
            "Global step: 8365,loss: 0.010126537\n",
            "\n",
            "Global step: 8366,loss: 0.010131239\n",
            "\n",
            "Global step: 8367,loss: 0.009518975\n",
            "\n",
            "Global step: 8368,loss: 0.009721018\n",
            "\n",
            "Global step: 8369,loss: 0.010113913\n",
            "\n",
            "Global step: 8370,loss: 0.009868401\n",
            "\n",
            "Global step: 8371,loss: 0.009775741\n",
            "\n",
            "Global step: 8372,loss: 0.010228689\n",
            "\n",
            "Global step: 8373,loss: 0.010504412\n",
            "\n",
            "Global step: 8374,loss: 0.01066102\n",
            "\n",
            "Global step: 8375,loss: 0.009855656\n",
            "\n",
            "Global step: 8376,loss: 0.00999989\n",
            "\n",
            "Global step: 8377,loss: 0.009740699\n",
            "\n",
            "Global step: 8378,loss: 0.00996816\n",
            "\n",
            "Global step: 8379,loss: 0.009913972\n",
            "\n",
            "Global step: 8380,loss: 0.009703688\n",
            "\n",
            "Global step: 8381,loss: 0.009869013\n",
            "\n",
            "Global step: 8382,loss: 0.01023439\n",
            "\n",
            "Global step: 8383,loss: 0.010208176\n",
            "\n",
            "Global step: 8384,loss: 0.009812773\n",
            "\n",
            "Global step: 8385,loss: 0.010189065\n",
            "\n",
            "Global step: 8386,loss: 0.009964842\n",
            "\n",
            "Global step: 8387,loss: 0.00982106\n",
            "\n",
            "Global step: 8388,loss: 0.0104072755\n",
            "\n",
            "Global step: 8389,loss: 0.009666423\n",
            "\n",
            "Global step: 8390,loss: 0.010009249\n",
            "\n",
            "Global step: 8391,loss: 0.010969605\n",
            "\n",
            "Global step: 8392,loss: 0.01045479\n",
            "\n",
            "Global step: 8393,loss: 0.009612186\n",
            "\n",
            "Global step: 8394,loss: 0.01000736\n",
            "\n",
            "Global step: 8395,loss: 0.010888405\n",
            "\n",
            "Global step: 8396,loss: 0.0099251615\n",
            "\n",
            "Global step: 8397,loss: 0.0101673165\n",
            "\n",
            "Global step: 8398,loss: 0.009981525\n",
            "\n",
            "Global step: 8399,loss: 0.010538029\n",
            "\n",
            "Global step: 8400,loss: 0.009526141\n",
            "\n",
            "Global step: 8401,loss: 0.01007062\n",
            "\n",
            "Global step: 8402,loss: 0.009830653\n",
            "\n",
            "Global step: 8403,loss: 0.010020234\n",
            "\n",
            "Global step: 8404,loss: 0.009556915\n",
            "\n",
            "Global step: 8405,loss: 0.010374758\n",
            "\n",
            "Global step: 8406,loss: 0.009984449\n",
            "\n",
            "Global step: 8407,loss: 0.010334785\n",
            "\n",
            "Global step: 8408,loss: 0.010105041\n",
            "\n",
            "Global step: 8409,loss: 0.010180769\n",
            "\n",
            "Global step: 8410,loss: 0.011065618\n",
            "\n",
            "Global step: 8411,loss: 0.010127862\n",
            "\n",
            "Global step: 8412,loss: 0.010030983\n",
            "\n",
            "Global step: 8413,loss: 0.0102214785\n",
            "\n",
            "Global step: 8414,loss: 0.009870139\n",
            "\n",
            "Global step: 8415,loss: 0.010515509\n",
            "\n",
            "Global step: 8416,loss: 0.01004476\n",
            "\n",
            "Global step: 8417,loss: 0.010471693\n",
            "\n",
            "Global step: 8418,loss: 0.010150925\n",
            "\n",
            "Global step: 8419,loss: 0.010440539\n",
            "\n",
            "Global step: 8420,loss: 0.010003393\n",
            "\n",
            "Global step: 8421,loss: 0.009876143\n",
            "\n",
            "Global step: 8422,loss: 0.010336694\n",
            "\n",
            "Global step: 8423,loss: 0.010015114\n",
            "\n",
            "Global step: 8424,loss: 0.010053628\n",
            "\n",
            "Global step: 8425,loss: 0.010718997\n",
            "\n",
            "Global step: 8426,loss: 0.010572543\n",
            "\n",
            "Global step: 8427,loss: 0.010518477\n",
            "\n",
            "Global step: 8428,loss: 0.010846181\n",
            "\n",
            "Global step: 8429,loss: 0.010277957\n",
            "\n",
            "Global step: 8430,loss: 0.010025026\n",
            "\n",
            "Global step: 8431,loss: 0.010970149\n",
            "\n",
            "Global step: 8432,loss: 0.010005539\n",
            "\n",
            "Global step: 8433,loss: 0.010462917\n",
            "\n",
            "Global step: 8434,loss: 0.010477551\n",
            "\n",
            "Global step: 8435,loss: 0.01083926\n",
            "\n",
            "Global step: 8436,loss: 0.010312105\n",
            "\n",
            "Global step: 8437,loss: 0.0102997525\n",
            "\n",
            "Global step: 8438,loss: 0.010574723\n",
            "\n",
            "Global step: 8439,loss: 0.009987224\n",
            "\n",
            "Global step: 8440,loss: 0.01033189\n",
            "\n",
            "Global step: 8441,loss: 0.010018567\n",
            "\n",
            "Global step: 8442,loss: 0.010060109\n",
            "\n",
            "Global step: 8443,loss: 0.010434955\n",
            "\n",
            "Global step: 8444,loss: 0.009922075\n",
            "\n",
            "Global step: 8445,loss: 0.010166774\n",
            "\n",
            "Global step: 8446,loss: 0.010458926\n",
            "\n",
            "Global step: 8447,loss: 0.010621453\n",
            "\n",
            "Global step: 8448,loss: 0.010370301\n",
            "\n",
            "Global step: 8449,loss: 0.010405762\n",
            "\n",
            "Global step: 8450,loss: 0.009992262\n",
            "\n",
            "Global step: 8451,loss: 0.010987788\n",
            "\n",
            "Global step: 8452,loss: 0.010593801\n",
            "\n",
            "Global step: 8453,loss: 0.010473276\n",
            "\n",
            "Global step: 8454,loss: 0.011004951\n",
            "\n",
            "Global step: 8455,loss: 0.010437496\n",
            "\n",
            "Global step: 8456,loss: 0.010649036\n",
            "\n",
            "Global step: 8457,loss: 0.010482891\n",
            "\n",
            "Global step: 8458,loss: 0.00983674\n",
            "\n",
            "Global step: 8459,loss: 0.009785948\n",
            "\n",
            "Global step: 8460,loss: 0.010350579\n",
            "\n",
            "Global step: 8461,loss: 0.010345532\n",
            "\n",
            "Global step: 8462,loss: 0.010365559\n",
            "\n",
            "Global step: 8463,loss: 0.010520361\n",
            "\n",
            "Global step: 8464,loss: 0.010164776\n",
            "\n",
            "Global step: 8465,loss: 0.009938765\n",
            "\n",
            "Global step: 8466,loss: 0.009710842\n",
            "\n",
            "Global step: 8467,loss: 0.0102135865\n",
            "\n",
            "Global step: 8468,loss: 0.010323079\n",
            "\n",
            "Global step: 8469,loss: 0.010117239\n",
            "\n",
            "Global step: 8470,loss: 0.0101770805\n",
            "\n",
            "Global step: 8471,loss: 0.0111722\n",
            "\n",
            "Global step: 8472,loss: 0.010187178\n",
            "\n",
            "Global step: 8473,loss: 0.0098657645\n",
            "\n",
            "Global step: 8474,loss: 0.010179423\n",
            "\n",
            "Global step: 8475,loss: 0.010317882\n",
            "\n",
            "Global step: 8476,loss: 0.010296811\n",
            "\n",
            "Global step: 8477,loss: 0.009671963\n",
            "\n",
            "Global step: 8478,loss: 0.010185587\n",
            "\n",
            "Global step: 8479,loss: 0.010312086\n",
            "\n",
            "Global step: 8480,loss: 0.010649966\n",
            "\n",
            "Global step: 8481,loss: 0.01040598\n",
            "\n",
            "Global step: 8482,loss: 0.010650554\n",
            "\n",
            "Global step: 8483,loss: 0.010101948\n",
            "\n",
            "Global step: 8484,loss: 0.010263757\n",
            "\n",
            "Global step: 8485,loss: 0.009876247\n",
            "\n",
            "Global step: 8486,loss: 0.010794418\n",
            "\n",
            "Global step: 8487,loss: 0.010069881\n",
            "\n",
            "Global step: 8488,loss: 0.010277908\n",
            "\n",
            "Global step: 8489,loss: 0.009729871\n",
            "\n",
            "Global step: 8490,loss: 0.010731014\n",
            "\n",
            "Global step: 8491,loss: 0.010625348\n",
            "\n",
            "Global step: 8492,loss: 0.010403634\n",
            "\n",
            "Global step: 8493,loss: 0.010427188\n",
            "\n",
            "Global step: 8494,loss: 0.010149862\n",
            "\n",
            "Global step: 8495,loss: 0.010378343\n",
            "\n",
            "Global step: 8496,loss: 0.009828849\n",
            "\n",
            "Global step: 8497,loss: 0.009784057\n",
            "\n",
            "Global step: 8498,loss: 0.0098398905\n",
            "\n",
            "Global step: 8499,loss: 0.010837591\n",
            "\n",
            "Global step: 8500,loss: 0.010238796\n",
            "\n",
            "Global step: 8501,loss: 0.010051501\n",
            "\n",
            "Global step: 8502,loss: 0.0102297105\n",
            "\n",
            "Global step: 8503,loss: 0.009997948\n",
            "\n",
            "Global step: 8504,loss: 0.010389688\n",
            "\n",
            "Global step: 8505,loss: 0.010133951\n",
            "\n",
            "Global step: 8506,loss: 0.010143999\n",
            "\n",
            "Global step: 8507,loss: 0.009803512\n",
            "\n",
            "Global step: 8508,loss: 0.0105074085\n",
            "\n",
            "Global step: 8509,loss: 0.00962156\n",
            "\n",
            "Global step: 8510,loss: 0.010697424\n",
            "\n",
            "Global step: 8511,loss: 0.010275661\n",
            "\n",
            "Global step: 8512,loss: 0.010291192\n",
            "\n",
            "Global step: 8513,loss: 0.010695343\n",
            "\n",
            "Global step: 8514,loss: 0.010158649\n",
            "\n",
            "Global step: 8515,loss: 0.010006242\n",
            "\n",
            "Global step: 8516,loss: 0.010307606\n",
            "\n",
            "Global step: 8517,loss: 0.010367296\n",
            "\n",
            "Global step: 8518,loss: 0.009942555\n",
            "\n",
            "Global step: 8519,loss: 0.010903238\n",
            "\n",
            "Global step: 8520,loss: 0.010274186\n",
            "\n",
            "Global step: 8521,loss: 0.010286871\n",
            "\n",
            "Global step: 8522,loss: 0.010369601\n",
            "\n",
            "Global step: 8523,loss: 0.009851786\n",
            "\n",
            "Global step: 8524,loss: 0.009799769\n",
            "\n",
            "Global step: 8525,loss: 0.010578209\n",
            "\n",
            "Global step: 8526,loss: 0.010265623\n",
            "\n",
            "Global step: 8527,loss: 0.010045329\n",
            "\n",
            "Global step: 8528,loss: 0.010031545\n",
            "\n",
            "Global step: 8529,loss: 0.010516892\n",
            "\n",
            "Global step: 8530,loss: 0.010315738\n",
            "\n",
            "Global step: 8531,loss: 0.010018093\n",
            "\n",
            "Global step: 8532,loss: 0.010241223\n",
            "\n",
            "Global step: 8533,loss: 0.010721769\n",
            "\n",
            "Global step: 8534,loss: 0.009805374\n",
            "\n",
            "Global step: 8535,loss: 0.009577427\n",
            "\n",
            "Global step: 8536,loss: 0.010560847\n",
            "\n",
            "Global step: 8537,loss: 0.0105321435\n",
            "\n",
            "Global step: 8538,loss: 0.010020586\n",
            "\n",
            "Global step: 8539,loss: 0.010456418\n",
            "\n",
            "Global step: 8540,loss: 0.010526342\n",
            "\n",
            "Global step: 8541,loss: 0.010358567\n",
            "\n",
            "Global step: 8542,loss: 0.010024459\n",
            "\n",
            "Global step: 8543,loss: 0.010014041\n",
            "\n",
            "Global step: 8544,loss: 0.010222577\n",
            "\n",
            "Global step: 8545,loss: 0.009480979\n",
            "\n",
            "Global step: 8546,loss: 0.0104186805\n",
            "\n",
            "Global step: 8547,loss: 0.010512821\n",
            "\n",
            "Global step: 8548,loss: 0.010636487\n",
            "\n",
            "Global step: 8549,loss: 0.010108442\n",
            "\n",
            "Global step: 8550,loss: 0.00996505\n",
            "\n",
            "Global step: 8551,loss: 0.010437264\n",
            "\n",
            "Global step: 8552,loss: 0.010370978\n",
            "\n",
            "Global step: 8553,loss: 0.0107168155\n",
            "\n",
            "Global step: 8554,loss: 0.010196763\n",
            "\n",
            "Global step: 8555,loss: 0.010158165\n",
            "\n",
            "Global step: 8556,loss: 0.010611119\n",
            "\n",
            "Global step: 8557,loss: 0.009747509\n",
            "\n",
            "Global step: 8558,loss: 0.010269734\n",
            "\n",
            "Global step: 8559,loss: 0.009957811\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 8559,val_loss: 0.014910888367969739\n",
            "\n",
            "Training for epoch 41/50:\n",
            "Global step: 8560,loss: 0.009880349\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 8561.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:02:29.239972 140167885084416 supervisor.py:1050] Recording summary at step 8561.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 8561,loss: 0.009809582\n",
            "\n",
            "Global step: 8562,loss: 0.010494308\n",
            "\n",
            "Global step: 8563,loss: 0.009795206\n",
            "\n",
            "Global step: 8564,loss: 0.0099935485\n",
            "\n",
            "Global step: 8565,loss: 0.010346503\n",
            "\n",
            "Global step: 8566,loss: 0.010738514\n",
            "\n",
            "Global step: 8567,loss: 0.00989902\n",
            "\n",
            "Global step: 8568,loss: 0.011013472\n",
            "\n",
            "Global step: 8569,loss: 0.010170835\n",
            "\n",
            "Global step: 8570,loss: 0.010333333\n",
            "\n",
            "Global step: 8571,loss: 0.010254315\n",
            "\n",
            "Global step: 8572,loss: 0.010614952\n",
            "\n",
            "Global step: 8573,loss: 0.009982056\n",
            "\n",
            "Global step: 8574,loss: 0.00961181\n",
            "\n",
            "Global step: 8575,loss: 0.010064839\n",
            "\n",
            "Global step: 8576,loss: 0.0095464485\n",
            "\n",
            "Global step: 8577,loss: 0.010481622\n",
            "\n",
            "Global step: 8578,loss: 0.010264914\n",
            "\n",
            "Global step: 8579,loss: 0.009511005\n",
            "\n",
            "Global step: 8580,loss: 0.009662985\n",
            "\n",
            "Global step: 8581,loss: 0.010171601\n",
            "\n",
            "Global step: 8582,loss: 0.010299137\n",
            "\n",
            "Global step: 8583,loss: 0.010511614\n",
            "\n",
            "Global step: 8584,loss: 0.01072762\n",
            "\n",
            "Global step: 8585,loss: 0.00986532\n",
            "\n",
            "Global step: 8586,loss: 0.010047085\n",
            "\n",
            "Global step: 8587,loss: 0.010565098\n",
            "\n",
            "Global step: 8588,loss: 0.0101005295\n",
            "\n",
            "Global step: 8589,loss: 0.009840047\n",
            "\n",
            "Global step: 8590,loss: 0.00995095\n",
            "\n",
            "Global step: 8591,loss: 0.010054482\n",
            "\n",
            "Global step: 8592,loss: 0.010296594\n",
            "\n",
            "Global step: 8593,loss: 0.009584916\n",
            "\n",
            "Global step: 8594,loss: 0.009938223\n",
            "\n",
            "Global step: 8595,loss: 0.009791891\n",
            "\n",
            "Global step: 8596,loss: 0.01037936\n",
            "\n",
            "Global step: 8597,loss: 0.010460462\n",
            "\n",
            "Global step: 8598,loss: 0.010366815\n",
            "\n",
            "Global step: 8599,loss: 0.009813117\n",
            "\n",
            "Global step: 8600,loss: 0.009632841\n",
            "\n",
            "Global step: 8601,loss: 0.010385739\n",
            "\n",
            "Global step: 8602,loss: 0.009753601\n",
            "\n",
            "Global step: 8603,loss: 0.010181165\n",
            "\n",
            "Global step: 8604,loss: 0.0103858905\n",
            "\n",
            "Global step: 8605,loss: 0.009487277\n",
            "\n",
            "Global step: 8606,loss: 0.009779094\n",
            "\n",
            "Global step: 8607,loss: 0.00986257\n",
            "\n",
            "Global step: 8608,loss: 0.010877896\n",
            "\n",
            "Global step: 8609,loss: 0.009851676\n",
            "\n",
            "Global step: 8610,loss: 0.010007162\n",
            "\n",
            "Global step: 8611,loss: 0.010220744\n",
            "\n",
            "Global step: 8612,loss: 0.009361103\n",
            "\n",
            "Global step: 8613,loss: 0.009633378\n",
            "\n",
            "Global step: 8614,loss: 0.010025476\n",
            "\n",
            "Global step: 8615,loss: 0.010309945\n",
            "\n",
            "Global step: 8616,loss: 0.009465364\n",
            "\n",
            "Global step: 8617,loss: 0.009859937\n",
            "\n",
            "Global step: 8618,loss: 0.009630088\n",
            "\n",
            "Global step: 8619,loss: 0.009758932\n",
            "\n",
            "Global step: 8620,loss: 0.011102626\n",
            "\n",
            "Global step: 8621,loss: 0.010001994\n",
            "\n",
            "Global step: 8622,loss: 0.010199646\n",
            "\n",
            "Global step: 8623,loss: 0.009912536\n",
            "\n",
            "Global step: 8624,loss: 0.010272961\n",
            "\n",
            "Global step: 8625,loss: 0.009806894\n",
            "\n",
            "Global step: 8626,loss: 0.00953404\n",
            "\n",
            "Global step: 8627,loss: 0.010352089\n",
            "\n",
            "Global step: 8628,loss: 0.010478064\n",
            "\n",
            "Global step: 8629,loss: 0.01035136\n",
            "\n",
            "Global step: 8630,loss: 0.010348676\n",
            "\n",
            "Global step: 8631,loss: 0.009549686\n",
            "\n",
            "Global step: 8632,loss: 0.009446104\n",
            "\n",
            "Global step: 8633,loss: 0.0100707\n",
            "\n",
            "Global step: 8634,loss: 0.010303072\n",
            "\n",
            "Global step: 8635,loss: 0.010437176\n",
            "\n",
            "Global step: 8636,loss: 0.009373175\n",
            "\n",
            "Global step: 8637,loss: 0.0097607635\n",
            "\n",
            "Global step: 8638,loss: 0.010110583\n",
            "\n",
            "Global step: 8639,loss: 0.010170572\n",
            "\n",
            "Global step: 8640,loss: 0.010315231\n",
            "\n",
            "Global step: 8641,loss: 0.009436299\n",
            "\n",
            "Global step: 8642,loss: 0.010328464\n",
            "\n",
            "Global step: 8643,loss: 0.009930351\n",
            "\n",
            "Global step: 8644,loss: 0.010315903\n",
            "\n",
            "Global step: 8645,loss: 0.009310677\n",
            "\n",
            "Global step: 8646,loss: 0.010357695\n",
            "\n",
            "Global step: 8647,loss: 0.010145979\n",
            "\n",
            "Global step: 8648,loss: 0.010208551\n",
            "\n",
            "Global step: 8649,loss: 0.009892728\n",
            "\n",
            "Global step: 8650,loss: 0.010261982\n",
            "\n",
            "Global step: 8651,loss: 0.009878052\n",
            "\n",
            "Global step: 8652,loss: 0.010047272\n",
            "\n",
            "Global step: 8653,loss: 0.010473062\n",
            "\n",
            "Global step: 8654,loss: 0.010170609\n",
            "\n",
            "Global step: 8655,loss: 0.0101946555\n",
            "\n",
            "Global step: 8656,loss: 0.009983798\n",
            "\n",
            "Global step: 8657,loss: 0.010595039\n",
            "\n",
            "Global step: 8658,loss: 0.010834316\n",
            "\n",
            "Global step: 8659,loss: 0.010196741\n",
            "\n",
            "Global step: 8660,loss: 0.009678069\n",
            "\n",
            "Global step: 8661,loss: 0.010017495\n",
            "\n",
            "Global step: 8662,loss: 0.009717529\n",
            "\n",
            "Global step: 8663,loss: 0.010047603\n",
            "\n",
            "Global step: 8664,loss: 0.0101815825\n",
            "\n",
            "Global step: 8665,loss: 0.010064997\n",
            "\n",
            "Global step: 8666,loss: 0.010483059\n",
            "\n",
            "Global step: 8667,loss: 0.010735878\n",
            "\n",
            "Global step: 8668,loss: 0.010212643\n",
            "\n",
            "Global step: 8669,loss: 0.0096166255\n",
            "\n",
            "Global step: 8670,loss: 0.010339348\n",
            "\n",
            "Global step: 8671,loss: 0.009844225\n",
            "\n",
            "Global step: 8672,loss: 0.009936218\n",
            "\n",
            "Global step: 8673,loss: 0.009842369\n",
            "\n",
            "Global step: 8674,loss: 0.01022827\n",
            "\n",
            "Global step: 8675,loss: 0.010542287\n",
            "\n",
            "Global step: 8676,loss: 0.009847556\n",
            "\n",
            "Global step: 8677,loss: 0.009819103\n",
            "\n",
            "Global step: 8678,loss: 0.009750876\n",
            "\n",
            "Global step: 8679,loss: 0.0096775945\n",
            "\n",
            "Global step: 8680,loss: 0.01003579\n",
            "\n",
            "Global step: 8681,loss: 0.010381696\n",
            "\n",
            "Global step: 8682,loss: 0.009471523\n",
            "\n",
            "Global step: 8683,loss: 0.0101845395\n",
            "\n",
            "Global step: 8684,loss: 0.009822644\n",
            "\n",
            "Global step: 8685,loss: 0.010626055\n",
            "\n",
            "Global step: 8686,loss: 0.010109433\n",
            "\n",
            "Global step: 8687,loss: 0.010439044\n",
            "\n",
            "Global step: 8688,loss: 0.010141984\n",
            "\n",
            "Global step: 8689,loss: 0.010232617\n",
            "\n",
            "Global step: 8690,loss: 0.010293548\n",
            "\n",
            "Global step: 8691,loss: 0.009667747\n",
            "\n",
            "Global step: 8692,loss: 0.010347798\n",
            "\n",
            "Global step: 8693,loss: 0.010156993\n",
            "\n",
            "Global step: 8694,loss: 0.009714969\n",
            "\n",
            "Global step: 8695,loss: 0.010694724\n",
            "\n",
            "Global step: 8696,loss: 0.010216954\n",
            "\n",
            "Global step: 8697,loss: 0.010787472\n",
            "\n",
            "Global step: 8698,loss: 0.009814008\n",
            "\n",
            "Global step: 8699,loss: 0.01027094\n",
            "\n",
            "Global step: 8700,loss: 0.010514458\n",
            "\n",
            "Global step: 8701,loss: 0.009761742\n",
            "\n",
            "Global step: 8702,loss: 0.010109882\n",
            "\n",
            "Global step: 8703,loss: 0.010398323\n",
            "\n",
            "Global step: 8704,loss: 0.0099976\n",
            "\n",
            "Global step: 8705,loss: 0.010094364\n",
            "\n",
            "Global step: 8706,loss: 0.010564578\n",
            "\n",
            "Global step: 8707,loss: 0.010137589\n",
            "\n",
            "Global step: 8708,loss: 0.0102767935\n",
            "\n",
            "Global step: 8709,loss: 0.010304884\n",
            "\n",
            "Global step: 8710,loss: 0.0105162\n",
            "\n",
            "Global step: 8711,loss: 0.010152651\n",
            "\n",
            "Global step: 8712,loss: 0.009870586\n",
            "\n",
            "Global step: 8713,loss: 0.0094379615\n",
            "\n",
            "Global step: 8714,loss: 0.010114881\n",
            "\n",
            "Global step: 8715,loss: 0.00992478\n",
            "\n",
            "Global step: 8716,loss: 0.010159496\n",
            "\n",
            "Global step: 8717,loss: 0.009924605\n",
            "\n",
            "Global step: 8718,loss: 0.01013615\n",
            "\n",
            "Global step: 8719,loss: 0.010271618\n",
            "\n",
            "Global step: 8720,loss: 0.0100359265\n",
            "\n",
            "Global step: 8721,loss: 0.010304692\n",
            "\n",
            "Global step: 8722,loss: 0.010582968\n",
            "\n",
            "Global step: 8723,loss: 0.010957695\n",
            "\n",
            "Global step: 8724,loss: 0.010043491\n",
            "\n",
            "Global step: 8725,loss: 0.010347687\n",
            "\n",
            "Global step: 8726,loss: 0.00991472\n",
            "\n",
            "Global step: 8727,loss: 0.010417593\n",
            "\n",
            "Global step: 8728,loss: 0.009585647\n",
            "\n",
            "Global step: 8729,loss: 0.010443141\n",
            "\n",
            "Global step: 8730,loss: 0.0107150795\n",
            "\n",
            "Global step: 8731,loss: 0.010215814\n",
            "\n",
            "Global step: 8732,loss: 0.010163227\n",
            "\n",
            "Global step: 8733,loss: 0.009614634\n",
            "\n",
            "Global step: 8734,loss: 0.010535536\n",
            "\n",
            "Global step: 8735,loss: 0.010427998\n",
            "\n",
            "Global step: 8736,loss: 0.010559095\n",
            "\n",
            "Global step: 8737,loss: 0.010380409\n",
            "\n",
            "Global step: 8738,loss: 0.010709446\n",
            "\n",
            "Global step: 8739,loss: 0.009340241\n",
            "\n",
            "Global step: 8740,loss: 0.0103252325\n",
            "\n",
            "Global step: 8741,loss: 0.010403006\n",
            "\n",
            "Global step: 8742,loss: 0.010317993\n",
            "\n",
            "Global step: 8743,loss: 0.010177678\n",
            "\n",
            "Global step: 8744,loss: 0.010062406\n",
            "\n",
            "Global step: 8745,loss: 0.010529908\n",
            "\n",
            "Global step: 8746,loss: 0.010241676\n",
            "\n",
            "Global step: 8747,loss: 0.0104182735\n",
            "\n",
            "Global step: 8748,loss: 0.011054617\n",
            "\n",
            "Global step: 8749,loss: 0.011029004\n",
            "\n",
            "Global step: 8750,loss: 0.01003727\n",
            "\n",
            "Global step: 8751,loss: 0.01008373\n",
            "\n",
            "Global step: 8752,loss: 0.009856201\n",
            "\n",
            "Global step: 8753,loss: 0.010412932\n",
            "\n",
            "Global step: 8754,loss: 0.00988306\n",
            "\n",
            "Global step: 8755,loss: 0.010007075\n",
            "\n",
            "Global step: 8756,loss: 0.010043481\n",
            "\n",
            "Global step: 8757,loss: 0.010244692\n",
            "\n",
            "Global step: 8758,loss: 0.009964533\n",
            "\n",
            "Global step: 8759,loss: 0.010023666\n",
            "\n",
            "Global step: 8760,loss: 0.010106642\n",
            "\n",
            "Global step: 8761,loss: 0.010181642\n",
            "\n",
            "Global step: 8762,loss: 0.009705553\n",
            "\n",
            "Global step: 8763,loss: 0.010034624\n",
            "\n",
            "Global step: 8764,loss: 0.009735834\n",
            "\n",
            "Global step: 8765,loss: 0.010338692\n",
            "\n",
            "Global step: 8766,loss: 0.009812183\n",
            "\n",
            "Global step: 8767,loss: 0.009918903\n",
            "\n",
            "Global step: 8768,loss: 0.010332635\n",
            "\n",
            "Global step: 8769,loss: 0.010230807\n",
            "\n",
            "Global step: 8770,loss: 0.0101042045\n",
            "\n",
            "Global step: 8771,loss: 0.010273224\n",
            "\n",
            "Global step: 8772,loss: 0.009887651\n",
            "\n",
            "Global step: 8773,loss: 0.009966428\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 8773,Val_Loss: 0.014801847699441407,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:03:21.695716 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 42/50:\n",
            "Global step: 8774,loss: 0.010290521\n",
            "\n",
            "Global step: 8775,loss: 0.009748911\n",
            "\n",
            "Global step: 8776,loss: 0.009902929\n",
            "\n",
            "Global step: 8777,loss: 0.010124348\n",
            "\n",
            "Global step: 8778,loss: 0.009959304\n",
            "\n",
            "Global step: 8779,loss: 0.010022839\n",
            "\n",
            "Global step: 8780,loss: 0.010379953\n",
            "\n",
            "Global step: 8781,loss: 0.0099484585\n",
            "\n",
            "Global step: 8782,loss: 0.010258269\n",
            "\n",
            "Global step: 8783,loss: 0.010098984\n",
            "\n",
            "Global step: 8784,loss: 0.009584209\n",
            "\n",
            "Global step: 8785,loss: 0.009930482\n",
            "\n",
            "Global step: 8786,loss: 0.009986633\n",
            "\n",
            "Global step: 8787,loss: 0.0097453175\n",
            "\n",
            "Global step: 8788,loss: 0.010166591\n",
            "\n",
            "Global step: 8789,loss: 0.010131642\n",
            "\n",
            "Global step: 8790,loss: 0.010248475\n",
            "\n",
            "Global step: 8791,loss: 0.009667899\n",
            "\n",
            "Global step: 8792,loss: 0.010441099\n",
            "\n",
            "Global step: 8793,loss: 0.009461051\n",
            "\n",
            "Global step: 8794,loss: 0.009729487\n",
            "\n",
            "Global step: 8795,loss: 0.009540402\n",
            "\n",
            "Global step: 8796,loss: 0.009825318\n",
            "\n",
            "Global step: 8797,loss: 0.010442913\n",
            "\n",
            "Global step: 8798,loss: 0.009934175\n",
            "\n",
            "Global step: 8799,loss: 0.009630006\n",
            "\n",
            "Global step: 8800,loss: 0.010400715\n",
            "\n",
            "Global step: 8801,loss: 0.010080583\n",
            "\n",
            "Global step: 8802,loss: 0.009773546\n",
            "\n",
            "Global step: 8803,loss: 0.010275665\n",
            "\n",
            "Global step: 8804,loss: 0.010185268\n",
            "\n",
            "Global step: 8805,loss: 0.009812766\n",
            "\n",
            "Global step: 8806,loss: 0.010196451\n",
            "\n",
            "Global step: 8807,loss: 0.009967796\n",
            "\n",
            "Global step: 8808,loss: 0.010013386\n",
            "\n",
            "Global step: 8809,loss: 0.009779441\n",
            "\n",
            "Global step: 8810,loss: 0.009892797\n",
            "\n",
            "Global step: 8811,loss: 0.009942348\n",
            "\n",
            "Global step: 8812,loss: 0.0096551245\n",
            "\n",
            "Global step: 8813,loss: 0.0098482445\n",
            "\n",
            "Global step: 8814,loss: 0.010007703\n",
            "\n",
            "Global step: 8815,loss: 0.010268391\n",
            "\n",
            "Global step: 8816,loss: 0.010392824\n",
            "\n",
            "Global step: 8817,loss: 0.00993749\n",
            "\n",
            "Global step: 8818,loss: 0.010398666\n",
            "\n",
            "Global step: 8819,loss: 0.009524374\n",
            "\n",
            "Global step: 8820,loss: 0.010318396\n",
            "\n",
            "Global step: 8821,loss: 0.009593655\n",
            "\n",
            "Global step: 8822,loss: 0.0108377235\n",
            "\n",
            "Global step: 8823,loss: 0.009789749\n",
            "\n",
            "Global step: 8824,loss: 0.009456303\n",
            "\n",
            "Global step: 8825,loss: 0.0098282145\n",
            "\n",
            "Global step: 8826,loss: 0.01002262\n",
            "\n",
            "Global step: 8827,loss: 0.0103524495\n",
            "\n",
            "Global step: 8828,loss: 0.010298724\n",
            "\n",
            "Global step: 8829,loss: 0.009695599\n",
            "\n",
            "Global step: 8830,loss: 0.010280562\n",
            "\n",
            "Global step: 8831,loss: 0.0095963245\n",
            "\n",
            "Global step: 8832,loss: 0.009690445\n",
            "\n",
            "Global step: 8833,loss: 0.010559322\n",
            "\n",
            "Global step: 8834,loss: 0.010014073\n",
            "\n",
            "Global step: 8835,loss: 0.010157262\n",
            "\n",
            "Global step: 8836,loss: 0.009966893\n",
            "\n",
            "Global step: 8837,loss: 0.010055778\n",
            "\n",
            "Global step: 8838,loss: 0.01064819\n",
            "\n",
            "Global step: 8839,loss: 0.010238761\n",
            "\n",
            "Global step: 8840,loss: 0.009945298\n",
            "\n",
            "Global step: 8841,loss: 0.009863824\n",
            "\n",
            "Global step: 8842,loss: 0.011085021\n",
            "\n",
            "Global step: 8843,loss: 0.0096173715\n",
            "\n",
            "Global step: 8844,loss: 0.01032116\n",
            "\n",
            "Global step: 8845,loss: 0.009752972\n",
            "\n",
            "Global step: 8846,loss: 0.009862757\n",
            "\n",
            "Global step: 8847,loss: 0.010041851\n",
            "\n",
            "Global step: 8848,loss: 0.010014044\n",
            "\n",
            "Global step: 8849,loss: 0.009753923\n",
            "\n",
            "Global step: 8850,loss: 0.009411338\n",
            "\n",
            "Global step: 8851,loss: 0.009725749\n",
            "\n",
            "Global step: 8852,loss: 0.009801665\n",
            "\n",
            "Global step: 8853,loss: 0.01034438\n",
            "\n",
            "Global step: 8854,loss: 0.009659682\n",
            "\n",
            "Global step: 8855,loss: 0.010196048\n",
            "\n",
            "Global step: 8856,loss: 0.009502396\n",
            "\n",
            "Global step: 8857,loss: 0.00988716\n",
            "\n",
            "Global step: 8858,loss: 0.009956017\n",
            "\n",
            "Global step: 8859,loss: 0.010226195\n",
            "\n",
            "Global step: 8860,loss: 0.009667748\n",
            "\n",
            "Global step: 8861,loss: 0.009640696\n",
            "\n",
            "Global step: 8862,loss: 0.00974288\n",
            "\n",
            "Global step: 8863,loss: 0.0099853445\n",
            "\n",
            "Global step: 8864,loss: 0.009762545\n",
            "\n",
            "Global step: 8865,loss: 0.009963354\n",
            "\n",
            "Global step: 8866,loss: 0.010484499\n",
            "\n",
            "Global step: 8867,loss: 0.009379265\n",
            "\n",
            "Global step: 8868,loss: 0.010645655\n",
            "\n",
            "Global step: 8869,loss: 0.009410234\n",
            "\n",
            "Global step: 8870,loss: 0.009889852\n",
            "\n",
            "Global step: 8871,loss: 0.009450019\n",
            "\n",
            "Global step: 8872,loss: 0.010253985\n",
            "\n",
            "Global step: 8873,loss: 0.009842957\n",
            "\n",
            "Global step: 8874,loss: 0.009839502\n",
            "\n",
            "Global step: 8875,loss: 0.01026573\n",
            "\n",
            "Global step: 8876,loss: 0.009672273\n",
            "\n",
            "Global step: 8877,loss: 0.010572201\n",
            "\n",
            "Global step: 8878,loss: 0.009739499\n",
            "\n",
            "Global step: 8879,loss: 0.009745032\n",
            "\n",
            "Global step: 8880,loss: 0.010377722\n",
            "\n",
            "Global step: 8881,loss: 0.010072962\n",
            "\n",
            "Global step: 8882,loss: 0.009517321\n",
            "\n",
            "Global step: 8883,loss: 0.009340068\n",
            "\n",
            "Global step: 8884,loss: 0.009465841\n",
            "\n",
            "Global step: 8885,loss: 0.010351322\n",
            "\n",
            "Global step: 8886,loss: 0.01026479\n",
            "\n",
            "Global step: 8887,loss: 0.010195505\n",
            "\n",
            "Global step: 8888,loss: 0.0101856515\n",
            "\n",
            "Global step: 8889,loss: 0.010093042\n",
            "\n",
            "Global step: 8890,loss: 0.0099974675\n",
            "\n",
            "Global step: 8891,loss: 0.0093023265\n",
            "\n",
            "Global step: 8892,loss: 0.010379482\n",
            "\n",
            "Global step: 8893,loss: 0.010265566\n",
            "\n",
            "Global step: 8894,loss: 0.010159425\n",
            "\n",
            "Global step: 8895,loss: 0.01061194\n",
            "\n",
            "Global step: 8896,loss: 0.010130302\n",
            "\n",
            "Global step: 8897,loss: 0.009902701\n",
            "\n",
            "Global step: 8898,loss: 0.010595707\n",
            "\n",
            "Global step: 8899,loss: 0.010063828\n",
            "\n",
            "Global step: 8900,loss: 0.010061811\n",
            "\n",
            "Global step: 8901,loss: 0.010198234\n",
            "\n",
            "Global step: 8902,loss: 0.009792673\n",
            "\n",
            "Global step: 8903,loss: 0.009392981\n",
            "\n",
            "Global step: 8904,loss: 0.010451743\n",
            "\n",
            "Global step: 8905,loss: 0.00981112\n",
            "\n",
            "Global step: 8906,loss: 0.009638873\n",
            "\n",
            "Global step: 8907,loss: 0.010336309\n",
            "\n",
            "Global step: 8908,loss: 0.009579812\n",
            "\n",
            "Global step: 8909,loss: 0.010251273\n",
            "\n",
            "Global step: 8910,loss: 0.009709569\n",
            "\n",
            "Global step: 8911,loss: 0.00956435\n",
            "\n",
            "Global step: 8912,loss: 0.009931755\n",
            "\n",
            "Global step: 8913,loss: 0.010233883\n",
            "\n",
            "Global step: 8914,loss: 0.009764424\n",
            "\n",
            "Global step: 8915,loss: 0.009672082\n",
            "\n",
            "Global step: 8916,loss: 0.010424605\n",
            "\n",
            "Global step: 8917,loss: 0.009984758\n",
            "\n",
            "Global step: 8918,loss: 0.010109184\n",
            "\n",
            "Global step: 8919,loss: 0.009677042\n",
            "\n",
            "Global step: 8920,loss: 0.010074214\n",
            "\n",
            "Global step: 8921,loss: 0.009678776\n",
            "\n",
            "Global step: 8922,loss: 0.00944116\n",
            "\n",
            "Global step: 8923,loss: 0.009615709\n",
            "\n",
            "Global step: 8924,loss: 0.01035485\n",
            "\n",
            "Global step: 8925,loss: 0.010593405\n",
            "\n",
            "Global step: 8926,loss: 0.009993163\n",
            "\n",
            "Global step: 8927,loss: 0.009671219\n",
            "\n",
            "Global step: 8928,loss: 0.0099054035\n",
            "\n",
            "Global step: 8929,loss: 0.010150277\n",
            "\n",
            "Global step: 8930,loss: 0.009738847\n",
            "\n",
            "Global step: 8931,loss: 0.00983257\n",
            "\n",
            "Global step: 8932,loss: 0.009753461\n",
            "\n",
            "Global step: 8933,loss: 0.010298602\n",
            "\n",
            "Global step: 8934,loss: 0.01045748\n",
            "\n",
            "Global step: 8935,loss: 0.009796774\n",
            "\n",
            "Global step: 8936,loss: 0.010140537\n",
            "\n",
            "Global step: 8937,loss: 0.0106225535\n",
            "\n",
            "Global step: 8938,loss: 0.010631848\n",
            "\n",
            "Global step: 8939,loss: 0.009583814\n",
            "\n",
            "Global step: 8940,loss: 0.009761388\n",
            "\n",
            "Global step: 8941,loss: 0.010188542\n",
            "\n",
            "Global step: 8942,loss: 0.010011239\n",
            "\n",
            "Global step: 8943,loss: 0.010656935\n",
            "\n",
            "Global step: 8944,loss: 0.009391524\n",
            "\n",
            "Global step: 8945,loss: 0.010597115\n",
            "\n",
            "Global step: 8946,loss: 0.0101465285\n",
            "\n",
            "Global step: 8947,loss: 0.009853329\n",
            "\n",
            "Global step: 8948,loss: 0.010133735\n",
            "\n",
            "Global step: 8949,loss: 0.010331291\n",
            "\n",
            "Global step: 8950,loss: 0.010098637\n",
            "\n",
            "Global step: 8951,loss: 0.0098770745\n",
            "\n",
            "Global step: 8952,loss: 0.009968593\n",
            "\n",
            "Global step: 8953,loss: 0.009821532\n",
            "\n",
            "Global step: 8954,loss: 0.010456031\n",
            "\n",
            "Global step: 8955,loss: 0.009899329\n",
            "\n",
            "Global step: 8956,loss: 0.009602275\n",
            "\n",
            "Global step: 8957,loss: 0.009693628\n",
            "\n",
            "Global step: 8958,loss: 0.0097724125\n",
            "\n",
            "Global step: 8959,loss: 0.010198522\n",
            "\n",
            "Global step: 8960,loss: 0.009480286\n",
            "\n",
            "Global step: 8961,loss: 0.010491839\n",
            "\n",
            "Global step: 8962,loss: 0.010288861\n",
            "\n",
            "Global step: 8963,loss: 0.010330624\n",
            "\n",
            "Global step: 8964,loss: 0.0104667675\n",
            "\n",
            "Global step: 8965,loss: 0.009481512\n",
            "\n",
            "Global step: 8966,loss: 0.009487448\n",
            "\n",
            "Global step: 8967,loss: 0.010247711\n",
            "\n",
            "Global step: 8968,loss: 0.009905564\n",
            "\n",
            "Global step: 8969,loss: 0.010204127\n",
            "\n",
            "Global step: 8970,loss: 0.009845648\n",
            "\n",
            "Global step: 8971,loss: 0.009838546\n",
            "\n",
            "Global step: 8972,loss: 0.009938846\n",
            "\n",
            "Global step: 8973,loss: 0.009944504\n",
            "\n",
            "Global step: 8974,loss: 0.010287739\n",
            "\n",
            "Global step: 8975,loss: 0.010134509\n",
            "\n",
            "Global step: 8976,loss: 0.009715158\n",
            "\n",
            "Global step: 8977,loss: 0.009457247\n",
            "\n",
            "Global step: 8978,loss: 0.010350303\n",
            "\n",
            "Global step: 8979,loss: 0.0096625155\n",
            "\n",
            "Global step: 8980,loss: 0.009855579\n",
            "\n",
            "Global step: 8981,loss: 0.009506316\n",
            "\n",
            "Global step: 8982,loss: 0.0097889\n",
            "\n",
            "Global step: 8983,loss: 0.009696462\n",
            "\n",
            "Global step: 8984,loss: 0.01032919\n",
            "\n",
            "Global step: 8985,loss: 0.009411932\n",
            "\n",
            "Global step: 8986,loss: 0.010592934\n",
            "\n",
            "Global step: 8987,loss: 0.01005529\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 8987,Val_Loss: 0.014617242458227434,  Val_acc: 0.9981496710526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:04:15.068994 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 43/50:\n",
            "Global step: 8988,loss: 0.010276165\n",
            "\n",
            "Global step: 8989,loss: 0.010255719\n",
            "\n",
            "Global step: 8990,loss: 0.009938479\n",
            "\n",
            "Global step: 8991,loss: 0.009654701\n",
            "\n",
            "Global step: 8992,loss: 0.009733766\n",
            "\n",
            "Global step: 8993,loss: 0.0097080385\n",
            "\n",
            "Global step: 8994,loss: 0.010250539\n",
            "\n",
            "Global step: 8995,loss: 0.009840179\n",
            "\n",
            "Global step: 8996,loss: 0.010778655\n",
            "\n",
            "Global step: 8997,loss: 0.009919375\n",
            "\n",
            "Global step: 8998,loss: 0.009644683\n",
            "\n",
            "Global step: 8999,loss: 0.009992561\n",
            "\n",
            "Global step: 9000,loss: 0.010103922\n",
            "\n",
            "Global step: 9001,loss: 0.009752642\n",
            "\n",
            "Global step: 9002,loss: 0.009548851\n",
            "\n",
            "Global step: 9003,loss: 0.0099812765\n",
            "\n",
            "Global step: 9004,loss: 0.010243225\n",
            "\n",
            "Global step: 9005,loss: 0.009969957\n",
            "\n",
            "Global step: 9006,loss: 0.010384492\n",
            "\n",
            "Global step: 9007,loss: 0.010360718\n",
            "\n",
            "Global step: 9008,loss: 0.010709495\n",
            "\n",
            "Global step: 9009,loss: 0.0098803975\n",
            "\n",
            "Global step: 9010,loss: 0.010378565\n",
            "\n",
            "Global step: 9011,loss: 0.010529122\n",
            "\n",
            "Global step: 9012,loss: 0.009795183\n",
            "\n",
            "Global step: 9013,loss: 0.01010867\n",
            "\n",
            "Global step: 9014,loss: 0.009704204\n",
            "\n",
            "Global step: 9015,loss: 0.009844907\n",
            "\n",
            "Global step: 9016,loss: 0.009667058\n",
            "\n",
            "Global step: 9017,loss: 0.009880925\n",
            "\n",
            "Global step: 9018,loss: 0.009609114\n",
            "\n",
            "Global step: 9019,loss: 0.009640871\n",
            "\n",
            "Global step: 9020,loss: 0.010098519\n",
            "\n",
            "Global step: 9021,loss: 0.00983711\n",
            "\n",
            "Global step: 9022,loss: 0.010093662\n",
            "\n",
            "Global step: 9023,loss: 0.009763984\n",
            "\n",
            "Global step: 9024,loss: 0.009348119\n",
            "\n",
            "Global step: 9025,loss: 0.010290786\n",
            "\n",
            "Global step: 9026,loss: 0.00972817\n",
            "\n",
            "Global step: 9027,loss: 0.009804559\n",
            "\n",
            "Global step: 9028,loss: 0.010030539\n",
            "\n",
            "Global step: 9029,loss: 0.010019059\n",
            "\n",
            "Global step: 9030,loss: 0.00978945\n",
            "\n",
            "Global step: 9031,loss: 0.010135837\n",
            "\n",
            "Global step: 9032,loss: 0.009454727\n",
            "\n",
            "Global step: 9033,loss: 0.009997461\n",
            "\n",
            "Global step: 9034,loss: 0.009758933\n",
            "\n",
            "Global step: 9035,loss: 0.009702763\n",
            "\n",
            "Global step: 9036,loss: 0.009466914\n",
            "\n",
            "Global step: 9037,loss: 0.010948534\n",
            "\n",
            "Global step: 9038,loss: 0.00989397\n",
            "\n",
            "Global step: 9039,loss: 0.009706302\n",
            "\n",
            "Global step: 9040,loss: 0.009898313\n",
            "\n",
            "Global step: 9041,loss: 0.009175585\n",
            "\n",
            "Global step: 9042,loss: 0.009568435\n",
            "\n",
            "Global step: 9043,loss: 0.009537134\n",
            "\n",
            "Global step: 9044,loss: 0.010099887\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 9045.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:04:29.198426 140167885084416 supervisor.py:1050] Recording summary at step 9045.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 9045,loss: 0.010179182\n",
            "\n",
            "Global step: 9046,loss: 0.00924596\n",
            "\n",
            "Global step: 9047,loss: 0.010026141\n",
            "\n",
            "Global step: 9048,loss: 0.009743469\n",
            "\n",
            "Global step: 9049,loss: 0.0096349595\n",
            "\n",
            "Global step: 9050,loss: 0.010414456\n",
            "\n",
            "Global step: 9051,loss: 0.00973044\n",
            "\n",
            "Global step: 9052,loss: 0.00982571\n",
            "\n",
            "Global step: 9053,loss: 0.009612541\n",
            "\n",
            "Global step: 9054,loss: 0.009959671\n",
            "\n",
            "Global step: 9055,loss: 0.010037126\n",
            "\n",
            "Global step: 9056,loss: 0.009382803\n",
            "\n",
            "Global step: 9057,loss: 0.0096895555\n",
            "\n",
            "Global step: 9058,loss: 0.009683761\n",
            "\n",
            "Global step: 9059,loss: 0.009562699\n",
            "\n",
            "Global step: 9060,loss: 0.009498475\n",
            "\n",
            "Global step: 9061,loss: 0.010227658\n",
            "\n",
            "Global step: 9062,loss: 0.009753161\n",
            "\n",
            "Global step: 9063,loss: 0.009966399\n",
            "\n",
            "Global step: 9064,loss: 0.009828173\n",
            "\n",
            "Global step: 9065,loss: 0.009813534\n",
            "\n",
            "Global step: 9066,loss: 0.009802608\n",
            "\n",
            "Global step: 9067,loss: 0.010211749\n",
            "\n",
            "Global step: 9068,loss: 0.0100251315\n",
            "\n",
            "Global step: 9069,loss: 0.009901874\n",
            "\n",
            "Global step: 9070,loss: 0.010107658\n",
            "\n",
            "Global step: 9071,loss: 0.00989453\n",
            "\n",
            "Global step: 9072,loss: 0.010094965\n",
            "\n",
            "Global step: 9073,loss: 0.009344659\n",
            "\n",
            "Global step: 9074,loss: 0.009745944\n",
            "\n",
            "Global step: 9075,loss: 0.009641721\n",
            "\n",
            "Global step: 9076,loss: 0.0095204525\n",
            "\n",
            "Global step: 9077,loss: 0.009570261\n",
            "\n",
            "Global step: 9078,loss: 0.009532445\n",
            "\n",
            "Global step: 9079,loss: 0.010035921\n",
            "\n",
            "Global step: 9080,loss: 0.009166376\n",
            "\n",
            "Global step: 9081,loss: 0.009690638\n",
            "\n",
            "Global step: 9082,loss: 0.010184072\n",
            "\n",
            "Global step: 9083,loss: 0.009723855\n",
            "\n",
            "Global step: 9084,loss: 0.010054023\n",
            "\n",
            "Global step: 9085,loss: 0.009711138\n",
            "\n",
            "Global step: 9086,loss: 0.010259252\n",
            "\n",
            "Global step: 9087,loss: 0.010709606\n",
            "\n",
            "Global step: 9088,loss: 0.009908265\n",
            "\n",
            "Global step: 9089,loss: 0.009960608\n",
            "\n",
            "Global step: 9090,loss: 0.009413034\n",
            "\n",
            "Global step: 9091,loss: 0.010319958\n",
            "\n",
            "Global step: 9092,loss: 0.009768795\n",
            "\n",
            "Global step: 9093,loss: 0.009548888\n",
            "\n",
            "Global step: 9094,loss: 0.009542174\n",
            "\n",
            "Global step: 9095,loss: 0.010546611\n",
            "\n",
            "Global step: 9096,loss: 0.010003235\n",
            "\n",
            "Global step: 9097,loss: 0.009832492\n",
            "\n",
            "Global step: 9098,loss: 0.010446644\n",
            "\n",
            "Global step: 9099,loss: 0.010625361\n",
            "\n",
            "Global step: 9100,loss: 0.009316266\n",
            "\n",
            "Global step: 9101,loss: 0.010276075\n",
            "\n",
            "Global step: 9102,loss: 0.010060722\n",
            "\n",
            "Global step: 9103,loss: 0.009790659\n",
            "\n",
            "Global step: 9104,loss: 0.009163503\n",
            "\n",
            "Global step: 9105,loss: 0.009991488\n",
            "\n",
            "Global step: 9106,loss: 0.010426358\n",
            "\n",
            "Global step: 9107,loss: 0.009969801\n",
            "\n",
            "Global step: 9108,loss: 0.009581869\n",
            "\n",
            "Global step: 9109,loss: 0.009709407\n",
            "\n",
            "Global step: 9110,loss: 0.010146271\n",
            "\n",
            "Global step: 9111,loss: 0.010147074\n",
            "\n",
            "Global step: 9112,loss: 0.010014942\n",
            "\n",
            "Global step: 9113,loss: 0.010327827\n",
            "\n",
            "Global step: 9114,loss: 0.009617579\n",
            "\n",
            "Global step: 9115,loss: 0.009503068\n",
            "\n",
            "Global step: 9116,loss: 0.009470345\n",
            "\n",
            "Global step: 9117,loss: 0.009482588\n",
            "\n",
            "Global step: 9118,loss: 0.010445396\n",
            "\n",
            "Global step: 9119,loss: 0.010265293\n",
            "\n",
            "Global step: 9120,loss: 0.010249617\n",
            "\n",
            "Global step: 9121,loss: 0.010190568\n",
            "\n",
            "Global step: 9122,loss: 0.009511879\n",
            "\n",
            "Global step: 9123,loss: 0.009477581\n",
            "\n",
            "Global step: 9124,loss: 0.010492318\n",
            "\n",
            "Global step: 9125,loss: 0.010223087\n",
            "\n",
            "Global step: 9126,loss: 0.009737471\n",
            "\n",
            "Global step: 9127,loss: 0.009897218\n",
            "\n",
            "Global step: 9128,loss: 0.0096911695\n",
            "\n",
            "Global step: 9129,loss: 0.009709954\n",
            "\n",
            "Global step: 9130,loss: 0.009699888\n",
            "\n",
            "Global step: 9131,loss: 0.010015158\n",
            "\n",
            "Global step: 9132,loss: 0.010152351\n",
            "\n",
            "Global step: 9133,loss: 0.0095670475\n",
            "\n",
            "Global step: 9134,loss: 0.009664114\n",
            "\n",
            "Global step: 9135,loss: 0.010235035\n",
            "\n",
            "Global step: 9136,loss: 0.009452203\n",
            "\n",
            "Global step: 9137,loss: 0.010269573\n",
            "\n",
            "Global step: 9138,loss: 0.010195786\n",
            "\n",
            "Global step: 9139,loss: 0.009833997\n",
            "\n",
            "Global step: 9140,loss: 0.010167628\n",
            "\n",
            "Global step: 9141,loss: 0.009571135\n",
            "\n",
            "Global step: 9142,loss: 0.0099191135\n",
            "\n",
            "Global step: 9143,loss: 0.009840494\n",
            "\n",
            "Global step: 9144,loss: 0.010089075\n",
            "\n",
            "Global step: 9145,loss: 0.00967892\n",
            "\n",
            "Global step: 9146,loss: 0.010404866\n",
            "\n",
            "Global step: 9147,loss: 0.009591786\n",
            "\n",
            "Global step: 9148,loss: 0.009883966\n",
            "\n",
            "Global step: 9149,loss: 0.010358408\n",
            "\n",
            "Global step: 9150,loss: 0.010082449\n",
            "\n",
            "Global step: 9151,loss: 0.009891757\n",
            "\n",
            "Global step: 9152,loss: 0.00970551\n",
            "\n",
            "Global step: 9153,loss: 0.009764745\n",
            "\n",
            "Global step: 9154,loss: 0.009421796\n",
            "\n",
            "Global step: 9155,loss: 0.010079504\n",
            "\n",
            "Global step: 9156,loss: 0.010415845\n",
            "\n",
            "Global step: 9157,loss: 0.009706052\n",
            "\n",
            "Global step: 9158,loss: 0.010370021\n",
            "\n",
            "Global step: 9159,loss: 0.0096851215\n",
            "\n",
            "Global step: 9160,loss: 0.0100955665\n",
            "\n",
            "Global step: 9161,loss: 0.01005613\n",
            "\n",
            "Global step: 9162,loss: 0.009763457\n",
            "\n",
            "Global step: 9163,loss: 0.009300017\n",
            "\n",
            "Global step: 9164,loss: 0.010266458\n",
            "\n",
            "Global step: 9165,loss: 0.009554987\n",
            "\n",
            "Global step: 9166,loss: 0.009991587\n",
            "\n",
            "Global step: 9167,loss: 0.010076946\n",
            "\n",
            "Global step: 9168,loss: 0.009891342\n",
            "\n",
            "Global step: 9169,loss: 0.009407319\n",
            "\n",
            "Global step: 9170,loss: 0.009786217\n",
            "\n",
            "Global step: 9171,loss: 0.010228219\n",
            "\n",
            "Global step: 9172,loss: 0.010168355\n",
            "\n",
            "Global step: 9173,loss: 0.010075621\n",
            "\n",
            "Global step: 9174,loss: 0.009138231\n",
            "\n",
            "Global step: 9175,loss: 0.009911169\n",
            "\n",
            "Global step: 9176,loss: 0.010052645\n",
            "\n",
            "Global step: 9177,loss: 0.009860326\n",
            "\n",
            "Global step: 9178,loss: 0.009464505\n",
            "\n",
            "Global step: 9179,loss: 0.01031137\n",
            "\n",
            "Global step: 9180,loss: 0.009749257\n",
            "\n",
            "Global step: 9181,loss: 0.009639853\n",
            "\n",
            "Global step: 9182,loss: 0.009553331\n",
            "\n",
            "Global step: 9183,loss: 0.010277874\n",
            "\n",
            "Global step: 9184,loss: 0.010089805\n",
            "\n",
            "Global step: 9185,loss: 0.010157857\n",
            "\n",
            "Global step: 9186,loss: 0.010040044\n",
            "\n",
            "Global step: 9187,loss: 0.0098751085\n",
            "\n",
            "Global step: 9188,loss: 0.009853832\n",
            "\n",
            "Global step: 9189,loss: 0.009814547\n",
            "\n",
            "Global step: 9190,loss: 0.009972208\n",
            "\n",
            "Global step: 9191,loss: 0.010263396\n",
            "\n",
            "Global step: 9192,loss: 0.009464953\n",
            "\n",
            "Global step: 9193,loss: 0.01016049\n",
            "\n",
            "Global step: 9194,loss: 0.009686211\n",
            "\n",
            "Global step: 9195,loss: 0.010469617\n",
            "\n",
            "Global step: 9196,loss: 0.010138093\n",
            "\n",
            "Global step: 9197,loss: 0.0096018845\n",
            "\n",
            "Global step: 9198,loss: 0.010692869\n",
            "\n",
            "Global step: 9199,loss: 0.009658653\n",
            "\n",
            "Global step: 9200,loss: 0.009685961\n",
            "\n",
            "Global step: 9201,loss: 0.009911465\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 9201,val_loss: 0.014655246546393946\n",
            "\n",
            "Training for epoch 44/50:\n",
            "Global step: 9202,loss: 0.010184039\n",
            "\n",
            "Global step: 9203,loss: 0.009454489\n",
            "\n",
            "Global step: 9204,loss: 0.0098778745\n",
            "\n",
            "Global step: 9205,loss: 0.009873126\n",
            "\n",
            "Global step: 9206,loss: 0.0104550775\n",
            "\n",
            "Global step: 9207,loss: 0.009430719\n",
            "\n",
            "Global step: 9208,loss: 0.0098462\n",
            "\n",
            "Global step: 9209,loss: 0.01012344\n",
            "\n",
            "Global step: 9210,loss: 0.009384016\n",
            "\n",
            "Global step: 9211,loss: 0.009701387\n",
            "\n",
            "Global step: 9212,loss: 0.010245185\n",
            "\n",
            "Global step: 9213,loss: 0.009737276\n",
            "\n",
            "Global step: 9214,loss: 0.009813996\n",
            "\n",
            "Global step: 9215,loss: 0.010065238\n",
            "\n",
            "Global step: 9216,loss: 0.009854141\n",
            "\n",
            "Global step: 9217,loss: 0.010162226\n",
            "\n",
            "Global step: 9218,loss: 0.009790839\n",
            "\n",
            "Global step: 9219,loss: 0.010012481\n",
            "\n",
            "Global step: 9220,loss: 0.009892112\n",
            "\n",
            "Global step: 9221,loss: 0.009795135\n",
            "\n",
            "Global step: 9222,loss: 0.0092934305\n",
            "\n",
            "Global step: 9223,loss: 0.009514233\n",
            "\n",
            "Global step: 9224,loss: 0.009852485\n",
            "\n",
            "Global step: 9225,loss: 0.00919903\n",
            "\n",
            "Global step: 9226,loss: 0.00896505\n",
            "\n",
            "Global step: 9227,loss: 0.009481651\n",
            "\n",
            "Global step: 9228,loss: 0.011054381\n",
            "\n",
            "Global step: 9229,loss: 0.0090295365\n",
            "\n",
            "Global step: 9230,loss: 0.010165563\n",
            "\n",
            "Global step: 9231,loss: 0.009964401\n",
            "\n",
            "Global step: 9232,loss: 0.010032046\n",
            "\n",
            "Global step: 9233,loss: 0.010169395\n",
            "\n",
            "Global step: 9234,loss: 0.009519634\n",
            "\n",
            "Global step: 9235,loss: 0.009591188\n",
            "\n",
            "Global step: 9236,loss: 0.009833053\n",
            "\n",
            "Global step: 9237,loss: 0.009622289\n",
            "\n",
            "Global step: 9238,loss: 0.009710894\n",
            "\n",
            "Global step: 9239,loss: 0.009238578\n",
            "\n",
            "Global step: 9240,loss: 0.009591233\n",
            "\n",
            "Global step: 9241,loss: 0.010013434\n",
            "\n",
            "Global step: 9242,loss: 0.009507651\n",
            "\n",
            "Global step: 9243,loss: 0.0103654275\n",
            "\n",
            "Global step: 9244,loss: 0.009816329\n",
            "\n",
            "Global step: 9245,loss: 0.009677038\n",
            "\n",
            "Global step: 9246,loss: 0.009787389\n",
            "\n",
            "Global step: 9247,loss: 0.009406928\n",
            "\n",
            "Global step: 9248,loss: 0.009406605\n",
            "\n",
            "Global step: 9249,loss: 0.009953385\n",
            "\n",
            "Global step: 9250,loss: 0.0093679065\n",
            "\n",
            "Global step: 9251,loss: 0.009927024\n",
            "\n",
            "Global step: 9252,loss: 0.009478294\n",
            "\n",
            "Global step: 9253,loss: 0.00978\n",
            "\n",
            "Global step: 9254,loss: 0.010332551\n",
            "\n",
            "Global step: 9255,loss: 0.009614595\n",
            "\n",
            "Global step: 9256,loss: 0.00952749\n",
            "\n",
            "Global step: 9257,loss: 0.010297169\n",
            "\n",
            "Global step: 9258,loss: 0.009951864\n",
            "\n",
            "Global step: 9259,loss: 0.009254726\n",
            "\n",
            "Global step: 9260,loss: 0.0093545085\n",
            "\n",
            "Global step: 9261,loss: 0.0093627805\n",
            "\n",
            "Global step: 9262,loss: 0.009541756\n",
            "\n",
            "Global step: 9263,loss: 0.010044234\n",
            "\n",
            "Global step: 9264,loss: 0.009245122\n",
            "\n",
            "Global step: 9265,loss: 0.010023495\n",
            "\n",
            "Global step: 9266,loss: 0.00976661\n",
            "\n",
            "Global step: 9267,loss: 0.009182749\n",
            "\n",
            "Global step: 9268,loss: 0.009716169\n",
            "\n",
            "Global step: 9269,loss: 0.009528296\n",
            "\n",
            "Global step: 9270,loss: 0.010004209\n",
            "\n",
            "Global step: 9271,loss: 0.009418235\n",
            "\n",
            "Global step: 9272,loss: 0.009618192\n",
            "\n",
            "Global step: 9273,loss: 0.009458925\n",
            "\n",
            "Global step: 9274,loss: 0.009636686\n",
            "\n",
            "Global step: 9275,loss: 0.010016555\n",
            "\n",
            "Global step: 9276,loss: 0.0096543\n",
            "\n",
            "Global step: 9277,loss: 0.009614873\n",
            "\n",
            "Global step: 9278,loss: 0.009953629\n",
            "\n",
            "Global step: 9279,loss: 0.009528696\n",
            "\n",
            "Global step: 9280,loss: 0.009143955\n",
            "\n",
            "Global step: 9281,loss: 0.009562861\n",
            "\n",
            "Global step: 9282,loss: 0.00989701\n",
            "\n",
            "Global step: 9283,loss: 0.009524296\n",
            "\n",
            "Global step: 9284,loss: 0.009858471\n",
            "\n",
            "Global step: 9285,loss: 0.0095888665\n",
            "\n",
            "Global step: 9286,loss: 0.009616753\n",
            "\n",
            "Global step: 9287,loss: 0.00957411\n",
            "\n",
            "Global step: 9288,loss: 0.009177022\n",
            "\n",
            "Global step: 9289,loss: 0.009926945\n",
            "\n",
            "Global step: 9290,loss: 0.009940754\n",
            "\n",
            "Global step: 9291,loss: 0.009251921\n",
            "\n",
            "Global step: 9292,loss: 0.010090061\n",
            "\n",
            "Global step: 9293,loss: 0.010215462\n",
            "\n",
            "Global step: 9294,loss: 0.0097862575\n",
            "\n",
            "Global step: 9295,loss: 0.009863937\n",
            "\n",
            "Global step: 9296,loss: 0.009886031\n",
            "\n",
            "Global step: 9297,loss: 0.009943107\n",
            "\n",
            "Global step: 9298,loss: 0.009583163\n",
            "\n",
            "Global step: 9299,loss: 0.009441246\n",
            "\n",
            "Global step: 9300,loss: 0.009494408\n",
            "\n",
            "Global step: 9301,loss: 0.009947953\n",
            "\n",
            "Global step: 9302,loss: 0.009771612\n",
            "\n",
            "Global step: 9303,loss: 0.009487071\n",
            "\n",
            "Global step: 9304,loss: 0.009858406\n",
            "\n",
            "Global step: 9305,loss: 0.008703894\n",
            "\n",
            "Global step: 9306,loss: 0.00958536\n",
            "\n",
            "Global step: 9307,loss: 0.009925541\n",
            "\n",
            "Global step: 9308,loss: 0.009635619\n",
            "\n",
            "Global step: 9309,loss: 0.009882335\n",
            "\n",
            "Global step: 9310,loss: 0.00944747\n",
            "\n",
            "Global step: 9311,loss: 0.010588862\n",
            "\n",
            "Global step: 9312,loss: 0.009693268\n",
            "\n",
            "Global step: 9313,loss: 0.009761027\n",
            "\n",
            "Global step: 9314,loss: 0.010206826\n",
            "\n",
            "Global step: 9315,loss: 0.00980987\n",
            "\n",
            "Global step: 9316,loss: 0.009058806\n",
            "\n",
            "Global step: 9317,loss: 0.010253834\n",
            "\n",
            "Global step: 9318,loss: 0.010053948\n",
            "\n",
            "Global step: 9319,loss: 0.010435213\n",
            "\n",
            "Global step: 9320,loss: 0.009578035\n",
            "\n",
            "Global step: 9321,loss: 0.0101404125\n",
            "\n",
            "Global step: 9322,loss: 0.009455886\n",
            "\n",
            "Global step: 9323,loss: 0.009776778\n",
            "\n",
            "Global step: 9324,loss: 0.009502488\n",
            "\n",
            "Global step: 9325,loss: 0.009500098\n",
            "\n",
            "Global step: 9326,loss: 0.01026644\n",
            "\n",
            "Global step: 9327,loss: 0.009630237\n",
            "\n",
            "Global step: 9328,loss: 0.009829423\n",
            "\n",
            "Global step: 9329,loss: 0.009839046\n",
            "\n",
            "Global step: 9330,loss: 0.01022818\n",
            "\n",
            "Global step: 9331,loss: 0.009504519\n",
            "\n",
            "Global step: 9332,loss: 0.0101992\n",
            "\n",
            "Global step: 9333,loss: 0.009981758\n",
            "\n",
            "Global step: 9334,loss: 0.009947826\n",
            "\n",
            "Global step: 9335,loss: 0.010006132\n",
            "\n",
            "Global step: 9336,loss: 0.009758269\n",
            "\n",
            "Global step: 9337,loss: 0.009576469\n",
            "\n",
            "Global step: 9338,loss: 0.0095487805\n",
            "\n",
            "Global step: 9339,loss: 0.009110756\n",
            "\n",
            "Global step: 9340,loss: 0.010297376\n",
            "\n",
            "Global step: 9341,loss: 0.009813125\n",
            "\n",
            "Global step: 9342,loss: 0.0099563515\n",
            "\n",
            "Global step: 9343,loss: 0.009333813\n",
            "\n",
            "Global step: 9344,loss: 0.009340329\n",
            "\n",
            "Global step: 9345,loss: 0.009589966\n",
            "\n",
            "Global step: 9346,loss: 0.009955308\n",
            "\n",
            "Global step: 9347,loss: 0.00938697\n",
            "\n",
            "Global step: 9348,loss: 0.010051873\n",
            "\n",
            "Global step: 9349,loss: 0.009916125\n",
            "\n",
            "Global step: 9350,loss: 0.009355556\n",
            "\n",
            "Global step: 9351,loss: 0.009871811\n",
            "\n",
            "Global step: 9352,loss: 0.009438669\n",
            "\n",
            "Global step: 9353,loss: 0.009327139\n",
            "\n",
            "Global step: 9354,loss: 0.009362383\n",
            "\n",
            "Global step: 9355,loss: 0.010055184\n",
            "\n",
            "Global step: 9356,loss: 0.009887449\n",
            "\n",
            "Global step: 9357,loss: 0.010211681\n",
            "\n",
            "Global step: 9358,loss: 0.009636236\n",
            "\n",
            "Global step: 9359,loss: 0.009562267\n",
            "\n",
            "Global step: 9360,loss: 0.010490796\n",
            "\n",
            "Global step: 9361,loss: 0.009439147\n",
            "\n",
            "Global step: 9362,loss: 0.009976539\n",
            "\n",
            "Global step: 9363,loss: 0.009880404\n",
            "\n",
            "Global step: 9364,loss: 0.00980104\n",
            "\n",
            "Global step: 9365,loss: 0.010133421\n",
            "\n",
            "Global step: 9366,loss: 0.009880525\n",
            "\n",
            "Global step: 9367,loss: 0.009749995\n",
            "\n",
            "Global step: 9368,loss: 0.00942439\n",
            "\n",
            "Global step: 9369,loss: 0.00996505\n",
            "\n",
            "Global step: 9370,loss: 0.01044015\n",
            "\n",
            "Global step: 9371,loss: 0.010218975\n",
            "\n",
            "Global step: 9372,loss: 0.010129798\n",
            "\n",
            "Global step: 9373,loss: 0.010305919\n",
            "\n",
            "Global step: 9374,loss: 0.0104995025\n",
            "\n",
            "Global step: 9375,loss: 0.009961824\n",
            "\n",
            "Global step: 9376,loss: 0.009626744\n",
            "\n",
            "Global step: 9377,loss: 0.009850281\n",
            "\n",
            "Global step: 9378,loss: 0.009801251\n",
            "\n",
            "Global step: 9379,loss: 0.009647198\n",
            "\n",
            "Global step: 9380,loss: 0.009457677\n",
            "\n",
            "Global step: 9381,loss: 0.009517285\n",
            "\n",
            "Global step: 9382,loss: 0.009648745\n",
            "\n",
            "Global step: 9383,loss: 0.009473341\n",
            "\n",
            "Global step: 9384,loss: 0.009640049\n",
            "\n",
            "Global step: 9385,loss: 0.01043176\n",
            "\n",
            "Global step: 9386,loss: 0.0099105155\n",
            "\n",
            "Global step: 9387,loss: 0.009959078\n",
            "\n",
            "Global step: 9388,loss: 0.010692131\n",
            "\n",
            "Global step: 9389,loss: 0.009690041\n",
            "\n",
            "Global step: 9390,loss: 0.009861501\n",
            "\n",
            "Global step: 9391,loss: 0.009615456\n",
            "\n",
            "Global step: 9392,loss: 0.009352908\n",
            "\n",
            "Global step: 9393,loss: 0.009899106\n",
            "\n",
            "Global step: 9394,loss: 0.009996628\n",
            "\n",
            "Global step: 9395,loss: 0.01047991\n",
            "\n",
            "Global step: 9396,loss: 0.010078673\n",
            "\n",
            "Global step: 9397,loss: 0.009898858\n",
            "\n",
            "Global step: 9398,loss: 0.010025692\n",
            "\n",
            "Global step: 9399,loss: 0.009886739\n",
            "\n",
            "Global step: 9400,loss: 0.009862089\n",
            "\n",
            "Global step: 9401,loss: 0.009565557\n",
            "\n",
            "Global step: 9402,loss: 0.009793784\n",
            "\n",
            "Global step: 9403,loss: 0.0101617975\n",
            "\n",
            "Global step: 9404,loss: 0.010387907\n",
            "\n",
            "Global step: 9405,loss: 0.009827693\n",
            "\n",
            "Global step: 9406,loss: 0.009861715\n",
            "\n",
            "Global step: 9407,loss: 0.009362371\n",
            "\n",
            "Global step: 9408,loss: 0.009813224\n",
            "\n",
            "Global step: 9409,loss: 0.0103001725\n",
            "\n",
            "Global step: 9410,loss: 0.009831796\n",
            "\n",
            "Global step: 9411,loss: 0.009287337\n",
            "\n",
            "Global step: 9412,loss: 0.010088681\n",
            "\n",
            "Global step: 9413,loss: 0.0100684855\n",
            "\n",
            "Global step: 9414,loss: 0.009525355\n",
            "\n",
            "Global step: 9415,loss: 0.009726292\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 9415,Val_Loss: 0.014494316162247407,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:06:00.423473 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 45/50:\n",
            "Global step: 9416,loss: 0.009473949\n",
            "\n",
            "Global step: 9417,loss: 0.009794021\n",
            "\n",
            "Global step: 9418,loss: 0.009355667\n",
            "\n",
            "Global step: 9419,loss: 0.010179656\n",
            "\n",
            "Global step: 9420,loss: 0.009814504\n",
            "\n",
            "Global step: 9421,loss: 0.00949845\n",
            "\n",
            "Global step: 9422,loss: 0.009695088\n",
            "\n",
            "Global step: 9423,loss: 0.009591001\n",
            "\n",
            "Global step: 9424,loss: 0.00914476\n",
            "\n",
            "Global step: 9425,loss: 0.009944007\n",
            "\n",
            "Global step: 9426,loss: 0.009474577\n",
            "\n",
            "Global step: 9427,loss: 0.009348793\n",
            "\n",
            "Global step: 9428,loss: 0.0093914755\n",
            "\n",
            "Global step: 9429,loss: 0.0095678205\n",
            "\n",
            "Global step: 9430,loss: 0.009687335\n",
            "\n",
            "Global step: 9431,loss: 0.009819492\n",
            "\n",
            "Global step: 9432,loss: 0.009687257\n",
            "\n",
            "Global step: 9433,loss: 0.010159431\n",
            "\n",
            "Global step: 9434,loss: 0.00960874\n",
            "\n",
            "Global step: 9435,loss: 0.010832835\n",
            "\n",
            "Global step: 9436,loss: 0.010025129\n",
            "\n",
            "Global step: 9437,loss: 0.009777718\n",
            "\n",
            "Global step: 9438,loss: 0.009370514\n",
            "\n",
            "Global step: 9439,loss: 0.009529866\n",
            "\n",
            "Global step: 9440,loss: 0.009774989\n",
            "\n",
            "Global step: 9441,loss: 0.00996759\n",
            "\n",
            "Global step: 9442,loss: 0.00964376\n",
            "\n",
            "Global step: 9443,loss: 0.010048736\n",
            "\n",
            "Global step: 9444,loss: 0.010096589\n",
            "\n",
            "Global step: 9445,loss: 0.010170744\n",
            "\n",
            "Global step: 9446,loss: 0.009966538\n",
            "\n",
            "Global step: 9447,loss: 0.0098388195\n",
            "\n",
            "Global step: 9448,loss: 0.009717538\n",
            "\n",
            "Global step: 9449,loss: 0.010032721\n",
            "\n",
            "Global step: 9450,loss: 0.009621194\n",
            "\n",
            "Global step: 9451,loss: 0.009259057\n",
            "\n",
            "Global step: 9452,loss: 0.009833245\n",
            "\n",
            "Global step: 9453,loss: 0.009979689\n",
            "\n",
            "Global step: 9454,loss: 0.0092867855\n",
            "\n",
            "Global step: 9455,loss: 0.009468701\n",
            "\n",
            "Global step: 9456,loss: 0.009610128\n",
            "\n",
            "Global step: 9457,loss: 0.009264459\n",
            "\n",
            "Global step: 9458,loss: 0.009558628\n",
            "\n",
            "Global step: 9459,loss: 0.010224209\n",
            "\n",
            "Global step: 9460,loss: 0.009771637\n",
            "\n",
            "Global step: 9461,loss: 0.008970978\n",
            "\n",
            "Global step: 9462,loss: 0.009524551\n",
            "\n",
            "Global step: 9463,loss: 0.00967672\n",
            "\n",
            "Global step: 9464,loss: 0.009247513\n",
            "\n",
            "Global step: 9465,loss: 0.010089941\n",
            "\n",
            "Global step: 9466,loss: 0.009539574\n",
            "\n",
            "Global step: 9467,loss: 0.009500155\n",
            "\n",
            "Global step: 9468,loss: 0.009317061\n",
            "\n",
            "Global step: 9469,loss: 0.009373624\n",
            "\n",
            "Global step: 9470,loss: 0.010194656\n",
            "\n",
            "Global step: 9471,loss: 0.01002305\n",
            "\n",
            "Global step: 9472,loss: 0.0092375735\n",
            "\n",
            "Global step: 9473,loss: 0.00948713\n",
            "\n",
            "Global step: 9474,loss: 0.008829742\n",
            "\n",
            "Global step: 9475,loss: 0.010423402\n",
            "\n",
            "Global step: 9476,loss: 0.009639226\n",
            "\n",
            "Global step: 9477,loss: 0.009068262\n",
            "\n",
            "Global step: 9478,loss: 0.0101437885\n",
            "\n",
            "Global step: 9479,loss: 0.010057733\n",
            "\n",
            "Global step: 9480,loss: 0.009686287\n",
            "\n",
            "Global step: 9481,loss: 0.009695523\n",
            "\n",
            "Global step: 9482,loss: 0.0099462345\n",
            "\n",
            "Global step: 9483,loss: 0.009637296\n",
            "\n",
            "Global step: 9484,loss: 0.009877814\n",
            "\n",
            "Global step: 9485,loss: 0.0103242\n",
            "\n",
            "Global step: 9486,loss: 0.010073365\n",
            "\n",
            "Global step: 9487,loss: 0.009630092\n",
            "\n",
            "Global step: 9488,loss: 0.010562272\n",
            "\n",
            "Global step: 9489,loss: 0.009120935\n",
            "\n",
            "Global step: 9490,loss: 0.00938271\n",
            "\n",
            "Global step: 9491,loss: 0.009431122\n",
            "\n",
            "Global step: 9492,loss: 0.009280736\n",
            "\n",
            "Global step: 9493,loss: 0.010083983\n",
            "\n",
            "Global step: 9494,loss: 0.009716849\n",
            "\n",
            "Global step: 9495,loss: 0.009421133\n",
            "\n",
            "Global step: 9496,loss: 0.009601448\n",
            "\n",
            "Global step: 9497,loss: 0.00975094\n",
            "\n",
            "Global step: 9498,loss: 0.009059004\n",
            "\n",
            "Global step: 9499,loss: 0.009829065\n",
            "\n",
            "Global step: 9500,loss: 0.009096712\n",
            "\n",
            "Global step: 9501,loss: 0.009257879\n",
            "\n",
            "Global step: 9502,loss: 0.009999995\n",
            "\n",
            "Global step: 9503,loss: 0.009515681\n",
            "\n",
            "Global step: 9504,loss: 0.009177023\n",
            "\n",
            "Global step: 9505,loss: 0.009528348\n",
            "\n",
            "Global step: 9506,loss: 0.009734569\n",
            "\n",
            "Global step: 9507,loss: 0.009839822\n",
            "\n",
            "Global step: 9508,loss: 0.009236012\n",
            "\n",
            "Global step: 9509,loss: 0.009686744\n",
            "\n",
            "Global step: 9510,loss: 0.010035874\n",
            "\n",
            "Global step: 9511,loss: 0.009691354\n",
            "\n",
            "Global step: 9512,loss: 0.009596003\n",
            "\n",
            "Global step: 9513,loss: 0.009750284\n",
            "\n",
            "Global step: 9514,loss: 0.009099119\n",
            "\n",
            "Global step: 9515,loss: 0.00971371\n",
            "\n",
            "Global step: 9516,loss: 0.0100338375\n",
            "\n",
            "Global step: 9517,loss: 0.009162828\n",
            "\n",
            "Global step: 9518,loss: 0.009825448\n",
            "\n",
            "Global step: 9519,loss: 0.009531447\n",
            "\n",
            "Global step: 9520,loss: 0.009981479\n",
            "\n",
            "Global step: 9521,loss: 0.009347673\n",
            "\n",
            "Global step: 9522,loss: 0.009206907\n",
            "\n",
            "Global step: 9523,loss: 0.010121137\n",
            "\n",
            "Global step: 9524,loss: 0.009260138\n",
            "\n",
            "Global step: 9525,loss: 0.009909408\n",
            "\n",
            "Global step: 9526,loss: 0.009779154\n",
            "\n",
            "Global step: 9527,loss: 0.009647902\n",
            "\n",
            "Global step: 9528,loss: 0.009316295\n",
            "\n",
            "Global step: 9529,loss: 0.010373918\n",
            "\n",
            "Global step: 9530,loss: 0.009721287\n",
            "\n",
            "Global step: 9531,loss: 0.0097070355\n",
            "\n",
            "Global step: 9532,loss: 0.00937\n",
            "\n",
            "Global step: 9533,loss: 0.009673478\n",
            "\n",
            "Global step: 9534,loss: 0.009375195\n",
            "\n",
            "Global step: 9535,loss: 0.01006556\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 9536.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:06:29.291814 140167885084416 supervisor.py:1050] Recording summary at step 9536.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 9536,loss: 0.009509911\n",
            "\n",
            "Global step: 9537,loss: 0.009054351\n",
            "\n",
            "Global step: 9538,loss: 0.009706541\n",
            "\n",
            "Global step: 9539,loss: 0.009454183\n",
            "\n",
            "Global step: 9540,loss: 0.010385135\n",
            "\n",
            "Global step: 9541,loss: 0.009501943\n",
            "\n",
            "Global step: 9542,loss: 0.010127753\n",
            "\n",
            "Global step: 9543,loss: 0.009209514\n",
            "\n",
            "Global step: 9544,loss: 0.009553127\n",
            "\n",
            "Global step: 9545,loss: 0.009655077\n",
            "\n",
            "Global step: 9546,loss: 0.009564415\n",
            "\n",
            "Global step: 9547,loss: 0.009401997\n",
            "\n",
            "Global step: 9548,loss: 0.009501292\n",
            "\n",
            "Global step: 9549,loss: 0.009973848\n",
            "\n",
            "Global step: 9550,loss: 0.010146135\n",
            "\n",
            "Global step: 9551,loss: 0.0099824825\n",
            "\n",
            "Global step: 9552,loss: 0.00955243\n",
            "\n",
            "Global step: 9553,loss: 0.009932585\n",
            "\n",
            "Global step: 9554,loss: 0.009342999\n",
            "\n",
            "Global step: 9555,loss: 0.009482713\n",
            "\n",
            "Global step: 9556,loss: 0.009298728\n",
            "\n",
            "Global step: 9557,loss: 0.009754882\n",
            "\n",
            "Global step: 9558,loss: 0.0095966095\n",
            "\n",
            "Global step: 9559,loss: 0.009515864\n",
            "\n",
            "Global step: 9560,loss: 0.009652557\n",
            "\n",
            "Global step: 9561,loss: 0.00924556\n",
            "\n",
            "Global step: 9562,loss: 0.009562406\n",
            "\n",
            "Global step: 9563,loss: 0.009388514\n",
            "\n",
            "Global step: 9564,loss: 0.009809195\n",
            "\n",
            "Global step: 9565,loss: 0.009367961\n",
            "\n",
            "Global step: 9566,loss: 0.009431785\n",
            "\n",
            "Global step: 9567,loss: 0.009715543\n",
            "\n",
            "Global step: 9568,loss: 0.010285681\n",
            "\n",
            "Global step: 9569,loss: 0.009421236\n",
            "\n",
            "Global step: 9570,loss: 0.009719281\n",
            "\n",
            "Global step: 9571,loss: 0.010137446\n",
            "\n",
            "Global step: 9572,loss: 0.0099420305\n",
            "\n",
            "Global step: 9573,loss: 0.009489436\n",
            "\n",
            "Global step: 9574,loss: 0.009678958\n",
            "\n",
            "Global step: 9575,loss: 0.009088388\n",
            "\n",
            "Global step: 9576,loss: 0.010137067\n",
            "\n",
            "Global step: 9577,loss: 0.009667171\n",
            "\n",
            "Global step: 9578,loss: 0.009634209\n",
            "\n",
            "Global step: 9579,loss: 0.009291735\n",
            "\n",
            "Global step: 9580,loss: 0.00958419\n",
            "\n",
            "Global step: 9581,loss: 0.009581046\n",
            "\n",
            "Global step: 9582,loss: 0.010057642\n",
            "\n",
            "Global step: 9583,loss: 0.009583127\n",
            "\n",
            "Global step: 9584,loss: 0.009706084\n",
            "\n",
            "Global step: 9585,loss: 0.009770794\n",
            "\n",
            "Global step: 9586,loss: 0.009137321\n",
            "\n",
            "Global step: 9587,loss: 0.009650404\n",
            "\n",
            "Global step: 9588,loss: 0.0097541\n",
            "\n",
            "Global step: 9589,loss: 0.00959498\n",
            "\n",
            "Global step: 9590,loss: 0.009854765\n",
            "\n",
            "Global step: 9591,loss: 0.009892887\n",
            "\n",
            "Global step: 9592,loss: 0.009732343\n",
            "\n",
            "Global step: 9593,loss: 0.009522177\n",
            "\n",
            "Global step: 9594,loss: 0.009068149\n",
            "\n",
            "Global step: 9595,loss: 0.009404882\n",
            "\n",
            "Global step: 9596,loss: 0.00960338\n",
            "\n",
            "Global step: 9597,loss: 0.0095728515\n",
            "\n",
            "Global step: 9598,loss: 0.009606383\n",
            "\n",
            "Global step: 9599,loss: 0.009973876\n",
            "\n",
            "Global step: 9600,loss: 0.009408872\n",
            "\n",
            "Global step: 9601,loss: 0.009442401\n",
            "\n",
            "Global step: 9602,loss: 0.010119037\n",
            "\n",
            "Global step: 9603,loss: 0.010016614\n",
            "\n",
            "Global step: 9604,loss: 0.009156943\n",
            "\n",
            "Global step: 9605,loss: 0.009744596\n",
            "\n",
            "Global step: 9606,loss: 0.009516999\n",
            "\n",
            "Global step: 9607,loss: 0.009661459\n",
            "\n",
            "Global step: 9608,loss: 0.009379256\n",
            "\n",
            "Global step: 9609,loss: 0.009713454\n",
            "\n",
            "Global step: 9610,loss: 0.009958954\n",
            "\n",
            "Global step: 9611,loss: 0.010168581\n",
            "\n",
            "Global step: 9612,loss: 0.009946312\n",
            "\n",
            "Global step: 9613,loss: 0.009747363\n",
            "\n",
            "Global step: 9614,loss: 0.009336074\n",
            "\n",
            "Global step: 9615,loss: 0.009774818\n",
            "\n",
            "Global step: 9616,loss: 0.009936756\n",
            "\n",
            "Global step: 9617,loss: 0.009478946\n",
            "\n",
            "Global step: 9618,loss: 0.009768061\n",
            "\n",
            "Global step: 9619,loss: 0.0097315265\n",
            "\n",
            "Global step: 9620,loss: 0.008982697\n",
            "\n",
            "Global step: 9621,loss: 0.009295368\n",
            "\n",
            "Global step: 9622,loss: 0.009893679\n",
            "\n",
            "Global step: 9623,loss: 0.009649885\n",
            "\n",
            "Global step: 9624,loss: 0.009421567\n",
            "\n",
            "Global step: 9625,loss: 0.009840548\n",
            "\n",
            "Global step: 9626,loss: 0.009664682\n",
            "\n",
            "Global step: 9627,loss: 0.009587882\n",
            "\n",
            "Global step: 9628,loss: 0.009906481\n",
            "\n",
            "Global step: 9629,loss: 0.009962516\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 9629,val_loss: 0.014578158722112053\n",
            "\n",
            "Training for epoch 46/50:\n",
            "Global step: 9630,loss: 0.009385985\n",
            "\n",
            "Global step: 9631,loss: 0.010712257\n",
            "\n",
            "Global step: 9632,loss: 0.009717685\n",
            "\n",
            "Global step: 9633,loss: 0.0095034\n",
            "\n",
            "Global step: 9634,loss: 0.00958025\n",
            "\n",
            "Global step: 9635,loss: 0.009643534\n",
            "\n",
            "Global step: 9636,loss: 0.009646478\n",
            "\n",
            "Global step: 9637,loss: 0.009709575\n",
            "\n",
            "Global step: 9638,loss: 0.009514531\n",
            "\n",
            "Global step: 9639,loss: 0.009168246\n",
            "\n",
            "Global step: 9640,loss: 0.009762592\n",
            "\n",
            "Global step: 9641,loss: 0.009528649\n",
            "\n",
            "Global step: 9642,loss: 0.009407564\n",
            "\n",
            "Global step: 9643,loss: 0.009543507\n",
            "\n",
            "Global step: 9644,loss: 0.009548581\n",
            "\n",
            "Global step: 9645,loss: 0.010134511\n",
            "\n",
            "Global step: 9646,loss: 0.009815792\n",
            "\n",
            "Global step: 9647,loss: 0.009626652\n",
            "\n",
            "Global step: 9648,loss: 0.009657347\n",
            "\n",
            "Global step: 9649,loss: 0.009862842\n",
            "\n",
            "Global step: 9650,loss: 0.009821882\n",
            "\n",
            "Global step: 9651,loss: 0.010159828\n",
            "\n",
            "Global step: 9652,loss: 0.0093383305\n",
            "\n",
            "Global step: 9653,loss: 0.009644834\n",
            "\n",
            "Global step: 9654,loss: 0.009055869\n",
            "\n",
            "Global step: 9655,loss: 0.009482242\n",
            "\n",
            "Global step: 9656,loss: 0.009423231\n",
            "\n",
            "Global step: 9657,loss: 0.009366608\n",
            "\n",
            "Global step: 9658,loss: 0.009510499\n",
            "\n",
            "Global step: 9659,loss: 0.009500643\n",
            "\n",
            "Global step: 9660,loss: 0.0095445765\n",
            "\n",
            "Global step: 9661,loss: 0.009552518\n",
            "\n",
            "Global step: 9662,loss: 0.01012284\n",
            "\n",
            "Global step: 9663,loss: 0.009950807\n",
            "\n",
            "Global step: 9664,loss: 0.009823268\n",
            "\n",
            "Global step: 9665,loss: 0.009959231\n",
            "\n",
            "Global step: 9666,loss: 0.009490239\n",
            "\n",
            "Global step: 9667,loss: 0.009065013\n",
            "\n",
            "Global step: 9668,loss: 0.0094010625\n",
            "\n",
            "Global step: 9669,loss: 0.009345237\n",
            "\n",
            "Global step: 9670,loss: 0.009649626\n",
            "\n",
            "Global step: 9671,loss: 0.009599226\n",
            "\n",
            "Global step: 9672,loss: 0.009955353\n",
            "\n",
            "Global step: 9673,loss: 0.009863181\n",
            "\n",
            "Global step: 9674,loss: 0.0095389765\n",
            "\n",
            "Global step: 9675,loss: 0.010223732\n",
            "\n",
            "Global step: 9676,loss: 0.009639837\n",
            "\n",
            "Global step: 9677,loss: 0.009421957\n",
            "\n",
            "Global step: 9678,loss: 0.010008871\n",
            "\n",
            "Global step: 9679,loss: 0.010141252\n",
            "\n",
            "Global step: 9680,loss: 0.009576677\n",
            "\n",
            "Global step: 9681,loss: 0.009701845\n",
            "\n",
            "Global step: 9682,loss: 0.009422501\n",
            "\n",
            "Global step: 9683,loss: 0.01033286\n",
            "\n",
            "Global step: 9684,loss: 0.009588622\n",
            "\n",
            "Global step: 9685,loss: 0.009146036\n",
            "\n",
            "Global step: 9686,loss: 0.00952834\n",
            "\n",
            "Global step: 9687,loss: 0.0093891835\n",
            "\n",
            "Global step: 9688,loss: 0.009336621\n",
            "\n",
            "Global step: 9689,loss: 0.009479695\n",
            "\n",
            "Global step: 9690,loss: 0.009422314\n",
            "\n",
            "Global step: 9691,loss: 0.0092535205\n",
            "\n",
            "Global step: 9692,loss: 0.009863539\n",
            "\n",
            "Global step: 9693,loss: 0.009961971\n",
            "\n",
            "Global step: 9694,loss: 0.009448864\n",
            "\n",
            "Global step: 9695,loss: 0.009999823\n",
            "\n",
            "Global step: 9696,loss: 0.009382135\n",
            "\n",
            "Global step: 9697,loss: 0.009633727\n",
            "\n",
            "Global step: 9698,loss: 0.009456895\n",
            "\n",
            "Global step: 9699,loss: 0.009634435\n",
            "\n",
            "Global step: 9700,loss: 0.009781403\n",
            "\n",
            "Global step: 9701,loss: 0.009190632\n",
            "\n",
            "Global step: 9702,loss: 0.009337394\n",
            "\n",
            "Global step: 9703,loss: 0.00916898\n",
            "\n",
            "Global step: 9704,loss: 0.009325992\n",
            "\n",
            "Global step: 9705,loss: 0.009172156\n",
            "\n",
            "Global step: 9706,loss: 0.009218603\n",
            "\n",
            "Global step: 9707,loss: 0.009057793\n",
            "\n",
            "Global step: 9708,loss: 0.009623844\n",
            "\n",
            "Global step: 9709,loss: 0.009422417\n",
            "\n",
            "Global step: 9710,loss: 0.009437764\n",
            "\n",
            "Global step: 9711,loss: 0.009388235\n",
            "\n",
            "Global step: 9712,loss: 0.009578219\n",
            "\n",
            "Global step: 9713,loss: 0.009733788\n",
            "\n",
            "Global step: 9714,loss: 0.008947853\n",
            "\n",
            "Global step: 9715,loss: 0.009220437\n",
            "\n",
            "Global step: 9716,loss: 0.009677619\n",
            "\n",
            "Global step: 9717,loss: 0.009325366\n",
            "\n",
            "Global step: 9718,loss: 0.0093716\n",
            "\n",
            "Global step: 9719,loss: 0.008890441\n",
            "\n",
            "Global step: 9720,loss: 0.009432678\n",
            "\n",
            "Global step: 9721,loss: 0.009673327\n",
            "\n",
            "Global step: 9722,loss: 0.009585745\n",
            "\n",
            "Global step: 9723,loss: 0.009853677\n",
            "\n",
            "Global step: 9724,loss: 0.010056327\n",
            "\n",
            "Global step: 9725,loss: 0.009610017\n",
            "\n",
            "Global step: 9726,loss: 0.009018867\n",
            "\n",
            "Global step: 9727,loss: 0.009291282\n",
            "\n",
            "Global step: 9728,loss: 0.010018853\n",
            "\n",
            "Global step: 9729,loss: 0.0097591095\n",
            "\n",
            "Global step: 9730,loss: 0.009180021\n",
            "\n",
            "Global step: 9731,loss: 0.009713878\n",
            "\n",
            "Global step: 9732,loss: 0.009341021\n",
            "\n",
            "Global step: 9733,loss: 0.00955939\n",
            "\n",
            "Global step: 9734,loss: 0.010044633\n",
            "\n",
            "Global step: 9735,loss: 0.009572916\n",
            "\n",
            "Global step: 9736,loss: 0.009283546\n",
            "\n",
            "Global step: 9737,loss: 0.009731366\n",
            "\n",
            "Global step: 9738,loss: 0.0093935495\n",
            "\n",
            "Global step: 9739,loss: 0.009987503\n",
            "\n",
            "Global step: 9740,loss: 0.009708589\n",
            "\n",
            "Global step: 9741,loss: 0.009621242\n",
            "\n",
            "Global step: 9742,loss: 0.009849118\n",
            "\n",
            "Global step: 9743,loss: 0.010020673\n",
            "\n",
            "Global step: 9744,loss: 0.009798959\n",
            "\n",
            "Global step: 9745,loss: 0.00993431\n",
            "\n",
            "Global step: 9746,loss: 0.010620929\n",
            "\n",
            "Global step: 9747,loss: 0.009329569\n",
            "\n",
            "Global step: 9748,loss: 0.009763459\n",
            "\n",
            "Global step: 9749,loss: 0.009857349\n",
            "\n",
            "Global step: 9750,loss: 0.00996694\n",
            "\n",
            "Global step: 9751,loss: 0.010118153\n",
            "\n",
            "Global step: 9752,loss: 0.010009096\n",
            "\n",
            "Global step: 9753,loss: 0.009624298\n",
            "\n",
            "Global step: 9754,loss: 0.010135024\n",
            "\n",
            "Global step: 9755,loss: 0.009455425\n",
            "\n",
            "Global step: 9756,loss: 0.009792547\n",
            "\n",
            "Global step: 9757,loss: 0.009746013\n",
            "\n",
            "Global step: 9758,loss: 0.009788233\n",
            "\n",
            "Global step: 9759,loss: 0.009077861\n",
            "\n",
            "Global step: 9760,loss: 0.008996659\n",
            "\n",
            "Global step: 9761,loss: 0.00954581\n",
            "\n",
            "Global step: 9762,loss: 0.010068139\n",
            "\n",
            "Global step: 9763,loss: 0.009535199\n",
            "\n",
            "Global step: 9764,loss: 0.009871394\n",
            "\n",
            "Global step: 9765,loss: 0.009542301\n",
            "\n",
            "Global step: 9766,loss: 0.00998386\n",
            "\n",
            "Global step: 9767,loss: 0.009443634\n",
            "\n",
            "Global step: 9768,loss: 0.00953542\n",
            "\n",
            "Global step: 9769,loss: 0.009644494\n",
            "\n",
            "Global step: 9770,loss: 0.009789248\n",
            "\n",
            "Global step: 9771,loss: 0.00891002\n",
            "\n",
            "Global step: 9772,loss: 0.00985231\n",
            "\n",
            "Global step: 9773,loss: 0.009812985\n",
            "\n",
            "Global step: 9774,loss: 0.009860771\n",
            "\n",
            "Global step: 9775,loss: 0.009225426\n",
            "\n",
            "Global step: 9776,loss: 0.009220694\n",
            "\n",
            "Global step: 9777,loss: 0.009823409\n",
            "\n",
            "Global step: 9778,loss: 0.009915641\n",
            "\n",
            "Global step: 9779,loss: 0.009402525\n",
            "\n",
            "Global step: 9780,loss: 0.009643299\n",
            "\n",
            "Global step: 9781,loss: 0.009480884\n",
            "\n",
            "Global step: 9782,loss: 0.00940934\n",
            "\n",
            "Global step: 9783,loss: 0.009875558\n",
            "\n",
            "Global step: 9784,loss: 0.009650936\n",
            "\n",
            "Global step: 9785,loss: 0.009960133\n",
            "\n",
            "Global step: 9786,loss: 0.009637801\n",
            "\n",
            "Global step: 9787,loss: 0.008893615\n",
            "\n",
            "Global step: 9788,loss: 0.0099694645\n",
            "\n",
            "Global step: 9789,loss: 0.009583345\n",
            "\n",
            "Global step: 9790,loss: 0.009735388\n",
            "\n",
            "Global step: 9791,loss: 0.009528069\n",
            "\n",
            "Global step: 9792,loss: 0.009945053\n",
            "\n",
            "Global step: 9793,loss: 0.00909099\n",
            "\n",
            "Global step: 9794,loss: 0.009879006\n",
            "\n",
            "Global step: 9795,loss: 0.00898874\n",
            "\n",
            "Global step: 9796,loss: 0.009879239\n",
            "\n",
            "Global step: 9797,loss: 0.009170618\n",
            "\n",
            "Global step: 9798,loss: 0.009766853\n",
            "\n",
            "Global step: 9799,loss: 0.009928954\n",
            "\n",
            "Global step: 9800,loss: 0.009195609\n",
            "\n",
            "Global step: 9801,loss: 0.008685777\n",
            "\n",
            "Global step: 9802,loss: 0.009269951\n",
            "\n",
            "Global step: 9803,loss: 0.009598945\n",
            "\n",
            "Global step: 9804,loss: 0.009194417\n",
            "\n",
            "Global step: 9805,loss: 0.009213922\n",
            "\n",
            "Global step: 9806,loss: 0.009459085\n",
            "\n",
            "Global step: 9807,loss: 0.009472002\n",
            "\n",
            "Global step: 9808,loss: 0.009513962\n",
            "\n",
            "Global step: 9809,loss: 0.009724481\n",
            "\n",
            "Global step: 9810,loss: 0.009197338\n",
            "\n",
            "Global step: 9811,loss: 0.010015771\n",
            "\n",
            "Global step: 9812,loss: 0.009727089\n",
            "\n",
            "Global step: 9813,loss: 0.009611683\n",
            "\n",
            "Global step: 9814,loss: 0.009621638\n",
            "\n",
            "Global step: 9815,loss: 0.010289933\n",
            "\n",
            "Global step: 9816,loss: 0.009881778\n",
            "\n",
            "Global step: 9817,loss: 0.009967246\n",
            "\n",
            "Global step: 9818,loss: 0.009649315\n",
            "\n",
            "Global step: 9819,loss: 0.009627883\n",
            "\n",
            "Global step: 9820,loss: 0.009831881\n",
            "\n",
            "Global step: 9821,loss: 0.009873817\n",
            "\n",
            "Global step: 9822,loss: 0.009591485\n",
            "\n",
            "Global step: 9823,loss: 0.010277329\n",
            "\n",
            "Global step: 9824,loss: 0.00981152\n",
            "\n",
            "Global step: 9825,loss: 0.009440569\n",
            "\n",
            "Global step: 9826,loss: 0.009320553\n",
            "\n",
            "Global step: 9827,loss: 0.008962603\n",
            "\n",
            "Global step: 9828,loss: 0.009001731\n",
            "\n",
            "Global step: 9829,loss: 0.0097448435\n",
            "\n",
            "Global step: 9830,loss: 0.010101458\n",
            "\n",
            "Global step: 9831,loss: 0.00908977\n",
            "\n",
            "Global step: 9832,loss: 0.010119483\n",
            "\n",
            "Global step: 9833,loss: 0.009280601\n",
            "\n",
            "Global step: 9834,loss: 0.009021151\n",
            "\n",
            "Global step: 9835,loss: 0.009841174\n",
            "\n",
            "Global step: 9836,loss: 0.010174743\n",
            "\n",
            "Global step: 9837,loss: 0.009643216\n",
            "\n",
            "Global step: 9838,loss: 0.009754406\n",
            "\n",
            "Global step: 9839,loss: 0.010053303\n",
            "\n",
            "Global step: 9840,loss: 0.009316154\n",
            "\n",
            "Global step: 9841,loss: 0.009366903\n",
            "\n",
            "Global step: 9842,loss: 0.009674214\n",
            "\n",
            "Global step: 9843,loss: 0.010237155\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 9843,val_loss: 0.014541434859366794\n",
            "\n",
            "Training for epoch 47/50:\n",
            "Global step: 9844,loss: 0.009257627\n",
            "\n",
            "Global step: 9845,loss: 0.009432369\n",
            "\n",
            "Global step: 9846,loss: 0.009770436\n",
            "\n",
            "Global step: 9847,loss: 0.009345138\n",
            "\n",
            "Global step: 9848,loss: 0.009560977\n",
            "\n",
            "Global step: 9849,loss: 0.009085196\n",
            "\n",
            "Global step: 9850,loss: 0.009470708\n",
            "\n",
            "Global step: 9851,loss: 0.0098122\n",
            "\n",
            "Global step: 9852,loss: 0.009563312\n",
            "\n",
            "Global step: 9853,loss: 0.010127294\n",
            "\n",
            "Global step: 9854,loss: 0.009329333\n",
            "\n",
            "Global step: 9855,loss: 0.009518611\n",
            "\n",
            "Global step: 9856,loss: 0.009228648\n",
            "\n",
            "Global step: 9857,loss: 0.009802309\n",
            "\n",
            "Global step: 9858,loss: 0.009743822\n",
            "\n",
            "Global step: 9859,loss: 0.009382827\n",
            "\n",
            "Global step: 9860,loss: 0.009300331\n",
            "\n",
            "Global step: 9861,loss: 0.00957955\n",
            "\n",
            "Global step: 9862,loss: 0.00977795\n",
            "\n",
            "Global step: 9863,loss: 0.009536501\n",
            "\n",
            "Global step: 9864,loss: 0.009538762\n",
            "\n",
            "Global step: 9865,loss: 0.009606294\n",
            "\n",
            "Global step: 9866,loss: 0.009306912\n",
            "\n",
            "Global step: 9867,loss: 0.00965823\n",
            "\n",
            "Global step: 9868,loss: 0.009762693\n",
            "\n",
            "Global step: 9869,loss: 0.009818804\n",
            "\n",
            "Global step: 9870,loss: 0.009729707\n",
            "\n",
            "Global step: 9871,loss: 0.009736339\n",
            "\n",
            "Global step: 9872,loss: 0.008979686\n",
            "\n",
            "Global step: 9873,loss: 0.009523291\n",
            "\n",
            "Global step: 9874,loss: 0.010050759\n",
            "\n",
            "Global step: 9875,loss: 0.009510269\n",
            "\n",
            "Global step: 9876,loss: 0.008938103\n",
            "\n",
            "Global step: 9877,loss: 0.009351226\n",
            "\n",
            "Global step: 9878,loss: 0.009807704\n",
            "\n",
            "Global step: 9879,loss: 0.009640922\n",
            "\n",
            "Global step: 9880,loss: 0.008816694\n",
            "\n",
            "Global step: 9881,loss: 0.0090124365\n",
            "\n",
            "Global step: 9882,loss: 0.009408388\n",
            "\n",
            "Global step: 9883,loss: 0.009487264\n",
            "\n",
            "Global step: 9884,loss: 0.009087669\n",
            "\n",
            "Global step: 9885,loss: 0.009220979\n",
            "\n",
            "Global step: 9886,loss: 0.009345496\n",
            "\n",
            "Global step: 9887,loss: 0.0093944045\n",
            "\n",
            "Global step: 9888,loss: 0.010273092\n",
            "\n",
            "Global step: 9889,loss: 0.009625076\n",
            "\n",
            "Global step: 9890,loss: 0.009624483\n",
            "\n",
            "Global step: 9891,loss: 0.009504076\n",
            "\n",
            "Global step: 9892,loss: 0.009745408\n",
            "\n",
            "Global step: 9893,loss: 0.009695726\n",
            "\n",
            "Global step: 9894,loss: 0.009673688\n",
            "\n",
            "Global step: 9895,loss: 0.009487034\n",
            "\n",
            "Global step: 9896,loss: 0.009216964\n",
            "\n",
            "Global step: 9897,loss: 0.009448236\n",
            "\n",
            "Global step: 9898,loss: 0.009638592\n",
            "\n",
            "Global step: 9899,loss: 0.0084200455\n",
            "\n",
            "Global step: 9900,loss: 0.009541527\n",
            "\n",
            "Global step: 9901,loss: 0.009033679\n",
            "\n",
            "Global step: 9902,loss: 0.009470086\n",
            "\n",
            "Global step: 9903,loss: 0.0092884125\n",
            "\n",
            "Global step: 9904,loss: 0.009075381\n",
            "\n",
            "Global step: 9905,loss: 0.009416987\n",
            "\n",
            "Global step: 9906,loss: 0.009512415\n",
            "\n",
            "Global step: 9907,loss: 0.008792266\n",
            "\n",
            "Global step: 9908,loss: 0.009253877\n",
            "\n",
            "Global step: 9909,loss: 0.009199759\n",
            "\n",
            "Global step: 9910,loss: 0.008856365\n",
            "\n",
            "Global step: 9911,loss: 0.009539358\n",
            "\n",
            "Global step: 9912,loss: 0.009228349\n",
            "\n",
            "Global step: 9913,loss: 0.009179986\n",
            "\n",
            "Global step: 9914,loss: 0.009847223\n",
            "\n",
            "Global step: 9915,loss: 0.009830848\n",
            "\n",
            "Global step: 9916,loss: 0.009892905\n",
            "\n",
            "Global step: 9917,loss: 0.009689925\n",
            "\n",
            "Global step: 9918,loss: 0.009120709\n",
            "\n",
            "Global step: 9919,loss: 0.0088511575\n",
            "\n",
            "Global step: 9920,loss: 0.009009808\n",
            "\n",
            "Global step: 9921,loss: 0.009340102\n",
            "\n",
            "Global step: 9922,loss: 0.009205781\n",
            "\n",
            "Global step: 9923,loss: 0.009911506\n",
            "\n",
            "Global step: 9924,loss: 0.009019676\n",
            "\n",
            "Global step: 9925,loss: 0.009605675\n",
            "\n",
            "Global step: 9926,loss: 0.009218107\n",
            "\n",
            "Global step: 9927,loss: 0.009273856\n",
            "\n",
            "Global step: 9928,loss: 0.009080812\n",
            "\n",
            "Global step: 9929,loss: 0.0097588785\n",
            "\n",
            "Global step: 9930,loss: 0.009437317\n",
            "\n",
            "Global step: 9931,loss: 0.009424863\n",
            "\n",
            "Global step: 9932,loss: 0.009445339\n",
            "\n",
            "Global step: 9933,loss: 0.009454629\n",
            "\n",
            "Global step: 9934,loss: 0.009136229\n",
            "\n",
            "Global step: 9935,loss: 0.0095098475\n",
            "\n",
            "Global step: 9936,loss: 0.009556574\n",
            "\n",
            "Global step: 9937,loss: 0.00966714\n",
            "\n",
            "Global step: 9938,loss: 0.010154806\n",
            "\n",
            "Global step: 9939,loss: 0.008880991\n",
            "\n",
            "Global step: 9940,loss: 0.009482927\n",
            "\n",
            "Global step: 9941,loss: 0.009185584\n",
            "\n",
            "Global step: 9942,loss: 0.008940087\n",
            "\n",
            "Global step: 9943,loss: 0.009243328\n",
            "\n",
            "Global step: 9944,loss: 0.009676591\n",
            "\n",
            "Global step: 9945,loss: 0.0097027\n",
            "\n",
            "Global step: 9946,loss: 0.009727987\n",
            "\n",
            "Global step: 9947,loss: 0.009129731\n",
            "\n",
            "Global step: 9948,loss: 0.008846447\n",
            "\n",
            "Global step: 9949,loss: 0.009155855\n",
            "\n",
            "Global step: 9950,loss: 0.009295402\n",
            "\n",
            "Global step: 9951,loss: 0.00972422\n",
            "\n",
            "Global step: 9952,loss: 0.009874841\n",
            "\n",
            "Global step: 9953,loss: 0.009904145\n",
            "\n",
            "Global step: 9954,loss: 0.009456063\n",
            "\n",
            "Global step: 9955,loss: 0.010048083\n",
            "\n",
            "Global step: 9956,loss: 0.009927671\n",
            "\n",
            "Global step: 9957,loss: 0.00947103\n",
            "\n",
            "Global step: 9958,loss: 0.009608088\n",
            "\n",
            "Global step: 9959,loss: 0.00993063\n",
            "\n",
            "Global step: 9960,loss: 0.009907923\n",
            "\n",
            "Global step: 9961,loss: 0.009688397\n",
            "\n",
            "Global step: 9962,loss: 0.009466857\n",
            "\n",
            "Global step: 9963,loss: 0.009513507\n",
            "\n",
            "Global step: 9964,loss: 0.009871342\n",
            "\n",
            "Global step: 9965,loss: 0.009242892\n",
            "\n",
            "Global step: 9966,loss: 0.009380352\n",
            "\n",
            "Global step: 9967,loss: 0.009311023\n",
            "\n",
            "Global step: 9968,loss: 0.010012204\n",
            "\n",
            "Global step: 9969,loss: 0.009177579\n",
            "\n",
            "Global step: 9970,loss: 0.009290446\n",
            "\n",
            "Global step: 9971,loss: 0.008733288\n",
            "\n",
            "Global step: 9972,loss: 0.009904616\n",
            "\n",
            "Global step: 9973,loss: 0.0093042\n",
            "\n",
            "Global step: 9974,loss: 0.009461452\n",
            "\n",
            "Global step: 9975,loss: 0.009465231\n",
            "\n",
            "Global step: 9976,loss: 0.009435304\n",
            "\n",
            "Global step: 9977,loss: 0.009637568\n",
            "\n",
            "Global step: 9978,loss: 0.009735707\n",
            "\n",
            "Global step: 9979,loss: 0.008795602\n",
            "\n",
            "Global step: 9980,loss: 0.008936514\n",
            "\n",
            "Global step: 9981,loss: 0.009658607\n",
            "\n",
            "Global step: 9982,loss: 0.009475483\n",
            "\n",
            "Global step: 9983,loss: 0.009830673\n",
            "\n",
            "Global step: 9984,loss: 0.00954827\n",
            "\n",
            "Global step: 9985,loss: 0.009235954\n",
            "\n",
            "Global step: 9986,loss: 0.009261704\n",
            "\n",
            "Global step: 9987,loss: 0.00948439\n",
            "\n",
            "Global step: 9988,loss: 0.009894621\n",
            "\n",
            "Global step: 9989,loss: 0.009608121\n",
            "\n",
            "Global step: 9990,loss: 0.009894824\n",
            "\n",
            "Global step: 9991,loss: 0.009924704\n",
            "\n",
            "Global step: 9992,loss: 0.009785552\n",
            "\n",
            "Global step: 9993,loss: 0.009010585\n",
            "\n",
            "Global step: 9994,loss: 0.0101057375\n",
            "\n",
            "Global step: 9995,loss: 0.009264525\n",
            "\n",
            "Global step: 9996,loss: 0.010267918\n",
            "\n",
            "Global step: 9997,loss: 0.0092146965\n",
            "\n",
            "Global step: 9998,loss: 0.009589757\n",
            "\n",
            "Global step: 9999,loss: 0.010013105\n",
            "\n",
            "Global step: 10000,loss: 0.009450391\n",
            "\n",
            "Global step: 10001,loss: 0.009851922\n",
            "\n",
            "Global step: 10002,loss: 0.00919634\n",
            "\n",
            "Global step: 10003,loss: 0.009878995\n",
            "\n",
            "Global step: 10004,loss: 0.009374193\n",
            "\n",
            "Global step: 10005,loss: 0.009423302\n",
            "\n",
            "Global step: 10006,loss: 0.00943948\n",
            "\n",
            "Global step: 10007,loss: 0.009404273\n",
            "\n",
            "Global step: 10008,loss: 0.009805294\n",
            "\n",
            "Global step: 10009,loss: 0.009555863\n",
            "\n",
            "Global step: 10010,loss: 0.0094388295\n",
            "\n",
            "Global step: 10011,loss: 0.009545039\n",
            "\n",
            "Global step: 10012,loss: 0.009610988\n",
            "\n",
            "Global step: 10013,loss: 0.009245032\n",
            "\n",
            "Global step: 10014,loss: 0.009288073\n",
            "\n",
            "Global step: 10015,loss: 0.009733252\n",
            "\n",
            "Global step: 10016,loss: 0.0098619955\n",
            "\n",
            "Global step: 10017,loss: 0.009490794\n",
            "\n",
            "Global step: 10018,loss: 0.009590477\n",
            "\n",
            "Global step: 10019,loss: 0.009798864\n",
            "\n",
            "Global step: 10020,loss: 0.009599402\n",
            "\n",
            "Global step: 10021,loss: 0.009807991\n",
            "\n",
            "Global step: 10022,loss: 0.009466288\n",
            "\n",
            "Global step: 10023,loss: 0.009835084\n",
            "\n",
            "Global step: 10024,loss: 0.009298577\n",
            "\n",
            "Global step: 10025,loss: 0.0090264175\n",
            "\n",
            "Global step: 10026,loss: 0.00955579\n",
            "\n",
            "Global step: 10027,loss: 0.009400234\n",
            "\n",
            "Global step: 10028,loss: 0.009557905\n",
            "\n",
            "Global step: 10029,loss: 0.009105594\n",
            "\n",
            "Global step: 10030,loss: 0.009579865\n",
            "\n",
            "Global step: 10031,loss: 0.009900283\n",
            "\n",
            "Global step: 10032,loss: 0.009444143\n",
            "\n",
            "Global step: 10033,loss: 0.009277907\n",
            "\n",
            "Global step: 10034,loss: 0.009070945\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 10035.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:08:29.283168 140167885084416 supervisor.py:1050] Recording summary at step 10035.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 10035,loss: 0.009720323\n",
            "\n",
            "Global step: 10036,loss: 0.009535112\n",
            "\n",
            "Global step: 10037,loss: 0.010063298\n",
            "\n",
            "Global step: 10038,loss: 0.009412476\n",
            "\n",
            "Global step: 10039,loss: 0.009830323\n",
            "\n",
            "Global step: 10040,loss: 0.009062044\n",
            "\n",
            "Global step: 10041,loss: 0.009485251\n",
            "\n",
            "Global step: 10042,loss: 0.009290757\n",
            "\n",
            "Global step: 10043,loss: 0.0104153585\n",
            "\n",
            "Global step: 10044,loss: 0.009067031\n",
            "\n",
            "Global step: 10045,loss: 0.009524749\n",
            "\n",
            "Global step: 10046,loss: 0.010095901\n",
            "\n",
            "Global step: 10047,loss: 0.009149841\n",
            "\n",
            "Global step: 10048,loss: 0.010171657\n",
            "\n",
            "Global step: 10049,loss: 0.0091475565\n",
            "\n",
            "Global step: 10050,loss: 0.0098534245\n",
            "\n",
            "Global step: 10051,loss: 0.009017913\n",
            "\n",
            "Global step: 10052,loss: 0.00876136\n",
            "\n",
            "Global step: 10053,loss: 0.009929526\n",
            "\n",
            "Global step: 10054,loss: 0.009084018\n",
            "\n",
            "Global step: 10055,loss: 0.009104132\n",
            "\n",
            "Global step: 10056,loss: 0.009230115\n",
            "\n",
            "Global step: 10057,loss: 0.010411736\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 10057,Val_Loss: 0.014380031853522124,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:08:37.281806 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 48/50:\n",
            "Global step: 10058,loss: 0.009171105\n",
            "\n",
            "Global step: 10059,loss: 0.009556087\n",
            "\n",
            "Global step: 10060,loss: 0.00921898\n",
            "\n",
            "Global step: 10061,loss: 0.009468347\n",
            "\n",
            "Global step: 10062,loss: 0.009582208\n",
            "\n",
            "Global step: 10063,loss: 0.009677213\n",
            "\n",
            "Global step: 10064,loss: 0.0094338795\n",
            "\n",
            "Global step: 10065,loss: 0.009292747\n",
            "\n",
            "Global step: 10066,loss: 0.009889352\n",
            "\n",
            "Global step: 10067,loss: 0.010095307\n",
            "\n",
            "Global step: 10068,loss: 0.009887515\n",
            "\n",
            "Global step: 10069,loss: 0.008951892\n",
            "\n",
            "Global step: 10070,loss: 0.009341509\n",
            "\n",
            "Global step: 10071,loss: 0.008780617\n",
            "\n",
            "Global step: 10072,loss: 0.008902242\n",
            "\n",
            "Global step: 10073,loss: 0.00937001\n",
            "\n",
            "Global step: 10074,loss: 0.009657074\n",
            "\n",
            "Global step: 10075,loss: 0.009686755\n",
            "\n",
            "Global step: 10076,loss: 0.010279796\n",
            "\n",
            "Global step: 10077,loss: 0.009246188\n",
            "\n",
            "Global step: 10078,loss: 0.009606117\n",
            "\n",
            "Global step: 10079,loss: 0.009531987\n",
            "\n",
            "Global step: 10080,loss: 0.0087400945\n",
            "\n",
            "Global step: 10081,loss: 0.009099147\n",
            "\n",
            "Global step: 10082,loss: 0.009216456\n",
            "\n",
            "Global step: 10083,loss: 0.008998652\n",
            "\n",
            "Global step: 10084,loss: 0.008825168\n",
            "\n",
            "Global step: 10085,loss: 0.009047623\n",
            "\n",
            "Global step: 10086,loss: 0.009635261\n",
            "\n",
            "Global step: 10087,loss: 0.009700479\n",
            "\n",
            "Global step: 10088,loss: 0.009249709\n",
            "\n",
            "Global step: 10089,loss: 0.009737039\n",
            "\n",
            "Global step: 10090,loss: 0.009454381\n",
            "\n",
            "Global step: 10091,loss: 0.009571538\n",
            "\n",
            "Global step: 10092,loss: 0.009751877\n",
            "\n",
            "Global step: 10093,loss: 0.009250842\n",
            "\n",
            "Global step: 10094,loss: 0.009080872\n",
            "\n",
            "Global step: 10095,loss: 0.009431673\n",
            "\n",
            "Global step: 10096,loss: 0.009295641\n",
            "\n",
            "Global step: 10097,loss: 0.009936922\n",
            "\n",
            "Global step: 10098,loss: 0.009403539\n",
            "\n",
            "Global step: 10099,loss: 0.009047288\n",
            "\n",
            "Global step: 10100,loss: 0.009447967\n",
            "\n",
            "Global step: 10101,loss: 0.009536377\n",
            "\n",
            "Global step: 10102,loss: 0.009505224\n",
            "\n",
            "Global step: 10103,loss: 0.00938133\n",
            "\n",
            "Global step: 10104,loss: 0.009825487\n",
            "\n",
            "Global step: 10105,loss: 0.009187498\n",
            "\n",
            "Global step: 10106,loss: 0.009559181\n",
            "\n",
            "Global step: 10107,loss: 0.00933667\n",
            "\n",
            "Global step: 10108,loss: 0.0094173215\n",
            "\n",
            "Global step: 10109,loss: 0.009229452\n",
            "\n",
            "Global step: 10110,loss: 0.009101725\n",
            "\n",
            "Global step: 10111,loss: 0.009318768\n",
            "\n",
            "Global step: 10112,loss: 0.009344559\n",
            "\n",
            "Global step: 10113,loss: 0.010138551\n",
            "\n",
            "Global step: 10114,loss: 0.009008193\n",
            "\n",
            "Global step: 10115,loss: 0.009384089\n",
            "\n",
            "Global step: 10116,loss: 0.008728377\n",
            "\n",
            "Global step: 10117,loss: 0.008808047\n",
            "\n",
            "Global step: 10118,loss: 0.009698669\n",
            "\n",
            "Global step: 10119,loss: 0.009952978\n",
            "\n",
            "Global step: 10120,loss: 0.009206249\n",
            "\n",
            "Global step: 10121,loss: 0.009396397\n",
            "\n",
            "Global step: 10122,loss: 0.00912572\n",
            "\n",
            "Global step: 10123,loss: 0.009184399\n",
            "\n",
            "Global step: 10124,loss: 0.008887052\n",
            "\n",
            "Global step: 10125,loss: 0.00949201\n",
            "\n",
            "Global step: 10126,loss: 0.009716136\n",
            "\n",
            "Global step: 10127,loss: 0.009305053\n",
            "\n",
            "Global step: 10128,loss: 0.00900505\n",
            "\n",
            "Global step: 10129,loss: 0.009444278\n",
            "\n",
            "Global step: 10130,loss: 0.009939318\n",
            "\n",
            "Global step: 10131,loss: 0.009235933\n",
            "\n",
            "Global step: 10132,loss: 0.009633887\n",
            "\n",
            "Global step: 10133,loss: 0.009556911\n",
            "\n",
            "Global step: 10134,loss: 0.009120521\n",
            "\n",
            "Global step: 10135,loss: 0.009671059\n",
            "\n",
            "Global step: 10136,loss: 0.009503821\n",
            "\n",
            "Global step: 10137,loss: 0.009325623\n",
            "\n",
            "Global step: 10138,loss: 0.009533588\n",
            "\n",
            "Global step: 10139,loss: 0.008689495\n",
            "\n",
            "Global step: 10140,loss: 0.009622672\n",
            "\n",
            "Global step: 10141,loss: 0.0096318545\n",
            "\n",
            "Global step: 10142,loss: 0.009596696\n",
            "\n",
            "Global step: 10143,loss: 0.009517696\n",
            "\n",
            "Global step: 10144,loss: 0.009533857\n",
            "\n",
            "Global step: 10145,loss: 0.009184972\n",
            "\n",
            "Global step: 10146,loss: 0.008775499\n",
            "\n",
            "Global step: 10147,loss: 0.008941294\n",
            "\n",
            "Global step: 10148,loss: 0.008982808\n",
            "\n",
            "Global step: 10149,loss: 0.009066337\n",
            "\n",
            "Global step: 10150,loss: 0.009229278\n",
            "\n",
            "Global step: 10151,loss: 0.009786015\n",
            "\n",
            "Global step: 10152,loss: 0.008907149\n",
            "\n",
            "Global step: 10153,loss: 0.009440882\n",
            "\n",
            "Global step: 10154,loss: 0.009193933\n",
            "\n",
            "Global step: 10155,loss: 0.009807475\n",
            "\n",
            "Global step: 10156,loss: 0.009404734\n",
            "\n",
            "Global step: 10157,loss: 0.00861291\n",
            "\n",
            "Global step: 10158,loss: 0.009204606\n",
            "\n",
            "Global step: 10159,loss: 0.0090152025\n",
            "\n",
            "Global step: 10160,loss: 0.008918126\n",
            "\n",
            "Global step: 10161,loss: 0.009257591\n",
            "\n",
            "Global step: 10162,loss: 0.008919135\n",
            "\n",
            "Global step: 10163,loss: 0.00911123\n",
            "\n",
            "Global step: 10164,loss: 0.009700861\n",
            "\n",
            "Global step: 10165,loss: 0.009473819\n",
            "\n",
            "Global step: 10166,loss: 0.009332164\n",
            "\n",
            "Global step: 10167,loss: 0.009672757\n",
            "\n",
            "Global step: 10168,loss: 0.008979735\n",
            "\n",
            "Global step: 10169,loss: 0.009074764\n",
            "\n",
            "Global step: 10170,loss: 0.009831304\n",
            "\n",
            "Global step: 10171,loss: 0.009379247\n",
            "\n",
            "Global step: 10172,loss: 0.009318632\n",
            "\n",
            "Global step: 10173,loss: 0.009698175\n",
            "\n",
            "Global step: 10174,loss: 0.009204907\n",
            "\n",
            "Global step: 10175,loss: 0.009131281\n",
            "\n",
            "Global step: 10176,loss: 0.00933913\n",
            "\n",
            "Global step: 10177,loss: 0.009711925\n",
            "\n",
            "Global step: 10178,loss: 0.009684476\n",
            "\n",
            "Global step: 10179,loss: 0.009526174\n",
            "\n",
            "Global step: 10180,loss: 0.008830771\n",
            "\n",
            "Global step: 10181,loss: 0.009765952\n",
            "\n",
            "Global step: 10182,loss: 0.010141025\n",
            "\n",
            "Global step: 10183,loss: 0.009219752\n",
            "\n",
            "Global step: 10184,loss: 0.009409987\n",
            "\n",
            "Global step: 10185,loss: 0.009439406\n",
            "\n",
            "Global step: 10186,loss: 0.009242885\n",
            "\n",
            "Global step: 10187,loss: 0.0099468045\n",
            "\n",
            "Global step: 10188,loss: 0.009014849\n",
            "\n",
            "Global step: 10189,loss: 0.009832291\n",
            "\n",
            "Global step: 10190,loss: 0.008862845\n",
            "\n",
            "Global step: 10191,loss: 0.009717495\n",
            "\n",
            "Global step: 10192,loss: 0.009927572\n",
            "\n",
            "Global step: 10193,loss: 0.009211655\n",
            "\n",
            "Global step: 10194,loss: 0.009333181\n",
            "\n",
            "Global step: 10195,loss: 0.009152994\n",
            "\n",
            "Global step: 10196,loss: 0.009255882\n",
            "\n",
            "Global step: 10197,loss: 0.009330462\n",
            "\n",
            "Global step: 10198,loss: 0.009056802\n",
            "\n",
            "Global step: 10199,loss: 0.009573607\n",
            "\n",
            "Global step: 10200,loss: 0.009851858\n",
            "\n",
            "Global step: 10201,loss: 0.009547008\n",
            "\n",
            "Global step: 10202,loss: 0.009145024\n",
            "\n",
            "Global step: 10203,loss: 0.009965319\n",
            "\n",
            "Global step: 10204,loss: 0.009405322\n",
            "\n",
            "Global step: 10205,loss: 0.009005737\n",
            "\n",
            "Global step: 10206,loss: 0.010046903\n",
            "\n",
            "Global step: 10207,loss: 0.009750109\n",
            "\n",
            "Global step: 10208,loss: 0.009544824\n",
            "\n",
            "Global step: 10209,loss: 0.009599434\n",
            "\n",
            "Global step: 10210,loss: 0.009174416\n",
            "\n",
            "Global step: 10211,loss: 0.009527744\n",
            "\n",
            "Global step: 10212,loss: 0.009429473\n",
            "\n",
            "Global step: 10213,loss: 0.009476297\n",
            "\n",
            "Global step: 10214,loss: 0.0096496\n",
            "\n",
            "Global step: 10215,loss: 0.009721832\n",
            "\n",
            "Global step: 10216,loss: 0.00954547\n",
            "\n",
            "Global step: 10217,loss: 0.009254942\n",
            "\n",
            "Global step: 10218,loss: 0.009650921\n",
            "\n",
            "Global step: 10219,loss: 0.009194507\n",
            "\n",
            "Global step: 10220,loss: 0.0092246905\n",
            "\n",
            "Global step: 10221,loss: 0.009063482\n",
            "\n",
            "Global step: 10222,loss: 0.009511245\n",
            "\n",
            "Global step: 10223,loss: 0.009348217\n",
            "\n",
            "Global step: 10224,loss: 0.0098397285\n",
            "\n",
            "Global step: 10225,loss: 0.009483173\n",
            "\n",
            "Global step: 10226,loss: 0.009901768\n",
            "\n",
            "Global step: 10227,loss: 0.009472816\n",
            "\n",
            "Global step: 10228,loss: 0.009425507\n",
            "\n",
            "Global step: 10229,loss: 0.009383694\n",
            "\n",
            "Global step: 10230,loss: 0.00932165\n",
            "\n",
            "Global step: 10231,loss: 0.009478664\n",
            "\n",
            "Global step: 10232,loss: 0.009229997\n",
            "\n",
            "Global step: 10233,loss: 0.008917376\n",
            "\n",
            "Global step: 10234,loss: 0.0092198355\n",
            "\n",
            "Global step: 10235,loss: 0.009497698\n",
            "\n",
            "Global step: 10236,loss: 0.009442632\n",
            "\n",
            "Global step: 10237,loss: 0.009044666\n",
            "\n",
            "Global step: 10238,loss: 0.009687685\n",
            "\n",
            "Global step: 10239,loss: 0.009153528\n",
            "\n",
            "Global step: 10240,loss: 0.00924181\n",
            "\n",
            "Global step: 10241,loss: 0.009559659\n",
            "\n",
            "Global step: 10242,loss: 0.009508089\n",
            "\n",
            "Global step: 10243,loss: 0.009256508\n",
            "\n",
            "Global step: 10244,loss: 0.009048494\n",
            "\n",
            "Global step: 10245,loss: 0.009505513\n",
            "\n",
            "Global step: 10246,loss: 0.00947541\n",
            "\n",
            "Global step: 10247,loss: 0.009550146\n",
            "\n",
            "Global step: 10248,loss: 0.00955267\n",
            "\n",
            "Global step: 10249,loss: 0.0092303185\n",
            "\n",
            "Global step: 10250,loss: 0.009775152\n",
            "\n",
            "Global step: 10251,loss: 0.009202382\n",
            "\n",
            "Global step: 10252,loss: 0.009811139\n",
            "\n",
            "Global step: 10253,loss: 0.009207965\n",
            "\n",
            "Global step: 10254,loss: 0.009532348\n",
            "\n",
            "Global step: 10255,loss: 0.009093327\n",
            "\n",
            "Global step: 10256,loss: 0.008932006\n",
            "\n",
            "Global step: 10257,loss: 0.009216226\n",
            "\n",
            "Global step: 10258,loss: 0.009662621\n",
            "\n",
            "Global step: 10259,loss: 0.009123013\n",
            "\n",
            "Global step: 10260,loss: 0.009443562\n",
            "\n",
            "Global step: 10261,loss: 0.009346963\n",
            "\n",
            "Global step: 10262,loss: 0.010184241\n",
            "\n",
            "Global step: 10263,loss: 0.0093403105\n",
            "\n",
            "Global step: 10264,loss: 0.009546332\n",
            "\n",
            "Global step: 10265,loss: 0.009447655\n",
            "\n",
            "Global step: 10266,loss: 0.009225042\n",
            "\n",
            "Global step: 10267,loss: 0.009556468\n",
            "\n",
            "Global step: 10268,loss: 0.009707703\n",
            "\n",
            "Global step: 10269,loss: 0.009991058\n",
            "\n",
            "Global step: 10270,loss: 0.009131103\n",
            "\n",
            "Global step: 10271,loss: 0.010016598\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 10271,val_loss: 0.014492133436234374\n",
            "\n",
            "Training for epoch 49/50:\n",
            "Global step: 10272,loss: 0.009817648\n",
            "\n",
            "Global step: 10273,loss: 0.009559439\n",
            "\n",
            "Global step: 10274,loss: 0.009357755\n",
            "\n",
            "Global step: 10275,loss: 0.009339256\n",
            "\n",
            "Global step: 10276,loss: 0.00955872\n",
            "\n",
            "Global step: 10277,loss: 0.008878989\n",
            "\n",
            "Global step: 10278,loss: 0.009192993\n",
            "\n",
            "Global step: 10279,loss: 0.009910099\n",
            "\n",
            "Global step: 10280,loss: 0.010407769\n",
            "\n",
            "Global step: 10281,loss: 0.009108708\n",
            "\n",
            "Global step: 10282,loss: 0.009186272\n",
            "\n",
            "Global step: 10283,loss: 0.009209102\n",
            "\n",
            "Global step: 10284,loss: 0.009489011\n",
            "\n",
            "Global step: 10285,loss: 0.009279214\n",
            "\n",
            "Global step: 10286,loss: 0.009395802\n",
            "\n",
            "Global step: 10287,loss: 0.009511347\n",
            "\n",
            "Global step: 10288,loss: 0.009179025\n",
            "\n",
            "Global step: 10289,loss: 0.009072644\n",
            "\n",
            "Global step: 10290,loss: 0.009915645\n",
            "\n",
            "Global step: 10291,loss: 0.009426064\n",
            "\n",
            "Global step: 10292,loss: 0.009357766\n",
            "\n",
            "Global step: 10293,loss: 0.00974022\n",
            "\n",
            "Global step: 10294,loss: 0.00952183\n",
            "\n",
            "Global step: 10295,loss: 0.009308946\n",
            "\n",
            "Global step: 10296,loss: 0.009518442\n",
            "\n",
            "Global step: 10297,loss: 0.008706725\n",
            "\n",
            "Global step: 10298,loss: 0.00958125\n",
            "\n",
            "Global step: 10299,loss: 0.009559291\n",
            "\n",
            "Global step: 10300,loss: 0.009598521\n",
            "\n",
            "Global step: 10301,loss: 0.009053393\n",
            "\n",
            "Global step: 10302,loss: 0.009190934\n",
            "\n",
            "Global step: 10303,loss: 0.0088523915\n",
            "\n",
            "Global step: 10304,loss: 0.009297554\n",
            "\n",
            "Global step: 10305,loss: 0.008826845\n",
            "\n",
            "Global step: 10306,loss: 0.00929538\n",
            "\n",
            "Global step: 10307,loss: 0.0089712115\n",
            "\n",
            "Global step: 10308,loss: 0.009051601\n",
            "\n",
            "Global step: 10309,loss: 0.009987687\n",
            "\n",
            "Global step: 10310,loss: 0.009296288\n",
            "\n",
            "Global step: 10311,loss: 0.0088914605\n",
            "\n",
            "Global step: 10312,loss: 0.009299619\n",
            "\n",
            "Global step: 10313,loss: 0.009433788\n",
            "\n",
            "Global step: 10314,loss: 0.009117051\n",
            "\n",
            "Global step: 10315,loss: 0.00936645\n",
            "\n",
            "Global step: 10316,loss: 0.009505824\n",
            "\n",
            "Global step: 10317,loss: 0.0089126555\n",
            "\n",
            "Global step: 10318,loss: 0.00894097\n",
            "\n",
            "Global step: 10319,loss: 0.008847336\n",
            "\n",
            "Global step: 10320,loss: 0.0092694145\n",
            "\n",
            "Global step: 10321,loss: 0.008406216\n",
            "\n",
            "Global step: 10322,loss: 0.009452293\n",
            "\n",
            "Global step: 10323,loss: 0.009463245\n",
            "\n",
            "Global step: 10324,loss: 0.009565192\n",
            "\n",
            "Global step: 10325,loss: 0.009479542\n",
            "\n",
            "Global step: 10326,loss: 0.009723732\n",
            "\n",
            "Global step: 10327,loss: 0.009730239\n",
            "\n",
            "Global step: 10328,loss: 0.009227679\n",
            "\n",
            "Global step: 10329,loss: 0.009912938\n",
            "\n",
            "Global step: 10330,loss: 0.009251956\n",
            "\n",
            "Global step: 10331,loss: 0.009319016\n",
            "\n",
            "Global step: 10332,loss: 0.009508744\n",
            "\n",
            "Global step: 10333,loss: 0.009798268\n",
            "\n",
            "Global step: 10334,loss: 0.008806264\n",
            "\n",
            "Global step: 10335,loss: 0.009522783\n",
            "\n",
            "Global step: 10336,loss: 0.009218353\n",
            "\n",
            "Global step: 10337,loss: 0.009819785\n",
            "\n",
            "Global step: 10338,loss: 0.008761643\n",
            "\n",
            "Global step: 10339,loss: 0.009090682\n",
            "\n",
            "Global step: 10340,loss: 0.009370106\n",
            "\n",
            "Global step: 10341,loss: 0.009130053\n",
            "\n",
            "Global step: 10342,loss: 0.00932551\n",
            "\n",
            "Global step: 10343,loss: 0.009014026\n",
            "\n",
            "Global step: 10344,loss: 0.009349018\n",
            "\n",
            "Global step: 10345,loss: 0.009567058\n",
            "\n",
            "Global step: 10346,loss: 0.009173465\n",
            "\n",
            "Global step: 10347,loss: 0.008495505\n",
            "\n",
            "Global step: 10348,loss: 0.009424567\n",
            "\n",
            "Global step: 10349,loss: 0.0090491045\n",
            "\n",
            "Global step: 10350,loss: 0.009395385\n",
            "\n",
            "Global step: 10351,loss: 0.009576487\n",
            "\n",
            "Global step: 10352,loss: 0.009404677\n",
            "\n",
            "Global step: 10353,loss: 0.009743214\n",
            "\n",
            "Global step: 10354,loss: 0.0091608465\n",
            "\n",
            "Global step: 10355,loss: 0.009286199\n",
            "\n",
            "Global step: 10356,loss: 0.009647395\n",
            "\n",
            "Global step: 10357,loss: 0.008672232\n",
            "\n",
            "Global step: 10358,loss: 0.009339295\n",
            "\n",
            "Global step: 10359,loss: 0.0093769\n",
            "\n",
            "Global step: 10360,loss: 0.008855899\n",
            "\n",
            "Global step: 10361,loss: 0.009467507\n",
            "\n",
            "Global step: 10362,loss: 0.009022593\n",
            "\n",
            "Global step: 10363,loss: 0.009132205\n",
            "\n",
            "Global step: 10364,loss: 0.009178599\n",
            "\n",
            "Global step: 10365,loss: 0.008781868\n",
            "\n",
            "Global step: 10366,loss: 0.009114032\n",
            "\n",
            "Global step: 10367,loss: 0.00976024\n",
            "\n",
            "Global step: 10368,loss: 0.009820205\n",
            "\n",
            "Global step: 10369,loss: 0.0090444535\n",
            "\n",
            "Global step: 10370,loss: 0.009424962\n",
            "\n",
            "Global step: 10371,loss: 0.009395662\n",
            "\n",
            "Global step: 10372,loss: 0.009560616\n",
            "\n",
            "Global step: 10373,loss: 0.009013626\n",
            "\n",
            "Global step: 10374,loss: 0.009095445\n",
            "\n",
            "Global step: 10375,loss: 0.00948162\n",
            "\n",
            "Global step: 10376,loss: 0.009172077\n",
            "\n",
            "Global step: 10377,loss: 0.009094512\n",
            "\n",
            "Global step: 10378,loss: 0.009886107\n",
            "\n",
            "Global step: 10379,loss: 0.009893443\n",
            "\n",
            "Global step: 10380,loss: 0.00975523\n",
            "\n",
            "Global step: 10381,loss: 0.009995325\n",
            "\n",
            "Global step: 10382,loss: 0.00949279\n",
            "\n",
            "Global step: 10383,loss: 0.0091191\n",
            "\n",
            "Global step: 10384,loss: 0.010104769\n",
            "\n",
            "Global step: 10385,loss: 0.00920434\n",
            "\n",
            "Global step: 10386,loss: 0.009443892\n",
            "\n",
            "Global step: 10387,loss: 0.009347995\n",
            "\n",
            "Global step: 10388,loss: 0.009349811\n",
            "\n",
            "Global step: 10389,loss: 0.008896809\n",
            "\n",
            "Global step: 10390,loss: 0.009708515\n",
            "\n",
            "Global step: 10391,loss: 0.008967707\n",
            "\n",
            "Global step: 10392,loss: 0.009750862\n",
            "\n",
            "Global step: 10393,loss: 0.0090040825\n",
            "\n",
            "Global step: 10394,loss: 0.009425047\n",
            "\n",
            "Global step: 10395,loss: 0.009469811\n",
            "\n",
            "Global step: 10396,loss: 0.008967741\n",
            "\n",
            "Global step: 10397,loss: 0.009050245\n",
            "\n",
            "Global step: 10398,loss: 0.009153256\n",
            "\n",
            "Global step: 10399,loss: 0.009204275\n",
            "\n",
            "Global step: 10400,loss: 0.00932895\n",
            "\n",
            "Global step: 10401,loss: 0.0088673085\n",
            "\n",
            "Global step: 10402,loss: 0.008955299\n",
            "\n",
            "Global step: 10403,loss: 0.008949979\n",
            "\n",
            "Global step: 10404,loss: 0.009042369\n",
            "\n",
            "Global step: 10405,loss: 0.010041978\n",
            "\n",
            "Global step: 10406,loss: 0.009707282\n",
            "\n",
            "Global step: 10407,loss: 0.009185872\n",
            "\n",
            "Global step: 10408,loss: 0.009214485\n",
            "\n",
            "Global step: 10409,loss: 0.009224154\n",
            "\n",
            "Global step: 10410,loss: 0.009092311\n",
            "\n",
            "Global step: 10411,loss: 0.009193116\n",
            "\n",
            "Global step: 10412,loss: 0.009452332\n",
            "\n",
            "Global step: 10413,loss: 0.008962015\n",
            "\n",
            "Global step: 10414,loss: 0.009377077\n",
            "\n",
            "Global step: 10415,loss: 0.009180479\n",
            "\n",
            "Global step: 10416,loss: 0.009888614\n",
            "\n",
            "Global step: 10417,loss: 0.0088713225\n",
            "\n",
            "Global step: 10418,loss: 0.008665278\n",
            "\n",
            "Global step: 10419,loss: 0.00898739\n",
            "\n",
            "Global step: 10420,loss: 0.009545581\n",
            "\n",
            "Global step: 10421,loss: 0.009435228\n",
            "\n",
            "Global step: 10422,loss: 0.009315191\n",
            "\n",
            "Global step: 10423,loss: 0.009314017\n",
            "\n",
            "Global step: 10424,loss: 0.009607373\n",
            "\n",
            "Global step: 10425,loss: 0.009384063\n",
            "\n",
            "Global step: 10426,loss: 0.009671391\n",
            "\n",
            "Global step: 10427,loss: 0.009398634\n",
            "\n",
            "Global step: 10428,loss: 0.009519449\n",
            "\n",
            "Global step: 10429,loss: 0.009234415\n",
            "\n",
            "Global step: 10430,loss: 0.009500479\n",
            "\n",
            "Global step: 10431,loss: 0.009314685\n",
            "\n",
            "Global step: 10432,loss: 0.009160705\n",
            "\n",
            "Global step: 10433,loss: 0.009566961\n",
            "\n",
            "Global step: 10434,loss: 0.009540194\n",
            "\n",
            "Global step: 10435,loss: 0.009084682\n",
            "\n",
            "Global step: 10436,loss: 0.009593441\n",
            "\n",
            "Global step: 10437,loss: 0.00895219\n",
            "\n",
            "Global step: 10438,loss: 0.009807702\n",
            "\n",
            "Global step: 10439,loss: 0.009756322\n",
            "\n",
            "Global step: 10440,loss: 0.009637271\n",
            "\n",
            "Global step: 10441,loss: 0.00945714\n",
            "\n",
            "Global step: 10442,loss: 0.008936094\n",
            "\n",
            "Global step: 10443,loss: 0.009304026\n",
            "\n",
            "Global step: 10444,loss: 0.009638528\n",
            "\n",
            "Global step: 10445,loss: 0.009380367\n",
            "\n",
            "Global step: 10446,loss: 0.008914511\n",
            "\n",
            "Global step: 10447,loss: 0.009304588\n",
            "\n",
            "Global step: 10448,loss: 0.009613031\n",
            "\n",
            "Global step: 10449,loss: 0.009060897\n",
            "\n",
            "Global step: 10450,loss: 0.009012307\n",
            "\n",
            "Global step: 10451,loss: 0.009255996\n",
            "\n",
            "Global step: 10452,loss: 0.009085536\n",
            "\n",
            "Global step: 10453,loss: 0.009117021\n",
            "\n",
            "Global step: 10454,loss: 0.009156447\n",
            "\n",
            "Global step: 10455,loss: 0.009903706\n",
            "\n",
            "Global step: 10456,loss: 0.009070991\n",
            "\n",
            "Global step: 10457,loss: 0.009235919\n",
            "\n",
            "Global step: 10458,loss: 0.009624007\n",
            "\n",
            "Global step: 10459,loss: 0.009895421\n",
            "\n",
            "Global step: 10460,loss: 0.0093071265\n",
            "\n",
            "Global step: 10461,loss: 0.0089612\n",
            "\n",
            "Global step: 10462,loss: 0.009115264\n",
            "\n",
            "Global step: 10463,loss: 0.008886713\n",
            "\n",
            "Global step: 10464,loss: 0.009568531\n",
            "\n",
            "Global step: 10465,loss: 0.009553395\n",
            "\n",
            "Global step: 10466,loss: 0.009483866\n",
            "\n",
            "Global step: 10467,loss: 0.009362195\n",
            "\n",
            "Global step: 10468,loss: 0.009401153\n",
            "\n",
            "Global step: 10469,loss: 0.009381018\n",
            "\n",
            "Global step: 10470,loss: 0.009172586\n",
            "\n",
            "Global step: 10471,loss: 0.009530229\n",
            "\n",
            "Global step: 10472,loss: 0.0098077385\n",
            "\n",
            "Global step: 10473,loss: 0.009077126\n",
            "\n",
            "Global step: 10474,loss: 0.009114956\n",
            "\n",
            "Global step: 10475,loss: 0.0094074\n",
            "\n",
            "Global step: 10476,loss: 0.009426228\n",
            "\n",
            "Global step: 10477,loss: 0.0096661\n",
            "\n",
            "Global step: 10478,loss: 0.010449362\n",
            "\n",
            "Global step: 10479,loss: 0.00913897\n",
            "\n",
            "Global step: 10480,loss: 0.009521543\n",
            "\n",
            "Global step: 10481,loss: 0.009064409\n",
            "\n",
            "Global step: 10482,loss: 0.00958677\n",
            "\n",
            "Global step: 10483,loss: 0.009383008\n",
            "\n",
            "Global step: 10484,loss: 0.009627831\n",
            "\n",
            "Global step: 10485,loss: 0.009223869\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 10485,val_loss: 0.014386506359043875\n",
            "\n",
            "Training for epoch 50/50:\n",
            "Global step: 10486,loss: 0.00979578\n",
            "\n",
            "Global step: 10487,loss: 0.00848114\n",
            "\n",
            "Global step: 10488,loss: 0.009594617\n",
            "\n",
            "Global step: 10489,loss: 0.009233149\n",
            "\n",
            "Global step: 10490,loss: 0.009230746\n",
            "\n",
            "Global step: 10491,loss: 0.009132704\n",
            "\n",
            "Global step: 10492,loss: 0.009124318\n",
            "\n",
            "Global step: 10493,loss: 0.009894157\n",
            "\n",
            "Global step: 10494,loss: 0.009004562\n",
            "\n",
            "Global step: 10495,loss: 0.009932079\n",
            "\n",
            "Global step: 10496,loss: 0.00929989\n",
            "\n",
            "Global step: 10497,loss: 0.009163698\n",
            "\n",
            "Global step: 10498,loss: 0.008917455\n",
            "\n",
            "Global step: 10499,loss: 0.009404331\n",
            "\n",
            "Global step: 10500,loss: 0.008861321\n",
            "\n",
            "Global step: 10501,loss: 0.009434884\n",
            "\n",
            "Global step: 10502,loss: 0.009132938\n",
            "\n",
            "Global step: 10503,loss: 0.008974183\n",
            "\n",
            "Global step: 10504,loss: 0.00940049\n",
            "\n",
            "Global step: 10505,loss: 0.008907619\n",
            "\n",
            "Global step: 10506,loss: 0.009609409\n",
            "\n",
            "Global step: 10507,loss: 0.008759863\n",
            "\n",
            "Global step: 10508,loss: 0.009019473\n",
            "\n",
            "Global step: 10509,loss: 0.009210669\n",
            "\n",
            "Global step: 10510,loss: 0.009194507\n",
            "\n",
            "Global step: 10511,loss: 0.009220921\n",
            "\n",
            "Global step: 10512,loss: 0.009013112\n",
            "\n",
            "Global step: 10513,loss: 0.009152445\n",
            "\n",
            "Global step: 10514,loss: 0.00846524\n",
            "\n",
            "Global step: 10515,loss: 0.008624796\n",
            "\n",
            "Global step: 10516,loss: 0.009194944\n",
            "\n",
            "Global step: 10517,loss: 0.009407952\n",
            "\n",
            "Global step: 10518,loss: 0.008678621\n",
            "\n",
            "Global step: 10519,loss: 0.009728262\n",
            "\n",
            "Global step: 10520,loss: 0.008553097\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 10521.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:10:29.418991 140167885084416 supervisor.py:1050] Recording summary at step 10521.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 10521,loss: 0.008387123\n",
            "\n",
            "Global step: 10522,loss: 0.009517279\n",
            "\n",
            "Global step: 10523,loss: 0.009065425\n",
            "\n",
            "Global step: 10524,loss: 0.009295419\n",
            "\n",
            "Global step: 10525,loss: 0.009494383\n",
            "\n",
            "Global step: 10526,loss: 0.009045905\n",
            "\n",
            "Global step: 10527,loss: 0.009662258\n",
            "\n",
            "Global step: 10528,loss: 0.008939296\n",
            "\n",
            "Global step: 10529,loss: 0.009446768\n",
            "\n",
            "Global step: 10530,loss: 0.008947546\n",
            "\n",
            "Global step: 10531,loss: 0.009256066\n",
            "\n",
            "Global step: 10532,loss: 0.009764227\n",
            "\n",
            "Global step: 10533,loss: 0.009062987\n",
            "\n",
            "Global step: 10534,loss: 0.009376945\n",
            "\n",
            "Global step: 10535,loss: 0.009486194\n",
            "\n",
            "Global step: 10536,loss: 0.009700551\n",
            "\n",
            "Global step: 10537,loss: 0.009469385\n",
            "\n",
            "Global step: 10538,loss: 0.00952445\n",
            "\n",
            "Global step: 10539,loss: 0.008844981\n",
            "\n",
            "Global step: 10540,loss: 0.009362525\n",
            "\n",
            "Global step: 10541,loss: 0.009357362\n",
            "\n",
            "Global step: 10542,loss: 0.009008368\n",
            "\n",
            "Global step: 10543,loss: 0.009000997\n",
            "\n",
            "Global step: 10544,loss: 0.009799469\n",
            "\n",
            "Global step: 10545,loss: 0.009849781\n",
            "\n",
            "Global step: 10546,loss: 0.009549172\n",
            "\n",
            "Global step: 10547,loss: 0.009223022\n",
            "\n",
            "Global step: 10548,loss: 0.008774566\n",
            "\n",
            "Global step: 10549,loss: 0.009669494\n",
            "\n",
            "Global step: 10550,loss: 0.009289169\n",
            "\n",
            "Global step: 10551,loss: 0.00945364\n",
            "\n",
            "Global step: 10552,loss: 0.008661748\n",
            "\n",
            "Global step: 10553,loss: 0.009028098\n",
            "\n",
            "Global step: 10554,loss: 0.008688566\n",
            "\n",
            "Global step: 10555,loss: 0.00891176\n",
            "\n",
            "Global step: 10556,loss: 0.009391556\n",
            "\n",
            "Global step: 10557,loss: 0.010019305\n",
            "\n",
            "Global step: 10558,loss: 0.008951373\n",
            "\n",
            "Global step: 10559,loss: 0.009649376\n",
            "\n",
            "Global step: 10560,loss: 0.008890697\n",
            "\n",
            "Global step: 10561,loss: 0.0089151\n",
            "\n",
            "Global step: 10562,loss: 0.008980554\n",
            "\n",
            "Global step: 10563,loss: 0.0099853985\n",
            "\n",
            "Global step: 10564,loss: 0.009513486\n",
            "\n",
            "Global step: 10565,loss: 0.009384718\n",
            "\n",
            "Global step: 10566,loss: 0.009492252\n",
            "\n",
            "Global step: 10567,loss: 0.008796982\n",
            "\n",
            "Global step: 10568,loss: 0.008898555\n",
            "\n",
            "Global step: 10569,loss: 0.009396752\n",
            "\n",
            "Global step: 10570,loss: 0.008522087\n",
            "\n",
            "Global step: 10571,loss: 0.008933728\n",
            "\n",
            "Global step: 10572,loss: 0.009324881\n",
            "\n",
            "Global step: 10573,loss: 0.009331757\n",
            "\n",
            "Global step: 10574,loss: 0.008472014\n",
            "\n",
            "Global step: 10575,loss: 0.009258046\n",
            "\n",
            "Global step: 10576,loss: 0.00894476\n",
            "\n",
            "Global step: 10577,loss: 0.009385471\n",
            "\n",
            "Global step: 10578,loss: 0.009082443\n",
            "\n",
            "Global step: 10579,loss: 0.009383682\n",
            "\n",
            "Global step: 10580,loss: 0.008909946\n",
            "\n",
            "Global step: 10581,loss: 0.009782955\n",
            "\n",
            "Global step: 10582,loss: 0.009086801\n",
            "\n",
            "Global step: 10583,loss: 0.008924035\n",
            "\n",
            "Global step: 10584,loss: 0.00966126\n",
            "\n",
            "Global step: 10585,loss: 0.009061138\n",
            "\n",
            "Global step: 10586,loss: 0.009236626\n",
            "\n",
            "Global step: 10587,loss: 0.009123194\n",
            "\n",
            "Global step: 10588,loss: 0.009266619\n",
            "\n",
            "Global step: 10589,loss: 0.008797757\n",
            "\n",
            "Global step: 10590,loss: 0.009150494\n",
            "\n",
            "Global step: 10591,loss: 0.009440626\n",
            "\n",
            "Global step: 10592,loss: 0.008994751\n",
            "\n",
            "Global step: 10593,loss: 0.009425498\n",
            "\n",
            "Global step: 10594,loss: 0.00907014\n",
            "\n",
            "Global step: 10595,loss: 0.009027386\n",
            "\n",
            "Global step: 10596,loss: 0.0094681075\n",
            "\n",
            "Global step: 10597,loss: 0.009303332\n",
            "\n",
            "Global step: 10598,loss: 0.009222553\n",
            "\n",
            "Global step: 10599,loss: 0.009604404\n",
            "\n",
            "Global step: 10600,loss: 0.008914088\n",
            "\n",
            "Global step: 10601,loss: 0.009458447\n",
            "\n",
            "Global step: 10602,loss: 0.0097050015\n",
            "\n",
            "Global step: 10603,loss: 0.009581727\n",
            "\n",
            "Global step: 10604,loss: 0.009223883\n",
            "\n",
            "Global step: 10605,loss: 0.009268411\n",
            "\n",
            "Global step: 10606,loss: 0.009142301\n",
            "\n",
            "Global step: 10607,loss: 0.008891815\n",
            "\n",
            "Global step: 10608,loss: 0.009050203\n",
            "\n",
            "Global step: 10609,loss: 0.009254906\n",
            "\n",
            "Global step: 10610,loss: 0.009588005\n",
            "\n",
            "Global step: 10611,loss: 0.009182805\n",
            "\n",
            "Global step: 10612,loss: 0.010138075\n",
            "\n",
            "Global step: 10613,loss: 0.009471917\n",
            "\n",
            "Global step: 10614,loss: 0.009467863\n",
            "\n",
            "Global step: 10615,loss: 0.00870286\n",
            "\n",
            "Global step: 10616,loss: 0.009574345\n",
            "\n",
            "Global step: 10617,loss: 0.0089676175\n",
            "\n",
            "Global step: 10618,loss: 0.009092222\n",
            "\n",
            "Global step: 10619,loss: 0.009560658\n",
            "\n",
            "Global step: 10620,loss: 0.009262697\n",
            "\n",
            "Global step: 10621,loss: 0.009328468\n",
            "\n",
            "Global step: 10622,loss: 0.008646279\n",
            "\n",
            "Global step: 10623,loss: 0.00938089\n",
            "\n",
            "Global step: 10624,loss: 0.0086590685\n",
            "\n",
            "Global step: 10625,loss: 0.009220346\n",
            "\n",
            "Global step: 10626,loss: 0.009250091\n",
            "\n",
            "Global step: 10627,loss: 0.0094192885\n",
            "\n",
            "Global step: 10628,loss: 0.009454791\n",
            "\n",
            "Global step: 10629,loss: 0.008737635\n",
            "\n",
            "Global step: 10630,loss: 0.009063059\n",
            "\n",
            "Global step: 10631,loss: 0.009050834\n",
            "\n",
            "Global step: 10632,loss: 0.008461829\n",
            "\n",
            "Global step: 10633,loss: 0.009426729\n",
            "\n",
            "Global step: 10634,loss: 0.009279849\n",
            "\n",
            "Global step: 10635,loss: 0.0094702775\n",
            "\n",
            "Global step: 10636,loss: 0.009033127\n",
            "\n",
            "Global step: 10637,loss: 0.008889815\n",
            "\n",
            "Global step: 10638,loss: 0.009861647\n",
            "\n",
            "Global step: 10639,loss: 0.009984603\n",
            "\n",
            "Global step: 10640,loss: 0.009500612\n",
            "\n",
            "Global step: 10641,loss: 0.009308013\n",
            "\n",
            "Global step: 10642,loss: 0.00869207\n",
            "\n",
            "Global step: 10643,loss: 0.009636963\n",
            "\n",
            "Global step: 10644,loss: 0.009124486\n",
            "\n",
            "Global step: 10645,loss: 0.009078484\n",
            "\n",
            "Global step: 10646,loss: 0.009353273\n",
            "\n",
            "Global step: 10647,loss: 0.009120092\n",
            "\n",
            "Global step: 10648,loss: 0.008769346\n",
            "\n",
            "Global step: 10649,loss: 0.009073795\n",
            "\n",
            "Global step: 10650,loss: 0.00931929\n",
            "\n",
            "Global step: 10651,loss: 0.009062465\n",
            "\n",
            "Global step: 10652,loss: 0.009722775\n",
            "\n",
            "Global step: 10653,loss: 0.008830354\n",
            "\n",
            "Global step: 10654,loss: 0.009062381\n",
            "\n",
            "Global step: 10655,loss: 0.009933901\n",
            "\n",
            "Global step: 10656,loss: 0.009202279\n",
            "\n",
            "Global step: 10657,loss: 0.009008829\n",
            "\n",
            "Global step: 10658,loss: 0.009246396\n",
            "\n",
            "Global step: 10659,loss: 0.008856739\n",
            "\n",
            "Global step: 10660,loss: 0.009389289\n",
            "\n",
            "Global step: 10661,loss: 0.009122541\n",
            "\n",
            "Global step: 10662,loss: 0.009802229\n",
            "\n",
            "Global step: 10663,loss: 0.0092893485\n",
            "\n",
            "Global step: 10664,loss: 0.009013446\n",
            "\n",
            "Global step: 10665,loss: 0.008895273\n",
            "\n",
            "Global step: 10666,loss: 0.009845031\n",
            "\n",
            "Global step: 10667,loss: 0.0096072415\n",
            "\n",
            "Global step: 10668,loss: 0.0092575895\n",
            "\n",
            "Global step: 10669,loss: 0.008835312\n",
            "\n",
            "Global step: 10670,loss: 0.0092143845\n",
            "\n",
            "Global step: 10671,loss: 0.009244998\n",
            "\n",
            "Global step: 10672,loss: 0.00924371\n",
            "\n",
            "Global step: 10673,loss: 0.009131368\n",
            "\n",
            "Global step: 10674,loss: 0.008661119\n",
            "\n",
            "Global step: 10675,loss: 0.008802925\n",
            "\n",
            "Global step: 10676,loss: 0.009656456\n",
            "\n",
            "Global step: 10677,loss: 0.009452121\n",
            "\n",
            "Global step: 10678,loss: 0.008939699\n",
            "\n",
            "Global step: 10679,loss: 0.008862726\n",
            "\n",
            "Global step: 10680,loss: 0.0090188915\n",
            "\n",
            "Global step: 10681,loss: 0.009205072\n",
            "\n",
            "Global step: 10682,loss: 0.009018322\n",
            "\n",
            "Global step: 10683,loss: 0.00878062\n",
            "\n",
            "Global step: 10684,loss: 0.009305491\n",
            "\n",
            "Global step: 10685,loss: 0.008657543\n",
            "\n",
            "Global step: 10686,loss: 0.0090753585\n",
            "\n",
            "Global step: 10687,loss: 0.00939077\n",
            "\n",
            "Global step: 10688,loss: 0.009417773\n",
            "\n",
            "Global step: 10689,loss: 0.009130982\n",
            "\n",
            "Global step: 10690,loss: 0.009051408\n",
            "\n",
            "Global step: 10691,loss: 0.0087445\n",
            "\n",
            "Global step: 10692,loss: 0.008835689\n",
            "\n",
            "Global step: 10693,loss: 0.008936517\n",
            "\n",
            "Global step: 10694,loss: 0.008587217\n",
            "\n",
            "Global step: 10695,loss: 0.009032981\n",
            "\n",
            "Global step: 10696,loss: 0.0096409675\n",
            "\n",
            "Global step: 10697,loss: 0.009324152\n",
            "\n",
            "Global step: 10698,loss: 0.00974449\n",
            "\n",
            "Global step: 10699,loss: 0.009923217\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 10699,Val_Loss: 0.014226198196411133,  Val_acc: 0.9983552631578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:11:13.697864 140170173187968 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:15.296746 140170173187968 <ipython-input-9-b2423ad1e833>:16] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Beginning Eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "outputId": "9ee2d9cc-81eb-4f4b-92d9-ebad4c02b903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "cfg.is_training=False\n",
        "\n",
        "try:\n",
        "  def main(_):\n",
        "      tf.logging.info(' Loading Graph...')\n",
        "      num_label = 10\n",
        "      model = CapsNet()\n",
        "      tf.logging.info(' Graph loaded')\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "      sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      if cfg.is_training:\n",
        "          tf.logging.info(' Start training...')\n",
        "          train(model, sv, num_label)\n",
        "          tf.logging.info('Training done')\n",
        "      else:\n",
        "          evaluation(model, sv, num_label)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"\\Completed\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:15.326179 140170173187968 <ipython-input-10-d583bcff4fbf>:5]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:16.480002 140170173187968 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:16.486317 140170173187968 <ipython-input-10-d583bcff4fbf>:8]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir_try8/model_epoch_0049_step_10699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:16.933589 140170173187968 saver.py:1284] Restoring parameters from logdir_try8/model_epoch_0049_step_10699\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 23:11:17.224179 140170173187968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:17.238649 140170173187968 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:17.266936 140170173187968 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:49.496747 140170173187968 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:50.165657 140170173187968 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir_try8/model_epoch_0049_step_10699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:50.185485 140170173187968 saver.py:1284] Restoring parameters from logdir_try8/model_epoch_0049_step_10699\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Model restored!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:51.999236 140170173187968 <ipython-input-8-04cc3fa079aa>:117] Model restored!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy is 0.81455078125:\n",
            "\n",
            "Test accuracy has been saved to results/test_acc\n",
            "INFO:tensorflow:Recording summary at step 10700.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 23:11:56.389215 140166173808384 supervisor.py:1050] Recording summary at step 10700.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W8Z-9sVTMtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}