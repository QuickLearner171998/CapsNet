{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "414a9dd1-8264-4dea-f266-183d2809c03a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "95027789-4ee1-4c30-fc75-109216ad8739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   data     logdir_try8   tensorboard.ipynb   Try-3   try-5\n",
            " capsnet_tf.py\t     logdir   results\t    Try-1\t        Try-4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "0d07e289-e70d-4eff-b0f2-2ddc865c8467"
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "import random\n",
        "import skimage.io\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3O76C7FuuN",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAbq_6IFxbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_aug(images,labels,angel,resize_rate, populate):\n",
        "\n",
        "\n",
        "  new_img = []\n",
        "  new_label = []\n",
        "  print(\"\\nStarting Data Augmentation\")\n",
        "  for img,label in zip(images,labels):\n",
        "\n",
        "    image = img\n",
        "    #label1 = label.reshape(1,1)\n",
        "    #flip = random.randint(0, 1)\n",
        "    size = image.shape[0]\n",
        "    rsize = random.randint(np.floor(resize_rate*size),size)\n",
        "    w_s = random.randint(0,size - rsize)\n",
        "    h_s = random.randint(0,size - rsize)\n",
        "    sh = random.random()/2-0.25\n",
        "    rotate_angel = random.random()/180*np.pi*angel\n",
        "    # Create Afine transform\n",
        "    afine_tf = transform.AffineTransform(shear=sh,rotation=rotate_angel)\n",
        "    # Apply transform to image data\n",
        "    image = transform.warp(image, inverse_map=afine_tf,mode='edge')\n",
        "    #label1 = transform.warp(label1, inverse_map=afine_tf,mode='edge')\n",
        "    # # Randomly corpping image frame\n",
        "    #image = image[w_s:w_s+size,h_s:h_s+size,:]\n",
        "    #label1 = label1[w_s:w_s+size,h_s:h_s+size]\n",
        "    new_img.append(image)\n",
        "    new_label.append(label)\n",
        "  \n",
        "  print(\"\\nFinished Augmentation\")\n",
        "  if(populate):\n",
        "\n",
        "    final_trX = np.asarray(images + new_img)\n",
        "    final_labels = np.asarray(labels + new_label)\n",
        "    return final_trX, final_labels\n",
        "  return (np.array(new_img)).astype('float32'), (np.array(labels,dtype='int32').astype('int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RGsn5xMxlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "# loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "# trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "# fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "# loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "# trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "# trX = trainX[:55000] / 255.\n",
        "# trY = trainY[:55000]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIvr5lF9NHOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X, Y = data_aug(list(trX),list(trY),angel=5,resize_rate=0.9,populate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh3BjXYyM8qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trY.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "        trX = trainX[:55000] / 255.\n",
        "        trY = trainY[:55000]\n",
        "\n",
        "        trX, trY = data_aug(list(trX),list(trY),angel=5,resize_rate=0.9,populate=False)\n",
        "\n",
        "        valX = trainX[55000:, ] / 255.\n",
        "        valY = trainY[55000:]\n",
        "\n",
        "        num_tr_batch = 55000 // batch_size\n",
        "        num_val_batch = 5000 // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=True)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 256, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 16, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.9,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "# ############################\n",
        "# #   distributed setting    #\n",
        "# ############################\n",
        "# flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "# flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "# flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                  best_val_loss = val_loss\n",
        "                  best_val_acc = val_acc\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######NOT SAVING MODEL #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1591de07-fc2f-4006-db39-ceaf2430764e"
      },
      "source": [
        "cfg.is_training=True\n",
        "# try:\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n",
        "\n",
        "# except:\n",
        "# print(\"\\nBeginning Eval\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:35:42.487443 140508081637248 <ipython-input-12-0eedbf08ec32>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "WARNING:tensorflow:From <ipython-input-7-8bf0e7710aa2>:56: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.677216 140508081637248 deprecation.py:323] From <ipython-input-7-8bf0e7710aa2>:56: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.880861 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.887304 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.891812 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.897613 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.903684 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-8bf0e7710aa2>:61: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.920552 140508081637248 deprecation.py:323] From <ipython-input-7-8bf0e7710aa2>:61: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.945137 140508081637248 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:51.951525 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:52.150729 140508081637248 deprecation.py:323] From <ipython-input-9-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:52.321627 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:35:52.941604 140508081637248 <ipython-input-9-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:35:52.944010 140508081637248 <ipython-input-12-0eedbf08ec32>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-0eedbf08ec32>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:35:52.946135 140508081637248 deprecation.py:323] From <ipython-input-12-0eedbf08ec32>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:35:53.248970 140508081637248 <ipython-input-12-0eedbf08ec32>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:02.932265 140508081637248 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:02.956902 140508081637248 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:37.809828 140508081637248 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:39.220708 140508081637248 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/16:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:40.875933 140504762959616 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:36:45.944998 140504771352320 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.8541331\n",
            "\n",
            "Global step: 1,loss: 0.8472937\n",
            "\n",
            "Global step: 2,loss: 0.84629244\n",
            "\n",
            "Global step: 3,loss: 0.81596494\n",
            "\n",
            "Global step: 4,loss: 0.8046347\n",
            "\n",
            "Global step: 5,loss: 0.7637103\n",
            "\n",
            "Global step: 6,loss: 0.6995053\n",
            "\n",
            "Global step: 7,loss: 0.63944757\n",
            "\n",
            "Global step: 8,loss: 0.5962452\n",
            "\n",
            "Global step: 9,loss: 0.58640504\n",
            "\n",
            "Global step: 10,loss: 0.5169201\n",
            "\n",
            "Global step: 11,loss: 0.48600578\n",
            "\n",
            "Global step: 12,loss: 0.44532296\n",
            "\n",
            "Global step: 13,loss: 0.43543807\n",
            "\n",
            "Global step: 14,loss: 0.3873208\n",
            "\n",
            "Global step: 15,loss: 0.37241125\n",
            "\n",
            "Global step: 16,loss: 0.34879196\n",
            "\n",
            "Global step: 17,loss: 0.34095806\n",
            "\n",
            "Global step: 18,loss: 0.34255192\n",
            "\n",
            "Global step: 19,loss: 0.31688374\n",
            "\n",
            "Global step: 20,loss: 0.31780183\n",
            "\n",
            "Global step: 21,loss: 0.3148379\n",
            "\n",
            "Global step: 22,loss: 0.31106633\n",
            "\n",
            "Global step: 23,loss: 0.28230843\n",
            "\n",
            "Global step: 24,loss: 0.2780279\n",
            "\n",
            "Global step: 25,loss: 0.27151328\n",
            "\n",
            "Global step: 26,loss: 0.26102152\n",
            "\n",
            "Global step: 27,loss: 0.24437454\n",
            "\n",
            "Global step: 28,loss: 0.2480422\n",
            "\n",
            "Global step: 29,loss: 0.23505557\n",
            "\n",
            "Global step: 30,loss: 0.2562781\n",
            "\n",
            "Global step: 31,loss: 0.23194417\n",
            "\n",
            "Global step: 32,loss: 0.23232949\n",
            "\n",
            "Global step: 33,loss: 0.21725541\n",
            "\n",
            "Global step: 34,loss: 0.1966795\n",
            "\n",
            "Global step: 35,loss: 0.20516022\n",
            "\n",
            "Global step: 36,loss: 0.18852934\n",
            "\n",
            "Global step: 37,loss: 0.19773605\n",
            "\n",
            "Global step: 38,loss: 0.18952584\n",
            "\n",
            "Global step: 39,loss: 0.20621875\n",
            "\n",
            "Global step: 40,loss: 0.16996454\n",
            "\n",
            "Global step: 41,loss: 0.18471225\n",
            "\n",
            "Global step: 42,loss: 0.18580353\n",
            "\n",
            "Global step: 43,loss: 0.16303962\n",
            "\n",
            "Global step: 44,loss: 0.16785276\n",
            "\n",
            "Global step: 45,loss: 0.15233205\n",
            "\n",
            "Global step: 46,loss: 0.15414393\n",
            "\n",
            "Global step: 47,loss: 0.16011858\n",
            "\n",
            "Global step: 48,loss: 0.16582206\n",
            "\n",
            "Global step: 49,loss: 0.15524413\n",
            "\n",
            "Global step: 50,loss: 0.1549728\n",
            "\n",
            "Global step: 51,loss: 0.14422157\n",
            "\n",
            "Global step: 52,loss: 0.15207365\n",
            "\n",
            "Global step: 53,loss: 0.14721373\n",
            "\n",
            "Global step: 54,loss: 0.12770978\n",
            "\n",
            "Global step: 55,loss: 0.16213003\n",
            "\n",
            "Global step: 56,loss: 0.1477965\n",
            "\n",
            "Global step: 57,loss: 0.13984367\n",
            "\n",
            "Global step: 58,loss: 0.1270074\n",
            "\n",
            "Global step: 59,loss: 0.12451648\n",
            "\n",
            "Global step: 60,loss: 0.13907841\n",
            "\n",
            "Global step: 61,loss: 0.13137832\n",
            "\n",
            "Global step: 62,loss: 0.1178655\n",
            "\n",
            "Global step: 63,loss: 0.13556679\n",
            "\n",
            "Global step: 64,loss: 0.12438299\n",
            "\n",
            "Global step: 65,loss: 0.12998334\n",
            "\n",
            "Global step: 66,loss: 0.122149006\n",
            "\n",
            "Global step: 67,loss: 0.114282906\n",
            "\n",
            "Global step: 68,loss: 0.12552376\n",
            "\n",
            "Global step: 69,loss: 0.12161697\n",
            "\n",
            "Global step: 70,loss: 0.10666194\n",
            "\n",
            "Global step: 71,loss: 0.10762921\n",
            "\n",
            "Global step: 72,loss: 0.117932156\n",
            "\n",
            "Global step: 73,loss: 0.10644002\n",
            "\n",
            "Global step: 74,loss: 0.1063288\n",
            "\n",
            "Global step: 75,loss: 0.112462685\n",
            "\n",
            "Global step: 76,loss: 0.09380445\n",
            "\n",
            "Global step: 77,loss: 0.113546714\n",
            "\n",
            "Global step: 78,loss: 0.095357984\n",
            "\n",
            "Global step: 79,loss: 0.10190138\n",
            "\n",
            "Global step: 80,loss: 0.10104501\n",
            "\n",
            "Global step: 81,loss: 0.09841196\n",
            "\n",
            "Global step: 82,loss: 0.0994288\n",
            "\n",
            "Global step: 83,loss: 0.09804561\n",
            "\n",
            "Global step: 84,loss: 0.092089854\n",
            "\n",
            "Global step: 85,loss: 0.11304638\n",
            "\n",
            "Global step: 86,loss: 0.10444246\n",
            "\n",
            "Global step: 87,loss: 0.10550094\n",
            "\n",
            "Global step: 88,loss: 0.093523845\n",
            "\n",
            "Global step: 89,loss: 0.097528145\n",
            "\n",
            "Global step: 90,loss: 0.09789044\n",
            "\n",
            "Global step: 91,loss: 0.09184049\n",
            "\n",
            "Global step: 92,loss: 0.100197166\n",
            "\n",
            "Global step: 93,loss: 0.10124335\n",
            "\n",
            "Global step: 94,loss: 0.09215677\n",
            "\n",
            "Global step: 95,loss: 0.08715464\n",
            "\n",
            "Global step: 96,loss: 0.099675655\n",
            "\n",
            "Global step: 97,loss: 0.089880496\n",
            "\n",
            "Global step: 98,loss: 0.09375557\n",
            "\n",
            "Global step: 99,loss: 0.0953435\n",
            "\n",
            "Global step: 100,loss: 0.10004595\n",
            "\n",
            "Global step: 101,loss: 0.092295855\n",
            "\n",
            "Global step: 102,loss: 0.09172365\n",
            "\n",
            "Global step: 103,loss: 0.08319649\n",
            "\n",
            "Global step: 104,loss: 0.0801886\n",
            "\n",
            "Global step: 105,loss: 0.100840926\n",
            "\n",
            "Global step: 106,loss: 0.09114231\n",
            "\n",
            "Global step: 107,loss: 0.0822163\n",
            "\n",
            "Global step: 108,loss: 0.07581154\n",
            "\n",
            "Global step: 109,loss: 0.094614826\n",
            "\n",
            "Global step: 110,loss: 0.084383875\n",
            "\n",
            "Global step: 111,loss: 0.07787281\n",
            "\n",
            "Global step: 112,loss: 0.08015018\n",
            "\n",
            "Global step: 113,loss: 0.08382067\n",
            "\n",
            "Global step: 114,loss: 0.09058371\n",
            "\n",
            "Global step: 115,loss: 0.07459231\n",
            "\n",
            "Global step: 116,loss: 0.07936154\n",
            "\n",
            "Global step: 117,loss: 0.07244607\n",
            "\n",
            "Global step: 118,loss: 0.08281201\n",
            "\n",
            "Global step: 119,loss: 0.07347168\n",
            "\n",
            "Global step: 120,loss: 0.09728059\n",
            "\n",
            "Global step: 121,loss: 0.07813275\n",
            "\n",
            "Global step: 122,loss: 0.077157184\n",
            "\n",
            "Global step: 123,loss: 0.081588864\n",
            "\n",
            "Global step: 124,loss: 0.071251765\n",
            "\n",
            "Global step: 125,loss: 0.093787715\n",
            "\n",
            "Global step: 126,loss: 0.07440041\n",
            "\n",
            "Global step: 127,loss: 0.078532055\n",
            "\n",
            "Global step: 128,loss: 0.09223662\n",
            "\n",
            "Global step: 129,loss: 0.085974246\n",
            "\n",
            "Global step: 130,loss: 0.073120706\n",
            "\n",
            "Global step: 131,loss: 0.08356946\n",
            "\n",
            "Global step: 132,loss: 0.06478928\n",
            "\n",
            "Global step: 133,loss: 0.07967096\n",
            "\n",
            "Global step: 134,loss: 0.08090128\n",
            "\n",
            "Global step: 135,loss: 0.100466475\n",
            "\n",
            "Global step: 136,loss: 0.06948911\n",
            "\n",
            "Global step: 137,loss: 0.06939705\n",
            "\n",
            "Global step: 138,loss: 0.07419188\n",
            "\n",
            "Global step: 139,loss: 0.08289072\n",
            "\n",
            "Global step: 140,loss: 0.08695105\n",
            "\n",
            "Global step: 141,loss: 0.08383122\n",
            "\n",
            "Global step: 142,loss: 0.07959692\n",
            "\n",
            "Global step: 143,loss: 0.07315858\n",
            "\n",
            "Global step: 144,loss: 0.07513502\n",
            "\n",
            "Global step: 145,loss: 0.07880288\n",
            "\n",
            "Global step: 146,loss: 0.06745039\n",
            "\n",
            "Global step: 147,loss: 0.07464774\n",
            "\n",
            "Global step: 148,loss: 0.06372702\n",
            "\n",
            "Global step: 149,loss: 0.0872705\n",
            "\n",
            "Global step: 150,loss: 0.067654595\n",
            "\n",
            "Global step: 151,loss: 0.07113975\n",
            "\n",
            "Global step: 152,loss: 0.07069967\n",
            "\n",
            "Global step: 153,loss: 0.07013523\n",
            "\n",
            "Global step: 154,loss: 0.07032868\n",
            "\n",
            "Global step: 155,loss: 0.06436652\n",
            "\n",
            "Global step: 156,loss: 0.07355049\n",
            "\n",
            "Global step: 157,loss: 0.07103732\n",
            "\n",
            "Global step: 158,loss: 0.07414789\n",
            "\n",
            "Global step: 159,loss: 0.07139017\n",
            "\n",
            "Global step: 160,loss: 0.08842738\n",
            "\n",
            "Global step: 161,loss: 0.0738266\n",
            "\n",
            "Global step: 162,loss: 0.06489734\n",
            "\n",
            "Global step: 163,loss: 0.07401787\n",
            "\n",
            "Global step: 164,loss: 0.074376576\n",
            "\n",
            "Global step: 165,loss: 0.058019415\n",
            "\n",
            "Global step: 166,loss: 0.08207176\n",
            "\n",
            "Global step: 167,loss: 0.0698515\n",
            "\n",
            "Global step: 168,loss: 0.07293783\n",
            "\n",
            "Global step: 169,loss: 0.061217405\n",
            "\n",
            "Global step: 170,loss: 0.07425616\n",
            "\n",
            "Global step: 171,loss: 0.07771088\n",
            "\n",
            "Global step: 172,loss: 0.071335435\n",
            "\n",
            "Global step: 173,loss: 0.070345774\n",
            "\n",
            "Global step: 174,loss: 0.07463996\n",
            "\n",
            "Global step: 175,loss: 0.0693845\n",
            "\n",
            "Global step: 176,loss: 0.064482324\n",
            "\n",
            "Global step: 177,loss: 0.06798431\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.5016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:38:39.415653 140504762959616 supervisor.py:1099] global_step/sec: 1.5016\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 178.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:38:39.731212 140504771352320 supervisor.py:1050] Recording summary at step 178.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 178,loss: 0.06857155\n",
            "\n",
            "Global step: 179,loss: 0.076735325\n",
            "\n",
            "Global step: 180,loss: 0.07654287\n",
            "\n",
            "Global step: 181,loss: 0.063023195\n",
            "\n",
            "Global step: 182,loss: 0.081937134\n",
            "\n",
            "Global step: 183,loss: 0.06537986\n",
            "\n",
            "Global step: 184,loss: 0.08005868\n",
            "\n",
            "Global step: 185,loss: 0.06662923\n",
            "\n",
            "Global step: 186,loss: 0.07383479\n",
            "\n",
            "Global step: 187,loss: 0.06812533\n",
            "\n",
            "Global step: 188,loss: 0.05990274\n",
            "\n",
            "Global step: 189,loss: 0.06122957\n",
            "\n",
            "Global step: 190,loss: 0.064530194\n",
            "\n",
            "Global step: 191,loss: 0.064158276\n",
            "\n",
            "Global step: 192,loss: 0.06904024\n",
            "\n",
            "Global step: 193,loss: 0.05482738\n",
            "\n",
            "Global step: 194,loss: 0.058386326\n",
            "\n",
            "Global step: 195,loss: 0.059490196\n",
            "\n",
            "Global step: 196,loss: 0.06391402\n",
            "\n",
            "Global step: 197,loss: 0.06380987\n",
            "\n",
            "Global step: 198,loss: 0.06286204\n",
            "\n",
            "Global step: 199,loss: 0.06749139\n",
            "\n",
            "Global step: 200,loss: 0.057703096\n",
            "\n",
            "Global step: 201,loss: 0.06458191\n",
            "\n",
            "Global step: 202,loss: 0.0645623\n",
            "\n",
            "Global step: 203,loss: 0.051637094\n",
            "\n",
            "Global step: 204,loss: 0.05815781\n",
            "\n",
            "Global step: 205,loss: 0.06186571\n",
            "\n",
            "Global step: 206,loss: 0.06534766\n",
            "\n",
            "Global step: 207,loss: 0.06072876\n",
            "\n",
            "Global step: 208,loss: 0.07010535\n",
            "\n",
            "Global step: 209,loss: 0.058696613\n",
            "\n",
            "Global step: 210,loss: 0.05560167\n",
            "\n",
            "Global step: 211,loss: 0.06873606\n",
            "\n",
            "Global step: 212,loss: 0.05601711\n",
            "\n",
            "Global step: 213,loss: 0.051532\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 213,Val_Loss: 0.05763819676480795,  Val_acc: 0.9944490131578947 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:39:08.541151 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/16:\n",
            "Global step: 214,loss: 0.06566085\n",
            "\n",
            "Global step: 215,loss: 0.072475985\n",
            "\n",
            "Global step: 216,loss: 0.063024305\n",
            "\n",
            "Global step: 217,loss: 0.065172374\n",
            "\n",
            "Global step: 218,loss: 0.062046707\n",
            "\n",
            "Global step: 219,loss: 0.05382322\n",
            "\n",
            "Global step: 220,loss: 0.056178022\n",
            "\n",
            "Global step: 221,loss: 0.056841902\n",
            "\n",
            "Global step: 222,loss: 0.05622477\n",
            "\n",
            "Global step: 223,loss: 0.06819785\n",
            "\n",
            "Global step: 224,loss: 0.06202869\n",
            "\n",
            "Global step: 225,loss: 0.06244783\n",
            "\n",
            "Global step: 226,loss: 0.061341304\n",
            "\n",
            "Global step: 227,loss: 0.06353097\n",
            "\n",
            "Global step: 228,loss: 0.06250304\n",
            "\n",
            "Global step: 229,loss: 0.05933589\n",
            "\n",
            "Global step: 230,loss: 0.06014415\n",
            "\n",
            "Global step: 231,loss: 0.061945055\n",
            "\n",
            "Global step: 232,loss: 0.055945553\n",
            "\n",
            "Global step: 233,loss: 0.059318542\n",
            "\n",
            "Global step: 234,loss: 0.05917605\n",
            "\n",
            "Global step: 235,loss: 0.054382253\n",
            "\n",
            "Global step: 236,loss: 0.05191445\n",
            "\n",
            "Global step: 237,loss: 0.058885083\n",
            "\n",
            "Global step: 238,loss: 0.053463772\n",
            "\n",
            "Global step: 239,loss: 0.052289736\n",
            "\n",
            "Global step: 240,loss: 0.053864814\n",
            "\n",
            "Global step: 241,loss: 0.057403512\n",
            "\n",
            "Global step: 242,loss: 0.06151645\n",
            "\n",
            "Global step: 243,loss: 0.053684432\n",
            "\n",
            "Global step: 244,loss: 0.05298748\n",
            "\n",
            "Global step: 245,loss: 0.061047338\n",
            "\n",
            "Global step: 246,loss: 0.053650036\n",
            "\n",
            "Global step: 247,loss: 0.052044295\n",
            "\n",
            "Global step: 248,loss: 0.055827294\n",
            "\n",
            "Global step: 249,loss: 0.054328248\n",
            "\n",
            "Global step: 250,loss: 0.05934844\n",
            "\n",
            "Global step: 251,loss: 0.060455337\n",
            "\n",
            "Global step: 252,loss: 0.0530899\n",
            "\n",
            "Global step: 253,loss: 0.06350276\n",
            "\n",
            "Global step: 254,loss: 0.05426842\n",
            "\n",
            "Global step: 255,loss: 0.057413183\n",
            "\n",
            "Global step: 256,loss: 0.053756386\n",
            "\n",
            "Global step: 257,loss: 0.050404124\n",
            "\n",
            "Global step: 258,loss: 0.05907758\n",
            "\n",
            "Global step: 259,loss: 0.053088367\n",
            "\n",
            "Global step: 260,loss: 0.059172746\n",
            "\n",
            "Global step: 261,loss: 0.058455113\n",
            "\n",
            "Global step: 262,loss: 0.059553348\n",
            "\n",
            "Global step: 263,loss: 0.049247794\n",
            "\n",
            "Global step: 264,loss: 0.051684007\n",
            "\n",
            "Global step: 265,loss: 0.062367957\n",
            "\n",
            "Global step: 266,loss: 0.062462263\n",
            "\n",
            "Global step: 267,loss: 0.05323663\n",
            "\n",
            "Global step: 268,loss: 0.054803565\n",
            "\n",
            "Global step: 269,loss: 0.06700852\n",
            "\n",
            "Global step: 270,loss: 0.048272703\n",
            "\n",
            "Global step: 271,loss: 0.054238643\n",
            "\n",
            "Global step: 272,loss: 0.054067288\n",
            "\n",
            "Global step: 273,loss: 0.05900403\n",
            "\n",
            "Global step: 274,loss: 0.056185216\n",
            "\n",
            "Global step: 275,loss: 0.05697596\n",
            "\n",
            "Global step: 276,loss: 0.05014111\n",
            "\n",
            "Global step: 277,loss: 0.05756181\n",
            "\n",
            "Global step: 278,loss: 0.056405183\n",
            "\n",
            "Global step: 279,loss: 0.053515676\n",
            "\n",
            "Global step: 280,loss: 0.06463082\n",
            "\n",
            "Global step: 281,loss: 0.05134017\n",
            "\n",
            "Global step: 282,loss: 0.0518696\n",
            "\n",
            "Global step: 283,loss: 0.05185137\n",
            "\n",
            "Global step: 284,loss: 0.04974407\n",
            "\n",
            "Global step: 285,loss: 0.056236614\n",
            "\n",
            "Global step: 286,loss: 0.052131984\n",
            "\n",
            "Global step: 287,loss: 0.054119702\n",
            "\n",
            "Global step: 288,loss: 0.05063426\n",
            "\n",
            "Global step: 289,loss: 0.05326888\n",
            "\n",
            "Global step: 290,loss: 0.053938434\n",
            "\n",
            "Global step: 291,loss: 0.059679404\n",
            "\n",
            "Global step: 292,loss: 0.04963379\n",
            "\n",
            "Global step: 293,loss: 0.05274761\n",
            "\n",
            "Global step: 294,loss: 0.059333406\n",
            "\n",
            "Global step: 295,loss: 0.05357301\n",
            "\n",
            "Global step: 296,loss: 0.04733655\n",
            "\n",
            "Global step: 297,loss: 0.048859328\n",
            "\n",
            "Global step: 298,loss: 0.055721387\n",
            "\n",
            "Global step: 299,loss: 0.05314325\n",
            "\n",
            "Global step: 300,loss: 0.057996\n",
            "\n",
            "Global step: 301,loss: 0.06355256\n",
            "\n",
            "Global step: 302,loss: 0.048434574\n",
            "\n",
            "Global step: 303,loss: 0.051512875\n",
            "\n",
            "Global step: 304,loss: 0.056085058\n",
            "\n",
            "Global step: 305,loss: 0.05614894\n",
            "\n",
            "Global step: 306,loss: 0.0503426\n",
            "\n",
            "Global step: 307,loss: 0.050539684\n",
            "\n",
            "Global step: 308,loss: 0.05075813\n",
            "\n",
            "Global step: 309,loss: 0.053232066\n",
            "\n",
            "Global step: 310,loss: 0.052565333\n",
            "\n",
            "Global step: 311,loss: 0.057906754\n",
            "\n",
            "Global step: 312,loss: 0.059305325\n",
            "\n",
            "Global step: 313,loss: 0.050664525\n",
            "\n",
            "Global step: 314,loss: 0.058887452\n",
            "\n",
            "Global step: 315,loss: 0.05792557\n",
            "\n",
            "Global step: 316,loss: 0.060212582\n",
            "\n",
            "Global step: 317,loss: 0.05810045\n",
            "\n",
            "Global step: 318,loss: 0.04953084\n",
            "\n",
            "Global step: 319,loss: 0.052641317\n",
            "\n",
            "Global step: 320,loss: 0.051143155\n",
            "\n",
            "Global step: 321,loss: 0.059343256\n",
            "\n",
            "Global step: 322,loss: 0.05253218\n",
            "\n",
            "Global step: 323,loss: 0.051422454\n",
            "\n",
            "Global step: 324,loss: 0.04637589\n",
            "\n",
            "Global step: 325,loss: 0.053729333\n",
            "\n",
            "Global step: 326,loss: 0.048314705\n",
            "\n",
            "Global step: 327,loss: 0.050675668\n",
            "\n",
            "Global step: 328,loss: 0.04934901\n",
            "\n",
            "Global step: 329,loss: 0.048712183\n",
            "\n",
            "Global step: 330,loss: 0.057763882\n",
            "\n",
            "Global step: 331,loss: 0.062126942\n",
            "\n",
            "Global step: 332,loss: 0.052083552\n",
            "\n",
            "Global step: 333,loss: 0.055556677\n",
            "\n",
            "Global step: 334,loss: 0.055272706\n",
            "\n",
            "Global step: 335,loss: 0.049192864\n",
            "\n",
            "Global step: 336,loss: 0.048146367\n",
            "\n",
            "Global step: 337,loss: 0.04808955\n",
            "\n",
            "Global step: 338,loss: 0.049535267\n",
            "\n",
            "Global step: 339,loss: 0.057530683\n",
            "\n",
            "Global step: 340,loss: 0.06356757\n",
            "\n",
            "Global step: 341,loss: 0.04844194\n",
            "\n",
            "Global step: 342,loss: 0.047274835\n",
            "\n",
            "Global step: 343,loss: 0.04759945\n",
            "\n",
            "Global step: 344,loss: 0.04570704\n",
            "\n",
            "Global step: 345,loss: 0.048230868\n",
            "\n",
            "Global step: 346,loss: 0.051455423\n",
            "\n",
            "Global step: 347,loss: 0.05904483\n",
            "\n",
            "Global step: 348,loss: 0.05475454\n",
            "\n",
            "Global step: 349,loss: 0.051340844\n",
            "\n",
            "Global step: 350,loss: 0.048850812\n",
            "\n",
            "Global step: 351,loss: 0.04853081\n",
            "\n",
            "Global step: 352,loss: 0.055438913\n",
            "\n",
            "Global step: 353,loss: 0.052653708\n",
            "\n",
            "Global step: 354,loss: 0.04673063\n",
            "\n",
            "Global step: 355,loss: 0.04543595\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.49165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:40:39.416886 140504762959616 supervisor.py:1099] global_step/sec: 1.49165\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 356,loss: 0.048946787\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 357.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:40:39.762895 140504771352320 supervisor.py:1050] Recording summary at step 357.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 357,loss: 0.04766004\n",
            "\n",
            "Global step: 358,loss: 0.05107341\n",
            "\n",
            "Global step: 359,loss: 0.047059447\n",
            "\n",
            "Global step: 360,loss: 0.04723593\n",
            "\n",
            "Global step: 361,loss: 0.047887262\n",
            "\n",
            "Global step: 362,loss: 0.04740447\n",
            "\n",
            "Global step: 363,loss: 0.048786066\n",
            "\n",
            "Global step: 364,loss: 0.047896978\n",
            "\n",
            "Global step: 365,loss: 0.049556725\n",
            "\n",
            "Global step: 366,loss: 0.055852666\n",
            "\n",
            "Global step: 367,loss: 0.047576398\n",
            "\n",
            "Global step: 368,loss: 0.05325354\n",
            "\n",
            "Global step: 369,loss: 0.05686803\n",
            "\n",
            "Global step: 370,loss: 0.04474154\n",
            "\n",
            "Global step: 371,loss: 0.05027703\n",
            "\n",
            "Global step: 372,loss: 0.049899135\n",
            "\n",
            "Global step: 373,loss: 0.056574583\n",
            "\n",
            "Global step: 374,loss: 0.052142244\n",
            "\n",
            "Global step: 375,loss: 0.046741143\n",
            "\n",
            "Global step: 376,loss: 0.047540765\n",
            "\n",
            "Global step: 377,loss: 0.05087989\n",
            "\n",
            "Global step: 378,loss: 0.053563826\n",
            "\n",
            "Global step: 379,loss: 0.048143085\n",
            "\n",
            "Global step: 380,loss: 0.04810147\n",
            "\n",
            "Global step: 381,loss: 0.04199155\n",
            "\n",
            "Global step: 382,loss: 0.049710743\n",
            "\n",
            "Global step: 383,loss: 0.05019147\n",
            "\n",
            "Global step: 384,loss: 0.041470066\n",
            "\n",
            "Global step: 385,loss: 0.04929124\n",
            "\n",
            "Global step: 386,loss: 0.052836634\n",
            "\n",
            "Global step: 387,loss: 0.045886017\n",
            "\n",
            "Global step: 388,loss: 0.047151517\n",
            "\n",
            "Global step: 389,loss: 0.043680042\n",
            "\n",
            "Global step: 390,loss: 0.048617\n",
            "\n",
            "Global step: 391,loss: 0.04865629\n",
            "\n",
            "Global step: 392,loss: 0.0455514\n",
            "\n",
            "Global step: 393,loss: 0.05347456\n",
            "\n",
            "Global step: 394,loss: 0.048962437\n",
            "\n",
            "Global step: 395,loss: 0.04596484\n",
            "\n",
            "Global step: 396,loss: 0.04947479\n",
            "\n",
            "Global step: 397,loss: 0.048326634\n",
            "\n",
            "Global step: 398,loss: 0.047679618\n",
            "\n",
            "Global step: 399,loss: 0.050994292\n",
            "\n",
            "Global step: 400,loss: 0.050281383\n",
            "\n",
            "Global step: 401,loss: 0.04980429\n",
            "\n",
            "Global step: 402,loss: 0.050285943\n",
            "\n",
            "Global step: 403,loss: 0.04849908\n",
            "\n",
            "Global step: 404,loss: 0.05778382\n",
            "\n",
            "Global step: 405,loss: 0.045450415\n",
            "\n",
            "Global step: 406,loss: 0.05305717\n",
            "\n",
            "Global step: 407,loss: 0.05021435\n",
            "\n",
            "Global step: 408,loss: 0.043492\n",
            "\n",
            "Global step: 409,loss: 0.047653314\n",
            "\n",
            "Global step: 410,loss: 0.047852732\n",
            "\n",
            "Global step: 411,loss: 0.05832357\n",
            "\n",
            "Global step: 412,loss: 0.045483086\n",
            "\n",
            "Global step: 413,loss: 0.05136221\n",
            "\n",
            "Global step: 414,loss: 0.04201077\n",
            "\n",
            "Global step: 415,loss: 0.049034476\n",
            "\n",
            "Global step: 416,loss: 0.046249095\n",
            "\n",
            "Global step: 417,loss: 0.04594255\n",
            "\n",
            "Global step: 418,loss: 0.04219371\n",
            "\n",
            "Global step: 419,loss: 0.0402447\n",
            "\n",
            "Global step: 420,loss: 0.042941764\n",
            "\n",
            "Global step: 421,loss: 0.054523602\n",
            "\n",
            "Global step: 422,loss: 0.048242465\n",
            "\n",
            "Global step: 423,loss: 0.04574305\n",
            "\n",
            "Global step: 424,loss: 0.04102952\n",
            "\n",
            "Global step: 425,loss: 0.049134325\n",
            "\n",
            "Global step: 426,loss: 0.04466588\n",
            "\n",
            "Global step: 427,loss: 0.049644582\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 427,Val_Loss: 0.04790133551547402,  Val_acc: 0.9946546052631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:41:30.441170 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/16:\n",
            "Global step: 428,loss: 0.04476338\n",
            "\n",
            "Global step: 429,loss: 0.048439227\n",
            "\n",
            "Global step: 430,loss: 0.05133552\n",
            "\n",
            "Global step: 431,loss: 0.039669484\n",
            "\n",
            "Global step: 432,loss: 0.046609778\n",
            "\n",
            "Global step: 433,loss: 0.053321492\n",
            "\n",
            "Global step: 434,loss: 0.044741534\n",
            "\n",
            "Global step: 435,loss: 0.04098496\n",
            "\n",
            "Global step: 436,loss: 0.048268966\n",
            "\n",
            "Global step: 437,loss: 0.04345821\n",
            "\n",
            "Global step: 438,loss: 0.044225827\n",
            "\n",
            "Global step: 439,loss: 0.04645516\n",
            "\n",
            "Global step: 440,loss: 0.04216706\n",
            "\n",
            "Global step: 441,loss: 0.04813701\n",
            "\n",
            "Global step: 442,loss: 0.049404714\n",
            "\n",
            "Global step: 443,loss: 0.03942216\n",
            "\n",
            "Global step: 444,loss: 0.04764858\n",
            "\n",
            "Global step: 445,loss: 0.039689787\n",
            "\n",
            "Global step: 446,loss: 0.04558242\n",
            "\n",
            "Global step: 447,loss: 0.046931572\n",
            "\n",
            "Global step: 448,loss: 0.05093313\n",
            "\n",
            "Global step: 449,loss: 0.042837065\n",
            "\n",
            "Global step: 450,loss: 0.041580968\n",
            "\n",
            "Global step: 451,loss: 0.046401653\n",
            "\n",
            "Global step: 452,loss: 0.0448728\n",
            "\n",
            "Global step: 453,loss: 0.044050865\n",
            "\n",
            "Global step: 454,loss: 0.05120877\n",
            "\n",
            "Global step: 455,loss: 0.04357221\n",
            "\n",
            "Global step: 456,loss: 0.04263899\n",
            "\n",
            "Global step: 457,loss: 0.041612335\n",
            "\n",
            "Global step: 458,loss: 0.047412165\n",
            "\n",
            "Global step: 459,loss: 0.045410044\n",
            "\n",
            "Global step: 460,loss: 0.04507844\n",
            "\n",
            "Global step: 461,loss: 0.04402441\n",
            "\n",
            "Global step: 462,loss: 0.039098725\n",
            "\n",
            "Global step: 463,loss: 0.04414772\n",
            "\n",
            "Global step: 464,loss: 0.040378563\n",
            "\n",
            "Global step: 465,loss: 0.03901927\n",
            "\n",
            "Global step: 466,loss: 0.04839559\n",
            "\n",
            "Global step: 467,loss: 0.04317525\n",
            "\n",
            "Global step: 468,loss: 0.054408282\n",
            "\n",
            "Global step: 469,loss: 0.04552579\n",
            "\n",
            "Global step: 470,loss: 0.041949883\n",
            "\n",
            "Global step: 471,loss: 0.04261706\n",
            "\n",
            "Global step: 472,loss: 0.040675614\n",
            "\n",
            "Global step: 473,loss: 0.046913013\n",
            "\n",
            "Global step: 474,loss: 0.04984928\n",
            "\n",
            "Global step: 475,loss: 0.04320688\n",
            "\n",
            "Global step: 476,loss: 0.05135962\n",
            "\n",
            "Global step: 477,loss: 0.04081808\n",
            "\n",
            "Global step: 478,loss: 0.040255453\n",
            "\n",
            "Global step: 479,loss: 0.0420164\n",
            "\n",
            "Global step: 480,loss: 0.041912116\n",
            "\n",
            "Global step: 481,loss: 0.039571755\n",
            "\n",
            "Global step: 482,loss: 0.04500971\n",
            "\n",
            "Global step: 483,loss: 0.04171353\n",
            "\n",
            "Global step: 484,loss: 0.046890877\n",
            "\n",
            "Global step: 485,loss: 0.045781873\n",
            "\n",
            "Global step: 486,loss: 0.04767682\n",
            "\n",
            "Global step: 487,loss: 0.040813245\n",
            "\n",
            "Global step: 488,loss: 0.04388782\n",
            "\n",
            "Global step: 489,loss: 0.045811236\n",
            "\n",
            "Global step: 490,loss: 0.044203483\n",
            "\n",
            "Global step: 491,loss: 0.039859306\n",
            "\n",
            "Global step: 492,loss: 0.043476515\n",
            "\n",
            "Global step: 493,loss: 0.04125926\n",
            "\n",
            "Global step: 494,loss: 0.042798556\n",
            "\n",
            "Global step: 495,loss: 0.0425054\n",
            "\n",
            "Global step: 496,loss: 0.041818433\n",
            "\n",
            "Global step: 497,loss: 0.040138375\n",
            "\n",
            "Global step: 498,loss: 0.044756327\n",
            "\n",
            "Global step: 499,loss: 0.040820166\n",
            "\n",
            "Global step: 500,loss: 0.040841606\n",
            "\n",
            "Global step: 501,loss: 0.046782013\n",
            "\n",
            "Global step: 502,loss: 0.044625573\n",
            "\n",
            "Global step: 503,loss: 0.039808646\n",
            "\n",
            "Global step: 504,loss: 0.039162174\n",
            "\n",
            "Global step: 505,loss: 0.044329867\n",
            "\n",
            "Global step: 506,loss: 0.050960608\n",
            "\n",
            "Global step: 507,loss: 0.042312533\n",
            "\n",
            "Global step: 508,loss: 0.041735563\n",
            "\n",
            "Global step: 509,loss: 0.03994917\n",
            "\n",
            "Global step: 510,loss: 0.04334994\n",
            "\n",
            "Global step: 511,loss: 0.041680988\n",
            "\n",
            "Global step: 512,loss: 0.04125313\n",
            "\n",
            "Global step: 513,loss: 0.053446256\n",
            "\n",
            "Global step: 514,loss: 0.049890272\n",
            "\n",
            "Global step: 515,loss: 0.03835383\n",
            "\n",
            "Global step: 516,loss: 0.051007837\n",
            "\n",
            "Global step: 517,loss: 0.04101426\n",
            "\n",
            "Global step: 518,loss: 0.0493424\n",
            "\n",
            "Global step: 519,loss: 0.037655853\n",
            "\n",
            "Global step: 520,loss: 0.045325145\n",
            "\n",
            "Global step: 521,loss: 0.04200904\n",
            "\n",
            "Global step: 522,loss: 0.044609714\n",
            "\n",
            "Global step: 523,loss: 0.03452752\n",
            "\n",
            "Global step: 524,loss: 0.04912569\n",
            "\n",
            "Global step: 525,loss: 0.041854467\n",
            "\n",
            "Global step: 526,loss: 0.03775586\n",
            "\n",
            "Global step: 527,loss: 0.040808633\n",
            "\n",
            "Global step: 528,loss: 0.038957786\n",
            "\n",
            "Global step: 529,loss: 0.037841067\n",
            "\n",
            "Global step: 530,loss: 0.042053066\n",
            "\n",
            "Global step: 531,loss: 0.039866928\n",
            "\n",
            "Global step: 532,loss: 0.038423777\n",
            "\n",
            "Global step: 533,loss: 0.04186113\n",
            "\n",
            "Global step: 534,loss: 0.043662682\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.48296\n",
            "Global step: 535,loss: 0.049886838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:42:39.447243 140504762959616 supervisor.py:1099] global_step/sec: 1.48296\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:tensorflow:Recording summary at step 536.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:42:39.787566 140504771352320 supervisor.py:1050] Recording summary at step 536.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 536,loss: 0.040468633\n",
            "\n",
            "Global step: 537,loss: 0.043369047\n",
            "\n",
            "Global step: 538,loss: 0.040921293\n",
            "\n",
            "Global step: 539,loss: 0.048718214\n",
            "\n",
            "Global step: 540,loss: 0.034461387\n",
            "\n",
            "Global step: 541,loss: 0.036596682\n",
            "\n",
            "Global step: 542,loss: 0.041269433\n",
            "\n",
            "Global step: 543,loss: 0.043409884\n",
            "\n",
            "Global step: 544,loss: 0.041515075\n",
            "\n",
            "Global step: 545,loss: 0.036985338\n",
            "\n",
            "Global step: 546,loss: 0.04414981\n",
            "\n",
            "Global step: 547,loss: 0.037820674\n",
            "\n",
            "Global step: 548,loss: 0.03326064\n",
            "\n",
            "Global step: 549,loss: 0.038394652\n",
            "\n",
            "Global step: 550,loss: 0.039195124\n",
            "\n",
            "Global step: 551,loss: 0.040474005\n",
            "\n",
            "Global step: 552,loss: 0.041573703\n",
            "\n",
            "Global step: 553,loss: 0.034879573\n",
            "\n",
            "Global step: 554,loss: 0.038623206\n",
            "\n",
            "Global step: 555,loss: 0.03951571\n",
            "\n",
            "Global step: 556,loss: 0.038785398\n",
            "\n",
            "Global step: 557,loss: 0.035705507\n",
            "\n",
            "Global step: 558,loss: 0.039208874\n",
            "\n",
            "Global step: 559,loss: 0.0390738\n",
            "\n",
            "Global step: 560,loss: 0.04298847\n",
            "\n",
            "Global step: 561,loss: 0.043346897\n",
            "\n",
            "Global step: 562,loss: 0.041166015\n",
            "\n",
            "Global step: 563,loss: 0.03965851\n",
            "\n",
            "Global step: 564,loss: 0.038109303\n",
            "\n",
            "Global step: 565,loss: 0.03674362\n",
            "\n",
            "Global step: 566,loss: 0.03883312\n",
            "\n",
            "Global step: 567,loss: 0.03663434\n",
            "\n",
            "Global step: 568,loss: 0.04197395\n",
            "\n",
            "Global step: 569,loss: 0.038699158\n",
            "\n",
            "Global step: 570,loss: 0.04079413\n",
            "\n",
            "Global step: 571,loss: 0.034528375\n",
            "\n",
            "Global step: 572,loss: 0.043774817\n",
            "\n",
            "Global step: 573,loss: 0.033197325\n",
            "\n",
            "Global step: 574,loss: 0.039872997\n",
            "\n",
            "Global step: 575,loss: 0.040736828\n",
            "\n",
            "Global step: 576,loss: 0.035457104\n",
            "\n",
            "Global step: 577,loss: 0.03698182\n",
            "\n",
            "Global step: 578,loss: 0.03824187\n",
            "\n",
            "Global step: 579,loss: 0.03573952\n",
            "\n",
            "Global step: 580,loss: 0.035693057\n",
            "\n",
            "Global step: 581,loss: 0.03615509\n",
            "\n",
            "Global step: 582,loss: 0.04266143\n",
            "\n",
            "Global step: 583,loss: 0.040227633\n",
            "\n",
            "Global step: 584,loss: 0.03548058\n",
            "\n",
            "Global step: 585,loss: 0.035887167\n",
            "\n",
            "Global step: 586,loss: 0.036448274\n",
            "\n",
            "Global step: 587,loss: 0.03455455\n",
            "\n",
            "Global step: 588,loss: 0.040411416\n",
            "\n",
            "Global step: 589,loss: 0.044695124\n",
            "\n",
            "Global step: 590,loss: 0.033158068\n",
            "\n",
            "Global step: 591,loss: 0.034291133\n",
            "\n",
            "Global step: 592,loss: 0.038509436\n",
            "\n",
            "Global step: 593,loss: 0.03671774\n",
            "\n",
            "Global step: 594,loss: 0.04018149\n",
            "\n",
            "Global step: 595,loss: 0.03418259\n",
            "\n",
            "Global step: 596,loss: 0.035675284\n",
            "\n",
            "Global step: 597,loss: 0.041626535\n",
            "\n",
            "Global step: 598,loss: 0.042940654\n",
            "\n",
            "Global step: 599,loss: 0.035124067\n",
            "\n",
            "Global step: 600,loss: 0.03364019\n",
            "\n",
            "Global step: 601,loss: 0.03971513\n",
            "\n",
            "Global step: 602,loss: 0.03799185\n",
            "\n",
            "Global step: 603,loss: 0.04037138\n",
            "\n",
            "Global step: 604,loss: 0.041360337\n",
            "\n",
            "Global step: 605,loss: 0.036592163\n",
            "\n",
            "Global step: 606,loss: 0.035910424\n",
            "\n",
            "Global step: 607,loss: 0.03891259\n",
            "\n",
            "Global step: 608,loss: 0.040163815\n",
            "\n",
            "Global step: 609,loss: 0.039267644\n",
            "\n",
            "Global step: 610,loss: 0.03446208\n",
            "\n",
            "Global step: 611,loss: 0.039338373\n",
            "\n",
            "Global step: 612,loss: 0.0384156\n",
            "\n",
            "Global step: 613,loss: 0.042065628\n",
            "\n",
            "Global step: 614,loss: 0.033993\n",
            "\n",
            "Global step: 615,loss: 0.036471855\n",
            "\n",
            "Global step: 616,loss: 0.04478921\n",
            "\n",
            "Global step: 617,loss: 0.0364597\n",
            "\n",
            "Global step: 618,loss: 0.03370239\n",
            "\n",
            "Global step: 619,loss: 0.039102506\n",
            "\n",
            "Global step: 620,loss: 0.04204949\n",
            "\n",
            "Global step: 621,loss: 0.04462827\n",
            "\n",
            "Global step: 622,loss: 0.042237073\n",
            "\n",
            "Global step: 623,loss: 0.04287549\n",
            "\n",
            "Global step: 624,loss: 0.03778446\n",
            "\n",
            "Global step: 625,loss: 0.04647309\n",
            "\n",
            "Global step: 626,loss: 0.037468813\n",
            "\n",
            "Global step: 627,loss: 0.040865857\n",
            "\n",
            "Global step: 628,loss: 0.04088217\n",
            "\n",
            "Global step: 629,loss: 0.042692114\n",
            "\n",
            "Global step: 630,loss: 0.039723855\n",
            "\n",
            "Global step: 631,loss: 0.041138\n",
            "\n",
            "Global step: 632,loss: 0.033295974\n",
            "\n",
            "Global step: 633,loss: 0.035823103\n",
            "\n",
            "Global step: 634,loss: 0.034716994\n",
            "\n",
            "Global step: 635,loss: 0.037030414\n",
            "\n",
            "Global step: 636,loss: 0.04330024\n",
            "\n",
            "Global step: 637,loss: 0.03278047\n",
            "\n",
            "Global step: 638,loss: 0.038955715\n",
            "\n",
            "Global step: 639,loss: 0.031057345\n",
            "\n",
            "Global step: 640,loss: 0.032801025\n",
            "\n",
            "Global step: 641,loss: 0.036453076\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 641,Val_Loss: 0.042454927571510016,  Val_acc: 0.9962993421052632 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:43:52.806351 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/16:\n",
            "Global step: 642,loss: 0.038834512\n",
            "\n",
            "Global step: 643,loss: 0.030161776\n",
            "\n",
            "Global step: 644,loss: 0.033004716\n",
            "\n",
            "Global step: 645,loss: 0.03967841\n",
            "\n",
            "Global step: 646,loss: 0.040720753\n",
            "\n",
            "Global step: 647,loss: 0.04046432\n",
            "\n",
            "Global step: 648,loss: 0.038283177\n",
            "\n",
            "Global step: 649,loss: 0.041771498\n",
            "\n",
            "Global step: 650,loss: 0.034748472\n",
            "\n",
            "Global step: 651,loss: 0.034130726\n",
            "\n",
            "Global step: 652,loss: 0.039578766\n",
            "\n",
            "Global step: 653,loss: 0.035216767\n",
            "\n",
            "Global step: 654,loss: 0.039444033\n",
            "\n",
            "Global step: 655,loss: 0.03463775\n",
            "\n",
            "Global step: 656,loss: 0.0361363\n",
            "\n",
            "Global step: 657,loss: 0.052840203\n",
            "\n",
            "Global step: 658,loss: 0.040312197\n",
            "\n",
            "Global step: 659,loss: 0.038319632\n",
            "\n",
            "Global step: 660,loss: 0.039176986\n",
            "\n",
            "Global step: 661,loss: 0.034911416\n",
            "\n",
            "Global step: 662,loss: 0.03756881\n",
            "\n",
            "Global step: 663,loss: 0.03306194\n",
            "\n",
            "Global step: 664,loss: 0.038201608\n",
            "\n",
            "Global step: 665,loss: 0.039930254\n",
            "\n",
            "Global step: 666,loss: 0.031636886\n",
            "\n",
            "Global step: 667,loss: 0.03559519\n",
            "\n",
            "Global step: 668,loss: 0.032777287\n",
            "\n",
            "Global step: 669,loss: 0.03014811\n",
            "\n",
            "Global step: 670,loss: 0.029597552\n",
            "\n",
            "Global step: 671,loss: 0.04336743\n",
            "\n",
            "Global step: 672,loss: 0.0377919\n",
            "\n",
            "Global step: 673,loss: 0.03534849\n",
            "\n",
            "Global step: 674,loss: 0.03431222\n",
            "\n",
            "Global step: 675,loss: 0.038762383\n",
            "\n",
            "Global step: 676,loss: 0.042064674\n",
            "\n",
            "Global step: 677,loss: 0.037168566\n",
            "\n",
            "Global step: 678,loss: 0.039165605\n",
            "\n",
            "Global step: 679,loss: 0.034100424\n",
            "\n",
            "Global step: 680,loss: 0.034623686\n",
            "\n",
            "Global step: 681,loss: 0.03539869\n",
            "\n",
            "Global step: 682,loss: 0.035665162\n",
            "\n",
            "Global step: 683,loss: 0.034112796\n",
            "\n",
            "Global step: 684,loss: 0.03449069\n",
            "\n",
            "Global step: 685,loss: 0.03511414\n",
            "\n",
            "Global step: 686,loss: 0.034071445\n",
            "\n",
            "Global step: 687,loss: 0.03373734\n",
            "\n",
            "Global step: 688,loss: 0.03274332\n",
            "\n",
            "Global step: 689,loss: 0.029649315\n",
            "\n",
            "Global step: 690,loss: 0.043575417\n",
            "\n",
            "Global step: 691,loss: 0.034086853\n",
            "\n",
            "Global step: 692,loss: 0.033633545\n",
            "\n",
            "Global step: 693,loss: 0.031488422\n",
            "\n",
            "Global step: 694,loss: 0.032198545\n",
            "\n",
            "Global step: 695,loss: 0.040898316\n",
            "\n",
            "Global step: 696,loss: 0.03700533\n",
            "\n",
            "Global step: 697,loss: 0.03626757\n",
            "\n",
            "Global step: 698,loss: 0.03421211\n",
            "\n",
            "Global step: 699,loss: 0.030397363\n",
            "\n",
            "Global step: 700,loss: 0.030400602\n",
            "\n",
            "Global step: 701,loss: 0.033318914\n",
            "\n",
            "Global step: 702,loss: 0.04090574\n",
            "\n",
            "Global step: 703,loss: 0.03154425\n",
            "\n",
            "Global step: 704,loss: 0.034459047\n",
            "\n",
            "Global step: 705,loss: 0.036886547\n",
            "\n",
            "Global step: 706,loss: 0.03553363\n",
            "\n",
            "Global step: 707,loss: 0.036097847\n",
            "\n",
            "Global step: 708,loss: 0.03481947\n",
            "\n",
            "Global step: 709,loss: 0.041050464\n",
            "\n",
            "Global step: 710,loss: 0.03561149\n",
            "\n",
            "Global step: 711,loss: 0.03772628\n",
            "\n",
            "Global step: 712,loss: 0.031151118\n",
            "\n",
            "Global step: 713,loss: 0.03219863\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.49157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:44:39.454694 140504762959616 supervisor.py:1099] global_step/sec: 1.49157\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 714,loss: 0.031064127\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 715.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:44:39.795891 140504771352320 supervisor.py:1050] Recording summary at step 715.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 715,loss: 0.03336446\n",
            "\n",
            "Global step: 716,loss: 0.036908414\n",
            "\n",
            "Global step: 717,loss: 0.03332546\n",
            "\n",
            "Global step: 718,loss: 0.040836573\n",
            "\n",
            "Global step: 719,loss: 0.03970866\n",
            "\n",
            "Global step: 720,loss: 0.03480779\n",
            "\n",
            "Global step: 721,loss: 0.035319153\n",
            "\n",
            "Global step: 722,loss: 0.031213213\n",
            "\n",
            "Global step: 723,loss: 0.032920886\n",
            "\n",
            "Global step: 724,loss: 0.03350266\n",
            "\n",
            "Global step: 725,loss: 0.03315524\n",
            "\n",
            "Global step: 726,loss: 0.04440068\n",
            "\n",
            "Global step: 727,loss: 0.03971721\n",
            "\n",
            "Global step: 728,loss: 0.036996752\n",
            "\n",
            "Global step: 729,loss: 0.034468964\n",
            "\n",
            "Global step: 730,loss: 0.036331587\n",
            "\n",
            "Global step: 731,loss: 0.033455186\n",
            "\n",
            "Global step: 732,loss: 0.033094205\n",
            "\n",
            "Global step: 733,loss: 0.037477355\n",
            "\n",
            "Global step: 734,loss: 0.029582497\n",
            "\n",
            "Global step: 735,loss: 0.04051119\n",
            "\n",
            "Global step: 736,loss: 0.034860596\n",
            "\n",
            "Global step: 737,loss: 0.030135179\n",
            "\n",
            "Global step: 738,loss: 0.030845152\n",
            "\n",
            "Global step: 739,loss: 0.036895737\n",
            "\n",
            "Global step: 740,loss: 0.034251817\n",
            "\n",
            "Global step: 741,loss: 0.0382168\n",
            "\n",
            "Global step: 742,loss: 0.038795236\n",
            "\n",
            "Global step: 743,loss: 0.03198072\n",
            "\n",
            "Global step: 744,loss: 0.033893928\n",
            "\n",
            "Global step: 745,loss: 0.03754027\n",
            "\n",
            "Global step: 746,loss: 0.036202986\n",
            "\n",
            "Global step: 747,loss: 0.031738374\n",
            "\n",
            "Global step: 748,loss: 0.03466504\n",
            "\n",
            "Global step: 749,loss: 0.03381616\n",
            "\n",
            "Global step: 750,loss: 0.032188322\n",
            "\n",
            "Global step: 751,loss: 0.030531157\n",
            "\n",
            "Global step: 752,loss: 0.031557713\n",
            "\n",
            "Global step: 753,loss: 0.032747578\n",
            "\n",
            "Global step: 754,loss: 0.031492025\n",
            "\n",
            "Global step: 755,loss: 0.032694306\n",
            "\n",
            "Global step: 756,loss: 0.0336646\n",
            "\n",
            "Global step: 757,loss: 0.03250646\n",
            "\n",
            "Global step: 758,loss: 0.031665668\n",
            "\n",
            "Global step: 759,loss: 0.034098253\n",
            "\n",
            "Global step: 760,loss: 0.036325417\n",
            "\n",
            "Global step: 761,loss: 0.035353787\n",
            "\n",
            "Global step: 762,loss: 0.030313976\n",
            "\n",
            "Global step: 763,loss: 0.028724039\n",
            "\n",
            "Global step: 764,loss: 0.03912721\n",
            "\n",
            "Global step: 765,loss: 0.036791194\n",
            "\n",
            "Global step: 766,loss: 0.02977266\n",
            "\n",
            "Global step: 767,loss: 0.029734077\n",
            "\n",
            "Global step: 768,loss: 0.030657474\n",
            "\n",
            "Global step: 769,loss: 0.031128205\n",
            "\n",
            "Global step: 770,loss: 0.031341128\n",
            "\n",
            "Global step: 771,loss: 0.033827446\n",
            "\n",
            "Global step: 772,loss: 0.03636593\n",
            "\n",
            "Global step: 773,loss: 0.031666167\n",
            "\n",
            "Global step: 774,loss: 0.038084462\n",
            "\n",
            "Global step: 775,loss: 0.02983491\n",
            "\n",
            "Global step: 776,loss: 0.03805264\n",
            "\n",
            "Global step: 777,loss: 0.031371996\n",
            "\n",
            "Global step: 778,loss: 0.029303685\n",
            "\n",
            "Global step: 779,loss: 0.032887414\n",
            "\n",
            "Global step: 780,loss: 0.029092662\n",
            "\n",
            "Global step: 781,loss: 0.032300025\n",
            "\n",
            "Global step: 782,loss: 0.03786979\n",
            "\n",
            "Global step: 783,loss: 0.029975362\n",
            "\n",
            "Global step: 784,loss: 0.029989805\n",
            "\n",
            "Global step: 785,loss: 0.044896603\n",
            "\n",
            "Global step: 786,loss: 0.030607078\n",
            "\n",
            "Global step: 787,loss: 0.03157762\n",
            "\n",
            "Global step: 788,loss: 0.029732138\n",
            "\n",
            "Global step: 789,loss: 0.02921804\n",
            "\n",
            "Global step: 790,loss: 0.03107195\n",
            "\n",
            "Global step: 791,loss: 0.029758938\n",
            "\n",
            "Global step: 792,loss: 0.03443645\n",
            "\n",
            "Global step: 793,loss: 0.031933103\n",
            "\n",
            "Global step: 794,loss: 0.03331973\n",
            "\n",
            "Global step: 795,loss: 0.033619493\n",
            "\n",
            "Global step: 796,loss: 0.033035737\n",
            "\n",
            "Global step: 797,loss: 0.03363223\n",
            "\n",
            "Global step: 798,loss: 0.030126046\n",
            "\n",
            "Global step: 799,loss: 0.02792789\n",
            "\n",
            "Global step: 800,loss: 0.032378294\n",
            "\n",
            "Global step: 801,loss: 0.031065322\n",
            "\n",
            "Global step: 802,loss: 0.03438592\n",
            "\n",
            "Global step: 803,loss: 0.04173195\n",
            "\n",
            "Global step: 804,loss: 0.032922897\n",
            "\n",
            "Global step: 805,loss: 0.03955145\n",
            "\n",
            "Global step: 806,loss: 0.030373974\n",
            "\n",
            "Global step: 807,loss: 0.0320183\n",
            "\n",
            "Global step: 808,loss: 0.035413437\n",
            "\n",
            "Global step: 809,loss: 0.0337072\n",
            "\n",
            "Global step: 810,loss: 0.03152284\n",
            "\n",
            "Global step: 811,loss: 0.037250437\n",
            "\n",
            "Global step: 812,loss: 0.028258082\n",
            "\n",
            "Global step: 813,loss: 0.03792137\n",
            "\n",
            "Global step: 814,loss: 0.0325145\n",
            "\n",
            "Global step: 815,loss: 0.032513767\n",
            "\n",
            "Global step: 816,loss: 0.03133604\n",
            "\n",
            "Global step: 817,loss: 0.03623633\n",
            "\n",
            "Global step: 818,loss: 0.030583447\n",
            "\n",
            "Global step: 819,loss: 0.038189676\n",
            "\n",
            "Global step: 820,loss: 0.03408273\n",
            "\n",
            "Global step: 821,loss: 0.031297784\n",
            "\n",
            "Global step: 822,loss: 0.032417584\n",
            "\n",
            "Global step: 823,loss: 0.036670547\n",
            "\n",
            "Global step: 824,loss: 0.026252043\n",
            "\n",
            "Global step: 825,loss: 0.030400371\n",
            "\n",
            "Global step: 826,loss: 0.028674126\n",
            "\n",
            "Global step: 827,loss: 0.03828426\n",
            "\n",
            "Global step: 828,loss: 0.029457783\n",
            "\n",
            "Global step: 829,loss: 0.026809528\n",
            "\n",
            "Global step: 830,loss: 0.02682737\n",
            "\n",
            "Global step: 831,loss: 0.032734204\n",
            "\n",
            "Global step: 832,loss: 0.03369588\n",
            "\n",
            "Global step: 833,loss: 0.030471662\n",
            "\n",
            "Global step: 834,loss: 0.029684393\n",
            "\n",
            "Global step: 835,loss: 0.028944392\n",
            "\n",
            "Global step: 836,loss: 0.03602943\n",
            "\n",
            "Global step: 837,loss: 0.030034654\n",
            "\n",
            "Global step: 838,loss: 0.031021178\n",
            "\n",
            "Global step: 839,loss: 0.028683405\n",
            "\n",
            "Global step: 840,loss: 0.029894765\n",
            "\n",
            "Global step: 841,loss: 0.026454467\n",
            "\n",
            "Global step: 842,loss: 0.028212938\n",
            "\n",
            "Global step: 843,loss: 0.02839599\n",
            "\n",
            "Global step: 844,loss: 0.03101534\n",
            "\n",
            "Global step: 845,loss: 0.030542664\n",
            "\n",
            "Global step: 846,loss: 0.032983277\n",
            "\n",
            "Global step: 847,loss: 0.036844127\n",
            "\n",
            "Global step: 848,loss: 0.034203053\n",
            "\n",
            "Global step: 849,loss: 0.028725192\n",
            "\n",
            "Global step: 850,loss: 0.029183445\n",
            "\n",
            "Global step: 851,loss: 0.026072215\n",
            "\n",
            "Global step: 852,loss: 0.036790587\n",
            "\n",
            "Global step: 853,loss: 0.03411608\n",
            "\n",
            "Global step: 854,loss: 0.027941875\n",
            "\n",
            "Global step: 855,loss: 0.027432457\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 855,Val_Loss: 0.03971823363711959,  Val_acc: 0.9967105263157895 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:46:14.970440 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/16:\n",
            "Global step: 856,loss: 0.03485677\n",
            "\n",
            "Global step: 857,loss: 0.031070914\n",
            "\n",
            "Global step: 858,loss: 0.031347796\n",
            "\n",
            "Global step: 859,loss: 0.035836123\n",
            "\n",
            "Global step: 860,loss: 0.031740006\n",
            "\n",
            "Global step: 861,loss: 0.028139174\n",
            "\n",
            "Global step: 862,loss: 0.029821476\n",
            "\n",
            "Global step: 863,loss: 0.031318694\n",
            "\n",
            "Global step: 864,loss: 0.034156516\n",
            "\n",
            "Global step: 865,loss: 0.027950797\n",
            "\n",
            "Global step: 866,loss: 0.029525317\n",
            "\n",
            "Global step: 867,loss: 0.028060673\n",
            "\n",
            "Global step: 868,loss: 0.030103242\n",
            "\n",
            "Global step: 869,loss: 0.031433165\n",
            "\n",
            "Global step: 870,loss: 0.032667547\n",
            "\n",
            "Global step: 871,loss: 0.031658962\n",
            "\n",
            "Global step: 872,loss: 0.03082132\n",
            "\n",
            "Global step: 873,loss: 0.033750944\n",
            "\n",
            "Global step: 874,loss: 0.027121989\n",
            "\n",
            "Global step: 875,loss: 0.03093964\n",
            "\n",
            "Global step: 876,loss: 0.031124637\n",
            "\n",
            "Global step: 877,loss: 0.034814246\n",
            "\n",
            "Global step: 878,loss: 0.029363241\n",
            "\n",
            "Global step: 879,loss: 0.032305706\n",
            "\n",
            "Global step: 880,loss: 0.032367148\n",
            "\n",
            "Global step: 881,loss: 0.027727611\n",
            "\n",
            "Global step: 882,loss: 0.02614723\n",
            "\n",
            "Global step: 883,loss: 0.03254217\n",
            "\n",
            "Global step: 884,loss: 0.031746954\n",
            "\n",
            "Global step: 885,loss: 0.02785979\n",
            "\n",
            "Global step: 886,loss: 0.028725471\n",
            "\n",
            "Global step: 887,loss: 0.029591307\n",
            "\n",
            "Global step: 888,loss: 0.028452061\n",
            "\n",
            "Global step: 889,loss: 0.025683433\n",
            "\n",
            "Global step: 890,loss: 0.0313016\n",
            "\n",
            "Global step: 891,loss: 0.03184813\n",
            "\n",
            "Global step: 892,loss: 0.02935481\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.49176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:46:39.447465 140504762959616 supervisor.py:1099] global_step/sec: 1.49176\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 893,loss: 0.026886703\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 894.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:46:39.901595 140504771352320 supervisor.py:1050] Recording summary at step 894.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 894,loss: 0.030128695\n",
            "\n",
            "Global step: 895,loss: 0.031180114\n",
            "\n",
            "Global step: 896,loss: 0.027481742\n",
            "\n",
            "Global step: 897,loss: 0.031904116\n",
            "\n",
            "Global step: 898,loss: 0.029511921\n",
            "\n",
            "Global step: 899,loss: 0.029259298\n",
            "\n",
            "Global step: 900,loss: 0.029087855\n",
            "\n",
            "Global step: 901,loss: 0.026021574\n",
            "\n",
            "Global step: 902,loss: 0.035345435\n",
            "\n",
            "Global step: 903,loss: 0.027979877\n",
            "\n",
            "Global step: 904,loss: 0.033604354\n",
            "\n",
            "Global step: 905,loss: 0.029259523\n",
            "\n",
            "Global step: 906,loss: 0.032088086\n",
            "\n",
            "Global step: 907,loss: 0.031710267\n",
            "\n",
            "Global step: 908,loss: 0.033726174\n",
            "\n",
            "Global step: 909,loss: 0.02990488\n",
            "\n",
            "Global step: 910,loss: 0.028356431\n",
            "\n",
            "Global step: 911,loss: 0.028889058\n",
            "\n",
            "Global step: 912,loss: 0.029352559\n",
            "\n",
            "Global step: 913,loss: 0.031212501\n",
            "\n",
            "Global step: 914,loss: 0.03399575\n",
            "\n",
            "Global step: 915,loss: 0.0270342\n",
            "\n",
            "Global step: 916,loss: 0.028093096\n",
            "\n",
            "Global step: 917,loss: 0.027159948\n",
            "\n",
            "Global step: 918,loss: 0.027002564\n",
            "\n",
            "Global step: 919,loss: 0.030735128\n",
            "\n",
            "Global step: 920,loss: 0.026293885\n",
            "\n",
            "Global step: 921,loss: 0.031208094\n",
            "\n",
            "Global step: 922,loss: 0.02892649\n",
            "\n",
            "Global step: 923,loss: 0.032196343\n",
            "\n",
            "Global step: 924,loss: 0.029611863\n",
            "\n",
            "Global step: 925,loss: 0.028029762\n",
            "\n",
            "Global step: 926,loss: 0.028226255\n",
            "\n",
            "Global step: 927,loss: 0.031547338\n",
            "\n",
            "Global step: 928,loss: 0.029300943\n",
            "\n",
            "Global step: 929,loss: 0.028539564\n",
            "\n",
            "Global step: 930,loss: 0.02690874\n",
            "\n",
            "Global step: 931,loss: 0.03312576\n",
            "\n",
            "Global step: 932,loss: 0.0363433\n",
            "\n",
            "Global step: 933,loss: 0.027335128\n",
            "\n",
            "Global step: 934,loss: 0.03131225\n",
            "\n",
            "Global step: 935,loss: 0.03078238\n",
            "\n",
            "Global step: 936,loss: 0.029722862\n",
            "\n",
            "Global step: 937,loss: 0.028839257\n",
            "\n",
            "Global step: 938,loss: 0.032135744\n",
            "\n",
            "Global step: 939,loss: 0.031779014\n",
            "\n",
            "Global step: 940,loss: 0.029668406\n",
            "\n",
            "Global step: 941,loss: 0.03381076\n",
            "\n",
            "Global step: 942,loss: 0.029473087\n",
            "\n",
            "Global step: 943,loss: 0.02749421\n",
            "\n",
            "Global step: 944,loss: 0.030619575\n",
            "\n",
            "Global step: 945,loss: 0.027369257\n",
            "\n",
            "Global step: 946,loss: 0.034189764\n",
            "\n",
            "Global step: 947,loss: 0.027252547\n",
            "\n",
            "Global step: 948,loss: 0.028764313\n",
            "\n",
            "Global step: 949,loss: 0.027283998\n",
            "\n",
            "Global step: 950,loss: 0.0309477\n",
            "\n",
            "Global step: 951,loss: 0.02489679\n",
            "\n",
            "Global step: 952,loss: 0.033102952\n",
            "\n",
            "Global step: 953,loss: 0.034182016\n",
            "\n",
            "Global step: 954,loss: 0.030355643\n",
            "\n",
            "Global step: 955,loss: 0.032610282\n",
            "\n",
            "Global step: 956,loss: 0.030416816\n",
            "\n",
            "Global step: 957,loss: 0.030771904\n",
            "\n",
            "Global step: 958,loss: 0.028933369\n",
            "\n",
            "Global step: 959,loss: 0.028186759\n",
            "\n",
            "Global step: 960,loss: 0.028695282\n",
            "\n",
            "Global step: 961,loss: 0.030673828\n",
            "\n",
            "Global step: 962,loss: 0.028969597\n",
            "\n",
            "Global step: 963,loss: 0.028138047\n",
            "\n",
            "Global step: 964,loss: 0.031593103\n",
            "\n",
            "Global step: 965,loss: 0.028609455\n",
            "\n",
            "Global step: 966,loss: 0.027947998\n",
            "\n",
            "Global step: 967,loss: 0.03267355\n",
            "\n",
            "Global step: 968,loss: 0.033659715\n",
            "\n",
            "Global step: 969,loss: 0.03795117\n",
            "\n",
            "Global step: 970,loss: 0.03090521\n",
            "\n",
            "Global step: 971,loss: 0.024924222\n",
            "\n",
            "Global step: 972,loss: 0.026021942\n",
            "\n",
            "Global step: 973,loss: 0.026920212\n",
            "\n",
            "Global step: 974,loss: 0.03142445\n",
            "\n",
            "Global step: 975,loss: 0.02988129\n",
            "\n",
            "Global step: 976,loss: 0.025555618\n",
            "\n",
            "Global step: 977,loss: 0.027808037\n",
            "\n",
            "Global step: 978,loss: 0.03157732\n",
            "\n",
            "Global step: 979,loss: 0.027361734\n",
            "\n",
            "Global step: 980,loss: 0.025419453\n",
            "\n",
            "Global step: 981,loss: 0.028243542\n",
            "\n",
            "Global step: 982,loss: 0.02463338\n",
            "\n",
            "Global step: 983,loss: 0.029054664\n",
            "\n",
            "Global step: 984,loss: 0.025723334\n",
            "\n",
            "Global step: 985,loss: 0.02749628\n",
            "\n",
            "Global step: 986,loss: 0.028242253\n",
            "\n",
            "Global step: 987,loss: 0.027837407\n",
            "\n",
            "Global step: 988,loss: 0.0319738\n",
            "\n",
            "Global step: 989,loss: 0.029933743\n",
            "\n",
            "Global step: 990,loss: 0.032355417\n",
            "\n",
            "Global step: 991,loss: 0.028795762\n",
            "\n",
            "Global step: 992,loss: 0.029812722\n",
            "\n",
            "Global step: 993,loss: 0.027950348\n",
            "\n",
            "Global step: 994,loss: 0.034377977\n",
            "\n",
            "Global step: 995,loss: 0.028910412\n",
            "\n",
            "Global step: 996,loss: 0.024476882\n",
            "\n",
            "Global step: 997,loss: 0.027736511\n",
            "\n",
            "Global step: 998,loss: 0.027095646\n",
            "\n",
            "Global step: 999,loss: 0.02706222\n",
            "\n",
            "Global step: 1000,loss: 0.029589314\n",
            "\n",
            "Global step: 1001,loss: 0.029970452\n",
            "\n",
            "Global step: 1002,loss: 0.0320801\n",
            "\n",
            "Global step: 1003,loss: 0.02560162\n",
            "\n",
            "Global step: 1004,loss: 0.025993584\n",
            "\n",
            "Global step: 1005,loss: 0.032529663\n",
            "\n",
            "Global step: 1006,loss: 0.026080007\n",
            "\n",
            "Global step: 1007,loss: 0.030352373\n",
            "\n",
            "Global step: 1008,loss: 0.025206473\n",
            "\n",
            "Global step: 1009,loss: 0.026893783\n",
            "\n",
            "Global step: 1010,loss: 0.026934814\n",
            "\n",
            "Global step: 1011,loss: 0.027381105\n",
            "\n",
            "Global step: 1012,loss: 0.032763064\n",
            "\n",
            "Global step: 1013,loss: 0.028917857\n",
            "\n",
            "Global step: 1014,loss: 0.030381568\n",
            "\n",
            "Global step: 1015,loss: 0.030139908\n",
            "\n",
            "Global step: 1016,loss: 0.027533624\n",
            "\n",
            "Global step: 1017,loss: 0.026075624\n",
            "\n",
            "Global step: 1018,loss: 0.029405981\n",
            "\n",
            "Global step: 1019,loss: 0.03137396\n",
            "\n",
            "Global step: 1020,loss: 0.02713042\n",
            "\n",
            "Global step: 1021,loss: 0.025342982\n",
            "\n",
            "Global step: 1022,loss: 0.028466668\n",
            "\n",
            "Global step: 1023,loss: 0.026397018\n",
            "\n",
            "Global step: 1024,loss: 0.029162303\n",
            "\n",
            "Global step: 1025,loss: 0.032042727\n",
            "\n",
            "Global step: 1026,loss: 0.027910478\n",
            "\n",
            "Global step: 1027,loss: 0.029759135\n",
            "\n",
            "Global step: 1028,loss: 0.025741505\n",
            "\n",
            "Global step: 1029,loss: 0.03461954\n",
            "\n",
            "Global step: 1030,loss: 0.025812384\n",
            "\n",
            "Global step: 1031,loss: 0.029107425\n",
            "\n",
            "Global step: 1032,loss: 0.030183794\n",
            "\n",
            "Global step: 1033,loss: 0.029093178\n",
            "\n",
            "Global step: 1034,loss: 0.029043006\n",
            "\n",
            "Global step: 1035,loss: 0.02690943\n",
            "\n",
            "Global step: 1036,loss: 0.033975076\n",
            "\n",
            "Global step: 1037,loss: 0.028778572\n",
            "\n",
            "Global step: 1038,loss: 0.030724715\n",
            "\n",
            "Global step: 1039,loss: 0.024987698\n",
            "\n",
            "Global step: 1040,loss: 0.027266044\n",
            "\n",
            "Global step: 1041,loss: 0.030378193\n",
            "\n",
            "Global step: 1042,loss: 0.037679672\n",
            "\n",
            "Global step: 1043,loss: 0.02911301\n",
            "\n",
            "Global step: 1044,loss: 0.032463394\n",
            "\n",
            "Global step: 1045,loss: 0.030502759\n",
            "\n",
            "Global step: 1046,loss: 0.028007794\n",
            "\n",
            "Global step: 1047,loss: 0.03849848\n",
            "\n",
            "Global step: 1048,loss: 0.030664567\n",
            "\n",
            "Global step: 1049,loss: 0.025050923\n",
            "\n",
            "Global step: 1050,loss: 0.023513725\n",
            "\n",
            "Global step: 1051,loss: 0.029668082\n",
            "\n",
            "Global step: 1052,loss: 0.02490696\n",
            "\n",
            "Global step: 1053,loss: 0.030448036\n",
            "\n",
            "Global step: 1054,loss: 0.025627008\n",
            "\n",
            "Global step: 1055,loss: 0.033076253\n",
            "\n",
            "Global step: 1056,loss: 0.02453668\n",
            "\n",
            "Global step: 1057,loss: 0.02551813\n",
            "\n",
            "Global step: 1058,loss: 0.033771172\n",
            "\n",
            "Global step: 1059,loss: 0.030570576\n",
            "\n",
            "Global step: 1060,loss: 0.026366603\n",
            "\n",
            "Global step: 1061,loss: 0.028292043\n",
            "\n",
            "Global step: 1062,loss: 0.025999803\n",
            "\n",
            "Global step: 1063,loss: 0.025901884\n",
            "\n",
            "Global step: 1064,loss: 0.02349104\n",
            "\n",
            "Global step: 1065,loss: 0.026914217\n",
            "\n",
            "Global step: 1066,loss: 0.029261105\n",
            "\n",
            "Global step: 1067,loss: 0.028982144\n",
            "\n",
            "Global step: 1068,loss: 0.025325036\n",
            "\n",
            "Global step: 1069,loss: 0.03111914\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1069,Val_Loss: 0.036610024147912076,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:48:36.938396 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/16:\n",
            "Global step: 1070,loss: 0.031087859\n",
            "\n",
            "Global step: 1071,loss: 0.029467959\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.49764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:48:39.636415 140504762959616 supervisor.py:1099] global_step/sec: 1.49764\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1072,loss: 0.028246198\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1073.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:48:39.904496 140504771352320 supervisor.py:1050] Recording summary at step 1073.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1073,loss: 0.025238065\n",
            "\n",
            "Global step: 1074,loss: 0.027625628\n",
            "\n",
            "Global step: 1075,loss: 0.023509756\n",
            "\n",
            "Global step: 1076,loss: 0.02469368\n",
            "\n",
            "Global step: 1077,loss: 0.025235318\n",
            "\n",
            "Global step: 1078,loss: 0.02561427\n",
            "\n",
            "Global step: 1079,loss: 0.027804263\n",
            "\n",
            "Global step: 1080,loss: 0.02654439\n",
            "\n",
            "Global step: 1081,loss: 0.027546212\n",
            "\n",
            "Global step: 1082,loss: 0.031250045\n",
            "\n",
            "Global step: 1083,loss: 0.0276381\n",
            "\n",
            "Global step: 1084,loss: 0.02519096\n",
            "\n",
            "Global step: 1085,loss: 0.024584813\n",
            "\n",
            "Global step: 1086,loss: 0.026179621\n",
            "\n",
            "Global step: 1087,loss: 0.027774459\n",
            "\n",
            "Global step: 1088,loss: 0.02714487\n",
            "\n",
            "Global step: 1089,loss: 0.028102936\n",
            "\n",
            "Global step: 1090,loss: 0.024686655\n",
            "\n",
            "Global step: 1091,loss: 0.023714064\n",
            "\n",
            "Global step: 1092,loss: 0.025908472\n",
            "\n",
            "Global step: 1093,loss: 0.031144986\n",
            "\n",
            "Global step: 1094,loss: 0.029082004\n",
            "\n",
            "Global step: 1095,loss: 0.031202253\n",
            "\n",
            "Global step: 1096,loss: 0.02520558\n",
            "\n",
            "Global step: 1097,loss: 0.030041192\n",
            "\n",
            "Global step: 1098,loss: 0.02828747\n",
            "\n",
            "Global step: 1099,loss: 0.031324994\n",
            "\n",
            "Global step: 1100,loss: 0.027387194\n",
            "\n",
            "Global step: 1101,loss: 0.024823207\n",
            "\n",
            "Global step: 1102,loss: 0.026545577\n",
            "\n",
            "Global step: 1103,loss: 0.026656922\n",
            "\n",
            "Global step: 1104,loss: 0.029604662\n",
            "\n",
            "Global step: 1105,loss: 0.027015347\n",
            "\n",
            "Global step: 1106,loss: 0.02898734\n",
            "\n",
            "Global step: 1107,loss: 0.03694535\n",
            "\n",
            "Global step: 1108,loss: 0.025422607\n",
            "\n",
            "Global step: 1109,loss: 0.02711584\n",
            "\n",
            "Global step: 1110,loss: 0.032224752\n",
            "\n",
            "Global step: 1111,loss: 0.025602207\n",
            "\n",
            "Global step: 1112,loss: 0.027131224\n",
            "\n",
            "Global step: 1113,loss: 0.027839608\n",
            "\n",
            "Global step: 1114,loss: 0.024690628\n",
            "\n",
            "Global step: 1115,loss: 0.027981102\n",
            "\n",
            "Global step: 1116,loss: 0.025548967\n",
            "\n",
            "Global step: 1117,loss: 0.027388757\n",
            "\n",
            "Global step: 1118,loss: 0.024279047\n",
            "\n",
            "Global step: 1119,loss: 0.025862137\n",
            "\n",
            "Global step: 1120,loss: 0.02613251\n",
            "\n",
            "Global step: 1121,loss: 0.027240902\n",
            "\n",
            "Global step: 1122,loss: 0.028048974\n",
            "\n",
            "Global step: 1123,loss: 0.0256348\n",
            "\n",
            "Global step: 1124,loss: 0.025726855\n",
            "\n",
            "Global step: 1125,loss: 0.02542491\n",
            "\n",
            "Global step: 1126,loss: 0.027441535\n",
            "\n",
            "Global step: 1127,loss: 0.025016986\n",
            "\n",
            "Global step: 1128,loss: 0.027486706\n",
            "\n",
            "Global step: 1129,loss: 0.03044147\n",
            "\n",
            "Global step: 1130,loss: 0.028187172\n",
            "\n",
            "Global step: 1131,loss: 0.028467972\n",
            "\n",
            "Global step: 1132,loss: 0.024865856\n",
            "\n",
            "Global step: 1133,loss: 0.03137754\n",
            "\n",
            "Global step: 1134,loss: 0.024750538\n",
            "\n",
            "Global step: 1135,loss: 0.026413826\n",
            "\n",
            "Global step: 1136,loss: 0.028888414\n",
            "\n",
            "Global step: 1137,loss: 0.02507381\n",
            "\n",
            "Global step: 1138,loss: 0.025113966\n",
            "\n",
            "Global step: 1139,loss: 0.02427623\n",
            "\n",
            "Global step: 1140,loss: 0.024035417\n",
            "\n",
            "Global step: 1141,loss: 0.023277512\n",
            "\n",
            "Global step: 1142,loss: 0.025767244\n",
            "\n",
            "Global step: 1143,loss: 0.02917995\n",
            "\n",
            "Global step: 1144,loss: 0.033657044\n",
            "\n",
            "Global step: 1145,loss: 0.023114743\n",
            "\n",
            "Global step: 1146,loss: 0.027826061\n",
            "\n",
            "Global step: 1147,loss: 0.024640381\n",
            "\n",
            "Global step: 1148,loss: 0.02638422\n",
            "\n",
            "Global step: 1149,loss: 0.025313994\n",
            "\n",
            "Global step: 1150,loss: 0.028976675\n",
            "\n",
            "Global step: 1151,loss: 0.029793791\n",
            "\n",
            "Global step: 1152,loss: 0.027191944\n",
            "\n",
            "Global step: 1153,loss: 0.024512671\n",
            "\n",
            "Global step: 1154,loss: 0.023767004\n",
            "\n",
            "Global step: 1155,loss: 0.026657674\n",
            "\n",
            "Global step: 1156,loss: 0.023090307\n",
            "\n",
            "Global step: 1157,loss: 0.024984432\n",
            "\n",
            "Global step: 1158,loss: 0.024994489\n",
            "\n",
            "Global step: 1159,loss: 0.02371048\n",
            "\n",
            "Global step: 1160,loss: 0.02407251\n",
            "\n",
            "Global step: 1161,loss: 0.027576538\n",
            "\n",
            "Global step: 1162,loss: 0.02643881\n",
            "\n",
            "Global step: 1163,loss: 0.023524676\n",
            "\n",
            "Global step: 1164,loss: 0.025489569\n",
            "\n",
            "Global step: 1165,loss: 0.022753913\n",
            "\n",
            "Global step: 1166,loss: 0.026490573\n",
            "\n",
            "Global step: 1167,loss: 0.023742916\n",
            "\n",
            "Global step: 1168,loss: 0.023789614\n",
            "\n",
            "Global step: 1169,loss: 0.026177302\n",
            "\n",
            "Global step: 1170,loss: 0.02827856\n",
            "\n",
            "Global step: 1171,loss: 0.027677288\n",
            "\n",
            "Global step: 1172,loss: 0.03231865\n",
            "\n",
            "Global step: 1173,loss: 0.023939628\n",
            "\n",
            "Global step: 1174,loss: 0.022965057\n",
            "\n",
            "Global step: 1175,loss: 0.024864426\n",
            "\n",
            "Global step: 1176,loss: 0.027362926\n",
            "\n",
            "Global step: 1177,loss: 0.025016524\n",
            "\n",
            "Global step: 1178,loss: 0.028866723\n",
            "\n",
            "Global step: 1179,loss: 0.028591704\n",
            "\n",
            "Global step: 1180,loss: 0.024157042\n",
            "\n",
            "Global step: 1181,loss: 0.02879512\n",
            "\n",
            "Global step: 1182,loss: 0.025892103\n",
            "\n",
            "Global step: 1183,loss: 0.023576194\n",
            "\n",
            "Global step: 1184,loss: 0.02511547\n",
            "\n",
            "Global step: 1185,loss: 0.024058353\n",
            "\n",
            "Global step: 1186,loss: 0.029143924\n",
            "\n",
            "Global step: 1187,loss: 0.02776644\n",
            "\n",
            "Global step: 1188,loss: 0.024353577\n",
            "\n",
            "Global step: 1189,loss: 0.02456468\n",
            "\n",
            "Global step: 1190,loss: 0.027545689\n",
            "\n",
            "Global step: 1191,loss: 0.023548074\n",
            "\n",
            "Global step: 1192,loss: 0.026536824\n",
            "\n",
            "Global step: 1193,loss: 0.026984511\n",
            "\n",
            "Global step: 1194,loss: 0.024218025\n",
            "\n",
            "Global step: 1195,loss: 0.031621125\n",
            "\n",
            "Global step: 1196,loss: 0.02879182\n",
            "\n",
            "Global step: 1197,loss: 0.024443636\n",
            "\n",
            "Global step: 1198,loss: 0.024998229\n",
            "\n",
            "Global step: 1199,loss: 0.02310917\n",
            "\n",
            "Global step: 1200,loss: 0.028686248\n",
            "\n",
            "Global step: 1201,loss: 0.02496487\n",
            "\n",
            "Global step: 1202,loss: 0.02556536\n",
            "\n",
            "Global step: 1203,loss: 0.029797982\n",
            "\n",
            "Global step: 1204,loss: 0.023195233\n",
            "\n",
            "Global step: 1205,loss: 0.023884367\n",
            "\n",
            "Global step: 1206,loss: 0.025200859\n",
            "\n",
            "Global step: 1207,loss: 0.025200583\n",
            "\n",
            "Global step: 1208,loss: 0.02635589\n",
            "\n",
            "Global step: 1209,loss: 0.026357114\n",
            "\n",
            "Global step: 1210,loss: 0.025480753\n",
            "\n",
            "Global step: 1211,loss: 0.025790319\n",
            "\n",
            "Global step: 1212,loss: 0.025466667\n",
            "\n",
            "Global step: 1213,loss: 0.025658727\n",
            "\n",
            "Global step: 1214,loss: 0.02372666\n",
            "\n",
            "Global step: 1215,loss: 0.025297955\n",
            "\n",
            "Global step: 1216,loss: 0.02443194\n",
            "\n",
            "Global step: 1217,loss: 0.029307282\n",
            "\n",
            "Global step: 1218,loss: 0.02567567\n",
            "\n",
            "Global step: 1219,loss: 0.026595527\n",
            "\n",
            "Global step: 1220,loss: 0.023241144\n",
            "\n",
            "Global step: 1221,loss: 0.028694667\n",
            "\n",
            "Global step: 1222,loss: 0.02483208\n",
            "\n",
            "Global step: 1223,loss: 0.024748746\n",
            "\n",
            "Global step: 1224,loss: 0.026232352\n",
            "\n",
            "Global step: 1225,loss: 0.026653154\n",
            "\n",
            "Global step: 1226,loss: 0.024644298\n",
            "\n",
            "Global step: 1227,loss: 0.027648833\n",
            "\n",
            "Global step: 1228,loss: 0.028327437\n",
            "\n",
            "Global step: 1229,loss: 0.024543036\n",
            "\n",
            "Global step: 1230,loss: 0.023485443\n",
            "\n",
            "Global step: 1231,loss: 0.026091544\n",
            "\n",
            "Global step: 1232,loss: 0.028186522\n",
            "\n",
            "Global step: 1233,loss: 0.026167676\n",
            "\n",
            "Global step: 1234,loss: 0.02831206\n",
            "\n",
            "Global step: 1235,loss: 0.02280777\n",
            "\n",
            "Global step: 1236,loss: 0.024822358\n",
            "\n",
            "Global step: 1237,loss: 0.028636586\n",
            "\n",
            "Global step: 1238,loss: 0.027590467\n",
            "\n",
            "Global step: 1239,loss: 0.024976641\n",
            "\n",
            "Global step: 1240,loss: 0.028066019\n",
            "\n",
            "Global step: 1241,loss: 0.022948181\n",
            "\n",
            "Global step: 1242,loss: 0.029995227\n",
            "\n",
            "Global step: 1243,loss: 0.022622805\n",
            "\n",
            "Global step: 1244,loss: 0.025201641\n",
            "\n",
            "Global step: 1245,loss: 0.026390161\n",
            "\n",
            "Global step: 1246,loss: 0.025493307\n",
            "\n",
            "Global step: 1247,loss: 0.024738556\n",
            "\n",
            "Global step: 1248,loss: 0.030757304\n",
            "\n",
            "Global step: 1249,loss: 0.025083585\n",
            "\n",
            "Global step: 1250,loss: 0.023784157\n",
            "\n",
            "Global step: 1251,loss: 0.027329922\n",
            "\n",
            "Global step: 1252,loss: 0.023577215\n",
            "\n",
            "Global step: 1253,loss: 0.025055973\n",
            "\n",
            "Global step: 1254,loss: 0.025893811\n",
            "\n",
            "Global step: 1255,loss: 0.029855922\n",
            "\n",
            "Global step: 1256,loss: 0.023807175\n",
            "\n",
            "Global step: 1257,loss: 0.030098978\n",
            "\n",
            "Global step: 1258,loss: 0.025547175\n",
            "\n",
            "Global step: 1259,loss: 0.02796286\n",
            "\n",
            "Global step: 1260,loss: 0.024574146\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.56955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:50:39.416196 140504762959616 supervisor.py:1099] global_step/sec: 1.56955\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1261,loss: 0.02582238\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1262.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:50:40.046000 140504771352320 supervisor.py:1050] Recording summary at step 1262.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1262,loss: 0.026822766\n",
            "\n",
            "Global step: 1263,loss: 0.025647897\n",
            "\n",
            "Global step: 1264,loss: 0.035112\n",
            "\n",
            "Global step: 1265,loss: 0.025533332\n",
            "\n",
            "Global step: 1266,loss: 0.027493373\n",
            "\n",
            "Global step: 1267,loss: 0.027736273\n",
            "\n",
            "Global step: 1268,loss: 0.023692505\n",
            "\n",
            "Global step: 1269,loss: 0.027958086\n",
            "\n",
            "Global step: 1270,loss: 0.031237423\n",
            "\n",
            "Global step: 1271,loss: 0.025023662\n",
            "\n",
            "Global step: 1272,loss: 0.026460107\n",
            "\n",
            "Global step: 1273,loss: 0.02629957\n",
            "\n",
            "Global step: 1274,loss: 0.026556224\n",
            "\n",
            "Global step: 1275,loss: 0.024339445\n",
            "\n",
            "Global step: 1276,loss: 0.028914992\n",
            "\n",
            "Global step: 1277,loss: 0.025256485\n",
            "\n",
            "Global step: 1278,loss: 0.023695122\n",
            "\n",
            "Global step: 1279,loss: 0.028682534\n",
            "\n",
            "Global step: 1280,loss: 0.02458608\n",
            "\n",
            "Global step: 1281,loss: 0.024913114\n",
            "\n",
            "Global step: 1282,loss: 0.024163617\n",
            "\n",
            "Global step: 1283,loss: 0.02286731\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 1283,val_loss: 0.03571888206428603\n",
            "\n",
            "Training for epoch 7/16:\n",
            "Global step: 1284,loss: 0.023818517\n",
            "\n",
            "Global step: 1285,loss: 0.024813415\n",
            "\n",
            "Global step: 1286,loss: 0.026817761\n",
            "\n",
            "Global step: 1287,loss: 0.026009604\n",
            "\n",
            "Global step: 1288,loss: 0.024939697\n",
            "\n",
            "Global step: 1289,loss: 0.027374115\n",
            "\n",
            "Global step: 1290,loss: 0.023745682\n",
            "\n",
            "Global step: 1291,loss: 0.024883594\n",
            "\n",
            "Global step: 1292,loss: 0.02299654\n",
            "\n",
            "Global step: 1293,loss: 0.021910667\n",
            "\n",
            "Global step: 1294,loss: 0.024923932\n",
            "\n",
            "Global step: 1295,loss: 0.02345508\n",
            "\n",
            "Global step: 1296,loss: 0.02592\n",
            "\n",
            "Global step: 1297,loss: 0.027154444\n",
            "\n",
            "Global step: 1298,loss: 0.022103732\n",
            "\n",
            "Global step: 1299,loss: 0.02434412\n",
            "\n",
            "Global step: 1300,loss: 0.021996314\n",
            "\n",
            "Global step: 1301,loss: 0.025822239\n",
            "\n",
            "Global step: 1302,loss: 0.024662733\n",
            "\n",
            "Global step: 1303,loss: 0.02225858\n",
            "\n",
            "Global step: 1304,loss: 0.022585722\n",
            "\n",
            "Global step: 1305,loss: 0.02585544\n",
            "\n",
            "Global step: 1306,loss: 0.028249094\n",
            "\n",
            "Global step: 1307,loss: 0.022443848\n",
            "\n",
            "Global step: 1308,loss: 0.025989003\n",
            "\n",
            "Global step: 1309,loss: 0.02142273\n",
            "\n",
            "Global step: 1310,loss: 0.024222946\n",
            "\n",
            "Global step: 1311,loss: 0.025252879\n",
            "\n",
            "Global step: 1312,loss: 0.025652492\n",
            "\n",
            "Global step: 1313,loss: 0.026576955\n",
            "\n",
            "Global step: 1314,loss: 0.023032514\n",
            "\n",
            "Global step: 1315,loss: 0.023506185\n",
            "\n",
            "Global step: 1316,loss: 0.023369676\n",
            "\n",
            "Global step: 1317,loss: 0.022178568\n",
            "\n",
            "Global step: 1318,loss: 0.024359109\n",
            "\n",
            "Global step: 1319,loss: 0.02241033\n",
            "\n",
            "Global step: 1320,loss: 0.02290674\n",
            "\n",
            "Global step: 1321,loss: 0.026259847\n",
            "\n",
            "Global step: 1322,loss: 0.023487568\n",
            "\n",
            "Global step: 1323,loss: 0.026267823\n",
            "\n",
            "Global step: 1324,loss: 0.024206528\n",
            "\n",
            "Global step: 1325,loss: 0.023844967\n",
            "\n",
            "Global step: 1326,loss: 0.025460187\n",
            "\n",
            "Global step: 1327,loss: 0.023202352\n",
            "\n",
            "Global step: 1328,loss: 0.028636979\n",
            "\n",
            "Global step: 1329,loss: 0.022893097\n",
            "\n",
            "Global step: 1330,loss: 0.02612056\n",
            "\n",
            "Global step: 1331,loss: 0.023158941\n",
            "\n",
            "Global step: 1332,loss: 0.02461412\n",
            "\n",
            "Global step: 1333,loss: 0.02340937\n",
            "\n",
            "Global step: 1334,loss: 0.02205938\n",
            "\n",
            "Global step: 1335,loss: 0.02417658\n",
            "\n",
            "Global step: 1336,loss: 0.024915408\n",
            "\n",
            "Global step: 1337,loss: 0.024340164\n",
            "\n",
            "Global step: 1338,loss: 0.028017947\n",
            "\n",
            "Global step: 1339,loss: 0.025922224\n",
            "\n",
            "Global step: 1340,loss: 0.022691948\n",
            "\n",
            "Global step: 1341,loss: 0.023272123\n",
            "\n",
            "Global step: 1342,loss: 0.025345722\n",
            "\n",
            "Global step: 1343,loss: 0.02711421\n",
            "\n",
            "Global step: 1344,loss: 0.022725172\n",
            "\n",
            "Global step: 1345,loss: 0.02441623\n",
            "\n",
            "Global step: 1346,loss: 0.021475347\n",
            "\n",
            "Global step: 1347,loss: 0.022990685\n",
            "\n",
            "Global step: 1348,loss: 0.024620239\n",
            "\n",
            "Global step: 1349,loss: 0.028568832\n",
            "\n",
            "Global step: 1350,loss: 0.023410661\n",
            "\n",
            "Global step: 1351,loss: 0.02457169\n",
            "\n",
            "Global step: 1352,loss: 0.022703636\n",
            "\n",
            "Global step: 1353,loss: 0.021704951\n",
            "\n",
            "Global step: 1354,loss: 0.0265878\n",
            "\n",
            "Global step: 1355,loss: 0.022707177\n",
            "\n",
            "Global step: 1356,loss: 0.0262101\n",
            "\n",
            "Global step: 1357,loss: 0.022562714\n",
            "\n",
            "Global step: 1358,loss: 0.027892865\n",
            "\n",
            "Global step: 1359,loss: 0.028052278\n",
            "\n",
            "Global step: 1360,loss: 0.02457659\n",
            "\n",
            "Global step: 1361,loss: 0.025251226\n",
            "\n",
            "Global step: 1362,loss: 0.028616754\n",
            "\n",
            "Global step: 1363,loss: 0.026701465\n",
            "\n",
            "Global step: 1364,loss: 0.021885574\n",
            "\n",
            "Global step: 1365,loss: 0.024908235\n",
            "\n",
            "Global step: 1366,loss: 0.026631478\n",
            "\n",
            "Global step: 1367,loss: 0.02493763\n",
            "\n",
            "Global step: 1368,loss: 0.02338355\n",
            "\n",
            "Global step: 1369,loss: 0.022967631\n",
            "\n",
            "Global step: 1370,loss: 0.02570632\n",
            "\n",
            "Global step: 1371,loss: 0.02547594\n",
            "\n",
            "Global step: 1372,loss: 0.027793428\n",
            "\n",
            "Global step: 1373,loss: 0.024077263\n",
            "\n",
            "Global step: 1374,loss: 0.02448174\n",
            "\n",
            "Global step: 1375,loss: 0.023223147\n",
            "\n",
            "Global step: 1376,loss: 0.02798936\n",
            "\n",
            "Global step: 1377,loss: 0.024621192\n",
            "\n",
            "Global step: 1378,loss: 0.023873849\n",
            "\n",
            "Global step: 1379,loss: 0.025601666\n",
            "\n",
            "Global step: 1380,loss: 0.023291387\n",
            "\n",
            "Global step: 1381,loss: 0.02644666\n",
            "\n",
            "Global step: 1382,loss: 0.02177526\n",
            "\n",
            "Global step: 1383,loss: 0.022651268\n",
            "\n",
            "Global step: 1384,loss: 0.024747288\n",
            "\n",
            "Global step: 1385,loss: 0.02694054\n",
            "\n",
            "Global step: 1386,loss: 0.024379218\n",
            "\n",
            "Global step: 1387,loss: 0.024091024\n",
            "\n",
            "Global step: 1388,loss: 0.02204531\n",
            "\n",
            "Global step: 1389,loss: 0.02200916\n",
            "\n",
            "Global step: 1390,loss: 0.023289287\n",
            "\n",
            "Global step: 1391,loss: 0.022787435\n",
            "\n",
            "Global step: 1392,loss: 0.023935672\n",
            "\n",
            "Global step: 1393,loss: 0.02366526\n",
            "\n",
            "Global step: 1394,loss: 0.023677465\n",
            "\n",
            "Global step: 1395,loss: 0.025107447\n",
            "\n",
            "Global step: 1396,loss: 0.023511473\n",
            "\n",
            "Global step: 1397,loss: 0.026014041\n",
            "\n",
            "Global step: 1398,loss: 0.023350023\n",
            "\n",
            "Global step: 1399,loss: 0.024166502\n",
            "\n",
            "Global step: 1400,loss: 0.025279775\n",
            "\n",
            "Global step: 1401,loss: 0.024439445\n",
            "\n",
            "Global step: 1402,loss: 0.024264526\n",
            "\n",
            "Global step: 1403,loss: 0.022378052\n",
            "\n",
            "Global step: 1404,loss: 0.023950728\n",
            "\n",
            "Global step: 1405,loss: 0.025980465\n",
            "\n",
            "Global step: 1406,loss: 0.021491848\n",
            "\n",
            "Global step: 1407,loss: 0.023064904\n",
            "\n",
            "Global step: 1408,loss: 0.023240605\n",
            "\n",
            "Global step: 1409,loss: 0.022604834\n",
            "\n",
            "Global step: 1410,loss: 0.024677113\n",
            "\n",
            "Global step: 1411,loss: 0.02201537\n",
            "\n",
            "Global step: 1412,loss: 0.02696917\n",
            "\n",
            "Global step: 1413,loss: 0.022719499\n",
            "\n",
            "Global step: 1414,loss: 0.022884967\n",
            "\n",
            "Global step: 1415,loss: 0.021755679\n",
            "\n",
            "Global step: 1416,loss: 0.024491642\n",
            "\n",
            "Global step: 1417,loss: 0.022957405\n",
            "\n",
            "Global step: 1418,loss: 0.023655163\n",
            "\n",
            "Global step: 1419,loss: 0.02399397\n",
            "\n",
            "Global step: 1420,loss: 0.022185594\n",
            "\n",
            "Global step: 1421,loss: 0.02498423\n",
            "\n",
            "Global step: 1422,loss: 0.021248298\n",
            "\n",
            "Global step: 1423,loss: 0.026560413\n",
            "\n",
            "Global step: 1424,loss: 0.027196312\n",
            "\n",
            "Global step: 1425,loss: 0.027751995\n",
            "\n",
            "Global step: 1426,loss: 0.027893491\n",
            "\n",
            "Global step: 1427,loss: 0.023566106\n",
            "\n",
            "Global step: 1428,loss: 0.025905544\n",
            "\n",
            "Global step: 1429,loss: 0.024097934\n",
            "\n",
            "Global step: 1430,loss: 0.022104122\n",
            "\n",
            "Global step: 1431,loss: 0.02248229\n",
            "\n",
            "Global step: 1432,loss: 0.022197759\n",
            "\n",
            "Global step: 1433,loss: 0.023925843\n",
            "\n",
            "Global step: 1434,loss: 0.027946036\n",
            "\n",
            "Global step: 1435,loss: 0.0244819\n",
            "\n",
            "Global step: 1436,loss: 0.024858277\n",
            "\n",
            "Global step: 1437,loss: 0.023638263\n",
            "\n",
            "Global step: 1438,loss: 0.023535104\n",
            "\n",
            "Global step: 1439,loss: 0.023314444\n",
            "\n",
            "Global step: 1440,loss: 0.02274674\n",
            "\n",
            "Global step: 1441,loss: 0.024771575\n",
            "\n",
            "Global step: 1442,loss: 0.02236009\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.51667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:52:39.415485 140504762959616 supervisor.py:1099] global_step/sec: 1.51667\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 1443.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:52:39.725858 140504771352320 supervisor.py:1050] Recording summary at step 1443.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1443,loss: 0.023139652\n",
            "\n",
            "Global step: 1444,loss: 0.021072634\n",
            "\n",
            "Global step: 1445,loss: 0.025853746\n",
            "\n",
            "Global step: 1446,loss: 0.027321965\n",
            "\n",
            "Global step: 1447,loss: 0.02213074\n",
            "\n",
            "Global step: 1448,loss: 0.021417143\n",
            "\n",
            "Global step: 1449,loss: 0.023849528\n",
            "\n",
            "Global step: 1450,loss: 0.02432869\n",
            "\n",
            "Global step: 1451,loss: 0.02515224\n",
            "\n",
            "Global step: 1452,loss: 0.02544171\n",
            "\n",
            "Global step: 1453,loss: 0.021863224\n",
            "\n",
            "Global step: 1454,loss: 0.025234781\n",
            "\n",
            "Global step: 1455,loss: 0.025056435\n",
            "\n",
            "Global step: 1456,loss: 0.020133384\n",
            "\n",
            "Global step: 1457,loss: 0.024619358\n",
            "\n",
            "Global step: 1458,loss: 0.023122206\n",
            "\n",
            "Global step: 1459,loss: 0.022716183\n",
            "\n",
            "Global step: 1460,loss: 0.022159763\n",
            "\n",
            "Global step: 1461,loss: 0.02384321\n",
            "\n",
            "Global step: 1462,loss: 0.031824727\n",
            "\n",
            "Global step: 1463,loss: 0.025364954\n",
            "\n",
            "Global step: 1464,loss: 0.021962142\n",
            "\n",
            "Global step: 1465,loss: 0.024741022\n",
            "\n",
            "Global step: 1466,loss: 0.027546288\n",
            "\n",
            "Global step: 1467,loss: 0.02456567\n",
            "\n",
            "Global step: 1468,loss: 0.022105517\n",
            "\n",
            "Global step: 1469,loss: 0.022905443\n",
            "\n",
            "Global step: 1470,loss: 0.022377368\n",
            "\n",
            "Global step: 1471,loss: 0.02247551\n",
            "\n",
            "Global step: 1472,loss: 0.022794852\n",
            "\n",
            "Global step: 1473,loss: 0.023743402\n",
            "\n",
            "Global step: 1474,loss: 0.023750424\n",
            "\n",
            "Global step: 1475,loss: 0.02252239\n",
            "\n",
            "Global step: 1476,loss: 0.022348333\n",
            "\n",
            "Global step: 1477,loss: 0.022697313\n",
            "\n",
            "Global step: 1478,loss: 0.023563921\n",
            "\n",
            "Global step: 1479,loss: 0.020732693\n",
            "\n",
            "Global step: 1480,loss: 0.020047491\n",
            "\n",
            "Global step: 1481,loss: 0.022057181\n",
            "\n",
            "Global step: 1482,loss: 0.024982281\n",
            "\n",
            "Global step: 1483,loss: 0.02441748\n",
            "\n",
            "Global step: 1484,loss: 0.021834692\n",
            "\n",
            "Global step: 1485,loss: 0.02355935\n",
            "\n",
            "Global step: 1486,loss: 0.023819737\n",
            "\n",
            "Global step: 1487,loss: 0.021136757\n",
            "\n",
            "Global step: 1488,loss: 0.022333072\n",
            "\n",
            "Global step: 1489,loss: 0.02236681\n",
            "\n",
            "Global step: 1490,loss: 0.025041917\n",
            "\n",
            "Global step: 1491,loss: 0.023930836\n",
            "\n",
            "Global step: 1492,loss: 0.022077536\n",
            "\n",
            "Global step: 1493,loss: 0.023099665\n",
            "\n",
            "Global step: 1494,loss: 0.024282476\n",
            "\n",
            "Global step: 1495,loss: 0.021389524\n",
            "\n",
            "Global step: 1496,loss: 0.022937635\n",
            "\n",
            "Global step: 1497,loss: 0.022633364\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1497,Val_Loss: 0.03352624099505575,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:53:19.835936 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 11:53:20.255744 140508081637248 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 8/16:\n",
            "Global step: 1498,loss: 0.021177588\n",
            "\n",
            "Global step: 1499,loss: 0.0221965\n",
            "\n",
            "Global step: 1500,loss: 0.022259085\n",
            "\n",
            "Global step: 1501,loss: 0.023552004\n",
            "\n",
            "Global step: 1502,loss: 0.022277538\n",
            "\n",
            "Global step: 1503,loss: 0.02555349\n",
            "\n",
            "Global step: 1504,loss: 0.023106286\n",
            "\n",
            "Global step: 1505,loss: 0.02262391\n",
            "\n",
            "Global step: 1506,loss: 0.021451784\n",
            "\n",
            "Global step: 1507,loss: 0.02080497\n",
            "\n",
            "Global step: 1508,loss: 0.021825891\n",
            "\n",
            "Global step: 1509,loss: 0.022141574\n",
            "\n",
            "Global step: 1510,loss: 0.024641918\n",
            "\n",
            "Global step: 1511,loss: 0.021620184\n",
            "\n",
            "Global step: 1512,loss: 0.021914212\n",
            "\n",
            "Global step: 1513,loss: 0.029915106\n",
            "\n",
            "Global step: 1514,loss: 0.026976855\n",
            "\n",
            "Global step: 1515,loss: 0.022768833\n",
            "\n",
            "Global step: 1516,loss: 0.021556208\n",
            "\n",
            "Global step: 1517,loss: 0.024519334\n",
            "\n",
            "Global step: 1518,loss: 0.024332533\n",
            "\n",
            "Global step: 1519,loss: 0.024686266\n",
            "\n",
            "Global step: 1520,loss: 0.028603617\n",
            "\n",
            "Global step: 1521,loss: 0.02257215\n",
            "\n",
            "Global step: 1522,loss: 0.022439262\n",
            "\n",
            "Global step: 1523,loss: 0.020104654\n",
            "\n",
            "Global step: 1524,loss: 0.021704845\n",
            "\n",
            "Global step: 1525,loss: 0.024214813\n",
            "\n",
            "Global step: 1526,loss: 0.021158539\n",
            "\n",
            "Global step: 1527,loss: 0.02276641\n",
            "\n",
            "Global step: 1528,loss: 0.02255992\n",
            "\n",
            "Global step: 1529,loss: 0.023862049\n",
            "\n",
            "Global step: 1530,loss: 0.02369113\n",
            "\n",
            "Global step: 1531,loss: 0.02049825\n",
            "\n",
            "Global step: 1532,loss: 0.02262363\n",
            "\n",
            "Global step: 1533,loss: 0.023039762\n",
            "\n",
            "Global step: 1534,loss: 0.023544548\n",
            "\n",
            "Global step: 1535,loss: 0.020343589\n",
            "\n",
            "Global step: 1536,loss: 0.022364868\n",
            "\n",
            "Global step: 1537,loss: 0.023100436\n",
            "\n",
            "Global step: 1538,loss: 0.024886597\n",
            "\n",
            "Global step: 1539,loss: 0.021251256\n",
            "\n",
            "Global step: 1540,loss: 0.022381688\n",
            "\n",
            "Global step: 1541,loss: 0.022570062\n",
            "\n",
            "Global step: 1542,loss: 0.021129444\n",
            "\n",
            "Global step: 1543,loss: 0.022127222\n",
            "\n",
            "Global step: 1544,loss: 0.024437077\n",
            "\n",
            "Global step: 1545,loss: 0.01982406\n",
            "\n",
            "Global step: 1546,loss: 0.024462495\n",
            "\n",
            "Global step: 1547,loss: 0.021591185\n",
            "\n",
            "Global step: 1548,loss: 0.021416418\n",
            "\n",
            "Global step: 1549,loss: 0.02172396\n",
            "\n",
            "Global step: 1550,loss: 0.023606824\n",
            "\n",
            "Global step: 1551,loss: 0.020874484\n",
            "\n",
            "Global step: 1552,loss: 0.020557627\n",
            "\n",
            "Global step: 1553,loss: 0.019977862\n",
            "\n",
            "Global step: 1554,loss: 0.023770217\n",
            "\n",
            "Global step: 1555,loss: 0.022201966\n",
            "\n",
            "Global step: 1556,loss: 0.021890225\n",
            "\n",
            "Global step: 1557,loss: 0.020411124\n",
            "\n",
            "Global step: 1558,loss: 0.019968439\n",
            "\n",
            "Global step: 1559,loss: 0.021782665\n",
            "\n",
            "Global step: 1560,loss: 0.025223736\n",
            "\n",
            "Global step: 1561,loss: 0.022988431\n",
            "\n",
            "Global step: 1562,loss: 0.024331316\n",
            "\n",
            "Global step: 1563,loss: 0.02348682\n",
            "\n",
            "Global step: 1564,loss: 0.02467021\n",
            "\n",
            "Global step: 1565,loss: 0.02162513\n",
            "\n",
            "Global step: 1566,loss: 0.020896312\n",
            "\n",
            "Global step: 1567,loss: 0.023761198\n",
            "\n",
            "Global step: 1568,loss: 0.019606005\n",
            "\n",
            "Global step: 1569,loss: 0.021925194\n",
            "\n",
            "Global step: 1570,loss: 0.02353155\n",
            "\n",
            "Global step: 1571,loss: 0.02382592\n",
            "\n",
            "Global step: 1572,loss: 0.021940548\n",
            "\n",
            "Global step: 1573,loss: 0.020175118\n",
            "\n",
            "Global step: 1574,loss: 0.024042113\n",
            "\n",
            "Global step: 1575,loss: 0.023551159\n",
            "\n",
            "Global step: 1576,loss: 0.020117754\n",
            "\n",
            "Global step: 1577,loss: 0.020890784\n",
            "\n",
            "Global step: 1578,loss: 0.023195945\n",
            "\n",
            "Global step: 1579,loss: 0.02468618\n",
            "\n",
            "Global step: 1580,loss: 0.020716451\n",
            "\n",
            "Global step: 1581,loss: 0.020701647\n",
            "\n",
            "Global step: 1582,loss: 0.024033237\n",
            "\n",
            "Global step: 1583,loss: 0.0245946\n",
            "\n",
            "Global step: 1584,loss: 0.021691624\n",
            "\n",
            "Global step: 1585,loss: 0.020784065\n",
            "\n",
            "Global step: 1586,loss: 0.019852482\n",
            "\n",
            "Global step: 1587,loss: 0.021216381\n",
            "\n",
            "Global step: 1588,loss: 0.022730974\n",
            "\n",
            "Global step: 1589,loss: 0.021129552\n",
            "\n",
            "Global step: 1590,loss: 0.021249652\n",
            "\n",
            "Global step: 1591,loss: 0.01989215\n",
            "\n",
            "Global step: 1592,loss: 0.02263473\n",
            "\n",
            "Global step: 1593,loss: 0.023048438\n",
            "\n",
            "Global step: 1594,loss: 0.025187196\n",
            "\n",
            "Global step: 1595,loss: 0.021695146\n",
            "\n",
            "Global step: 1596,loss: 0.025682598\n",
            "\n",
            "Global step: 1597,loss: 0.024360895\n",
            "\n",
            "Global step: 1598,loss: 0.021341333\n",
            "\n",
            "Global step: 1599,loss: 0.026911166\n",
            "\n",
            "Global step: 1600,loss: 0.021540198\n",
            "\n",
            "Global step: 1601,loss: 0.02179045\n",
            "\n",
            "Global step: 1602,loss: 0.021644387\n",
            "\n",
            "Global step: 1603,loss: 0.021020768\n",
            "\n",
            "Global step: 1604,loss: 0.027692115\n",
            "\n",
            "Global step: 1605,loss: 0.020697443\n",
            "\n",
            "Global step: 1606,loss: 0.021022025\n",
            "\n",
            "Global step: 1607,loss: 0.021941274\n",
            "\n",
            "Global step: 1608,loss: 0.02150178\n",
            "\n",
            "Global step: 1609,loss: 0.020466631\n",
            "\n",
            "Global step: 1610,loss: 0.020813422\n",
            "\n",
            "Global step: 1611,loss: 0.02117838\n",
            "\n",
            "Global step: 1612,loss: 0.023153853\n",
            "\n",
            "Global step: 1613,loss: 0.023795784\n",
            "\n",
            "Global step: 1614,loss: 0.022784553\n",
            "\n",
            "Global step: 1615,loss: 0.024243161\n",
            "\n",
            "Global step: 1616,loss: 0.021334603\n",
            "\n",
            "Global step: 1617,loss: 0.020828672\n",
            "\n",
            "Global step: 1618,loss: 0.023975134\n",
            "\n",
            "Global step: 1619,loss: 0.021354264\n",
            "\n",
            "Global step: 1620,loss: 0.024023894\n",
            "\n",
            "Global step: 1621,loss: 0.026677025\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.48888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:54:39.640480 140504762959616 supervisor.py:1099] global_step/sec: 1.48888\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 1622.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:54:39.643257 140504771352320 supervisor.py:1050] Recording summary at step 1622.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1622,loss: 0.021184364\n",
            "\n",
            "Global step: 1623,loss: 0.020919582\n",
            "\n",
            "Global step: 1624,loss: 0.021929054\n",
            "\n",
            "Global step: 1625,loss: 0.025185231\n",
            "\n",
            "Global step: 1626,loss: 0.022649301\n",
            "\n",
            "Global step: 1627,loss: 0.019870736\n",
            "\n",
            "Global step: 1628,loss: 0.022789422\n",
            "\n",
            "Global step: 1629,loss: 0.021385035\n",
            "\n",
            "Global step: 1630,loss: 0.021329824\n",
            "\n",
            "Global step: 1631,loss: 0.023665832\n",
            "\n",
            "Global step: 1632,loss: 0.02186713\n",
            "\n",
            "Global step: 1633,loss: 0.021058084\n",
            "\n",
            "Global step: 1634,loss: 0.02085199\n",
            "\n",
            "Global step: 1635,loss: 0.021821287\n",
            "\n",
            "Global step: 1636,loss: 0.022582833\n",
            "\n",
            "Global step: 1637,loss: 0.021233752\n",
            "\n",
            "Global step: 1638,loss: 0.022261612\n",
            "\n",
            "Global step: 1639,loss: 0.02336752\n",
            "\n",
            "Global step: 1640,loss: 0.023884734\n",
            "\n",
            "Global step: 1641,loss: 0.02366397\n",
            "\n",
            "Global step: 1642,loss: 0.020338502\n",
            "\n",
            "Global step: 1643,loss: 0.023600865\n",
            "\n",
            "Global step: 1644,loss: 0.021885937\n",
            "\n",
            "Global step: 1645,loss: 0.024376333\n",
            "\n",
            "Global step: 1646,loss: 0.024327857\n",
            "\n",
            "Global step: 1647,loss: 0.022269096\n",
            "\n",
            "Global step: 1648,loss: 0.021175388\n",
            "\n",
            "Global step: 1649,loss: 0.01994557\n",
            "\n",
            "Global step: 1650,loss: 0.021695344\n",
            "\n",
            "Global step: 1651,loss: 0.025648886\n",
            "\n",
            "Global step: 1652,loss: 0.021695167\n",
            "\n",
            "Global step: 1653,loss: 0.021272376\n",
            "\n",
            "Global step: 1654,loss: 0.023647286\n",
            "\n",
            "Global step: 1655,loss: 0.022538483\n",
            "\n",
            "Global step: 1656,loss: 0.021746976\n",
            "\n",
            "Global step: 1657,loss: 0.021804165\n",
            "\n",
            "Global step: 1658,loss: 0.019982515\n",
            "\n",
            "Global step: 1659,loss: 0.022441076\n",
            "\n",
            "Global step: 1660,loss: 0.021823235\n",
            "\n",
            "Global step: 1661,loss: 0.025715701\n",
            "\n",
            "Global step: 1662,loss: 0.02428311\n",
            "\n",
            "Global step: 1663,loss: 0.021914557\n",
            "\n",
            "Global step: 1664,loss: 0.023109047\n",
            "\n",
            "Global step: 1665,loss: 0.020689767\n",
            "\n",
            "Global step: 1666,loss: 0.024388295\n",
            "\n",
            "Global step: 1667,loss: 0.021981698\n",
            "\n",
            "Global step: 1668,loss: 0.021083647\n",
            "\n",
            "Global step: 1669,loss: 0.020892216\n",
            "\n",
            "Global step: 1670,loss: 0.02103331\n",
            "\n",
            "Global step: 1671,loss: 0.02069343\n",
            "\n",
            "Global step: 1672,loss: 0.028266292\n",
            "\n",
            "Global step: 1673,loss: 0.020647857\n",
            "\n",
            "Global step: 1674,loss: 0.023685964\n",
            "\n",
            "Global step: 1675,loss: 0.021822711\n",
            "\n",
            "Global step: 1676,loss: 0.0220495\n",
            "\n",
            "Global step: 1677,loss: 0.020475222\n",
            "\n",
            "Global step: 1678,loss: 0.021436734\n",
            "\n",
            "Global step: 1679,loss: 0.022643788\n",
            "\n",
            "Global step: 1680,loss: 0.020000286\n",
            "\n",
            "Global step: 1681,loss: 0.020198166\n",
            "\n",
            "Global step: 1682,loss: 0.0227608\n",
            "\n",
            "Global step: 1683,loss: 0.02036091\n",
            "\n",
            "Global step: 1684,loss: 0.020631304\n",
            "\n",
            "Global step: 1685,loss: 0.023090417\n",
            "\n",
            "Global step: 1686,loss: 0.023098346\n",
            "\n",
            "Global step: 1687,loss: 0.027682934\n",
            "\n",
            "Global step: 1688,loss: 0.021440003\n",
            "\n",
            "Global step: 1689,loss: 0.02601571\n",
            "\n",
            "Global step: 1690,loss: 0.021045402\n",
            "\n",
            "Global step: 1691,loss: 0.023336705\n",
            "\n",
            "Global step: 1692,loss: 0.021009311\n",
            "\n",
            "Global step: 1693,loss: 0.020062093\n",
            "\n",
            "Global step: 1694,loss: 0.021075582\n",
            "\n",
            "Global step: 1695,loss: 0.019390643\n",
            "\n",
            "Global step: 1696,loss: 0.020705614\n",
            "\n",
            "Global step: 1697,loss: 0.02040332\n",
            "\n",
            "Global step: 1698,loss: 0.020726375\n",
            "\n",
            "Global step: 1699,loss: 0.022435099\n",
            "\n",
            "Global step: 1700,loss: 0.024455793\n",
            "\n",
            "Global step: 1701,loss: 0.021073867\n",
            "\n",
            "Global step: 1702,loss: 0.022657914\n",
            "\n",
            "Global step: 1703,loss: 0.019571086\n",
            "\n",
            "Global step: 1704,loss: 0.021059943\n",
            "\n",
            "Global step: 1705,loss: 0.020942759\n",
            "\n",
            "Global step: 1706,loss: 0.019801112\n",
            "\n",
            "Global step: 1707,loss: 0.024914749\n",
            "\n",
            "Global step: 1708,loss: 0.019871186\n",
            "\n",
            "Global step: 1709,loss: 0.022625335\n",
            "\n",
            "Global step: 1710,loss: 0.021627182\n",
            "\n",
            "Global step: 1711,loss: 0.022404926\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 1711,val_loss: 0.03338591440727836\n",
            "\n",
            "Training for epoch 9/16:\n",
            "Global step: 1712,loss: 0.020463865\n",
            "\n",
            "Global step: 1713,loss: 0.020853851\n",
            "\n",
            "Global step: 1714,loss: 0.024753004\n",
            "\n",
            "Global step: 1715,loss: 0.019915668\n",
            "\n",
            "Global step: 1716,loss: 0.019187424\n",
            "\n",
            "Global step: 1717,loss: 0.022037901\n",
            "\n",
            "Global step: 1718,loss: 0.020051137\n",
            "\n",
            "Global step: 1719,loss: 0.020980995\n",
            "\n",
            "Global step: 1720,loss: 0.020162744\n",
            "\n",
            "Global step: 1721,loss: 0.020404577\n",
            "\n",
            "Global step: 1722,loss: 0.019670907\n",
            "\n",
            "Global step: 1723,loss: 0.019768909\n",
            "\n",
            "Global step: 1724,loss: 0.019505823\n",
            "\n",
            "Global step: 1725,loss: 0.019625116\n",
            "\n",
            "Global step: 1726,loss: 0.020791903\n",
            "\n",
            "Global step: 1727,loss: 0.020552345\n",
            "\n",
            "Global step: 1728,loss: 0.022895906\n",
            "\n",
            "Global step: 1729,loss: 0.021942014\n",
            "\n",
            "Global step: 1730,loss: 0.019767676\n",
            "\n",
            "Global step: 1731,loss: 0.021311007\n",
            "\n",
            "Global step: 1732,loss: 0.020212486\n",
            "\n",
            "Global step: 1733,loss: 0.019978486\n",
            "\n",
            "Global step: 1734,loss: 0.021226458\n",
            "\n",
            "Global step: 1735,loss: 0.021520661\n",
            "\n",
            "Global step: 1736,loss: 0.019849785\n",
            "\n",
            "Global step: 1737,loss: 0.023710135\n",
            "\n",
            "Global step: 1738,loss: 0.020518623\n",
            "\n",
            "Global step: 1739,loss: 0.021031937\n",
            "\n",
            "Global step: 1740,loss: 0.02234721\n",
            "\n",
            "Global step: 1741,loss: 0.020780804\n",
            "\n",
            "Global step: 1742,loss: 0.019208778\n",
            "\n",
            "Global step: 1743,loss: 0.020275433\n",
            "\n",
            "Global step: 1744,loss: 0.01931\n",
            "\n",
            "Global step: 1745,loss: 0.020162364\n",
            "\n",
            "Global step: 1746,loss: 0.021335172\n",
            "\n",
            "Global step: 1747,loss: 0.021233078\n",
            "\n",
            "Global step: 1748,loss: 0.019386921\n",
            "\n",
            "Global step: 1749,loss: 0.021278428\n",
            "\n",
            "Global step: 1750,loss: 0.022507375\n",
            "\n",
            "Global step: 1751,loss: 0.019302567\n",
            "\n",
            "Global step: 1752,loss: 0.02005182\n",
            "\n",
            "Global step: 1753,loss: 0.018729297\n",
            "\n",
            "Global step: 1754,loss: 0.019759482\n",
            "\n",
            "Global step: 1755,loss: 0.021827897\n",
            "\n",
            "Global step: 1756,loss: 0.019961683\n",
            "\n",
            "Global step: 1757,loss: 0.021100445\n",
            "\n",
            "Global step: 1758,loss: 0.021298843\n",
            "\n",
            "Global step: 1759,loss: 0.021158379\n",
            "\n",
            "Global step: 1760,loss: 0.020924833\n",
            "\n",
            "Global step: 1761,loss: 0.022897016\n",
            "\n",
            "Global step: 1762,loss: 0.0209779\n",
            "\n",
            "Global step: 1763,loss: 0.02281831\n",
            "\n",
            "Global step: 1764,loss: 0.02290399\n",
            "\n",
            "Global step: 1765,loss: 0.019813437\n",
            "\n",
            "Global step: 1766,loss: 0.020530492\n",
            "\n",
            "Global step: 1767,loss: 0.022062905\n",
            "\n",
            "Global step: 1768,loss: 0.0193492\n",
            "\n",
            "Global step: 1769,loss: 0.021052886\n",
            "\n",
            "Global step: 1770,loss: 0.019639224\n",
            "\n",
            "Global step: 1771,loss: 0.020199757\n",
            "\n",
            "Global step: 1772,loss: 0.020537164\n",
            "\n",
            "Global step: 1773,loss: 0.019759167\n",
            "\n",
            "Global step: 1774,loss: 0.020245913\n",
            "\n",
            "Global step: 1775,loss: 0.019347839\n",
            "\n",
            "Global step: 1776,loss: 0.019084169\n",
            "\n",
            "Global step: 1777,loss: 0.019140968\n",
            "\n",
            "Global step: 1778,loss: 0.021418301\n",
            "\n",
            "Global step: 1779,loss: 0.019683132\n",
            "\n",
            "Global step: 1780,loss: 0.020234542\n",
            "\n",
            "Global step: 1781,loss: 0.02028022\n",
            "\n",
            "Global step: 1782,loss: 0.01984118\n",
            "\n",
            "Global step: 1783,loss: 0.020113062\n",
            "\n",
            "Global step: 1784,loss: 0.021019414\n",
            "\n",
            "Global step: 1785,loss: 0.020199692\n",
            "\n",
            "Global step: 1786,loss: 0.022231944\n",
            "\n",
            "Global step: 1787,loss: 0.019096166\n",
            "\n",
            "Global step: 1788,loss: 0.02094913\n",
            "\n",
            "Global step: 1789,loss: 0.019647503\n",
            "\n",
            "Global step: 1790,loss: 0.021562511\n",
            "\n",
            "Global step: 1791,loss: 0.018736819\n",
            "\n",
            "Global step: 1792,loss: 0.020632284\n",
            "\n",
            "Global step: 1793,loss: 0.02059066\n",
            "\n",
            "Global step: 1794,loss: 0.018981174\n",
            "\n",
            "Global step: 1795,loss: 0.020675566\n",
            "\n",
            "Global step: 1796,loss: 0.020370783\n",
            "\n",
            "Global step: 1797,loss: 0.019485513\n",
            "\n",
            "Global step: 1798,loss: 0.020548386\n",
            "\n",
            "Global step: 1799,loss: 0.020985184\n",
            "\n",
            "Global step: 1800,loss: 0.01983662\n",
            "\n",
            "Global step: 1801,loss: 0.021226225\n",
            "\n",
            "Global step: 1802,loss: 0.018184472\n",
            "\n",
            "Global step: 1803,loss: 0.020313013\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1804.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:56:40.082550 140504771352320 supervisor.py:1050] Recording summary at step 1804.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1804,loss: 0.020963171\n",
            "\n",
            "Global step: 1805,loss: 0.020658627\n",
            "\n",
            "Global step: 1806,loss: 0.019234218\n",
            "\n",
            "Global step: 1807,loss: 0.019209875\n",
            "\n",
            "Global step: 1808,loss: 0.018677022\n",
            "\n",
            "Global step: 1809,loss: 0.023835538\n",
            "\n",
            "Global step: 1810,loss: 0.019589175\n",
            "\n",
            "Global step: 1811,loss: 0.019191027\n",
            "\n",
            "Global step: 1812,loss: 0.020973155\n",
            "\n",
            "Global step: 1813,loss: 0.023929691\n",
            "\n",
            "Global step: 1814,loss: 0.01871146\n",
            "\n",
            "Global step: 1815,loss: 0.019083155\n",
            "\n",
            "Global step: 1816,loss: 0.022088246\n",
            "\n",
            "Global step: 1817,loss: 0.019529967\n",
            "\n",
            "Global step: 1818,loss: 0.020846898\n",
            "\n",
            "Global step: 1819,loss: 0.018851016\n",
            "\n",
            "Global step: 1820,loss: 0.022862028\n",
            "\n",
            "Global step: 1821,loss: 0.024767797\n",
            "\n",
            "Global step: 1822,loss: 0.020546248\n",
            "\n",
            "Global step: 1823,loss: 0.027210858\n",
            "\n",
            "Global step: 1824,loss: 0.020416632\n",
            "\n",
            "Global step: 1825,loss: 0.02057524\n",
            "\n",
            "Global step: 1826,loss: 0.020505242\n",
            "\n",
            "Global step: 1827,loss: 0.022460759\n",
            "\n",
            "Global step: 1828,loss: 0.020743916\n",
            "\n",
            "Global step: 1829,loss: 0.020265346\n",
            "\n",
            "Global step: 1830,loss: 0.025158076\n",
            "\n",
            "Global step: 1831,loss: 0.01994412\n",
            "\n",
            "Global step: 1832,loss: 0.01921966\n",
            "\n",
            "Global step: 1833,loss: 0.018345464\n",
            "\n",
            "Global step: 1834,loss: 0.020322952\n",
            "\n",
            "Global step: 1835,loss: 0.018611643\n",
            "\n",
            "Global step: 1836,loss: 0.021668136\n",
            "\n",
            "Global step: 1837,loss: 0.019073535\n",
            "\n",
            "Global step: 1838,loss: 0.020132478\n",
            "\n",
            "Global step: 1839,loss: 0.021122929\n",
            "\n",
            "Global step: 1840,loss: 0.020138338\n",
            "\n",
            "Global step: 1841,loss: 0.02094506\n",
            "\n",
            "Global step: 1842,loss: 0.021385103\n",
            "\n",
            "Global step: 1843,loss: 0.019448131\n",
            "\n",
            "Global step: 1844,loss: 0.019137887\n",
            "\n",
            "Global step: 1845,loss: 0.02011004\n",
            "\n",
            "Global step: 1846,loss: 0.020603087\n",
            "\n",
            "Global step: 1847,loss: 0.020688495\n",
            "\n",
            "Global step: 1848,loss: 0.02204714\n",
            "\n",
            "Global step: 1849,loss: 0.020561712\n",
            "\n",
            "Global step: 1850,loss: 0.023581041\n",
            "\n",
            "Global step: 1851,loss: 0.019653738\n",
            "\n",
            "Global step: 1852,loss: 0.021283258\n",
            "\n",
            "Global step: 1853,loss: 0.019716272\n",
            "\n",
            "Global step: 1854,loss: 0.02114272\n",
            "\n",
            "Global step: 1855,loss: 0.022273077\n",
            "\n",
            "Global step: 1856,loss: 0.018922286\n",
            "\n",
            "Global step: 1857,loss: 0.023079244\n",
            "\n",
            "Global step: 1858,loss: 0.01897855\n",
            "\n",
            "Global step: 1859,loss: 0.021619547\n",
            "\n",
            "Global step: 1860,loss: 0.02098775\n",
            "\n",
            "Global step: 1861,loss: 0.01890449\n",
            "\n",
            "Global step: 1862,loss: 0.019903451\n",
            "\n",
            "Global step: 1863,loss: 0.020450223\n",
            "\n",
            "Global step: 1864,loss: 0.021782756\n",
            "\n",
            "Global step: 1865,loss: 0.02007834\n",
            "\n",
            "Global step: 1866,loss: 0.020963876\n",
            "\n",
            "Global step: 1867,loss: 0.019844253\n",
            "\n",
            "Global step: 1868,loss: 0.020108523\n",
            "\n",
            "Global step: 1869,loss: 0.019115333\n",
            "\n",
            "Global step: 1870,loss: 0.019402135\n",
            "\n",
            "Global step: 1871,loss: 0.019032106\n",
            "\n",
            "Global step: 1872,loss: 0.022020813\n",
            "\n",
            "Global step: 1873,loss: 0.020110643\n",
            "\n",
            "Global step: 1874,loss: 0.02146494\n",
            "\n",
            "Global step: 1875,loss: 0.022131393\n",
            "\n",
            "Global step: 1876,loss: 0.021627143\n",
            "\n",
            "Global step: 1877,loss: 0.018217197\n",
            "\n",
            "Global step: 1878,loss: 0.021326883\n",
            "\n",
            "Global step: 1879,loss: 0.019027952\n",
            "\n",
            "Global step: 1880,loss: 0.02378966\n",
            "\n",
            "Global step: 1881,loss: 0.019915007\n",
            "\n",
            "Global step: 1882,loss: 0.019526502\n",
            "\n",
            "Global step: 1883,loss: 0.023271991\n",
            "\n",
            "Global step: 1884,loss: 0.01991211\n",
            "\n",
            "Global step: 1885,loss: 0.021707434\n",
            "\n",
            "Global step: 1886,loss: 0.02089626\n",
            "\n",
            "Global step: 1887,loss: 0.020199265\n",
            "\n",
            "Global step: 1888,loss: 0.018509371\n",
            "\n",
            "Global step: 1889,loss: 0.020738492\n",
            "\n",
            "Global step: 1890,loss: 0.019320562\n",
            "\n",
            "Global step: 1891,loss: 0.022008434\n",
            "\n",
            "Global step: 1892,loss: 0.023410527\n",
            "\n",
            "Global step: 1893,loss: 0.018672727\n",
            "\n",
            "Global step: 1894,loss: 0.020003648\n",
            "\n",
            "Global step: 1895,loss: 0.022190856\n",
            "\n",
            "Global step: 1896,loss: 0.020281516\n",
            "\n",
            "Global step: 1897,loss: 0.021223307\n",
            "\n",
            "Global step: 1898,loss: 0.019711813\n",
            "\n",
            "Global step: 1899,loss: 0.022372523\n",
            "\n",
            "Global step: 1900,loss: 0.02178184\n",
            "\n",
            "Global step: 1901,loss: 0.024890259\n",
            "\n",
            "Global step: 1902,loss: 0.022237161\n",
            "\n",
            "Global step: 1903,loss: 0.021953842\n",
            "\n",
            "Global step: 1904,loss: 0.019575274\n",
            "\n",
            "Global step: 1905,loss: 0.018950764\n",
            "\n",
            "Global step: 1906,loss: 0.02025752\n",
            "\n",
            "Global step: 1907,loss: 0.01838716\n",
            "\n",
            "Global step: 1908,loss: 0.021201216\n",
            "\n",
            "Global step: 1909,loss: 0.018873975\n",
            "\n",
            "Global step: 1910,loss: 0.01973642\n",
            "\n",
            "Global step: 1911,loss: 0.020211438\n",
            "\n",
            "Global step: 1912,loss: 0.02014749\n",
            "\n",
            "Global step: 1913,loss: 0.021212792\n",
            "\n",
            "Global step: 1914,loss: 0.019488668\n",
            "\n",
            "Global step: 1915,loss: 0.018837517\n",
            "\n",
            "Global step: 1916,loss: 0.019471178\n",
            "\n",
            "Global step: 1917,loss: 0.019501425\n",
            "\n",
            "Global step: 1918,loss: 0.018042425\n",
            "\n",
            "Global step: 1919,loss: 0.021784939\n",
            "\n",
            "Global step: 1920,loss: 0.01924348\n",
            "\n",
            "Global step: 1921,loss: 0.019560302\n",
            "\n",
            "Global step: 1922,loss: 0.017984163\n",
            "\n",
            "Global step: 1923,loss: 0.019974701\n",
            "\n",
            "Global step: 1924,loss: 0.018825313\n",
            "\n",
            "Global step: 1925,loss: 0.02390852\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 1925,val_loss: 0.0313467859829727\n",
            "\n",
            "Training for epoch 10/16:\n",
            "Global step: 1926,loss: 0.020803433\n",
            "\n",
            "Global step: 1927,loss: 0.018488588\n",
            "\n",
            "Global step: 1928,loss: 0.020338045\n",
            "\n",
            "Global step: 1929,loss: 0.018541504\n",
            "\n",
            "Global step: 1930,loss: 0.01938937\n",
            "\n",
            "Global step: 1931,loss: 0.019818764\n",
            "\n",
            "Global step: 1932,loss: 0.018519625\n",
            "\n",
            "Global step: 1933,loss: 0.018422237\n",
            "\n",
            "Global step: 1934,loss: 0.018338613\n",
            "\n",
            "Global step: 1935,loss: 0.01833842\n",
            "\n",
            "Global step: 1936,loss: 0.019263316\n",
            "\n",
            "Global step: 1937,loss: 0.020187587\n",
            "\n",
            "Global step: 1938,loss: 0.019014962\n",
            "\n",
            "Global step: 1939,loss: 0.01881956\n",
            "\n",
            "Global step: 1940,loss: 0.01803216\n",
            "\n",
            "Global step: 1941,loss: 0.01947315\n",
            "\n",
            "Global step: 1942,loss: 0.022115823\n",
            "\n",
            "Global step: 1943,loss: 0.01869999\n",
            "\n",
            "Global step: 1944,loss: 0.020881247\n",
            "\n",
            "Global step: 1945,loss: 0.018204713\n",
            "\n",
            "Global step: 1946,loss: 0.020220432\n",
            "\n",
            "Global step: 1947,loss: 0.019206528\n",
            "\n",
            "Global step: 1948,loss: 0.019820638\n",
            "\n",
            "Global step: 1949,loss: 0.021179743\n",
            "\n",
            "Global step: 1950,loss: 0.018560633\n",
            "\n",
            "Global step: 1951,loss: 0.019728662\n",
            "\n",
            "Global step: 1952,loss: 0.01828798\n",
            "\n",
            "Global step: 1953,loss: 0.018921465\n",
            "\n",
            "Global step: 1954,loss: 0.019361682\n",
            "\n",
            "Global step: 1955,loss: 0.019103957\n",
            "\n",
            "Global step: 1956,loss: 0.018413663\n",
            "\n",
            "Global step: 1957,loss: 0.01939411\n",
            "\n",
            "Global step: 1958,loss: 0.018776234\n",
            "\n",
            "Global step: 1959,loss: 0.02167012\n",
            "\n",
            "Global step: 1960,loss: 0.018672295\n",
            "\n",
            "Global step: 1961,loss: 0.017621448\n",
            "\n",
            "Global step: 1962,loss: 0.019068383\n",
            "\n",
            "Global step: 1963,loss: 0.019067671\n",
            "\n",
            "Global step: 1964,loss: 0.019833632\n",
            "\n",
            "Global step: 1965,loss: 0.020374026\n",
            "\n",
            "Global step: 1966,loss: 0.018369012\n",
            "\n",
            "Global step: 1967,loss: 0.01839618\n",
            "\n",
            "Global step: 1968,loss: 0.02074851\n",
            "\n",
            "Global step: 1969,loss: 0.021459911\n",
            "\n",
            "Global step: 1970,loss: 0.018339124\n",
            "\n",
            "Global step: 1971,loss: 0.0181784\n",
            "\n",
            "Global step: 1972,loss: 0.018507758\n",
            "\n",
            "Global step: 1973,loss: 0.01857717\n",
            "\n",
            "Global step: 1974,loss: 0.022595461\n",
            "\n",
            "Global step: 1975,loss: 0.01795341\n",
            "\n",
            "Global step: 1976,loss: 0.018250734\n",
            "\n",
            "Global step: 1977,loss: 0.018241268\n",
            "\n",
            "Global step: 1978,loss: 0.020788144\n",
            "\n",
            "Global step: 1979,loss: 0.018905759\n",
            "\n",
            "Global step: 1980,loss: 0.018150907\n",
            "\n",
            "Global step: 1981,loss: 0.018317835\n",
            "\n",
            "Global step: 1982,loss: 0.019245764\n",
            "\n",
            "Global step: 1983,loss: 0.017295426\n",
            "\n",
            "Global step: 1984,loss: 0.020147651\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1985.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 11:58:39.988175 140504771352320 supervisor.py:1050] Recording summary at step 1985.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1985,loss: 0.02082246\n",
            "\n",
            "Global step: 1986,loss: 0.01819683\n",
            "\n",
            "Global step: 1987,loss: 0.018428558\n",
            "\n",
            "Global step: 1988,loss: 0.017767718\n",
            "\n",
            "Global step: 1989,loss: 0.021408929\n",
            "\n",
            "Global step: 1990,loss: 0.020090567\n",
            "\n",
            "Global step: 1991,loss: 0.018622624\n",
            "\n",
            "Global step: 1992,loss: 0.020074382\n",
            "\n",
            "Global step: 1993,loss: 0.01879355\n",
            "\n",
            "Global step: 1994,loss: 0.01888201\n",
            "\n",
            "Global step: 1995,loss: 0.017539632\n",
            "\n",
            "Global step: 1996,loss: 0.018625442\n",
            "\n",
            "Global step: 1997,loss: 0.019305442\n",
            "\n",
            "Global step: 1998,loss: 0.01850241\n",
            "\n",
            "Global step: 1999,loss: 0.018682886\n",
            "\n",
            "Global step: 2000,loss: 0.018078003\n",
            "\n",
            "Global step: 2001,loss: 0.01821357\n",
            "\n",
            "Global step: 2002,loss: 0.019496208\n",
            "\n",
            "Global step: 2003,loss: 0.021840941\n",
            "\n",
            "Global step: 2004,loss: 0.020338574\n",
            "\n",
            "Global step: 2005,loss: 0.017826308\n",
            "\n",
            "Global step: 2006,loss: 0.018843178\n",
            "\n",
            "Global step: 2007,loss: 0.020645786\n",
            "\n",
            "Global step: 2008,loss: 0.01758492\n",
            "\n",
            "Global step: 2009,loss: 0.018790504\n",
            "\n",
            "Global step: 2010,loss: 0.017917398\n",
            "\n",
            "Global step: 2011,loss: 0.019447004\n",
            "\n",
            "Global step: 2012,loss: 0.019211205\n",
            "\n",
            "Global step: 2013,loss: 0.018835062\n",
            "\n",
            "Global step: 2014,loss: 0.021220742\n",
            "\n",
            "Global step: 2015,loss: 0.017911129\n",
            "\n",
            "Global step: 2016,loss: 0.019609764\n",
            "\n",
            "Global step: 2017,loss: 0.019020744\n",
            "\n",
            "Global step: 2018,loss: 0.018059973\n",
            "\n",
            "Global step: 2019,loss: 0.019219048\n",
            "\n",
            "Global step: 2020,loss: 0.018719096\n",
            "\n",
            "Global step: 2021,loss: 0.018290024\n",
            "\n",
            "Global step: 2022,loss: 0.018803896\n",
            "\n",
            "Global step: 2023,loss: 0.019509764\n",
            "\n",
            "Global step: 2024,loss: 0.01792961\n",
            "\n",
            "Global step: 2025,loss: 0.018781561\n",
            "\n",
            "Global step: 2026,loss: 0.018353177\n",
            "\n",
            "Global step: 2027,loss: 0.022904811\n",
            "\n",
            "Global step: 2028,loss: 0.017300459\n",
            "\n",
            "Global step: 2029,loss: 0.018241815\n",
            "\n",
            "Global step: 2030,loss: 0.019340392\n",
            "\n",
            "Global step: 2031,loss: 0.018612022\n",
            "\n",
            "Global step: 2032,loss: 0.01717433\n",
            "\n",
            "Global step: 2033,loss: 0.018848442\n",
            "\n",
            "Global step: 2034,loss: 0.017962053\n",
            "\n",
            "Global step: 2035,loss: 0.018590841\n",
            "\n",
            "Global step: 2036,loss: 0.019573648\n",
            "\n",
            "Global step: 2037,loss: 0.021040786\n",
            "\n",
            "Global step: 2038,loss: 0.019305034\n",
            "\n",
            "Global step: 2039,loss: 0.01837967\n",
            "\n",
            "Global step: 2040,loss: 0.01778289\n",
            "\n",
            "Global step: 2041,loss: 0.017462581\n",
            "\n",
            "Global step: 2042,loss: 0.020063018\n",
            "\n",
            "Global step: 2043,loss: 0.017780019\n",
            "\n",
            "Global step: 2044,loss: 0.022219209\n",
            "\n",
            "Global step: 2045,loss: 0.01930795\n",
            "\n",
            "Global step: 2046,loss: 0.019510588\n",
            "\n",
            "Global step: 2047,loss: 0.01903576\n",
            "\n",
            "Global step: 2048,loss: 0.01785795\n",
            "\n",
            "Global step: 2049,loss: 0.017211437\n",
            "\n",
            "Global step: 2050,loss: 0.017317599\n",
            "\n",
            "Global step: 2051,loss: 0.01877829\n",
            "\n",
            "Global step: 2052,loss: 0.019241827\n",
            "\n",
            "Global step: 2053,loss: 0.019314295\n",
            "\n",
            "Global step: 2054,loss: 0.019052487\n",
            "\n",
            "Global step: 2055,loss: 0.017545901\n",
            "\n",
            "Global step: 2056,loss: 0.017248461\n",
            "\n",
            "Global step: 2057,loss: 0.019511895\n",
            "\n",
            "Global step: 2058,loss: 0.017635394\n",
            "\n",
            "Global step: 2059,loss: 0.017170379\n",
            "\n",
            "Global step: 2060,loss: 0.020636903\n",
            "\n",
            "Global step: 2061,loss: 0.017661119\n",
            "\n",
            "Global step: 2062,loss: 0.020686021\n",
            "\n",
            "Global step: 2063,loss: 0.017445365\n",
            "\n",
            "Global step: 2064,loss: 0.019545022\n",
            "\n",
            "Global step: 2065,loss: 0.020083113\n",
            "\n",
            "Global step: 2066,loss: 0.018075781\n",
            "\n",
            "Global step: 2067,loss: 0.019217962\n",
            "\n",
            "Global step: 2068,loss: 0.019518888\n",
            "\n",
            "Global step: 2069,loss: 0.018858623\n",
            "\n",
            "Global step: 2070,loss: 0.020375337\n",
            "\n",
            "Global step: 2071,loss: 0.018136263\n",
            "\n",
            "Global step: 2072,loss: 0.018409904\n",
            "\n",
            "Global step: 2073,loss: 0.019398678\n",
            "\n",
            "Global step: 2074,loss: 0.018848276\n",
            "\n",
            "Global step: 2075,loss: 0.02054615\n",
            "\n",
            "Global step: 2076,loss: 0.018931262\n",
            "\n",
            "Global step: 2077,loss: 0.017426467\n",
            "\n",
            "Global step: 2078,loss: 0.018355915\n",
            "\n",
            "Global step: 2079,loss: 0.018834608\n",
            "\n",
            "Global step: 2080,loss: 0.018442428\n",
            "\n",
            "Global step: 2081,loss: 0.01808636\n",
            "\n",
            "Global step: 2082,loss: 0.018687531\n",
            "\n",
            "Global step: 2083,loss: 0.017526023\n",
            "\n",
            "Global step: 2084,loss: 0.017857911\n",
            "\n",
            "Global step: 2085,loss: 0.018429527\n",
            "\n",
            "Global step: 2086,loss: 0.018818201\n",
            "\n",
            "Global step: 2087,loss: 0.022844288\n",
            "\n",
            "Global step: 2088,loss: 0.019873068\n",
            "\n",
            "Global step: 2089,loss: 0.017607803\n",
            "\n",
            "Global step: 2090,loss: 0.017532159\n",
            "\n",
            "Global step: 2091,loss: 0.018249985\n",
            "\n",
            "Global step: 2092,loss: 0.021064196\n",
            "\n",
            "Global step: 2093,loss: 0.017501818\n",
            "\n",
            "Global step: 2094,loss: 0.02044472\n",
            "\n",
            "Global step: 2095,loss: 0.01830498\n",
            "\n",
            "Global step: 2096,loss: 0.019206498\n",
            "\n",
            "Global step: 2097,loss: 0.020190567\n",
            "\n",
            "Global step: 2098,loss: 0.018748831\n",
            "\n",
            "Global step: 2099,loss: 0.0187785\n",
            "\n",
            "Global step: 2100,loss: 0.018902807\n",
            "\n",
            "Global step: 2101,loss: 0.022541687\n",
            "\n",
            "Global step: 2102,loss: 0.019233804\n",
            "\n",
            "Global step: 2103,loss: 0.018488131\n",
            "\n",
            "Global step: 2104,loss: 0.018875219\n",
            "\n",
            "Global step: 2105,loss: 0.01913921\n",
            "\n",
            "Global step: 2106,loss: 0.018390713\n",
            "\n",
            "Global step: 2107,loss: 0.018737212\n",
            "\n",
            "Global step: 2108,loss: 0.018102646\n",
            "\n",
            "Global step: 2109,loss: 0.019091774\n",
            "\n",
            "Global step: 2110,loss: 0.017273951\n",
            "\n",
            "Global step: 2111,loss: 0.01941562\n",
            "\n",
            "Global step: 2112,loss: 0.019919518\n",
            "\n",
            "Global step: 2113,loss: 0.017660368\n",
            "\n",
            "Global step: 2114,loss: 0.020331353\n",
            "\n",
            "Global step: 2115,loss: 0.019101946\n",
            "\n",
            "Global step: 2116,loss: 0.018119674\n",
            "\n",
            "Global step: 2117,loss: 0.017943684\n",
            "\n",
            "Global step: 2118,loss: 0.01723333\n",
            "\n",
            "Global step: 2119,loss: 0.017261025\n",
            "\n",
            "Global step: 2120,loss: 0.017271867\n",
            "\n",
            "Global step: 2121,loss: 0.017680505\n",
            "\n",
            "Global step: 2122,loss: 0.020409396\n",
            "\n",
            "Global step: 2123,loss: 0.018944379\n",
            "\n",
            "Global step: 2124,loss: 0.025547672\n",
            "\n",
            "Global step: 2125,loss: 0.018568002\n",
            "\n",
            "Global step: 2126,loss: 0.020293407\n",
            "\n",
            "Global step: 2127,loss: 0.017737687\n",
            "\n",
            "Global step: 2128,loss: 0.017475747\n",
            "\n",
            "Global step: 2129,loss: 0.017649012\n",
            "\n",
            "Global step: 2130,loss: 0.018008962\n",
            "\n",
            "Global step: 2131,loss: 0.019442718\n",
            "\n",
            "Global step: 2132,loss: 0.018656766\n",
            "\n",
            "Global step: 2133,loss: 0.018075837\n",
            "\n",
            "Global step: 2134,loss: 0.020718746\n",
            "\n",
            "Global step: 2135,loss: 0.019122418\n",
            "\n",
            "Global step: 2136,loss: 0.01738834\n",
            "\n",
            "Global step: 2137,loss: 0.018977236\n",
            "\n",
            "Global step: 2138,loss: 0.021134097\n",
            "\n",
            "Global step: 2139,loss: 0.019452376\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2139,val_loss: 0.031224355787823076\n",
            "\n",
            "Training for epoch 11/16:\n",
            "Global step: 2140,loss: 0.018139048\n",
            "\n",
            "Global step: 2141,loss: 0.017406913\n",
            "\n",
            "Global step: 2142,loss: 0.01728299\n",
            "\n",
            "Global step: 2143,loss: 0.020927735\n",
            "\n",
            "Global step: 2144,loss: 0.018474631\n",
            "\n",
            "Global step: 2145,loss: 0.019736681\n",
            "\n",
            "Global step: 2146,loss: 0.018026883\n",
            "\n",
            "Global step: 2147,loss: 0.01728926\n",
            "\n",
            "Global step: 2148,loss: 0.017706506\n",
            "\n",
            "Global step: 2149,loss: 0.01812458\n",
            "\n",
            "Global step: 2150,loss: 0.018212449\n",
            "\n",
            "Global step: 2151,loss: 0.01715955\n",
            "\n",
            "Global step: 2152,loss: 0.019025378\n",
            "\n",
            "Global step: 2153,loss: 0.019155463\n",
            "\n",
            "Global step: 2154,loss: 0.019580647\n",
            "\n",
            "Global step: 2155,loss: 0.018135246\n",
            "\n",
            "Global step: 2156,loss: 0.020221267\n",
            "\n",
            "Global step: 2157,loss: 0.018792883\n",
            "\n",
            "Global step: 2158,loss: 0.01869604\n",
            "\n",
            "Global step: 2159,loss: 0.018432412\n",
            "\n",
            "Global step: 2160,loss: 0.018275259\n",
            "\n",
            "Global step: 2161,loss: 0.018303446\n",
            "\n",
            "Global step: 2162,loss: 0.016902467\n",
            "\n",
            "Global step: 2163,loss: 0.018796168\n",
            "\n",
            "Global step: 2164,loss: 0.017069716\n",
            "\n",
            "Global step: 2165,loss: 0.01671252\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2166.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:00:39.832250 140504771352320 supervisor.py:1050] Recording summary at step 2166.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2166,loss: 0.017285299\n",
            "\n",
            "Global step: 2167,loss: 0.017084192\n",
            "\n",
            "Global step: 2168,loss: 0.017156743\n",
            "\n",
            "Global step: 2169,loss: 0.017892456\n",
            "\n",
            "Global step: 2170,loss: 0.017801082\n",
            "\n",
            "Global step: 2171,loss: 0.017457154\n",
            "\n",
            "Global step: 2172,loss: 0.016507778\n",
            "\n",
            "Global step: 2173,loss: 0.016666602\n",
            "\n",
            "Global step: 2174,loss: 0.01847009\n",
            "\n",
            "Global step: 2175,loss: 0.017618736\n",
            "\n",
            "Global step: 2176,loss: 0.017493745\n",
            "\n",
            "Global step: 2177,loss: 0.018856592\n",
            "\n",
            "Global step: 2178,loss: 0.017329803\n",
            "\n",
            "Global step: 2179,loss: 0.01728419\n",
            "\n",
            "Global step: 2180,loss: 0.017321425\n",
            "\n",
            "Global step: 2181,loss: 0.018012537\n",
            "\n",
            "Global step: 2182,loss: 0.019319657\n",
            "\n",
            "Global step: 2183,loss: 0.016969737\n",
            "\n",
            "Global step: 2184,loss: 0.019215731\n",
            "\n",
            "Global step: 2185,loss: 0.018230544\n",
            "\n",
            "Global step: 2186,loss: 0.017161004\n",
            "\n",
            "Global step: 2187,loss: 0.016462026\n",
            "\n",
            "Global step: 2188,loss: 0.018084485\n",
            "\n",
            "Global step: 2189,loss: 0.018969068\n",
            "\n",
            "Global step: 2190,loss: 0.017411532\n",
            "\n",
            "Global step: 2191,loss: 0.018418876\n",
            "\n",
            "Global step: 2192,loss: 0.017200347\n",
            "\n",
            "Global step: 2193,loss: 0.019833183\n",
            "\n",
            "Global step: 2194,loss: 0.016803822\n",
            "\n",
            "Global step: 2195,loss: 0.017007414\n",
            "\n",
            "Global step: 2196,loss: 0.019664042\n",
            "\n",
            "Global step: 2197,loss: 0.017341083\n",
            "\n",
            "Global step: 2198,loss: 0.019586587\n",
            "\n",
            "Global step: 2199,loss: 0.018334243\n",
            "\n",
            "Global step: 2200,loss: 0.017274138\n",
            "\n",
            "Global step: 2201,loss: 0.017981967\n",
            "\n",
            "Global step: 2202,loss: 0.0190172\n",
            "\n",
            "Global step: 2203,loss: 0.016967451\n",
            "\n",
            "Global step: 2204,loss: 0.017339852\n",
            "\n",
            "Global step: 2205,loss: 0.01787036\n",
            "\n",
            "Global step: 2206,loss: 0.019450137\n",
            "\n",
            "Global step: 2207,loss: 0.018307371\n",
            "\n",
            "Global step: 2208,loss: 0.020481534\n",
            "\n",
            "Global step: 2209,loss: 0.01750305\n",
            "\n",
            "Global step: 2210,loss: 0.019277688\n",
            "\n",
            "Global step: 2211,loss: 0.017096084\n",
            "\n",
            "Global step: 2212,loss: 0.016570963\n",
            "\n",
            "Global step: 2213,loss: 0.01703325\n",
            "\n",
            "Global step: 2214,loss: 0.016942613\n",
            "\n",
            "Global step: 2215,loss: 0.019092748\n",
            "\n",
            "Global step: 2216,loss: 0.01731466\n",
            "\n",
            "Global step: 2217,loss: 0.01675389\n",
            "\n",
            "Global step: 2218,loss: 0.019903509\n",
            "\n",
            "Global step: 2219,loss: 0.017654642\n",
            "\n",
            "Global step: 2220,loss: 0.018469956\n",
            "\n",
            "Global step: 2221,loss: 0.01742177\n",
            "\n",
            "Global step: 2222,loss: 0.018626893\n",
            "\n",
            "Global step: 2223,loss: 0.016691728\n",
            "\n",
            "Global step: 2224,loss: 0.018259931\n",
            "\n",
            "Global step: 2225,loss: 0.019130066\n",
            "\n",
            "Global step: 2226,loss: 0.016441781\n",
            "\n",
            "Global step: 2227,loss: 0.017067611\n",
            "\n",
            "Global step: 2228,loss: 0.018063657\n",
            "\n",
            "Global step: 2229,loss: 0.016691063\n",
            "\n",
            "Global step: 2230,loss: 0.017414369\n",
            "\n",
            "Global step: 2231,loss: 0.017005347\n",
            "\n",
            "Global step: 2232,loss: 0.017484874\n",
            "\n",
            "Global step: 2233,loss: 0.018541\n",
            "\n",
            "Global step: 2234,loss: 0.018567804\n",
            "\n",
            "Global step: 2235,loss: 0.016423054\n",
            "\n",
            "Global step: 2236,loss: 0.016768971\n",
            "\n",
            "Global step: 2237,loss: 0.017227944\n",
            "\n",
            "Global step: 2238,loss: 0.01760261\n",
            "\n",
            "Global step: 2239,loss: 0.016868649\n",
            "\n",
            "Global step: 2240,loss: 0.016669601\n",
            "\n",
            "Global step: 2241,loss: 0.016940223\n",
            "\n",
            "Global step: 2242,loss: 0.017024312\n",
            "\n",
            "Global step: 2243,loss: 0.018975874\n",
            "\n",
            "Global step: 2244,loss: 0.018762715\n",
            "\n",
            "Global step: 2245,loss: 0.01872274\n",
            "\n",
            "Global step: 2246,loss: 0.018201817\n",
            "\n",
            "Global step: 2247,loss: 0.016882036\n",
            "\n",
            "Global step: 2248,loss: 0.01994721\n",
            "\n",
            "Global step: 2249,loss: 0.017079407\n",
            "\n",
            "Global step: 2250,loss: 0.017616598\n",
            "\n",
            "Global step: 2251,loss: 0.020515043\n",
            "\n",
            "Global step: 2252,loss: 0.016959848\n",
            "\n",
            "Global step: 2253,loss: 0.017677333\n",
            "\n",
            "Global step: 2254,loss: 0.016552065\n",
            "\n",
            "Global step: 2255,loss: 0.018965805\n",
            "\n",
            "Global step: 2256,loss: 0.019402433\n",
            "\n",
            "Global step: 2257,loss: 0.017908635\n",
            "\n",
            "Global step: 2258,loss: 0.017243236\n",
            "\n",
            "Global step: 2259,loss: 0.017091954\n",
            "\n",
            "Global step: 2260,loss: 0.017023431\n",
            "\n",
            "Global step: 2261,loss: 0.020240115\n",
            "\n",
            "Global step: 2262,loss: 0.017835982\n",
            "\n",
            "Global step: 2263,loss: 0.017248461\n",
            "\n",
            "Global step: 2264,loss: 0.017539699\n",
            "\n",
            "Global step: 2265,loss: 0.016455643\n",
            "\n",
            "Global step: 2266,loss: 0.018561484\n",
            "\n",
            "Global step: 2267,loss: 0.019305214\n",
            "\n",
            "Global step: 2268,loss: 0.017495902\n",
            "\n",
            "Global step: 2269,loss: 0.01729653\n",
            "\n",
            "Global step: 2270,loss: 0.017753229\n",
            "\n",
            "Global step: 2271,loss: 0.01689877\n",
            "\n",
            "Global step: 2272,loss: 0.017548565\n",
            "\n",
            "Global step: 2273,loss: 0.019417562\n",
            "\n",
            "Global step: 2274,loss: 0.019477654\n",
            "\n",
            "Global step: 2275,loss: 0.017409142\n",
            "\n",
            "Global step: 2276,loss: 0.017017221\n",
            "\n",
            "Global step: 2277,loss: 0.01717999\n",
            "\n",
            "Global step: 2278,loss: 0.018314946\n",
            "\n",
            "Global step: 2279,loss: 0.01816497\n",
            "\n",
            "Global step: 2280,loss: 0.017582385\n",
            "\n",
            "Global step: 2281,loss: 0.017399862\n",
            "\n",
            "Global step: 2282,loss: 0.016976591\n",
            "\n",
            "Global step: 2283,loss: 0.016972356\n",
            "\n",
            "Global step: 2284,loss: 0.01678796\n",
            "\n",
            "Global step: 2285,loss: 0.018637858\n",
            "\n",
            "Global step: 2286,loss: 0.016786233\n",
            "\n",
            "Global step: 2287,loss: 0.017652407\n",
            "\n",
            "Global step: 2288,loss: 0.017712547\n",
            "\n",
            "Global step: 2289,loss: 0.018247362\n",
            "\n",
            "Global step: 2290,loss: 0.017381785\n",
            "\n",
            "Global step: 2291,loss: 0.017982475\n",
            "\n",
            "Global step: 2292,loss: 0.018857302\n",
            "\n",
            "Global step: 2293,loss: 0.017183324\n",
            "\n",
            "Global step: 2294,loss: 0.017134007\n",
            "\n",
            "Global step: 2295,loss: 0.016874397\n",
            "\n",
            "Global step: 2296,loss: 0.017038401\n",
            "\n",
            "Global step: 2297,loss: 0.016955541\n",
            "\n",
            "Global step: 2298,loss: 0.017942563\n",
            "\n",
            "Global step: 2299,loss: 0.017853903\n",
            "\n",
            "Global step: 2300,loss: 0.017017186\n",
            "\n",
            "Global step: 2301,loss: 0.01676913\n",
            "\n",
            "Global step: 2302,loss: 0.016277377\n",
            "\n",
            "Global step: 2303,loss: 0.016611641\n",
            "\n",
            "Global step: 2304,loss: 0.016913591\n",
            "\n",
            "Global step: 2305,loss: 0.017289514\n",
            "\n",
            "Global step: 2306,loss: 0.017694173\n",
            "\n",
            "Global step: 2307,loss: 0.01749166\n",
            "\n",
            "Global step: 2308,loss: 0.018167185\n",
            "\n",
            "Global step: 2309,loss: 0.016566223\n",
            "\n",
            "Global step: 2310,loss: 0.017997297\n",
            "\n",
            "Global step: 2311,loss: 0.017323177\n",
            "\n",
            "Global step: 2312,loss: 0.016951825\n",
            "\n",
            "Global step: 2313,loss: 0.017617533\n",
            "\n",
            "Global step: 2314,loss: 0.019934462\n",
            "\n",
            "Global step: 2315,loss: 0.017043713\n",
            "\n",
            "Global step: 2316,loss: 0.021743484\n",
            "\n",
            "Global step: 2317,loss: 0.016795272\n",
            "\n",
            "Global step: 2318,loss: 0.01633594\n",
            "\n",
            "Global step: 2319,loss: 0.016998585\n",
            "\n",
            "Global step: 2320,loss: 0.017192286\n",
            "\n",
            "Global step: 2321,loss: 0.018222906\n",
            "\n",
            "Global step: 2322,loss: 0.016944528\n",
            "\n",
            "Global step: 2323,loss: 0.01755643\n",
            "\n",
            "Global step: 2324,loss: 0.017781738\n",
            "\n",
            "Global step: 2325,loss: 0.017101116\n",
            "\n",
            "Global step: 2326,loss: 0.018630337\n",
            "\n",
            "Global step: 2327,loss: 0.019964622\n",
            "\n",
            "Global step: 2328,loss: 0.017154507\n",
            "\n",
            "Global step: 2329,loss: 0.017024143\n",
            "\n",
            "Global step: 2330,loss: 0.017429514\n",
            "\n",
            "Global step: 2331,loss: 0.016745219\n",
            "\n",
            "Global step: 2332,loss: 0.01679059\n",
            "\n",
            "Global step: 2333,loss: 0.016536528\n",
            "\n",
            "Global step: 2334,loss: 0.016800167\n",
            "\n",
            "Global step: 2335,loss: 0.017283702\n",
            "\n",
            "Global step: 2336,loss: 0.016279472\n",
            "\n",
            "Global step: 2337,loss: 0.016442155\n",
            "\n",
            "Global step: 2338,loss: 0.017154133\n",
            "\n",
            "Global step: 2339,loss: 0.01604248\n",
            "\n",
            "Global step: 2340,loss: 0.016991235\n",
            "\n",
            "Global step: 2341,loss: 0.017066691\n",
            "\n",
            "Global step: 2342,loss: 0.016359698\n",
            "\n",
            "Global step: 2343,loss: 0.018478438\n",
            "\n",
            "Global step: 2344,loss: 0.016810492\n",
            "\n",
            "Global step: 2345,loss: 0.016600069\n",
            "\n",
            "Global step: 2346,loss: 0.016561124\n",
            "\n",
            "Global step: 2347,loss: 0.018809674\n",
            "\n",
            "Global step: 2348,loss: 0.016369026\n",
            "\n",
            "Global step: 2349,loss: 0.019650077\n",
            "\n",
            "Global step: 2350,loss: 0.016348422\n",
            "\n",
            "Global step: 2351,loss: 0.018342616\n",
            "\n",
            "Global step: 2352,loss: 0.016685309\n",
            "\n",
            "Global step: 2353,loss: 0.016351055\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2354.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:02:42.650414 140504771352320 supervisor.py:1050] Recording summary at step 2354.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2353,val_loss: 0.029543214330547733\n",
            "\n",
            "Training for epoch 12/16:\n",
            "Global step: 2354,loss: 0.0165913\n",
            "\n",
            "Global step: 2355,loss: 0.01734741\n",
            "\n",
            "Global step: 2356,loss: 0.017556662\n",
            "\n",
            "Global step: 2357,loss: 0.015736056\n",
            "\n",
            "Global step: 2358,loss: 0.016546305\n",
            "\n",
            "Global step: 2359,loss: 0.015674407\n",
            "\n",
            "Global step: 2360,loss: 0.016226012\n",
            "\n",
            "Global step: 2361,loss: 0.016647084\n",
            "\n",
            "Global step: 2362,loss: 0.01657774\n",
            "\n",
            "Global step: 2363,loss: 0.019988086\n",
            "\n",
            "Global step: 2364,loss: 0.016913516\n",
            "\n",
            "Global step: 2365,loss: 0.015672127\n",
            "\n",
            "Global step: 2366,loss: 0.016398026\n",
            "\n",
            "Global step: 2367,loss: 0.018430533\n",
            "\n",
            "Global step: 2368,loss: 0.017172871\n",
            "\n",
            "Global step: 2369,loss: 0.016567728\n",
            "\n",
            "Global step: 2370,loss: 0.017045211\n",
            "\n",
            "Global step: 2371,loss: 0.015583397\n",
            "\n",
            "Global step: 2372,loss: 0.015917653\n",
            "\n",
            "Global step: 2373,loss: 0.016971597\n",
            "\n",
            "Global step: 2374,loss: 0.016941587\n",
            "\n",
            "Global step: 2375,loss: 0.01686779\n",
            "\n",
            "Global step: 2376,loss: 0.01730374\n",
            "\n",
            "Global step: 2377,loss: 0.016472781\n",
            "\n",
            "Global step: 2378,loss: 0.01635398\n",
            "\n",
            "Global step: 2379,loss: 0.017267387\n",
            "\n",
            "Global step: 2380,loss: 0.01704776\n",
            "\n",
            "Global step: 2381,loss: 0.017057903\n",
            "\n",
            "Global step: 2382,loss: 0.016178101\n",
            "\n",
            "Global step: 2383,loss: 0.016022818\n",
            "\n",
            "Global step: 2384,loss: 0.016031042\n",
            "\n",
            "Global step: 2385,loss: 0.016393026\n",
            "\n",
            "Global step: 2386,loss: 0.016149852\n",
            "\n",
            "Global step: 2387,loss: 0.016154408\n",
            "\n",
            "Global step: 2388,loss: 0.018864643\n",
            "\n",
            "Global step: 2389,loss: 0.016376503\n",
            "\n",
            "Global step: 2390,loss: 0.01577607\n",
            "\n",
            "Global step: 2391,loss: 0.01601035\n",
            "\n",
            "Global step: 2392,loss: 0.015778046\n",
            "\n",
            "Global step: 2393,loss: 0.016483175\n",
            "\n",
            "Global step: 2394,loss: 0.016021378\n",
            "\n",
            "Global step: 2395,loss: 0.018142812\n",
            "\n",
            "Global step: 2396,loss: 0.015793063\n",
            "\n",
            "Global step: 2397,loss: 0.01785678\n",
            "\n",
            "Global step: 2398,loss: 0.020182863\n",
            "\n",
            "Global step: 2399,loss: 0.016123341\n",
            "\n",
            "Global step: 2400,loss: 0.016995244\n",
            "\n",
            "Global step: 2401,loss: 0.018076891\n",
            "\n",
            "Global step: 2402,loss: 0.017183455\n",
            "\n",
            "Global step: 2403,loss: 0.018146347\n",
            "\n",
            "Global step: 2404,loss: 0.016037125\n",
            "\n",
            "Global step: 2405,loss: 0.016301038\n",
            "\n",
            "Global step: 2406,loss: 0.016619226\n",
            "\n",
            "Global step: 2407,loss: 0.01652718\n",
            "\n",
            "Global step: 2408,loss: 0.016143138\n",
            "\n",
            "Global step: 2409,loss: 0.01631929\n",
            "\n",
            "Global step: 2410,loss: 0.018457187\n",
            "\n",
            "Global step: 2411,loss: 0.016216826\n",
            "\n",
            "Global step: 2412,loss: 0.016601322\n",
            "\n",
            "Global step: 2413,loss: 0.015788851\n",
            "\n",
            "Global step: 2414,loss: 0.016143067\n",
            "\n",
            "Global step: 2415,loss: 0.01580856\n",
            "\n",
            "Global step: 2416,loss: 0.016473146\n",
            "\n",
            "Global step: 2417,loss: 0.01663044\n",
            "\n",
            "Global step: 2418,loss: 0.01664938\n",
            "\n",
            "Global step: 2419,loss: 0.015731724\n",
            "\n",
            "Global step: 2420,loss: 0.01563697\n",
            "\n",
            "Global step: 2421,loss: 0.017781686\n",
            "\n",
            "Global step: 2422,loss: 0.016354328\n",
            "\n",
            "Global step: 2423,loss: 0.016045662\n",
            "\n",
            "Global step: 2424,loss: 0.015847279\n",
            "\n",
            "Global step: 2425,loss: 0.017762499\n",
            "\n",
            "Global step: 2426,loss: 0.016886406\n",
            "\n",
            "Global step: 2427,loss: 0.015357305\n",
            "\n",
            "Global step: 2428,loss: 0.015811387\n",
            "\n",
            "Global step: 2429,loss: 0.016957037\n",
            "\n",
            "Global step: 2430,loss: 0.016605109\n",
            "\n",
            "Global step: 2431,loss: 0.015573715\n",
            "\n",
            "Global step: 2432,loss: 0.016133713\n",
            "\n",
            "Global step: 2433,loss: 0.01937737\n",
            "\n",
            "Global step: 2434,loss: 0.016333101\n",
            "\n",
            "Global step: 2435,loss: 0.017135588\n",
            "\n",
            "Global step: 2436,loss: 0.016851667\n",
            "\n",
            "Global step: 2437,loss: 0.016400797\n",
            "\n",
            "Global step: 2438,loss: 0.016198888\n",
            "\n",
            "Global step: 2439,loss: 0.016994612\n",
            "\n",
            "Global step: 2440,loss: 0.016348038\n",
            "\n",
            "Global step: 2441,loss: 0.01577986\n",
            "\n",
            "Global step: 2442,loss: 0.01657454\n",
            "\n",
            "Global step: 2443,loss: 0.016582012\n",
            "\n",
            "Global step: 2444,loss: 0.016662998\n",
            "\n",
            "Global step: 2445,loss: 0.01664727\n",
            "\n",
            "Global step: 2446,loss: 0.017182587\n",
            "\n",
            "Global step: 2447,loss: 0.016032811\n",
            "\n",
            "Global step: 2448,loss: 0.016151732\n",
            "\n",
            "Global step: 2449,loss: 0.016822428\n",
            "\n",
            "Global step: 2450,loss: 0.01597426\n",
            "\n",
            "Global step: 2451,loss: 0.01570806\n",
            "\n",
            "Global step: 2452,loss: 0.016508088\n",
            "\n",
            "Global step: 2453,loss: 0.016384214\n",
            "\n",
            "Global step: 2454,loss: 0.0149757685\n",
            "\n",
            "Global step: 2455,loss: 0.01698764\n",
            "\n",
            "Global step: 2456,loss: 0.018819688\n",
            "\n",
            "Global step: 2457,loss: 0.016929412\n",
            "\n",
            "Global step: 2458,loss: 0.017123392\n",
            "\n",
            "Global step: 2459,loss: 0.016273098\n",
            "\n",
            "Global step: 2460,loss: 0.016748074\n",
            "\n",
            "Global step: 2461,loss: 0.016011657\n",
            "\n",
            "Global step: 2462,loss: 0.0175796\n",
            "\n",
            "Global step: 2463,loss: 0.017023368\n",
            "\n",
            "Global step: 2464,loss: 0.01658705\n",
            "\n",
            "Global step: 2465,loss: 0.01699003\n",
            "\n",
            "Global step: 2466,loss: 0.015986321\n",
            "\n",
            "Global step: 2467,loss: 0.017409336\n",
            "\n",
            "Global step: 2468,loss: 0.016659997\n",
            "\n",
            "Global step: 2469,loss: 0.016705059\n",
            "\n",
            "Global step: 2470,loss: 0.015853114\n",
            "\n",
            "Global step: 2471,loss: 0.01616662\n",
            "\n",
            "Global step: 2472,loss: 0.015514249\n",
            "\n",
            "Global step: 2473,loss: 0.016027614\n",
            "\n",
            "Global step: 2474,loss: 0.016286012\n",
            "\n",
            "Global step: 2475,loss: 0.016039986\n",
            "\n",
            "Global step: 2476,loss: 0.017235063\n",
            "\n",
            "Global step: 2477,loss: 0.018044423\n",
            "\n",
            "Global step: 2478,loss: 0.017709423\n",
            "\n",
            "Global step: 2479,loss: 0.01638185\n",
            "\n",
            "Global step: 2480,loss: 0.016020816\n",
            "\n",
            "Global step: 2481,loss: 0.01623927\n",
            "\n",
            "Global step: 2482,loss: 0.016651746\n",
            "\n",
            "Global step: 2483,loss: 0.019233944\n",
            "\n",
            "Global step: 2484,loss: 0.0166063\n",
            "\n",
            "Global step: 2485,loss: 0.01505423\n",
            "\n",
            "Global step: 2486,loss: 0.01646837\n",
            "\n",
            "Global step: 2487,loss: 0.01702162\n",
            "\n",
            "Global step: 2488,loss: 0.016168762\n",
            "\n",
            "Global step: 2489,loss: 0.016078345\n",
            "\n",
            "Global step: 2490,loss: 0.01617757\n",
            "\n",
            "Global step: 2491,loss: 0.015922252\n",
            "\n",
            "Global step: 2492,loss: 0.016298825\n",
            "\n",
            "Global step: 2493,loss: 0.016857678\n",
            "\n",
            "Global step: 2494,loss: 0.017563153\n",
            "\n",
            "Global step: 2495,loss: 0.016234687\n",
            "\n",
            "Global step: 2496,loss: 0.016884264\n",
            "\n",
            "Global step: 2497,loss: 0.015982242\n",
            "\n",
            "Global step: 2498,loss: 0.015822764\n",
            "\n",
            "Global step: 2499,loss: 0.01597725\n",
            "\n",
            "Global step: 2500,loss: 0.015745547\n",
            "\n",
            "Global step: 2501,loss: 0.016234798\n",
            "\n",
            "Global step: 2502,loss: 0.016766062\n",
            "\n",
            "Global step: 2503,loss: 0.017585475\n",
            "\n",
            "Global step: 2504,loss: 0.015653234\n",
            "\n",
            "Global step: 2505,loss: 0.017599\n",
            "\n",
            "Global step: 2506,loss: 0.016064357\n",
            "\n",
            "Global step: 2507,loss: 0.016730336\n",
            "\n",
            "Global step: 2508,loss: 0.016206782\n",
            "\n",
            "Global step: 2509,loss: 0.016523013\n",
            "\n",
            "Global step: 2510,loss: 0.016119778\n",
            "\n",
            "Global step: 2511,loss: 0.0155252535\n",
            "\n",
            "Global step: 2512,loss: 0.015689895\n",
            "\n",
            "Global step: 2513,loss: 0.017951246\n",
            "\n",
            "Global step: 2514,loss: 0.01632933\n",
            "\n",
            "Global step: 2515,loss: 0.017840022\n",
            "\n",
            "Global step: 2516,loss: 0.015991723\n",
            "\n",
            "Global step: 2517,loss: 0.01632456\n",
            "\n",
            "Global step: 2518,loss: 0.016521025\n",
            "\n",
            "Global step: 2519,loss: 0.015768109\n",
            "\n",
            "Global step: 2520,loss: 0.019102812\n",
            "\n",
            "Global step: 2521,loss: 0.016522951\n",
            "\n",
            "Global step: 2522,loss: 0.017063959\n",
            "\n",
            "Global step: 2523,loss: 0.016316064\n",
            "\n",
            "Global step: 2524,loss: 0.017835762\n",
            "\n",
            "Global step: 2525,loss: 0.015344292\n",
            "\n",
            "Global step: 2526,loss: 0.015730457\n",
            "\n",
            "Global step: 2527,loss: 0.015448429\n",
            "\n",
            "Global step: 2528,loss: 0.018187962\n",
            "\n",
            "Global step: 2529,loss: 0.016573973\n",
            "\n",
            "Global step: 2530,loss: 0.016487975\n",
            "\n",
            "Global step: 2531,loss: 0.015559135\n",
            "\n",
            "Global step: 2532,loss: 0.016320826\n",
            "\n",
            "Global step: 2533,loss: 0.019388033\n",
            "\n",
            "Global step: 2534,loss: 0.016568786\n",
            "\n",
            "Global step: 2535,loss: 0.015229778\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2536.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:04:39.867715 140504771352320 supervisor.py:1050] Recording summary at step 2536.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2536,loss: 0.020065786\n",
            "\n",
            "Global step: 2537,loss: 0.016437467\n",
            "\n",
            "Global step: 2538,loss: 0.01681045\n",
            "\n",
            "Global step: 2539,loss: 0.015532376\n",
            "\n",
            "Global step: 2540,loss: 0.015813837\n",
            "\n",
            "Global step: 2541,loss: 0.015263125\n",
            "\n",
            "Global step: 2542,loss: 0.015227424\n",
            "\n",
            "Global step: 2543,loss: 0.017795395\n",
            "\n",
            "Global step: 2544,loss: 0.016739966\n",
            "\n",
            "Global step: 2545,loss: 0.016622817\n",
            "\n",
            "Global step: 2546,loss: 0.018180296\n",
            "\n",
            "Global step: 2547,loss: 0.016527865\n",
            "\n",
            "Global step: 2548,loss: 0.016927838\n",
            "\n",
            "Global step: 2549,loss: 0.016127806\n",
            "\n",
            "Global step: 2550,loss: 0.016882263\n",
            "\n",
            "Global step: 2551,loss: 0.015971836\n",
            "\n",
            "Global step: 2552,loss: 0.018333506\n",
            "\n",
            "Global step: 2553,loss: 0.015534209\n",
            "\n",
            "Global step: 2554,loss: 0.016492238\n",
            "\n",
            "Global step: 2555,loss: 0.01610023\n",
            "\n",
            "Global step: 2556,loss: 0.016503748\n",
            "\n",
            "Global step: 2557,loss: 0.0171999\n",
            "\n",
            "Global step: 2558,loss: 0.016223766\n",
            "\n",
            "Global step: 2559,loss: 0.016790364\n",
            "\n",
            "Global step: 2560,loss: 0.018200457\n",
            "\n",
            "Global step: 2561,loss: 0.01704361\n",
            "\n",
            "Global step: 2562,loss: 0.016199902\n",
            "\n",
            "Global step: 2563,loss: 0.016235728\n",
            "\n",
            "Global step: 2564,loss: 0.015603581\n",
            "\n",
            "Global step: 2565,loss: 0.01580271\n",
            "\n",
            "Global step: 2566,loss: 0.01582402\n",
            "\n",
            "Global step: 2567,loss: 0.016038356\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2567,val_loss: 0.028830468164462792\n",
            "\n",
            "Training for epoch 13/16:\n",
            "Global step: 2568,loss: 0.016074818\n",
            "\n",
            "Global step: 2569,loss: 0.018047467\n",
            "\n",
            "Global step: 2570,loss: 0.015320141\n",
            "\n",
            "Global step: 2571,loss: 0.015711188\n",
            "\n",
            "Global step: 2572,loss: 0.017980069\n",
            "\n",
            "Global step: 2573,loss: 0.015652813\n",
            "\n",
            "Global step: 2574,loss: 0.017936552\n",
            "\n",
            "Global step: 2575,loss: 0.015831526\n",
            "\n",
            "Global step: 2576,loss: 0.01663788\n",
            "\n",
            "Global step: 2577,loss: 0.01665055\n",
            "\n",
            "Global step: 2578,loss: 0.015831929\n",
            "\n",
            "Global step: 2579,loss: 0.015049872\n",
            "\n",
            "Global step: 2580,loss: 0.015581879\n",
            "\n",
            "Global step: 2581,loss: 0.017734913\n",
            "\n",
            "Global step: 2582,loss: 0.015121301\n",
            "\n",
            "Global step: 2583,loss: 0.016189152\n",
            "\n",
            "Global step: 2584,loss: 0.01593017\n",
            "\n",
            "Global step: 2585,loss: 0.015804972\n",
            "\n",
            "Global step: 2586,loss: 0.017466735\n",
            "\n",
            "Global step: 2587,loss: 0.015332374\n",
            "\n",
            "Global step: 2588,loss: 0.016632032\n",
            "\n",
            "Global step: 2589,loss: 0.01535755\n",
            "\n",
            "Global step: 2590,loss: 0.01600428\n",
            "\n",
            "Global step: 2591,loss: 0.015919434\n",
            "\n",
            "Global step: 2592,loss: 0.016339928\n",
            "\n",
            "Global step: 2593,loss: 0.016783236\n",
            "\n",
            "Global step: 2594,loss: 0.015180129\n",
            "\n",
            "Global step: 2595,loss: 0.015153272\n",
            "\n",
            "Global step: 2596,loss: 0.0164209\n",
            "\n",
            "Global step: 2597,loss: 0.016120777\n",
            "\n",
            "Global step: 2598,loss: 0.015745511\n",
            "\n",
            "Global step: 2599,loss: 0.01495704\n",
            "\n",
            "Global step: 2600,loss: 0.01552792\n",
            "\n",
            "Global step: 2601,loss: 0.016100636\n",
            "\n",
            "Global step: 2602,loss: 0.017694557\n",
            "\n",
            "Global step: 2603,loss: 0.015175676\n",
            "\n",
            "Global step: 2604,loss: 0.016957805\n",
            "\n",
            "Global step: 2605,loss: 0.016586052\n",
            "\n",
            "Global step: 2606,loss: 0.0155981\n",
            "\n",
            "Global step: 2607,loss: 0.016862707\n",
            "\n",
            "Global step: 2608,loss: 0.014497294\n",
            "\n",
            "Global step: 2609,loss: 0.015482373\n",
            "\n",
            "Global step: 2610,loss: 0.01610121\n",
            "\n",
            "Global step: 2611,loss: 0.015898103\n",
            "\n",
            "Global step: 2612,loss: 0.015851397\n",
            "\n",
            "Global step: 2613,loss: 0.01595972\n",
            "\n",
            "Global step: 2614,loss: 0.014965436\n",
            "\n",
            "Global step: 2615,loss: 0.020412397\n",
            "\n",
            "Global step: 2616,loss: 0.015432157\n",
            "\n",
            "Global step: 2617,loss: 0.015708467\n",
            "\n",
            "Global step: 2618,loss: 0.015619385\n",
            "\n",
            "Global step: 2619,loss: 0.017920291\n",
            "\n",
            "Global step: 2620,loss: 0.015641177\n",
            "\n",
            "Global step: 2621,loss: 0.016085735\n",
            "\n",
            "Global step: 2622,loss: 0.016199522\n",
            "\n",
            "Global step: 2623,loss: 0.016328417\n",
            "\n",
            "Global step: 2624,loss: 0.015690979\n",
            "\n",
            "Global step: 2625,loss: 0.015830327\n",
            "\n",
            "Global step: 2626,loss: 0.015186103\n",
            "\n",
            "Global step: 2627,loss: 0.015370071\n",
            "\n",
            "Global step: 2628,loss: 0.015566949\n",
            "\n",
            "Global step: 2629,loss: 0.016825106\n",
            "\n",
            "Global step: 2630,loss: 0.015903281\n",
            "\n",
            "Global step: 2631,loss: 0.017844561\n",
            "\n",
            "Global step: 2632,loss: 0.016418763\n",
            "\n",
            "Global step: 2633,loss: 0.017040214\n",
            "\n",
            "Global step: 2634,loss: 0.016104605\n",
            "\n",
            "Global step: 2635,loss: 0.01492936\n",
            "\n",
            "Global step: 2636,loss: 0.015481009\n",
            "\n",
            "Global step: 2637,loss: 0.0152588915\n",
            "\n",
            "Global step: 2638,loss: 0.017836047\n",
            "\n",
            "Global step: 2639,loss: 0.016752653\n",
            "\n",
            "Global step: 2640,loss: 0.015329457\n",
            "\n",
            "Global step: 2641,loss: 0.014760704\n",
            "\n",
            "Global step: 2642,loss: 0.015903296\n",
            "\n",
            "Global step: 2643,loss: 0.015361739\n",
            "\n",
            "Global step: 2644,loss: 0.015989946\n",
            "\n",
            "Global step: 2645,loss: 0.015345185\n",
            "\n",
            "Global step: 2646,loss: 0.015162194\n",
            "\n",
            "Global step: 2647,loss: 0.015119551\n",
            "\n",
            "Global step: 2648,loss: 0.015644265\n",
            "\n",
            "Global step: 2649,loss: 0.015077408\n",
            "\n",
            "Global step: 2650,loss: 0.015783463\n",
            "\n",
            "Global step: 2651,loss: 0.01529201\n",
            "\n",
            "Global step: 2652,loss: 0.015137248\n",
            "\n",
            "Global step: 2653,loss: 0.01612266\n",
            "\n",
            "Global step: 2654,loss: 0.015912818\n",
            "\n",
            "Global step: 2655,loss: 0.014683348\n",
            "\n",
            "Global step: 2656,loss: 0.016155273\n",
            "\n",
            "Global step: 2657,loss: 0.015677068\n",
            "\n",
            "Global step: 2658,loss: 0.015379198\n",
            "\n",
            "Global step: 2659,loss: 0.0150064975\n",
            "\n",
            "Global step: 2660,loss: 0.014454968\n",
            "\n",
            "Global step: 2661,loss: 0.015860444\n",
            "\n",
            "Global step: 2662,loss: 0.01575653\n",
            "\n",
            "Global step: 2663,loss: 0.016341582\n",
            "\n",
            "Global step: 2664,loss: 0.014683561\n",
            "\n",
            "Global step: 2665,loss: 0.016208176\n",
            "\n",
            "Global step: 2666,loss: 0.017822199\n",
            "\n",
            "Global step: 2667,loss: 0.015657613\n",
            "\n",
            "Global step: 2668,loss: 0.016240764\n",
            "\n",
            "Global step: 2669,loss: 0.01638349\n",
            "\n",
            "Global step: 2670,loss: 0.015364665\n",
            "\n",
            "Global step: 2671,loss: 0.016073665\n",
            "\n",
            "Global step: 2672,loss: 0.01534755\n",
            "\n",
            "Global step: 2673,loss: 0.014417007\n",
            "\n",
            "Global step: 2674,loss: 0.015362828\n",
            "\n",
            "Global step: 2675,loss: 0.015166925\n",
            "\n",
            "Global step: 2676,loss: 0.015316451\n",
            "\n",
            "Global step: 2677,loss: 0.015666345\n",
            "\n",
            "Global step: 2678,loss: 0.015990894\n",
            "\n",
            "Global step: 2679,loss: 0.0156442\n",
            "\n",
            "Global step: 2680,loss: 0.015927374\n",
            "\n",
            "Global step: 2681,loss: 0.014802123\n",
            "\n",
            "Global step: 2682,loss: 0.017325211\n",
            "\n",
            "Global step: 2683,loss: 0.015260136\n",
            "\n",
            "Global step: 2684,loss: 0.015062064\n",
            "\n",
            "Global step: 2685,loss: 0.016582394\n",
            "\n",
            "Global step: 2686,loss: 0.015488859\n",
            "\n",
            "Global step: 2687,loss: 0.015126767\n",
            "\n",
            "Global step: 2688,loss: 0.017766058\n",
            "\n",
            "Global step: 2689,loss: 0.014946732\n",
            "\n",
            "Global step: 2690,loss: 0.01538609\n",
            "\n",
            "Global step: 2691,loss: 0.01595467\n",
            "\n",
            "Global step: 2692,loss: 0.015068355\n",
            "\n",
            "Global step: 2693,loss: 0.015177105\n",
            "\n",
            "Global step: 2694,loss: 0.015772814\n",
            "\n",
            "Global step: 2695,loss: 0.014700472\n",
            "\n",
            "Global step: 2696,loss: 0.015193989\n",
            "\n",
            "Global step: 2697,loss: 0.015461586\n",
            "\n",
            "Global step: 2698,loss: 0.016919505\n",
            "\n",
            "Global step: 2699,loss: 0.014931054\n",
            "\n",
            "Global step: 2700,loss: 0.015005595\n",
            "\n",
            "Global step: 2701,loss: 0.014573567\n",
            "\n",
            "Global step: 2702,loss: 0.014736345\n",
            "\n",
            "Global step: 2703,loss: 0.01634497\n",
            "\n",
            "Global step: 2704,loss: 0.015147142\n",
            "\n",
            "Global step: 2705,loss: 0.015224716\n",
            "\n",
            "Global step: 2706,loss: 0.01611426\n",
            "\n",
            "Global step: 2707,loss: 0.014333402\n",
            "\n",
            "Global step: 2708,loss: 0.016389718\n",
            "\n",
            "Global step: 2709,loss: 0.015569389\n",
            "\n",
            "Global step: 2710,loss: 0.014749341\n",
            "\n",
            "Global step: 2711,loss: 0.015098889\n",
            "\n",
            "Global step: 2712,loss: 0.015768304\n",
            "\n",
            "Global step: 2713,loss: 0.016200472\n",
            "\n",
            "Global step: 2714,loss: 0.015500593\n",
            "\n",
            "Global step: 2715,loss: 0.01461526\n",
            "\n",
            "Global step: 2716,loss: 0.014990136\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2717.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:06:39.552807 140504771352320 supervisor.py:1050] Recording summary at step 2717.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2717,loss: 0.0165729\n",
            "\n",
            "Global step: 2718,loss: 0.016645623\n",
            "\n",
            "Global step: 2719,loss: 0.015616127\n",
            "\n",
            "Global step: 2720,loss: 0.0152704455\n",
            "\n",
            "Global step: 2721,loss: 0.017027613\n",
            "\n",
            "Global step: 2722,loss: 0.014662005\n",
            "\n",
            "Global step: 2723,loss: 0.015706372\n",
            "\n",
            "Global step: 2724,loss: 0.015485239\n",
            "\n",
            "Global step: 2725,loss: 0.015028365\n",
            "\n",
            "Global step: 2726,loss: 0.01435212\n",
            "\n",
            "Global step: 2727,loss: 0.014313814\n",
            "\n",
            "Global step: 2728,loss: 0.0150153395\n",
            "\n",
            "Global step: 2729,loss: 0.015754716\n",
            "\n",
            "Global step: 2730,loss: 0.0154598355\n",
            "\n",
            "Global step: 2731,loss: 0.017316429\n",
            "\n",
            "Global step: 2732,loss: 0.015237772\n",
            "\n",
            "Global step: 2733,loss: 0.015734212\n",
            "\n",
            "Global step: 2734,loss: 0.0151954265\n",
            "\n",
            "Global step: 2735,loss: 0.014697816\n",
            "\n",
            "Global step: 2736,loss: 0.014914667\n",
            "\n",
            "Global step: 2737,loss: 0.015050577\n",
            "\n",
            "Global step: 2738,loss: 0.015496954\n",
            "\n",
            "Global step: 2739,loss: 0.015817076\n",
            "\n",
            "Global step: 2740,loss: 0.014882324\n",
            "\n",
            "Global step: 2741,loss: 0.0149017535\n",
            "\n",
            "Global step: 2742,loss: 0.014850719\n",
            "\n",
            "Global step: 2743,loss: 0.015537848\n",
            "\n",
            "Global step: 2744,loss: 0.014499688\n",
            "\n",
            "Global step: 2745,loss: 0.01462992\n",
            "\n",
            "Global step: 2746,loss: 0.016327426\n",
            "\n",
            "Global step: 2747,loss: 0.015166046\n",
            "\n",
            "Global step: 2748,loss: 0.014525618\n",
            "\n",
            "Global step: 2749,loss: 0.015996236\n",
            "\n",
            "Global step: 2750,loss: 0.014727198\n",
            "\n",
            "Global step: 2751,loss: 0.015105277\n",
            "\n",
            "Global step: 2752,loss: 0.015192533\n",
            "\n",
            "Global step: 2753,loss: 0.014532876\n",
            "\n",
            "Global step: 2754,loss: 0.015250551\n",
            "\n",
            "Global step: 2755,loss: 0.01548393\n",
            "\n",
            "Global step: 2756,loss: 0.015177293\n",
            "\n",
            "Global step: 2757,loss: 0.01492473\n",
            "\n",
            "Global step: 2758,loss: 0.014889001\n",
            "\n",
            "Global step: 2759,loss: 0.015163251\n",
            "\n",
            "Global step: 2760,loss: 0.014839485\n",
            "\n",
            "Global step: 2761,loss: 0.015128323\n",
            "\n",
            "Global step: 2762,loss: 0.01507406\n",
            "\n",
            "Global step: 2763,loss: 0.015788095\n",
            "\n",
            "Global step: 2764,loss: 0.014639036\n",
            "\n",
            "Global step: 2765,loss: 0.015453861\n",
            "\n",
            "Global step: 2766,loss: 0.01500218\n",
            "\n",
            "Global step: 2767,loss: 0.015182877\n",
            "\n",
            "Global step: 2768,loss: 0.014452908\n",
            "\n",
            "Global step: 2769,loss: 0.015722431\n",
            "\n",
            "Global step: 2770,loss: 0.015050743\n",
            "\n",
            "Global step: 2771,loss: 0.016241875\n",
            "\n",
            "Global step: 2772,loss: 0.014594995\n",
            "\n",
            "Global step: 2773,loss: 0.01482578\n",
            "\n",
            "Global step: 2774,loss: 0.01415956\n",
            "\n",
            "Global step: 2775,loss: 0.015416721\n",
            "\n",
            "Global step: 2776,loss: 0.016644176\n",
            "\n",
            "Global step: 2777,loss: 0.014810819\n",
            "\n",
            "Global step: 2778,loss: 0.0149718765\n",
            "\n",
            "Global step: 2779,loss: 0.014920818\n",
            "\n",
            "Global step: 2780,loss: 0.014695755\n",
            "\n",
            "Global step: 2781,loss: 0.015496549\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2781,val_loss: 0.027605079702640836\n",
            "\n",
            "Training for epoch 14/16:\n",
            "Global step: 2782,loss: 0.015065205\n",
            "\n",
            "Global step: 2783,loss: 0.015012131\n",
            "\n",
            "Global step: 2784,loss: 0.015735528\n",
            "\n",
            "Global step: 2785,loss: 0.014499624\n",
            "\n",
            "Global step: 2786,loss: 0.014970539\n",
            "\n",
            "Global step: 2787,loss: 0.014445176\n",
            "\n",
            "Global step: 2788,loss: 0.014695486\n",
            "\n",
            "Global step: 2789,loss: 0.014843424\n",
            "\n",
            "Global step: 2790,loss: 0.014330944\n",
            "\n",
            "Global step: 2791,loss: 0.014458887\n",
            "\n",
            "Global step: 2792,loss: 0.015653376\n",
            "\n",
            "Global step: 2793,loss: 0.015425703\n",
            "\n",
            "Global step: 2794,loss: 0.015200851\n",
            "\n",
            "Global step: 2795,loss: 0.015930444\n",
            "\n",
            "Global step: 2796,loss: 0.014235337\n",
            "\n",
            "Global step: 2797,loss: 0.015835434\n",
            "\n",
            "Global step: 2798,loss: 0.016667837\n",
            "\n",
            "Global step: 2799,loss: 0.016442282\n",
            "\n",
            "Global step: 2800,loss: 0.015595293\n",
            "\n",
            "Global step: 2801,loss: 0.014800733\n",
            "\n",
            "Global step: 2802,loss: 0.014557584\n",
            "\n",
            "Global step: 2803,loss: 0.014927241\n",
            "\n",
            "Global step: 2804,loss: 0.01523374\n",
            "\n",
            "Global step: 2805,loss: 0.015318081\n",
            "\n",
            "Global step: 2806,loss: 0.014604345\n",
            "\n",
            "Global step: 2807,loss: 0.014580498\n",
            "\n",
            "Global step: 2808,loss: 0.014592889\n",
            "\n",
            "Global step: 2809,loss: 0.014918327\n",
            "\n",
            "Global step: 2810,loss: 0.014261465\n",
            "\n",
            "Global step: 2811,loss: 0.014593026\n",
            "\n",
            "Global step: 2812,loss: 0.0143506825\n",
            "\n",
            "Global step: 2813,loss: 0.014107615\n",
            "\n",
            "Global step: 2814,loss: 0.014730262\n",
            "\n",
            "Global step: 2815,loss: 0.015448572\n",
            "\n",
            "Global step: 2816,loss: 0.014198123\n",
            "\n",
            "Global step: 2817,loss: 0.01450945\n",
            "\n",
            "Global step: 2818,loss: 0.014781788\n",
            "\n",
            "Global step: 2819,loss: 0.014950173\n",
            "\n",
            "Global step: 2820,loss: 0.0150792785\n",
            "\n",
            "Global step: 2821,loss: 0.0147469975\n",
            "\n",
            "Global step: 2822,loss: 0.014262118\n",
            "\n",
            "Global step: 2823,loss: 0.013641028\n",
            "\n",
            "Global step: 2824,loss: 0.014239487\n",
            "\n",
            "Global step: 2825,loss: 0.015569656\n",
            "\n",
            "Global step: 2826,loss: 0.015092314\n",
            "\n",
            "Global step: 2827,loss: 0.014614035\n",
            "\n",
            "Global step: 2828,loss: 0.016411029\n",
            "\n",
            "Global step: 2829,loss: 0.014388455\n",
            "\n",
            "Global step: 2830,loss: 0.014521067\n",
            "\n",
            "Global step: 2831,loss: 0.014890079\n",
            "\n",
            "Global step: 2832,loss: 0.014770606\n",
            "\n",
            "Global step: 2833,loss: 0.015110504\n",
            "\n",
            "Global step: 2834,loss: 0.014741669\n",
            "\n",
            "Global step: 2835,loss: 0.014277963\n",
            "\n",
            "Global step: 2836,loss: 0.014098743\n",
            "\n",
            "Global step: 2837,loss: 0.0144946575\n",
            "\n",
            "Global step: 2838,loss: 0.016343541\n",
            "\n",
            "Global step: 2839,loss: 0.014503643\n",
            "\n",
            "Global step: 2840,loss: 0.015839271\n",
            "\n",
            "Global step: 2841,loss: 0.014762879\n",
            "\n",
            "Global step: 2842,loss: 0.015275769\n",
            "\n",
            "Global step: 2843,loss: 0.015102492\n",
            "\n",
            "Global step: 2844,loss: 0.0142046\n",
            "\n",
            "Global step: 2845,loss: 0.0150714815\n",
            "\n",
            "Global step: 2846,loss: 0.014547954\n",
            "\n",
            "Global step: 2847,loss: 0.015440504\n",
            "\n",
            "Global step: 2848,loss: 0.0144706275\n",
            "\n",
            "Global step: 2849,loss: 0.0142129045\n",
            "\n",
            "Global step: 2850,loss: 0.015973061\n",
            "\n",
            "Global step: 2851,loss: 0.013907862\n",
            "\n",
            "Global step: 2852,loss: 0.015442815\n",
            "\n",
            "Global step: 2853,loss: 0.014694188\n",
            "\n",
            "Global step: 2854,loss: 0.014654968\n",
            "\n",
            "Global step: 2855,loss: 0.014127031\n",
            "\n",
            "Global step: 2856,loss: 0.014261086\n",
            "\n",
            "Global step: 2857,loss: 0.01632483\n",
            "\n",
            "Global step: 2858,loss: 0.01429142\n",
            "\n",
            "Global step: 2859,loss: 0.014880484\n",
            "\n",
            "Global step: 2860,loss: 0.016391061\n",
            "\n",
            "Global step: 2861,loss: 0.015990965\n",
            "\n",
            "Global step: 2862,loss: 0.014157925\n",
            "\n",
            "Global step: 2863,loss: 0.015707927\n",
            "\n",
            "Global step: 2864,loss: 0.0146095315\n",
            "\n",
            "Global step: 2865,loss: 0.014775431\n",
            "\n",
            "Global step: 2866,loss: 0.0155198295\n",
            "\n",
            "Global step: 2867,loss: 0.014578798\n",
            "\n",
            "Global step: 2868,loss: 0.014992248\n",
            "\n",
            "Global step: 2869,loss: 0.013997697\n",
            "\n",
            "Global step: 2870,loss: 0.014535926\n",
            "\n",
            "Global step: 2871,loss: 0.014764259\n",
            "\n",
            "Global step: 2872,loss: 0.01407478\n",
            "\n",
            "Global step: 2873,loss: 0.014458104\n",
            "\n",
            "Global step: 2874,loss: 0.014801126\n",
            "\n",
            "Global step: 2875,loss: 0.014494936\n",
            "\n",
            "Global step: 2876,loss: 0.015106646\n",
            "\n",
            "Global step: 2877,loss: 0.014739468\n",
            "\n",
            "Global step: 2878,loss: 0.01408793\n",
            "\n",
            "Global step: 2879,loss: 0.014100465\n",
            "\n",
            "Global step: 2880,loss: 0.014010587\n",
            "\n",
            "Global step: 2881,loss: 0.014368268\n",
            "\n",
            "Global step: 2882,loss: 0.014679016\n",
            "\n",
            "Global step: 2883,loss: 0.0150338765\n",
            "\n",
            "Global step: 2884,loss: 0.014289192\n",
            "\n",
            "Global step: 2885,loss: 0.01469389\n",
            "\n",
            "Global step: 2886,loss: 0.015073144\n",
            "\n",
            "Global step: 2887,loss: 0.015139846\n",
            "\n",
            "Global step: 2888,loss: 0.01427207\n",
            "\n",
            "Global step: 2889,loss: 0.014645521\n",
            "\n",
            "Global step: 2890,loss: 0.013924899\n",
            "\n",
            "Global step: 2891,loss: 0.014131115\n",
            "\n",
            "Global step: 2892,loss: 0.014221832\n",
            "\n",
            "Global step: 2893,loss: 0.015018839\n",
            "\n",
            "Global step: 2894,loss: 0.013635486\n",
            "\n",
            "Global step: 2895,loss: 0.0142353205\n",
            "\n",
            "Global step: 2896,loss: 0.014783433\n",
            "\n",
            "Global step: 2897,loss: 0.014878788\n",
            "\n",
            "Global step: 2898,loss: 0.014510053\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2899.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:08:39.978442 140504771352320 supervisor.py:1050] Recording summary at step 2899.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2899,loss: 0.014383844\n",
            "\n",
            "Global step: 2900,loss: 0.014569625\n",
            "\n",
            "Global step: 2901,loss: 0.014742132\n",
            "\n",
            "Global step: 2902,loss: 0.014524461\n",
            "\n",
            "Global step: 2903,loss: 0.014481691\n",
            "\n",
            "Global step: 2904,loss: 0.015936049\n",
            "\n",
            "Global step: 2905,loss: 0.013975011\n",
            "\n",
            "Global step: 2906,loss: 0.014299024\n",
            "\n",
            "Global step: 2907,loss: 0.014640186\n",
            "\n",
            "Global step: 2908,loss: 0.015168138\n",
            "\n",
            "Global step: 2909,loss: 0.014762514\n",
            "\n",
            "Global step: 2910,loss: 0.015476327\n",
            "\n",
            "Global step: 2911,loss: 0.0147066945\n",
            "\n",
            "Global step: 2912,loss: 0.016202973\n",
            "\n",
            "Global step: 2913,loss: 0.01481201\n",
            "\n",
            "Global step: 2914,loss: 0.0153392255\n",
            "\n",
            "Global step: 2915,loss: 0.01467934\n",
            "\n",
            "Global step: 2916,loss: 0.014420735\n",
            "\n",
            "Global step: 2917,loss: 0.01424525\n",
            "\n",
            "Global step: 2918,loss: 0.014082089\n",
            "\n",
            "Global step: 2919,loss: 0.014868316\n",
            "\n",
            "Global step: 2920,loss: 0.014981435\n",
            "\n",
            "Global step: 2921,loss: 0.014485119\n",
            "\n",
            "Global step: 2922,loss: 0.01361687\n",
            "\n",
            "Global step: 2923,loss: 0.016361197\n",
            "\n",
            "Global step: 2924,loss: 0.015408842\n",
            "\n",
            "Global step: 2925,loss: 0.0143096745\n",
            "\n",
            "Global step: 2926,loss: 0.014969485\n",
            "\n",
            "Global step: 2927,loss: 0.015234361\n",
            "\n",
            "Global step: 2928,loss: 0.016255906\n",
            "\n",
            "Global step: 2929,loss: 0.01493649\n",
            "\n",
            "Global step: 2930,loss: 0.0147314165\n",
            "\n",
            "Global step: 2931,loss: 0.01397202\n",
            "\n",
            "Global step: 2932,loss: 0.01476006\n",
            "\n",
            "Global step: 2933,loss: 0.015054077\n",
            "\n",
            "Global step: 2934,loss: 0.016422635\n",
            "\n",
            "Global step: 2935,loss: 0.01403588\n",
            "\n",
            "Global step: 2936,loss: 0.014490449\n",
            "\n",
            "Global step: 2937,loss: 0.01369911\n",
            "\n",
            "Global step: 2938,loss: 0.015053211\n",
            "\n",
            "Global step: 2939,loss: 0.01456259\n",
            "\n",
            "Global step: 2940,loss: 0.016562648\n",
            "\n",
            "Global step: 2941,loss: 0.013935034\n",
            "\n",
            "Global step: 2942,loss: 0.017650796\n",
            "\n",
            "Global step: 2943,loss: 0.015231208\n",
            "\n",
            "Global step: 2944,loss: 0.015013315\n",
            "\n",
            "Global step: 2945,loss: 0.01499042\n",
            "\n",
            "Global step: 2946,loss: 0.013858012\n",
            "\n",
            "Global step: 2947,loss: 0.0145225\n",
            "\n",
            "Global step: 2948,loss: 0.014959944\n",
            "\n",
            "Global step: 2949,loss: 0.014476493\n",
            "\n",
            "Global step: 2950,loss: 0.015025198\n",
            "\n",
            "Global step: 2951,loss: 0.01567415\n",
            "\n",
            "Global step: 2952,loss: 0.014809302\n",
            "\n",
            "Global step: 2953,loss: 0.014266646\n",
            "\n",
            "Global step: 2954,loss: 0.014534909\n",
            "\n",
            "Global step: 2955,loss: 0.01531158\n",
            "\n",
            "Global step: 2956,loss: 0.015044071\n",
            "\n",
            "Global step: 2957,loss: 0.015368296\n",
            "\n",
            "Global step: 2958,loss: 0.014543031\n",
            "\n",
            "Global step: 2959,loss: 0.015436656\n",
            "\n",
            "Global step: 2960,loss: 0.014455175\n",
            "\n",
            "Global step: 2961,loss: 0.014558763\n",
            "\n",
            "Global step: 2962,loss: 0.014930065\n",
            "\n",
            "Global step: 2963,loss: 0.013992187\n",
            "\n",
            "Global step: 2964,loss: 0.014906472\n",
            "\n",
            "Global step: 2965,loss: 0.015856871\n",
            "\n",
            "Global step: 2966,loss: 0.016237674\n",
            "\n",
            "Global step: 2967,loss: 0.016616479\n",
            "\n",
            "Global step: 2968,loss: 0.013654972\n",
            "\n",
            "Global step: 2969,loss: 0.014078702\n",
            "\n",
            "Global step: 2970,loss: 0.0145025235\n",
            "\n",
            "Global step: 2971,loss: 0.014411074\n",
            "\n",
            "Global step: 2972,loss: 0.014451275\n",
            "\n",
            "Global step: 2973,loss: 0.016329957\n",
            "\n",
            "Global step: 2974,loss: 0.013685575\n",
            "\n",
            "Global step: 2975,loss: 0.014968169\n",
            "\n",
            "Global step: 2976,loss: 0.013840368\n",
            "\n",
            "Global step: 2977,loss: 0.014206121\n",
            "\n",
            "Global step: 2978,loss: 0.013882666\n",
            "\n",
            "Global step: 2979,loss: 0.014648946\n",
            "\n",
            "Global step: 2980,loss: 0.0144204525\n",
            "\n",
            "Global step: 2981,loss: 0.013827554\n",
            "\n",
            "Global step: 2982,loss: 0.013855566\n",
            "\n",
            "Global step: 2983,loss: 0.014202388\n",
            "\n",
            "Global step: 2984,loss: 0.014603909\n",
            "\n",
            "Global step: 2985,loss: 0.014819579\n",
            "\n",
            "Global step: 2986,loss: 0.0139270825\n",
            "\n",
            "Global step: 2987,loss: 0.013650253\n",
            "\n",
            "Global step: 2988,loss: 0.014595195\n",
            "\n",
            "Global step: 2989,loss: 0.014718443\n",
            "\n",
            "Global step: 2990,loss: 0.014075264\n",
            "\n",
            "Global step: 2991,loss: 0.013671906\n",
            "\n",
            "Global step: 2992,loss: 0.014225436\n",
            "\n",
            "Global step: 2993,loss: 0.0141087\n",
            "\n",
            "Global step: 2994,loss: 0.01430905\n",
            "\n",
            "Global step: 2995,loss: 0.014552988\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2995,val_loss: 0.026950614624901822\n",
            "\n",
            "Training for epoch 15/16:\n",
            "Global step: 2996,loss: 0.013820347\n",
            "\n",
            "Global step: 2997,loss: 0.014751097\n",
            "\n",
            "Global step: 2998,loss: 0.014603396\n",
            "\n",
            "Global step: 2999,loss: 0.014273451\n",
            "\n",
            "Global step: 3000,loss: 0.015384627\n",
            "\n",
            "Global step: 3001,loss: 0.014046218\n",
            "\n",
            "Global step: 3002,loss: 0.014054607\n",
            "\n",
            "Global step: 3003,loss: 0.014213032\n",
            "\n",
            "Global step: 3004,loss: 0.013802884\n",
            "\n",
            "Global step: 3005,loss: 0.0133633325\n",
            "\n",
            "Global step: 3006,loss: 0.013797611\n",
            "\n",
            "Global step: 3007,loss: 0.013638822\n",
            "\n",
            "Global step: 3008,loss: 0.014212332\n",
            "\n",
            "Global step: 3009,loss: 0.014038275\n",
            "\n",
            "Global step: 3010,loss: 0.015404134\n",
            "\n",
            "Global step: 3011,loss: 0.014306636\n",
            "\n",
            "Global step: 3012,loss: 0.016345914\n",
            "\n",
            "Global step: 3013,loss: 0.014330337\n",
            "\n",
            "Global step: 3014,loss: 0.014402283\n",
            "\n",
            "Global step: 3015,loss: 0.014669643\n",
            "\n",
            "Global step: 3016,loss: 0.014257436\n",
            "\n",
            "Global step: 3017,loss: 0.014167946\n",
            "\n",
            "Global step: 3018,loss: 0.014287253\n",
            "\n",
            "Global step: 3019,loss: 0.014888682\n",
            "\n",
            "Global step: 3020,loss: 0.014182435\n",
            "\n",
            "Global step: 3021,loss: 0.01422968\n",
            "\n",
            "Global step: 3022,loss: 0.015244836\n",
            "\n",
            "Global step: 3023,loss: 0.014351785\n",
            "\n",
            "Global step: 3024,loss: 0.014348987\n",
            "\n",
            "Global step: 3025,loss: 0.013758415\n",
            "\n",
            "Global step: 3026,loss: 0.013948988\n",
            "\n",
            "Global step: 3027,loss: 0.01472931\n",
            "\n",
            "Global step: 3028,loss: 0.014088621\n",
            "\n",
            "Global step: 3029,loss: 0.014004563\n",
            "\n",
            "Global step: 3030,loss: 0.014481721\n",
            "\n",
            "Global step: 3031,loss: 0.015169875\n",
            "\n",
            "Global step: 3032,loss: 0.013922667\n",
            "\n",
            "Global step: 3033,loss: 0.013930834\n",
            "\n",
            "Global step: 3034,loss: 0.0143036395\n",
            "\n",
            "Global step: 3035,loss: 0.013547903\n",
            "\n",
            "Global step: 3036,loss: 0.014693337\n",
            "\n",
            "Global step: 3037,loss: 0.01367157\n",
            "\n",
            "Global step: 3038,loss: 0.013542378\n",
            "\n",
            "Global step: 3039,loss: 0.01386614\n",
            "\n",
            "Global step: 3040,loss: 0.013697991\n",
            "\n",
            "Global step: 3041,loss: 0.013581133\n",
            "\n",
            "Global step: 3042,loss: 0.014170416\n",
            "\n",
            "Global step: 3043,loss: 0.0140253715\n",
            "\n",
            "Global step: 3044,loss: 0.014386625\n",
            "\n",
            "Global step: 3045,loss: 0.013884985\n",
            "\n",
            "Global step: 3046,loss: 0.013895379\n",
            "\n",
            "Global step: 3047,loss: 0.014500639\n",
            "\n",
            "Global step: 3048,loss: 0.014002846\n",
            "\n",
            "Global step: 3049,loss: 0.014696776\n",
            "\n",
            "Global step: 3050,loss: 0.0141465645\n",
            "\n",
            "Global step: 3051,loss: 0.013751368\n",
            "\n",
            "Global step: 3052,loss: 0.01377573\n",
            "\n",
            "Global step: 3053,loss: 0.013618062\n",
            "\n",
            "Global step: 3054,loss: 0.0139768515\n",
            "\n",
            "Global step: 3055,loss: 0.014037842\n",
            "\n",
            "Global step: 3056,loss: 0.01337226\n",
            "\n",
            "Global step: 3057,loss: 0.014323296\n",
            "\n",
            "Global step: 3058,loss: 0.013723315\n",
            "\n",
            "Global step: 3059,loss: 0.014231883\n",
            "\n",
            "Global step: 3060,loss: 0.013639538\n",
            "\n",
            "Global step: 3061,loss: 0.013855105\n",
            "\n",
            "Global step: 3062,loss: 0.013879943\n",
            "\n",
            "Global step: 3063,loss: 0.014022723\n",
            "\n",
            "Global step: 3064,loss: 0.013897302\n",
            "\n",
            "Global step: 3065,loss: 0.013914691\n",
            "\n",
            "Global step: 3066,loss: 0.013869411\n",
            "\n",
            "Global step: 3067,loss: 0.013943453\n",
            "\n",
            "Global step: 3068,loss: 0.013807872\n",
            "\n",
            "Global step: 3069,loss: 0.01342973\n",
            "\n",
            "Global step: 3070,loss: 0.013704953\n",
            "\n",
            "Global step: 3071,loss: 0.013772264\n",
            "\n",
            "Global step: 3072,loss: 0.013735772\n",
            "\n",
            "Global step: 3073,loss: 0.013501866\n",
            "\n",
            "Global step: 3074,loss: 0.01357546\n",
            "\n",
            "Global step: 3075,loss: 0.015931694\n",
            "\n",
            "Global step: 3076,loss: 0.013154278\n",
            "\n",
            "Global step: 3077,loss: 0.014151482\n",
            "\n",
            "Global step: 3078,loss: 0.013738041\n",
            "\n",
            "Global step: 3079,loss: 0.014229684\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3080.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:10:39.743977 140504771352320 supervisor.py:1050] Recording summary at step 3080.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3080,loss: 0.014093267\n",
            "\n",
            "Global step: 3081,loss: 0.014563382\n",
            "\n",
            "Global step: 3082,loss: 0.0140602505\n",
            "\n",
            "Global step: 3083,loss: 0.014751067\n",
            "\n",
            "Global step: 3084,loss: 0.013935933\n",
            "\n",
            "Global step: 3085,loss: 0.013533821\n",
            "\n",
            "Global step: 3086,loss: 0.014184978\n",
            "\n",
            "Global step: 3087,loss: 0.014115607\n",
            "\n",
            "Global step: 3088,loss: 0.014094251\n",
            "\n",
            "Global step: 3089,loss: 0.014072244\n",
            "\n",
            "Global step: 3090,loss: 0.013963727\n",
            "\n",
            "Global step: 3091,loss: 0.01397721\n",
            "\n",
            "Global step: 3092,loss: 0.0131961815\n",
            "\n",
            "Global step: 3093,loss: 0.01474764\n",
            "\n",
            "Global step: 3094,loss: 0.0140526965\n",
            "\n",
            "Global step: 3095,loss: 0.014149143\n",
            "\n",
            "Global step: 3096,loss: 0.014138675\n",
            "\n",
            "Global step: 3097,loss: 0.013613864\n",
            "\n",
            "Global step: 3098,loss: 0.014021019\n",
            "\n",
            "Global step: 3099,loss: 0.013875574\n",
            "\n",
            "Global step: 3100,loss: 0.0133630745\n",
            "\n",
            "Global step: 3101,loss: 0.013615265\n",
            "\n",
            "Global step: 3102,loss: 0.014812724\n",
            "\n",
            "Global step: 3103,loss: 0.0140653225\n",
            "\n",
            "Global step: 3104,loss: 0.0135717755\n",
            "\n",
            "Global step: 3105,loss: 0.013670829\n",
            "\n",
            "Global step: 3106,loss: 0.013309446\n",
            "\n",
            "Global step: 3107,loss: 0.014123925\n",
            "\n",
            "Global step: 3108,loss: 0.013814467\n",
            "\n",
            "Global step: 3109,loss: 0.014455162\n",
            "\n",
            "Global step: 3110,loss: 0.01455315\n",
            "\n",
            "Global step: 3111,loss: 0.013539897\n",
            "\n",
            "Global step: 3112,loss: 0.014136193\n",
            "\n",
            "Global step: 3113,loss: 0.014271986\n",
            "\n",
            "Global step: 3114,loss: 0.013586272\n",
            "\n",
            "Global step: 3115,loss: 0.014441704\n",
            "\n",
            "Global step: 3116,loss: 0.0137430485\n",
            "\n",
            "Global step: 3117,loss: 0.014140359\n",
            "\n",
            "Global step: 3118,loss: 0.013642369\n",
            "\n",
            "Global step: 3119,loss: 0.014332618\n",
            "\n",
            "Global step: 3120,loss: 0.013665264\n",
            "\n",
            "Global step: 3121,loss: 0.014261898\n",
            "\n",
            "Global step: 3122,loss: 0.014379542\n",
            "\n",
            "Global step: 3123,loss: 0.013912067\n",
            "\n",
            "Global step: 3124,loss: 0.013842394\n",
            "\n",
            "Global step: 3125,loss: 0.013745456\n",
            "\n",
            "Global step: 3126,loss: 0.013616289\n",
            "\n",
            "Global step: 3127,loss: 0.014918443\n",
            "\n",
            "Global step: 3128,loss: 0.015549269\n",
            "\n",
            "Global step: 3129,loss: 0.013998744\n",
            "\n",
            "Global step: 3130,loss: 0.013803847\n",
            "\n",
            "Global step: 3131,loss: 0.013944268\n",
            "\n",
            "Global step: 3132,loss: 0.016052596\n",
            "\n",
            "Global step: 3133,loss: 0.013352195\n",
            "\n",
            "Global step: 3134,loss: 0.013924588\n",
            "\n",
            "Global step: 3135,loss: 0.013503562\n",
            "\n",
            "Global step: 3136,loss: 0.0139270825\n",
            "\n",
            "Global step: 3137,loss: 0.015079051\n",
            "\n",
            "Global step: 3138,loss: 0.013623377\n",
            "\n",
            "Global step: 3139,loss: 0.013607959\n",
            "\n",
            "Global step: 3140,loss: 0.013514391\n",
            "\n",
            "Global step: 3141,loss: 0.013772597\n",
            "\n",
            "Global step: 3142,loss: 0.01392458\n",
            "\n",
            "Global step: 3143,loss: 0.01351058\n",
            "\n",
            "Global step: 3144,loss: 0.015458643\n",
            "\n",
            "Global step: 3145,loss: 0.014583738\n",
            "\n",
            "Global step: 3146,loss: 0.014108232\n",
            "\n",
            "Global step: 3147,loss: 0.013997455\n",
            "\n",
            "Global step: 3148,loss: 0.015493351\n",
            "\n",
            "Global step: 3149,loss: 0.013388048\n",
            "\n",
            "Global step: 3150,loss: 0.014291681\n",
            "\n",
            "Global step: 3151,loss: 0.015547048\n",
            "\n",
            "Global step: 3152,loss: 0.0140008405\n",
            "\n",
            "Global step: 3153,loss: 0.013863372\n",
            "\n",
            "Global step: 3154,loss: 0.013241588\n",
            "\n",
            "Global step: 3155,loss: 0.013775466\n",
            "\n",
            "Global step: 3156,loss: 0.013644128\n",
            "\n",
            "Global step: 3157,loss: 0.014158863\n",
            "\n",
            "Global step: 3158,loss: 0.013909977\n",
            "\n",
            "Global step: 3159,loss: 0.013558758\n",
            "\n",
            "Global step: 3160,loss: 0.015309229\n",
            "\n",
            "Global step: 3161,loss: 0.013872533\n",
            "\n",
            "Global step: 3162,loss: 0.013633353\n",
            "\n",
            "Global step: 3163,loss: 0.013717347\n",
            "\n",
            "Global step: 3164,loss: 0.014307168\n",
            "\n",
            "Global step: 3165,loss: 0.01349837\n",
            "\n",
            "Global step: 3166,loss: 0.013729083\n",
            "\n",
            "Global step: 3167,loss: 0.013476515\n",
            "\n",
            "Global step: 3168,loss: 0.013463377\n",
            "\n",
            "Global step: 3169,loss: 0.015103515\n",
            "\n",
            "Global step: 3170,loss: 0.013519301\n",
            "\n",
            "Global step: 3171,loss: 0.014462719\n",
            "\n",
            "Global step: 3172,loss: 0.013323409\n",
            "\n",
            "Global step: 3173,loss: 0.01308324\n",
            "\n",
            "Global step: 3174,loss: 0.01416079\n",
            "\n",
            "Global step: 3175,loss: 0.015211017\n",
            "\n",
            "Global step: 3176,loss: 0.013722426\n",
            "\n",
            "Global step: 3177,loss: 0.013795912\n",
            "\n",
            "Global step: 3178,loss: 0.0134820985\n",
            "\n",
            "Global step: 3179,loss: 0.0130726285\n",
            "\n",
            "Global step: 3180,loss: 0.0136541575\n",
            "\n",
            "Global step: 3181,loss: 0.013387949\n",
            "\n",
            "Global step: 3182,loss: 0.013728919\n",
            "\n",
            "Global step: 3183,loss: 0.013317052\n",
            "\n",
            "Global step: 3184,loss: 0.013005445\n",
            "\n",
            "Global step: 3185,loss: 0.016170861\n",
            "\n",
            "Global step: 3186,loss: 0.013370259\n",
            "\n",
            "Global step: 3187,loss: 0.014003632\n",
            "\n",
            "Global step: 3188,loss: 0.014579706\n",
            "\n",
            "Global step: 3189,loss: 0.014047685\n",
            "\n",
            "Global step: 3190,loss: 0.013872796\n",
            "\n",
            "Global step: 3191,loss: 0.0146761965\n",
            "\n",
            "Global step: 3192,loss: 0.013439979\n",
            "\n",
            "Global step: 3193,loss: 0.013480498\n",
            "\n",
            "Global step: 3194,loss: 0.013626252\n",
            "\n",
            "Global step: 3195,loss: 0.015166916\n",
            "\n",
            "Global step: 3196,loss: 0.013741941\n",
            "\n",
            "Global step: 3197,loss: 0.013877293\n",
            "\n",
            "Global step: 3198,loss: 0.013305171\n",
            "\n",
            "Global step: 3199,loss: 0.013962018\n",
            "\n",
            "Global step: 3200,loss: 0.013862917\n",
            "\n",
            "Global step: 3201,loss: 0.013755058\n",
            "\n",
            "Global step: 3202,loss: 0.0137389405\n",
            "\n",
            "Global step: 3203,loss: 0.013917242\n",
            "\n",
            "Global step: 3204,loss: 0.013026276\n",
            "\n",
            "Global step: 3205,loss: 0.014138464\n",
            "\n",
            "Global step: 3206,loss: 0.01311061\n",
            "\n",
            "Global step: 3207,loss: 0.013376381\n",
            "\n",
            "Global step: 3208,loss: 0.01541014\n",
            "\n",
            "Global step: 3209,loss: 0.013615422\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 3209,val_loss: 0.026023721812587036\n",
            "\n",
            "Training for epoch 16/16:\n",
            "Global step: 3210,loss: 0.013537514\n",
            "\n",
            "Global step: 3211,loss: 0.013881372\n",
            "\n",
            "Global step: 3212,loss: 0.013531197\n",
            "\n",
            "Global step: 3213,loss: 0.013124251\n",
            "\n",
            "Global step: 3214,loss: 0.013240083\n",
            "\n",
            "Global step: 3215,loss: 0.014258789\n",
            "\n",
            "Global step: 3216,loss: 0.0135373855\n",
            "\n",
            "Global step: 3217,loss: 0.013344344\n",
            "\n",
            "Global step: 3218,loss: 0.013492031\n",
            "\n",
            "Global step: 3219,loss: 0.013773482\n",
            "\n",
            "Global step: 3220,loss: 0.013502239\n",
            "\n",
            "Global step: 3221,loss: 0.013537583\n",
            "\n",
            "Global step: 3222,loss: 0.013873149\n",
            "\n",
            "Global step: 3223,loss: 0.0134092225\n",
            "\n",
            "Global step: 3224,loss: 0.012911751\n",
            "\n",
            "Global step: 3225,loss: 0.013311518\n",
            "\n",
            "Global step: 3226,loss: 0.013067551\n",
            "\n",
            "Global step: 3227,loss: 0.014929636\n",
            "\n",
            "Global step: 3228,loss: 0.013696734\n",
            "\n",
            "Global step: 3229,loss: 0.013604755\n",
            "\n",
            "Global step: 3230,loss: 0.012947163\n",
            "\n",
            "Global step: 3231,loss: 0.01301805\n",
            "\n",
            "Global step: 3232,loss: 0.01394347\n",
            "\n",
            "Global step: 3233,loss: 0.012881378\n",
            "\n",
            "Global step: 3234,loss: 0.013806806\n",
            "\n",
            "Global step: 3235,loss: 0.012956625\n",
            "\n",
            "Global step: 3236,loss: 0.012810342\n",
            "\n",
            "Global step: 3237,loss: 0.013220515\n",
            "\n",
            "Global step: 3238,loss: 0.0141046\n",
            "\n",
            "Global step: 3239,loss: 0.013666028\n",
            "\n",
            "Global step: 3240,loss: 0.0132904705\n",
            "\n",
            "Global step: 3241,loss: 0.014027439\n",
            "\n",
            "Global step: 3242,loss: 0.013566694\n",
            "\n",
            "Global step: 3243,loss: 0.013545353\n",
            "\n",
            "Global step: 3244,loss: 0.013430928\n",
            "\n",
            "Global step: 3245,loss: 0.012881031\n",
            "\n",
            "Global step: 3246,loss: 0.013315975\n",
            "\n",
            "Global step: 3247,loss: 0.012953846\n",
            "\n",
            "Global step: 3248,loss: 0.01344038\n",
            "\n",
            "Global step: 3249,loss: 0.01405459\n",
            "\n",
            "Global step: 3250,loss: 0.014103055\n",
            "\n",
            "Global step: 3251,loss: 0.013248615\n",
            "\n",
            "Global step: 3252,loss: 0.012946431\n",
            "\n",
            "Global step: 3253,loss: 0.013356766\n",
            "\n",
            "Global step: 3254,loss: 0.013378518\n",
            "\n",
            "Global step: 3255,loss: 0.013663717\n",
            "\n",
            "Global step: 3256,loss: 0.013503704\n",
            "\n",
            "Global step: 3257,loss: 0.012818817\n",
            "\n",
            "Global step: 3258,loss: 0.013256479\n",
            "\n",
            "Global step: 3259,loss: 0.014193468\n",
            "\n",
            "Global step: 3260,loss: 0.012995211\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3261.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:12:39.618525 140504771352320 supervisor.py:1050] Recording summary at step 3261.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3261,loss: 0.0138955405\n",
            "\n",
            "Global step: 3262,loss: 0.013959078\n",
            "\n",
            "Global step: 3263,loss: 0.013982537\n",
            "\n",
            "Global step: 3264,loss: 0.013487438\n",
            "\n",
            "Global step: 3265,loss: 0.013148213\n",
            "\n",
            "Global step: 3266,loss: 0.013381321\n",
            "\n",
            "Global step: 3267,loss: 0.013732853\n",
            "\n",
            "Global step: 3268,loss: 0.013019086\n",
            "\n",
            "Global step: 3269,loss: 0.014099351\n",
            "\n",
            "Global step: 3270,loss: 0.013929298\n",
            "\n",
            "Global step: 3271,loss: 0.013308508\n",
            "\n",
            "Global step: 3272,loss: 0.01534508\n",
            "\n",
            "Global step: 3273,loss: 0.013140891\n",
            "\n",
            "Global step: 3274,loss: 0.013445226\n",
            "\n",
            "Global step: 3275,loss: 0.013256067\n",
            "\n",
            "Global step: 3276,loss: 0.013611074\n",
            "\n",
            "Global step: 3277,loss: 0.013864629\n",
            "\n",
            "Global step: 3278,loss: 0.01349075\n",
            "\n",
            "Global step: 3279,loss: 0.013569185\n",
            "\n",
            "Global step: 3280,loss: 0.012972337\n",
            "\n",
            "Global step: 3281,loss: 0.0133419875\n",
            "\n",
            "Global step: 3282,loss: 0.0132247005\n",
            "\n",
            "Global step: 3283,loss: 0.014183681\n",
            "\n",
            "Global step: 3284,loss: 0.013091173\n",
            "\n",
            "Global step: 3285,loss: 0.013546633\n",
            "\n",
            "Global step: 3286,loss: 0.0138724875\n",
            "\n",
            "Global step: 3287,loss: 0.0149406325\n",
            "\n",
            "Global step: 3288,loss: 0.014590265\n",
            "\n",
            "Global step: 3289,loss: 0.012971544\n",
            "\n",
            "Global step: 3290,loss: 0.013205024\n",
            "\n",
            "Global step: 3291,loss: 0.013770224\n",
            "\n",
            "Global step: 3292,loss: 0.014040251\n",
            "\n",
            "Global step: 3293,loss: 0.01321129\n",
            "\n",
            "Global step: 3294,loss: 0.012820138\n",
            "\n",
            "Global step: 3295,loss: 0.013901416\n",
            "\n",
            "Global step: 3296,loss: 0.014111564\n",
            "\n",
            "Global step: 3297,loss: 0.013486848\n",
            "\n",
            "Global step: 3298,loss: 0.0139506105\n",
            "\n",
            "Global step: 3299,loss: 0.013093652\n",
            "\n",
            "Global step: 3300,loss: 0.013561559\n",
            "\n",
            "Global step: 3301,loss: 0.012997548\n",
            "\n",
            "Global step: 3302,loss: 0.013453777\n",
            "\n",
            "Global step: 3303,loss: 0.01336706\n",
            "\n",
            "Global step: 3304,loss: 0.013585433\n",
            "\n",
            "Global step: 3305,loss: 0.013772769\n",
            "\n",
            "Global step: 3306,loss: 0.013757234\n",
            "\n",
            "Global step: 3307,loss: 0.012636106\n",
            "\n",
            "Global step: 3308,loss: 0.012757112\n",
            "\n",
            "Global step: 3309,loss: 0.014026215\n",
            "\n",
            "Global step: 3310,loss: 0.012987675\n",
            "\n",
            "Global step: 3311,loss: 0.012597592\n",
            "\n",
            "Global step: 3312,loss: 0.014263624\n",
            "\n",
            "Global step: 3313,loss: 0.013122041\n",
            "\n",
            "Global step: 3314,loss: 0.013171871\n",
            "\n",
            "Global step: 3315,loss: 0.014533637\n",
            "\n",
            "Global step: 3316,loss: 0.013303076\n",
            "\n",
            "Global step: 3317,loss: 0.013542302\n",
            "\n",
            "Global step: 3318,loss: 0.0134860845\n",
            "\n",
            "Global step: 3319,loss: 0.013185329\n",
            "\n",
            "Global step: 3320,loss: 0.013684671\n",
            "\n",
            "Global step: 3321,loss: 0.012903394\n",
            "\n",
            "Global step: 3322,loss: 0.01431905\n",
            "\n",
            "Global step: 3323,loss: 0.013530563\n",
            "\n",
            "Global step: 3324,loss: 0.01324041\n",
            "\n",
            "Global step: 3325,loss: 0.014082038\n",
            "\n",
            "Global step: 3326,loss: 0.01359296\n",
            "\n",
            "Global step: 3327,loss: 0.014381491\n",
            "\n",
            "Global step: 3328,loss: 0.013598751\n",
            "\n",
            "Global step: 3329,loss: 0.012791353\n",
            "\n",
            "Global step: 3330,loss: 0.012667682\n",
            "\n",
            "Global step: 3331,loss: 0.013457342\n",
            "\n",
            "Global step: 3332,loss: 0.013451987\n",
            "\n",
            "Global step: 3333,loss: 0.013571495\n",
            "\n",
            "Global step: 3334,loss: 0.012963817\n",
            "\n",
            "Global step: 3335,loss: 0.012893301\n",
            "\n",
            "Global step: 3336,loss: 0.013958262\n",
            "\n",
            "Global step: 3337,loss: 0.0129467305\n",
            "\n",
            "Global step: 3338,loss: 0.0131755145\n",
            "\n",
            "Global step: 3339,loss: 0.013222667\n",
            "\n",
            "Global step: 3340,loss: 0.012884973\n",
            "\n",
            "Global step: 3341,loss: 0.013260231\n",
            "\n",
            "Global step: 3342,loss: 0.013086788\n",
            "\n",
            "Global step: 3343,loss: 0.012992204\n",
            "\n",
            "Global step: 3344,loss: 0.0140714785\n",
            "\n",
            "Global step: 3345,loss: 0.012567668\n",
            "\n",
            "Global step: 3346,loss: 0.012775203\n",
            "\n",
            "Global step: 3347,loss: 0.012961151\n",
            "\n",
            "Global step: 3348,loss: 0.013355174\n",
            "\n",
            "Global step: 3349,loss: 0.01335843\n",
            "\n",
            "Global step: 3350,loss: 0.014313859\n",
            "\n",
            "Global step: 3351,loss: 0.013160603\n",
            "\n",
            "Global step: 3352,loss: 0.012599221\n",
            "\n",
            "Global step: 3353,loss: 0.012126202\n",
            "\n",
            "Global step: 3354,loss: 0.013299878\n",
            "\n",
            "Global step: 3355,loss: 0.013493952\n",
            "\n",
            "Global step: 3356,loss: 0.013627639\n",
            "\n",
            "Global step: 3357,loss: 0.013294961\n",
            "\n",
            "Global step: 3358,loss: 0.013423242\n",
            "\n",
            "Global step: 3359,loss: 0.013525358\n",
            "\n",
            "Global step: 3360,loss: 0.012884199\n",
            "\n",
            "Global step: 3361,loss: 0.012676212\n",
            "\n",
            "Global step: 3362,loss: 0.0148650585\n",
            "\n",
            "Global step: 3363,loss: 0.013348857\n",
            "\n",
            "Global step: 3364,loss: 0.013393934\n",
            "\n",
            "Global step: 3365,loss: 0.013854933\n",
            "\n",
            "Global step: 3366,loss: 0.01367172\n",
            "\n",
            "Global step: 3367,loss: 0.012780146\n",
            "\n",
            "Global step: 3368,loss: 0.013245707\n",
            "\n",
            "Global step: 3369,loss: 0.012242323\n",
            "\n",
            "Global step: 3370,loss: 0.013177943\n",
            "\n",
            "Global step: 3371,loss: 0.012952722\n",
            "\n",
            "Global step: 3372,loss: 0.012903785\n",
            "\n",
            "Global step: 3373,loss: 0.013594297\n",
            "\n",
            "Global step: 3374,loss: 0.013097106\n",
            "\n",
            "Global step: 3375,loss: 0.013407944\n",
            "\n",
            "Global step: 3376,loss: 0.013159948\n",
            "\n",
            "Global step: 3377,loss: 0.013167947\n",
            "\n",
            "Global step: 3378,loss: 0.013298925\n",
            "\n",
            "Global step: 3379,loss: 0.013274083\n",
            "\n",
            "Global step: 3380,loss: 0.01519408\n",
            "\n",
            "Global step: 3381,loss: 0.013061872\n",
            "\n",
            "Global step: 3382,loss: 0.013357356\n",
            "\n",
            "Global step: 3383,loss: 0.0142142195\n",
            "\n",
            "Global step: 3384,loss: 0.013268386\n",
            "\n",
            "Global step: 3385,loss: 0.013111325\n",
            "\n",
            "Global step: 3386,loss: 0.013696147\n",
            "\n",
            "Global step: 3387,loss: 0.015830949\n",
            "\n",
            "Global step: 3388,loss: 0.014927008\n",
            "\n",
            "Global step: 3389,loss: 0.014813738\n",
            "\n",
            "Global step: 3390,loss: 0.0134968525\n",
            "\n",
            "Global step: 3391,loss: 0.013758775\n",
            "\n",
            "Global step: 3392,loss: 0.013394563\n",
            "\n",
            "Global step: 3393,loss: 0.013318605\n",
            "\n",
            "Global step: 3394,loss: 0.014122858\n",
            "\n",
            "Global step: 3395,loss: 0.0125110885\n",
            "\n",
            "Global step: 3396,loss: 0.012997586\n",
            "\n",
            "Global step: 3397,loss: 0.013482456\n",
            "\n",
            "Global step: 3398,loss: 0.01336144\n",
            "\n",
            "Global step: 3399,loss: 0.012004112\n",
            "\n",
            "Global step: 3400,loss: 0.013905778\n",
            "\n",
            "Global step: 3401,loss: 0.012631874\n",
            "\n",
            "Global step: 3402,loss: 0.012637703\n",
            "\n",
            "Global step: 3403,loss: 0.012922818\n",
            "\n",
            "Global step: 3404,loss: 0.013109927\n",
            "\n",
            "Global step: 3405,loss: 0.013274318\n",
            "\n",
            "Global step: 3406,loss: 0.013162154\n",
            "\n",
            "Global step: 3407,loss: 0.013348364\n",
            "\n",
            "Global step: 3408,loss: 0.013258193\n",
            "\n",
            "Global step: 3409,loss: 0.0129199885\n",
            "\n",
            "Global step: 3410,loss: 0.013348214\n",
            "\n",
            "Global step: 3411,loss: 0.013570602\n",
            "\n",
            "Global step: 3412,loss: 0.012007233\n",
            "\n",
            "Global step: 3413,loss: 0.013471248\n",
            "\n",
            "Global step: 3414,loss: 0.012571797\n",
            "\n",
            "Global step: 3415,loss: 0.013001294\n",
            "\n",
            "Global step: 3416,loss: 0.012559237\n",
            "\n",
            "Global step: 3417,loss: 0.01329747\n",
            "\n",
            "Global step: 3418,loss: 0.012816896\n",
            "\n",
            "Global step: 3419,loss: 0.013056081\n",
            "\n",
            "Global step: 3420,loss: 0.013558744\n",
            "\n",
            "Global step: 3421,loss: 0.01307937\n",
            "\n",
            "Global step: 3422,loss: 0.012654477\n",
            "\n",
            "Global step: 3423,loss: 0.01352486\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 3423,val_loss: 0.025344876082319962\n",
            "\n",
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:14:28.021179 140508081637248 <ipython-input-12-0eedbf08ec32>:16] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "0b766465-5a68-474b-f9d2-6b6bf7c76055"
      },
      "source": [
        "cfg.is_training=False\n",
        "\n",
        "try:\n",
        "  def main(_):\n",
        "      tf.logging.info(' Loading Graph...')\n",
        "      num_label = 10\n",
        "      model = CapsNet()\n",
        "      tf.logging.info(' Graph loaded')\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "      sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      if cfg.is_training:\n",
        "          tf.logging.info(' Start training...')\n",
        "          train(model, sv, num_label)\n",
        "          tf.logging.info('Training done')\n",
        "      else:\n",
        "          evaluation(model, sv, num_label)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"\\Completed\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:03.315546 140508081637248 <ipython-input-13-d583bcff4fbf>:5]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:13.423232 140508081637248 <ipython-input-9-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:13.428409 140508081637248 <ipython-input-13-d583bcff4fbf>:8]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0006_step_1497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:13.873123 140508081637248 saver.py:1284] Restoring parameters from logdir/model_epoch_0006_step_1497\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1111 12:15:14.263049 140508081637248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:14.277583 140508081637248 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:14.303579 140508081637248 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:49.118244 140508081637248 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:49.874116 140508081637248 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0006_step_1497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:49.894010 140508081637248 saver.py:1284] Restoring parameters from logdir/model_epoch_0006_step_1497\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Model restored!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:15:51.759684 140508081637248 <ipython-input-11-5c14e32e29ea>:118] Model restored!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy is 0.8099609375:\n",
            "\n",
            "Test accuracy has been saved to results/test_acc\n",
            "INFO:tensorflow:Recording summary at step 1498.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1111 12:16:03.947683 140504720996096 supervisor.py:1050] Recording summary at step 1498.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rC99Lwvyr5A",
        "colab_type": "text"
      },
      "source": [
        "Data Aug"
      ]
    }
  ]
}