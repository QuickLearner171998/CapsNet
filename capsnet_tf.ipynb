{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "022e44de-5827-4b67-ef02-dadcec33ee22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "9560f8f8-7299-443c-9443-6249efbbca30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive'\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "[Errno 2] No such file or directory: 'My Drive'\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "[Errno 2] No such file or directory: 'MY Projects'\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "[Errno 2] No such file or directory: 'EEE lop'\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "[Errno 2] No such file or directory: 'tensorflow_implementation'\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   logdir\t    tensorboard.ipynb   Try-12_BEST_83.63\n",
            " capsnet_tf.py\t     logdir_try8    Try-10\t        TRY_13\n",
            " data\t\t     results\t   'Try-11_Best _83%'\n",
            " graph.pbtxt\t     results_best   Try-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "import random\n",
        "import skimage.io\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "from scipy.ndimage.interpolation import map_coordinates, shift\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from scipy.ndimage import rotate, zoom\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3O76C7FuuN",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAbq_6IFxbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_aug(images,labels,angle, populate):\n",
        "\n",
        "\n",
        "  new_img = []\n",
        "  new_label = []\n",
        "  print(\"\\nStarting Data Augmentation\")\n",
        "  for img,label in zip(images,labels):\n",
        "\n",
        "    image = img\n",
        "    size = image.shape[0]\n",
        "\n",
        "    # Random ZOOM\n",
        "    if(random.randint(0,1)):\n",
        "      zoom_factor = random.uniform(0.8, 1.5)\n",
        "      h, w = image.shape[:2]\n",
        "\n",
        "      # Zooming out\n",
        "      if zoom_factor < 1:\n",
        "\n",
        "          # Bounding box of the zoomed-out image within the output array\n",
        "          zh = int(np.round(h * zoom_factor))\n",
        "          zw = int(np.round(w * zoom_factor))\n",
        "          top = (h - zh) // 2\n",
        "          left = (w - zw) // 2\n",
        "\n",
        "          # Zero-padding\n",
        "          image = np.zeros_like(image)\n",
        "          image[top:top+zh, left:left+zw] = zoom(image, zoom_factor)\n",
        "\n",
        "      # Zooming in\n",
        "      elif zoom_factor > 1:\n",
        "\n",
        "          # Bounding box of the zoomed-in region within the input array\n",
        "          zh = int(np.round(h / zoom_factor))\n",
        "          zw = int(np.round(w / zoom_factor))\n",
        "          top = (h - zh) // 2\n",
        "          left = (w - zw) // 2\n",
        "\n",
        "          image = zoom(image[top:top+zh, left:left+zw], zoom_factor)\n",
        "\n",
        "          trim_top = ((image.shape[0] - h) // 2)\n",
        "          trim_left = ((image.shape[1] - w) // 2)\n",
        "          image = image[trim_top:trim_top+h, trim_left:trim_left+w]\n",
        "\n",
        "      if image.shape != (h, w):         # If the zoom has failed\n",
        "          image = img\n",
        "      image = np.array(image)\n",
        "\n",
        "    \n",
        "    # Create Afine transform\n",
        "    sh = random.random()/2-0.25\n",
        "    rotate_angle = random.random()/180*np.pi*angle\n",
        "    afine_tf = transform.AffineTransform(shear=sh,rotation=rotate_angle)\n",
        "    # Apply transform to image data\n",
        "    image = transform.warp(image, inverse_map=afine_tf,mode='edge')\n",
        "    \n",
        "    #random blurr\n",
        "    if(random.randint(0,1)):\n",
        "\n",
        "      sigma = 0.3\n",
        "      image = gaussian_filter(image, sigma)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    new_img.append(image)\n",
        "    new_label.append(label)\n",
        "  \n",
        "  print(\"\\nFinished Augmentation\")\n",
        "  if(populate):\n",
        "\n",
        "    final_trX = np.asarray(images + new_img)\n",
        "    final_labels = np.asarray(labels + new_label)\n",
        "    return final_trX.astype('float32'), final_labels.astype('int32')\n",
        "  return (np.array(new_img)).astype('float32'), (np.array(labels,dtype='int32').astype('int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "        ind = np.arange(trainX.shape[0])\n",
        "        np.random.shuffle(ind)\n",
        "        trainX = trainX[ind]\n",
        "        trainY = trainY[ind]\n",
        "        \n",
        "        trX = trainX[:50000] / 255.\n",
        "        trY = trainY[:50000]   \n",
        "        \n",
        "        trX, trY = data_aug(list(trX),list(trY),angle=10,populate=True)\n",
        "\n",
        "        print(\"Train data Size: \",trX.shape[0])\n",
        "\n",
        "        valX = trainX[50000:, ] / 255.\n",
        "        valY = trainY[50000:]\n",
        "\n",
        "        num_tr_batch = trX.shape[0] // batch_size\n",
        "        num_val_batch = valX.shape[0] // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 20, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 4, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.392,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                # if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                  best_val_loss = val_loss\n",
        "                  best_val_acc = val_acc\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######  NOT SAVING MODEL  #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-d11ZQj701L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INIT Model\n",
        "tf.logging.info(' Loading Graph...')\n",
        "num_label = 10\n",
        "model = CapsNet()\n",
        "tf.logging.info(' Graph loaded')\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "# \n",
        "\n",
        "sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "outputId": "99f275c0-ee13-49ea-f494-5878cff1c508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cfg.is_training=True\n",
        "# try:\n",
        "def main(_):\n",
        "    # tf.logging.info(' Loading Graph...')\n",
        "    # num_label = 10\n",
        "    # model = CapsNet()\n",
        "    # tf.logging.info(' Graph loaded')\n",
        "    # tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    # sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n",
        "\n",
        "# except:\n",
        "  # print(\"\\nBeginning Eval\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:51:07.658824 139641102628736 <ipython-input-36-deb5ab324590>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
            "  \"the returned array has changed.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished Augmentation\n",
            "WARNING:tensorflow:From <ipython-input-31-7cce95fb450a>:60: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.532557 139641102628736 deprecation.py:323] From <ipython-input-31-7cce95fb450a>:60: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.875532 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.901751 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.907392 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.912702 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.916967 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-7cce95fb450a>:65: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.937182 139641102628736 deprecation.py:323] From <ipython-input-31-7cce95fb450a>:65: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.957601 139641102628736 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:38.963003 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:39.236382 139641102628736 deprecation.py:323] From <ipython-input-33-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:39.422992 139641102628736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:51:40.118394 139641102628736 <ipython-input-33-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:51:40.120968 139641102628736 <ipython-input-36-deb5ab324590>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-36-deb5ab324590>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:51:40.123719 139641102628736 deprecation.py:323] From <ipython-input-36-deb5ab324590>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:51:40.651998 139641102628736 <ipython-input-36-deb5ab324590>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
            "  \"the returned array has changed.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:52:15.276157 139641102628736 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:52:15.305441 139641102628736 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:53:18.952324 139641102628736 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:53:20.406630 139641102628736 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/20:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:53:22.943243 139635366639360 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:53:27.950373 139637785016064 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.7480811\n",
            "\n",
            "Global step: 1,loss: 0.7358222\n",
            "\n",
            "Global step: 2,loss: 0.74615157\n",
            "\n",
            "Global step: 3,loss: 0.72273946\n",
            "\n",
            "Global step: 4,loss: 0.70991033\n",
            "\n",
            "Global step: 5,loss: 0.6857057\n",
            "\n",
            "Global step: 6,loss: 0.65190655\n",
            "\n",
            "Global step: 7,loss: 0.609644\n",
            "\n",
            "Global step: 8,loss: 0.54247934\n",
            "\n",
            "Global step: 9,loss: 0.5149594\n",
            "\n",
            "Global step: 10,loss: 0.47216862\n",
            "\n",
            "Global step: 11,loss: 0.48159337\n",
            "\n",
            "Global step: 12,loss: 0.47072056\n",
            "\n",
            "Global step: 13,loss: 0.4071454\n",
            "\n",
            "Global step: 14,loss: 0.45195162\n",
            "\n",
            "Global step: 15,loss: 0.38499072\n",
            "\n",
            "Global step: 16,loss: 0.35927463\n",
            "\n",
            "Global step: 17,loss: 0.34623143\n",
            "\n",
            "Global step: 18,loss: 0.3473691\n",
            "\n",
            "Global step: 19,loss: 0.32707494\n",
            "\n",
            "Global step: 20,loss: 0.33408073\n",
            "\n",
            "Global step: 21,loss: 0.34150502\n",
            "\n",
            "Global step: 22,loss: 0.3135019\n",
            "\n",
            "Global step: 23,loss: 0.29343286\n",
            "\n",
            "Global step: 24,loss: 0.28721964\n",
            "\n",
            "Global step: 25,loss: 0.2966351\n",
            "\n",
            "Global step: 26,loss: 0.26863986\n",
            "\n",
            "Global step: 27,loss: 0.24141365\n",
            "\n",
            "Global step: 28,loss: 0.28127185\n",
            "\n",
            "Global step: 29,loss: 0.26501885\n",
            "\n",
            "Global step: 30,loss: 0.24454626\n",
            "\n",
            "Global step: 31,loss: 0.2886781\n",
            "\n",
            "Global step: 32,loss: 0.22014272\n",
            "\n",
            "Global step: 33,loss: 0.21425937\n",
            "\n",
            "Global step: 34,loss: 0.233681\n",
            "\n",
            "Global step: 35,loss: 0.24676827\n",
            "\n",
            "Global step: 36,loss: 0.226037\n",
            "\n",
            "Global step: 37,loss: 0.20000182\n",
            "\n",
            "Global step: 38,loss: 0.2113976\n",
            "\n",
            "Global step: 39,loss: 0.20806468\n",
            "\n",
            "Global step: 40,loss: 0.18789768\n",
            "\n",
            "Global step: 41,loss: 0.16836485\n",
            "\n",
            "Global step: 42,loss: 0.18629085\n",
            "\n",
            "Global step: 43,loss: 0.17897688\n",
            "\n",
            "Global step: 44,loss: 0.16689724\n",
            "\n",
            "Global step: 45,loss: 0.151777\n",
            "\n",
            "Global step: 46,loss: 0.17773446\n",
            "\n",
            "Global step: 47,loss: 0.17206614\n",
            "\n",
            "Global step: 48,loss: 0.15136267\n",
            "\n",
            "Global step: 49,loss: 0.16742414\n",
            "\n",
            "Global step: 50,loss: 0.14238024\n",
            "\n",
            "Global step: 51,loss: 0.15055121\n",
            "\n",
            "Global step: 52,loss: 0.15008907\n",
            "\n",
            "Global step: 53,loss: 0.12151943\n",
            "\n",
            "Global step: 54,loss: 0.13219287\n",
            "\n",
            "Global step: 55,loss: 0.12770778\n",
            "\n",
            "Global step: 56,loss: 0.12696122\n",
            "\n",
            "Global step: 57,loss: 0.12609175\n",
            "\n",
            "Global step: 58,loss: 0.1183392\n",
            "\n",
            "Global step: 59,loss: 0.12426432\n",
            "\n",
            "Global step: 60,loss: 0.120225415\n",
            "\n",
            "Global step: 61,loss: 0.1299794\n",
            "\n",
            "Global step: 62,loss: 0.11394275\n",
            "\n",
            "Global step: 63,loss: 0.10750891\n",
            "\n",
            "Global step: 64,loss: 0.11430313\n",
            "\n",
            "Global step: 65,loss: 0.117659256\n",
            "\n",
            "Global step: 66,loss: 0.11880716\n",
            "\n",
            "Global step: 67,loss: 0.11884332\n",
            "\n",
            "Global step: 68,loss: 0.10971574\n",
            "\n",
            "Global step: 69,loss: 0.11655972\n",
            "\n",
            "Global step: 70,loss: 0.10686939\n",
            "\n",
            "Global step: 71,loss: 0.09463392\n",
            "\n",
            "Global step: 72,loss: 0.123816654\n",
            "\n",
            "Global step: 73,loss: 0.119694434\n",
            "\n",
            "Global step: 74,loss: 0.108382985\n",
            "\n",
            "Global step: 75,loss: 0.079506956\n",
            "\n",
            "Global step: 76,loss: 0.086360015\n",
            "\n",
            "Global step: 77,loss: 0.08920194\n",
            "\n",
            "Global step: 78,loss: 0.123600006\n",
            "\n",
            "Global step: 79,loss: 0.10587396\n",
            "\n",
            "Global step: 80,loss: 0.10297051\n",
            "\n",
            "Global step: 81,loss: 0.089841515\n",
            "\n",
            "Global step: 82,loss: 0.0907504\n",
            "\n",
            "Global step: 83,loss: 0.09577061\n",
            "\n",
            "Global step: 84,loss: 0.09742007\n",
            "\n",
            "Global step: 85,loss: 0.09829153\n",
            "\n",
            "Global step: 86,loss: 0.08066031\n",
            "\n",
            "Global step: 87,loss: 0.08346866\n",
            "\n",
            "Global step: 88,loss: 0.091992\n",
            "\n",
            "Global step: 89,loss: 0.103797056\n",
            "\n",
            "Global step: 90,loss: 0.10031436\n",
            "\n",
            "Global step: 91,loss: 0.08531423\n",
            "\n",
            "Global step: 92,loss: 0.092963964\n",
            "\n",
            "Global step: 93,loss: 0.07055839\n",
            "\n",
            "Global step: 94,loss: 0.08657223\n",
            "\n",
            "Global step: 95,loss: 0.08852811\n",
            "\n",
            "Global step: 96,loss: 0.094206184\n",
            "\n",
            "Global step: 97,loss: 0.065582566\n",
            "\n",
            "Global step: 98,loss: 0.07733664\n",
            "\n",
            "Global step: 99,loss: 0.07366869\n",
            "\n",
            "Global step: 100,loss: 0.07720175\n",
            "\n",
            "Global step: 101,loss: 0.08434415\n",
            "\n",
            "Global step: 102,loss: 0.070419654\n",
            "\n",
            "Global step: 103,loss: 0.07182475\n",
            "\n",
            "Global step: 104,loss: 0.076316975\n",
            "\n",
            "Global step: 105,loss: 0.08510228\n",
            "\n",
            "Global step: 106,loss: 0.09445379\n",
            "\n",
            "Global step: 107,loss: 0.06199891\n",
            "\n",
            "Global step: 108,loss: 0.07797215\n",
            "\n",
            "Global step: 109,loss: 0.08546051\n",
            "\n",
            "Global step: 110,loss: 0.06659913\n",
            "\n",
            "Global step: 111,loss: 0.08060749\n",
            "\n",
            "Global step: 112,loss: 0.103986874\n",
            "\n",
            "Global step: 113,loss: 0.07720436\n",
            "\n",
            "Global step: 114,loss: 0.088243626\n",
            "\n",
            "Global step: 115,loss: 0.08746366\n",
            "\n",
            "Global step: 116,loss: 0.07735048\n",
            "\n",
            "Global step: 117,loss: 0.07707528\n",
            "\n",
            "Global step: 118,loss: 0.07094923\n",
            "\n",
            "Global step: 119,loss: 0.08660505\n",
            "\n",
            "Global step: 120,loss: 0.070121765\n",
            "\n",
            "Global step: 121,loss: 0.07341039\n",
            "\n",
            "Global step: 122,loss: 0.061054397\n",
            "\n",
            "Global step: 123,loss: 0.06948008\n",
            "\n",
            "Global step: 124,loss: 0.09268796\n",
            "\n",
            "Global step: 125,loss: 0.059531398\n",
            "\n",
            "Global step: 126,loss: 0.0654212\n",
            "\n",
            "Global step: 127,loss: 0.064804256\n",
            "\n",
            "Global step: 128,loss: 0.07486823\n",
            "\n",
            "Global step: 129,loss: 0.06265623\n",
            "\n",
            "Global step: 130,loss: 0.0693376\n",
            "\n",
            "Global step: 131,loss: 0.08088318\n",
            "\n",
            "Global step: 132,loss: 0.070464015\n",
            "\n",
            "Global step: 133,loss: 0.087741636\n",
            "\n",
            "Global step: 134,loss: 0.067723416\n",
            "\n",
            "Global step: 135,loss: 0.08416454\n",
            "\n",
            "Global step: 136,loss: 0.050583787\n",
            "\n",
            "Global step: 137,loss: 0.06261723\n",
            "\n",
            "Global step: 138,loss: 0.06651631\n",
            "\n",
            "Global step: 139,loss: 0.07119188\n",
            "\n",
            "Global step: 140,loss: 0.061813675\n",
            "\n",
            "Global step: 141,loss: 0.073959775\n",
            "\n",
            "Global step: 142,loss: 0.0759435\n",
            "\n",
            "Global step: 143,loss: 0.057964474\n",
            "\n",
            "Global step: 144,loss: 0.06677472\n",
            "\n",
            "Global step: 145,loss: 0.066529155\n",
            "\n",
            "Global step: 146,loss: 0.07898927\n",
            "\n",
            "Global step: 147,loss: 0.07488758\n",
            "\n",
            "Global step: 148,loss: 0.046858083\n",
            "\n",
            "Global step: 149,loss: 0.06798184\n",
            "\n",
            "Global step: 150,loss: 0.06586948\n",
            "\n",
            "Global step: 151,loss: 0.07179323\n",
            "\n",
            "Global step: 152,loss: 0.07192546\n",
            "\n",
            "Global step: 153,loss: 0.064616844\n",
            "\n",
            "Global step: 154,loss: 0.060226563\n",
            "\n",
            "Global step: 155,loss: 0.07068243\n",
            "\n",
            "Global step: 156,loss: 0.0561409\n",
            "\n",
            "Global step: 157,loss: 0.05908382\n",
            "\n",
            "Global step: 158,loss: 0.0671039\n",
            "\n",
            "Global step: 159,loss: 0.05703532\n",
            "\n",
            "Global step: 160,loss: 0.060839362\n",
            "\n",
            "Global step: 161,loss: 0.053834632\n",
            "\n",
            "Global step: 162,loss: 0.044259496\n",
            "\n",
            "Global step: 163,loss: 0.0554082\n",
            "\n",
            "Global step: 164,loss: 0.052386787\n",
            "\n",
            "Global step: 165,loss: 0.061477978\n",
            "\n",
            "Global step: 166,loss: 0.054201875\n",
            "\n",
            "Global step: 167,loss: 0.052174643\n",
            "\n",
            "Global step: 168,loss: 0.049471773\n",
            "\n",
            "Global step: 169,loss: 0.0526207\n",
            "\n",
            "Global step: 170,loss: 0.06922207\n",
            "\n",
            "Global step: 171,loss: 0.06607261\n",
            "\n",
            "Global step: 172,loss: 0.052577175\n",
            "\n",
            "Global step: 173,loss: 0.04928941\n",
            "\n",
            "Global step: 174,loss: 0.050663732\n",
            "\n",
            "Global step: 175,loss: 0.050960146\n",
            "\n",
            "Global step: 176,loss: 0.070235685\n",
            "\n",
            "Global step: 177,loss: 0.056789026\n",
            "\n",
            "Global step: 178,loss: 0.06912996\n",
            "\n",
            "Global step: 179,loss: 0.054153576\n",
            "\n",
            "Global step: 180,loss: 0.04752133\n",
            "\n",
            "Global step: 181,loss: 0.06101491\n",
            "\n",
            "Global step: 182,loss: 0.055448428\n",
            "\n",
            "Global step: 183,loss: 0.056520335\n",
            "\n",
            "Global step: 184,loss: 0.04436021\n",
            "\n",
            "Global step: 185,loss: 0.05329805\n",
            "\n",
            "Global step: 186,loss: 0.0551538\n",
            "\n",
            "Global step: 187,loss: 0.060629454\n",
            "\n",
            "Global step: 188,loss: 0.06737697\n",
            "\n",
            "Global step: 189,loss: 0.069068655\n",
            "\n",
            "Global step: 190,loss: 0.048165325\n",
            "\n",
            "Global step: 191,loss: 0.041092575\n",
            "\n",
            "Global step: 192,loss: 0.0532005\n",
            "\n",
            "Global step: 193,loss: 0.04616725\n",
            "\n",
            "Global step: 194,loss: 0.05177047\n",
            "\n",
            "Global step: 195,loss: 0.049866658\n",
            "\n",
            "Global step: 196,loss: 0.05167544\n",
            "\n",
            "Global step: 197,loss: 0.063135326\n",
            "\n",
            "Global step: 198,loss: 0.049975924\n",
            "\n",
            "Global step: 199,loss: 0.05061574\n",
            "\n",
            "Global step: 200,loss: 0.06467773\n",
            "\n",
            "Global step: 201,loss: 0.052801497\n",
            "\n",
            "Global step: 202,loss: 0.056226548\n",
            "\n",
            "Global step: 203,loss: 0.067937866\n",
            "\n",
            "Global step: 204,loss: 0.07025691\n",
            "\n",
            "Global step: 205,loss: 0.047253706\n",
            "\n",
            "Global step: 206,loss: 0.0513065\n",
            "\n",
            "Global step: 207,loss: 0.059448242\n",
            "\n",
            "Global step: 208,loss: 0.060328968\n",
            "\n",
            "Global step: 209,loss: 0.057792433\n",
            "\n",
            "Global step: 210,loss: 0.058802553\n",
            "\n",
            "Global step: 211,loss: 0.055707753\n",
            "\n",
            "Global step: 212,loss: 0.059409693\n",
            "\n",
            "Global step: 213,loss: 0.05662848\n",
            "\n",
            "Global step: 214,loss: 0.05247109\n",
            "\n",
            "Global step: 215,loss: 0.05071855\n",
            "\n",
            "Global step: 216,loss: 0.0581283\n",
            "\n",
            "Global step: 217,loss: 0.06069721\n",
            "\n",
            "Global step: 218,loss: 0.053234946\n",
            "\n",
            "Global step: 219,loss: 0.04741992\n",
            "\n",
            "Global step: 220,loss: 0.034377556\n",
            "\n",
            "Global step: 221,loss: 0.04620371\n",
            "\n",
            "Global step: 222,loss: 0.055882767\n",
            "\n",
            "Global step: 223,loss: 0.040416073\n",
            "\n",
            "Global step: 224,loss: 0.04037144\n",
            "\n",
            "Global step: 225,loss: 0.046268806\n",
            "\n",
            "Global step: 226,loss: 0.08098915\n",
            "\n",
            "Global step: 227,loss: 0.04781039\n",
            "\n",
            "Global step: 228,loss: 0.041504025\n",
            "\n",
            "Global step: 229,loss: 0.051319115\n",
            "\n",
            "Global step: 230,loss: 0.04732444\n",
            "\n",
            "Global step: 231,loss: 0.038469903\n",
            "\n",
            "Global step: 232,loss: 0.07008548\n",
            "\n",
            "Global step: 233,loss: 0.058336053\n",
            "\n",
            "Global step: 234,loss: 0.05123407\n",
            "\n",
            "Global step: 235,loss: 0.051795818\n",
            "\n",
            "Global step: 236,loss: 0.043942533\n",
            "\n",
            "Global step: 237,loss: 0.042851932\n",
            "\n",
            "Global step: 238,loss: 0.049839787\n",
            "\n",
            "Global step: 239,loss: 0.056091927\n",
            "\n",
            "Global step: 240,loss: 0.033920195\n",
            "\n",
            "Global step: 241,loss: 0.046732888\n",
            "\n",
            "Global step: 242,loss: 0.055114396\n",
            "\n",
            "Global step: 243,loss: 0.04202039\n",
            "\n",
            "Global step: 244,loss: 0.053153727\n",
            "\n",
            "Global step: 245,loss: 0.045625877\n",
            "\n",
            "Global step: 246,loss: 0.053294532\n",
            "\n",
            "Global step: 247,loss: 0.05001746\n",
            "\n",
            "Global step: 248,loss: 0.043443248\n",
            "\n",
            "Global step: 249,loss: 0.04682531\n",
            "\n",
            "Global step: 250,loss: 0.05930662\n",
            "\n",
            "Global step: 251,loss: 0.04279707\n",
            "\n",
            "Global step: 252,loss: 0.04424184\n",
            "\n",
            "Global step: 253,loss: 0.034831412\n",
            "\n",
            "Global step: 254,loss: 0.038127907\n",
            "\n",
            "Global step: 255,loss: 0.048307933\n",
            "\n",
            "Global step: 256,loss: 0.04493543\n",
            "\n",
            "Global step: 257,loss: 0.04350874\n",
            "\n",
            "Global step: 258,loss: 0.05035656\n",
            "\n",
            "Global step: 259,loss: 0.044818215\n",
            "\n",
            "Global step: 260,loss: 0.0396244\n",
            "\n",
            "Global step: 261,loss: 0.037924662\n",
            "\n",
            "Global step: 262,loss: 0.03384945\n",
            "\n",
            "Global step: 263,loss: 0.045176603\n",
            "\n",
            "Global step: 264,loss: 0.041724723\n",
            "\n",
            "Global step: 265,loss: 0.044188\n",
            "\n",
            "Global step: 266,loss: 0.052342344\n",
            "\n",
            "Global step: 267,loss: 0.038463365\n",
            "\n",
            "Global step: 268,loss: 0.048021354\n",
            "\n",
            "Global step: 269,loss: 0.03606931\n",
            "\n",
            "Global step: 270,loss: 0.04463079\n",
            "\n",
            "Global step: 271,loss: 0.038867515\n",
            "\n",
            "Global step: 272,loss: 0.052236963\n",
            "\n",
            "Global step: 273,loss: 0.038608707\n",
            "\n",
            "Global step: 274,loss: 0.03600423\n",
            "\n",
            "Global step: 275,loss: 0.04070456\n",
            "\n",
            "Global step: 276,loss: 0.039598316\n",
            "\n",
            "Global step: 277,loss: 0.042530283\n",
            "\n",
            "Global step: 278,loss: 0.05646752\n",
            "\n",
            "Global step: 279,loss: 0.047495466\n",
            "\n",
            "Global step: 280,loss: 0.03253502\n",
            "\n",
            "Global step: 281,loss: 0.04464993\n",
            "\n",
            "Global step: 282,loss: 0.0476238\n",
            "\n",
            "Global step: 283,loss: 0.047558684\n",
            "\n",
            "Global step: 284,loss: 0.049172908\n",
            "\n",
            "Global step: 285,loss: 0.031062745\n",
            "\n",
            "Global step: 286,loss: 0.04039609\n",
            "\n",
            "Global step: 287,loss: 0.033745676\n",
            "\n",
            "Global step: 288,loss: 0.043713596\n",
            "\n",
            "Global step: 289,loss: 0.056433704\n",
            "\n",
            "Global step: 290,loss: 0.04403976\n",
            "\n",
            "Global step: 291,loss: 0.046061244\n",
            "\n",
            "Global step: 292,loss: 0.035094373\n",
            "\n",
            "Global step: 293,loss: 0.044684708\n",
            "\n",
            "Global step: 294,loss: 0.043794923\n",
            "\n",
            "Global step: 295,loss: 0.05665378\n",
            "\n",
            "Global step: 296,loss: 0.061854087\n",
            "\n",
            "Global step: 297,loss: 0.05615908\n",
            "\n",
            "Global step: 298,loss: 0.045299724\n",
            "\n",
            "Global step: 299,loss: 0.03046724\n",
            "\n",
            "Global step: 300,loss: 0.049156398\n",
            "\n",
            "Global step: 301,loss: 0.050957896\n",
            "\n",
            "Global step: 302,loss: 0.030305812\n",
            "\n",
            "Global step: 303,loss: 0.0304185\n",
            "\n",
            "Global step: 304,loss: 0.03796282\n",
            "\n",
            "Global step: 305,loss: 0.03855224\n",
            "\n",
            "Global step: 306,loss: 0.03264109\n",
            "\n",
            "Global step: 307,loss: 0.042398766\n",
            "\n",
            "Global step: 308,loss: 0.037885062\n",
            "\n",
            "Global step: 309,loss: 0.040007815\n",
            "\n",
            "Global step: 310,loss: 0.031662006\n",
            "\n",
            "Global step: 311,loss: 0.039743334\n",
            "\n",
            "Global step: 312,loss: 0.03904317\n",
            "\n",
            "Global step: 313,loss: 0.050061695\n",
            "\n",
            "Global step: 314,loss: 0.04640489\n",
            "\n",
            "Global step: 315,loss: 0.04867679\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.69262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:55:20.672600 139635366639360 supervisor.py:1099] global_step/sec: 2.69262\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 316,loss: 0.057533942\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 317.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:55:20.859049 139637785016064 supervisor.py:1050] Recording summary at step 317.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 317,loss: 0.03747029\n",
            "\n",
            "Global step: 318,loss: 0.033298366\n",
            "\n",
            "Global step: 319,loss: 0.034844264\n",
            "\n",
            "Global step: 320,loss: 0.043646786\n",
            "\n",
            "Global step: 321,loss: 0.04541778\n",
            "\n",
            "Global step: 322,loss: 0.028111748\n",
            "\n",
            "Global step: 323,loss: 0.033846725\n",
            "\n",
            "Global step: 324,loss: 0.0436331\n",
            "\n",
            "Global step: 325,loss: 0.056211688\n",
            "\n",
            "Global step: 326,loss: 0.05054201\n",
            "\n",
            "Global step: 327,loss: 0.04709708\n",
            "\n",
            "Global step: 328,loss: 0.039205953\n",
            "\n",
            "Global step: 329,loss: 0.062691316\n",
            "\n",
            "Global step: 330,loss: 0.050538097\n",
            "\n",
            "Global step: 331,loss: 0.038449697\n",
            "\n",
            "Global step: 332,loss: 0.028969988\n",
            "\n",
            "Global step: 333,loss: 0.033846922\n",
            "\n",
            "Global step: 334,loss: 0.04022939\n",
            "\n",
            "Global step: 335,loss: 0.038555168\n",
            "\n",
            "Global step: 336,loss: 0.047577564\n",
            "\n",
            "Global step: 337,loss: 0.051429782\n",
            "\n",
            "Global step: 338,loss: 0.038546026\n",
            "\n",
            "Global step: 339,loss: 0.035469566\n",
            "\n",
            "Global step: 340,loss: 0.03121599\n",
            "\n",
            "Global step: 341,loss: 0.045511536\n",
            "\n",
            "Global step: 342,loss: 0.03113401\n",
            "\n",
            "Global step: 343,loss: 0.041769646\n",
            "\n",
            "Global step: 344,loss: 0.056914218\n",
            "\n",
            "Global step: 345,loss: 0.03421524\n",
            "\n",
            "Global step: 346,loss: 0.035314508\n",
            "\n",
            "Global step: 347,loss: 0.053164523\n",
            "\n",
            "Global step: 348,loss: 0.037534725\n",
            "\n",
            "Global step: 349,loss: 0.03958395\n",
            "\n",
            "Global step: 350,loss: 0.03568162\n",
            "\n",
            "Global step: 351,loss: 0.04204095\n",
            "\n",
            "Global step: 352,loss: 0.052396905\n",
            "\n",
            "Global step: 353,loss: 0.037215967\n",
            "\n",
            "Global step: 354,loss: 0.03982859\n",
            "\n",
            "Global step: 355,loss: 0.035794213\n",
            "\n",
            "Global step: 356,loss: 0.039718024\n",
            "\n",
            "Global step: 357,loss: 0.041924942\n",
            "\n",
            "Global step: 358,loss: 0.03908726\n",
            "\n",
            "Global step: 359,loss: 0.032304868\n",
            "\n",
            "Global step: 360,loss: 0.037632868\n",
            "\n",
            "Global step: 361,loss: 0.039381202\n",
            "\n",
            "Global step: 362,loss: 0.03449402\n",
            "\n",
            "Global step: 363,loss: 0.039664425\n",
            "\n",
            "Global step: 364,loss: 0.045210227\n",
            "\n",
            "Global step: 365,loss: 0.039695524\n",
            "\n",
            "Global step: 366,loss: 0.045094855\n",
            "\n",
            "Global step: 367,loss: 0.038173266\n",
            "\n",
            "Global step: 368,loss: 0.03237094\n",
            "\n",
            "Global step: 369,loss: 0.0393796\n",
            "\n",
            "Global step: 370,loss: 0.044080865\n",
            "\n",
            "Global step: 371,loss: 0.032017406\n",
            "\n",
            "Global step: 372,loss: 0.044955306\n",
            "\n",
            "Global step: 373,loss: 0.048550285\n",
            "\n",
            "Global step: 374,loss: 0.031435642\n",
            "\n",
            "Global step: 375,loss: 0.03967472\n",
            "\n",
            "Global step: 376,loss: 0.033401757\n",
            "\n",
            "Global step: 377,loss: 0.04067261\n",
            "\n",
            "Global step: 378,loss: 0.03427424\n",
            "\n",
            "Global step: 379,loss: 0.04746775\n",
            "\n",
            "Global step: 380,loss: 0.03940676\n",
            "\n",
            "Global step: 381,loss: 0.05098099\n",
            "\n",
            "Global step: 382,loss: 0.03831644\n",
            "\n",
            "Global step: 383,loss: 0.031098928\n",
            "\n",
            "Global step: 384,loss: 0.034591094\n",
            "\n",
            "Global step: 385,loss: 0.053612325\n",
            "\n",
            "Global step: 386,loss: 0.04665955\n",
            "\n",
            "Global step: 387,loss: 0.038773663\n",
            "\n",
            "Global step: 388,loss: 0.035331294\n",
            "\n",
            "Global step: 389,loss: 0.038569003\n",
            "\n",
            "Global step: 390,loss: 0.04413538\n",
            "\n",
            "Global step: 391,loss: 0.027883621\n",
            "\n",
            "Global step: 392,loss: 0.04858028\n",
            "\n",
            "Global step: 393,loss: 0.032838088\n",
            "\n",
            "Global step: 394,loss: 0.031962518\n",
            "\n",
            "Global step: 395,loss: 0.03806276\n",
            "\n",
            "Global step: 396,loss: 0.034796685\n",
            "\n",
            "Global step: 397,loss: 0.036837548\n",
            "\n",
            "Global step: 398,loss: 0.03336934\n",
            "\n",
            "Global step: 399,loss: 0.026339635\n",
            "\n",
            "Global step: 400,loss: 0.035737347\n",
            "\n",
            "Global step: 401,loss: 0.050711796\n",
            "\n",
            "Global step: 402,loss: 0.04431831\n",
            "\n",
            "Global step: 403,loss: 0.03735601\n",
            "\n",
            "Global step: 404,loss: 0.029641524\n",
            "\n",
            "Global step: 405,loss: 0.036722403\n",
            "\n",
            "Global step: 406,loss: 0.03633258\n",
            "\n",
            "Global step: 407,loss: 0.036775794\n",
            "\n",
            "Global step: 408,loss: 0.041776977\n",
            "\n",
            "Global step: 409,loss: 0.041350715\n",
            "\n",
            "Global step: 410,loss: 0.033820905\n",
            "\n",
            "Global step: 411,loss: 0.033098865\n",
            "\n",
            "Global step: 412,loss: 0.024334319\n",
            "\n",
            "Global step: 413,loss: 0.044897795\n",
            "\n",
            "Global step: 414,loss: 0.037724696\n",
            "\n",
            "Global step: 415,loss: 0.031841617\n",
            "\n",
            "Global step: 416,loss: 0.030885909\n",
            "\n",
            "Global step: 417,loss: 0.03727645\n",
            "\n",
            "Global step: 418,loss: 0.04275679\n",
            "\n",
            "Global step: 419,loss: 0.03626901\n",
            "\n",
            "Global step: 420,loss: 0.040162053\n",
            "\n",
            "Global step: 421,loss: 0.037374698\n",
            "\n",
            "Global step: 422,loss: 0.034427688\n",
            "\n",
            "Global step: 423,loss: 0.03462003\n",
            "\n",
            "Global step: 424,loss: 0.03418629\n",
            "\n",
            "Global step: 425,loss: 0.034062743\n",
            "\n",
            "Global step: 426,loss: 0.029494043\n",
            "\n",
            "Global step: 427,loss: 0.03276225\n",
            "\n",
            "Global step: 428,loss: 0.035272945\n",
            "\n",
            "Global step: 429,loss: 0.037427284\n",
            "\n",
            "Global step: 430,loss: 0.028806098\n",
            "\n",
            "Global step: 431,loss: 0.030651353\n",
            "\n",
            "Global step: 432,loss: 0.029719215\n",
            "\n",
            "Global step: 433,loss: 0.042554952\n",
            "\n",
            "Global step: 434,loss: 0.035980016\n",
            "\n",
            "Global step: 435,loss: 0.04719052\n",
            "\n",
            "Global step: 436,loss: 0.036843117\n",
            "\n",
            "Global step: 437,loss: 0.028888015\n",
            "\n",
            "Global step: 438,loss: 0.044881932\n",
            "\n",
            "Global step: 439,loss: 0.029318418\n",
            "\n",
            "Global step: 440,loss: 0.03725855\n",
            "\n",
            "Global step: 441,loss: 0.029891761\n",
            "\n",
            "Global step: 442,loss: 0.0367369\n",
            "\n",
            "Global step: 443,loss: 0.039066393\n",
            "\n",
            "Global step: 444,loss: 0.0337093\n",
            "\n",
            "Global step: 445,loss: 0.0381815\n",
            "\n",
            "Global step: 446,loss: 0.027822606\n",
            "\n",
            "Global step: 447,loss: 0.042447634\n",
            "\n",
            "Global step: 448,loss: 0.03308603\n",
            "\n",
            "Global step: 449,loss: 0.04302457\n",
            "\n",
            "Global step: 450,loss: 0.04635624\n",
            "\n",
            "Global step: 451,loss: 0.04569681\n",
            "\n",
            "Global step: 452,loss: 0.027926993\n",
            "\n",
            "Global step: 453,loss: 0.03273841\n",
            "\n",
            "Global step: 454,loss: 0.037840378\n",
            "\n",
            "Global step: 455,loss: 0.031947482\n",
            "\n",
            "Global step: 456,loss: 0.033955242\n",
            "\n",
            "Global step: 457,loss: 0.030102214\n",
            "\n",
            "Global step: 458,loss: 0.025820833\n",
            "\n",
            "Global step: 459,loss: 0.04025622\n",
            "\n",
            "Global step: 460,loss: 0.027860876\n",
            "\n",
            "Global step: 461,loss: 0.036770858\n",
            "\n",
            "Global step: 462,loss: 0.03398789\n",
            "\n",
            "Global step: 463,loss: 0.03738136\n",
            "\n",
            "Global step: 464,loss: 0.03246936\n",
            "\n",
            "Global step: 465,loss: 0.038274415\n",
            "\n",
            "Global step: 466,loss: 0.028231043\n",
            "\n",
            "Global step: 467,loss: 0.036983598\n",
            "\n",
            "Global step: 468,loss: 0.032954317\n",
            "\n",
            "Global step: 469,loss: 0.04267502\n",
            "\n",
            "Global step: 470,loss: 0.039095484\n",
            "\n",
            "Global step: 471,loss: 0.047662396\n",
            "\n",
            "Global step: 472,loss: 0.049968548\n",
            "\n",
            "Global step: 473,loss: 0.025321877\n",
            "\n",
            "Global step: 474,loss: 0.040728793\n",
            "\n",
            "Global step: 475,loss: 0.04076924\n",
            "\n",
            "Global step: 476,loss: 0.02546465\n",
            "\n",
            "Global step: 477,loss: 0.032239776\n",
            "\n",
            "Global step: 478,loss: 0.03139238\n",
            "\n",
            "Global step: 479,loss: 0.0404891\n",
            "\n",
            "Global step: 480,loss: 0.041615553\n",
            "\n",
            "Global step: 481,loss: 0.02990245\n",
            "\n",
            "Global step: 482,loss: 0.043486938\n",
            "\n",
            "Global step: 483,loss: 0.033099443\n",
            "\n",
            "Global step: 484,loss: 0.03842568\n",
            "\n",
            "Global step: 485,loss: 0.027407087\n",
            "\n",
            "Global step: 486,loss: 0.038890388\n",
            "\n",
            "Global step: 487,loss: 0.06466098\n",
            "\n",
            "Global step: 488,loss: 0.044013053\n",
            "\n",
            "Global step: 489,loss: 0.030010996\n",
            "\n",
            "Global step: 490,loss: 0.03215455\n",
            "\n",
            "Global step: 491,loss: 0.033202462\n",
            "\n",
            "Global step: 492,loss: 0.035885938\n",
            "\n",
            "Global step: 493,loss: 0.043554194\n",
            "\n",
            "Global step: 494,loss: 0.030758677\n",
            "\n",
            "Global step: 495,loss: 0.034895513\n",
            "\n",
            "Global step: 496,loss: 0.032028873\n",
            "\n",
            "Global step: 497,loss: 0.039402056\n",
            "\n",
            "Global step: 498,loss: 0.03511269\n",
            "\n",
            "Global step: 499,loss: 0.045597784\n",
            "\n",
            "Global step: 500,loss: 0.040225517\n",
            "\n",
            "Global step: 501,loss: 0.03995206\n",
            "\n",
            "Global step: 502,loss: 0.029475406\n",
            "\n",
            "Global step: 503,loss: 0.048701346\n",
            "\n",
            "Global step: 504,loss: 0.03201514\n",
            "\n",
            "Global step: 505,loss: 0.037891246\n",
            "\n",
            "Global step: 506,loss: 0.039554454\n",
            "\n",
            "Global step: 507,loss: 0.032469753\n",
            "\n",
            "Global step: 508,loss: 0.038227633\n",
            "\n",
            "Global step: 509,loss: 0.031868376\n",
            "\n",
            "Global step: 510,loss: 0.021088466\n",
            "\n",
            "Global step: 511,loss: 0.0243247\n",
            "\n",
            "Global step: 512,loss: 0.025791636\n",
            "\n",
            "Global step: 513,loss: 0.025627851\n",
            "\n",
            "Global step: 514,loss: 0.031377565\n",
            "\n",
            "Global step: 515,loss: 0.035868242\n",
            "\n",
            "Global step: 516,loss: 0.027438294\n",
            "\n",
            "Global step: 517,loss: 0.024422668\n",
            "\n",
            "Global step: 518,loss: 0.024943907\n",
            "\n",
            "Global step: 519,loss: 0.04288792\n",
            "\n",
            "Global step: 520,loss: 0.03866158\n",
            "\n",
            "Global step: 521,loss: 0.036138758\n",
            "\n",
            "Global step: 522,loss: 0.0351184\n",
            "\n",
            "Global step: 523,loss: 0.0386095\n",
            "\n",
            "Global step: 524,loss: 0.03829306\n",
            "\n",
            "Global step: 525,loss: 0.051329233\n",
            "\n",
            "Global step: 526,loss: 0.02987902\n",
            "\n",
            "Global step: 527,loss: 0.044518977\n",
            "\n",
            "Global step: 528,loss: 0.024064768\n",
            "\n",
            "Global step: 529,loss: 0.027164388\n",
            "\n",
            "Global step: 530,loss: 0.036783103\n",
            "\n",
            "Global step: 531,loss: 0.03921709\n",
            "\n",
            "Global step: 532,loss: 0.053447824\n",
            "\n",
            "Global step: 533,loss: 0.040949136\n",
            "\n",
            "Global step: 534,loss: 0.029614892\n",
            "\n",
            "Global step: 535,loss: 0.02811011\n",
            "\n",
            "Global step: 536,loss: 0.034831066\n",
            "\n",
            "Global step: 537,loss: 0.03908453\n",
            "\n",
            "Global step: 538,loss: 0.031004999\n",
            "\n",
            "Global step: 539,loss: 0.02405612\n",
            "\n",
            "Global step: 540,loss: 0.036488663\n",
            "\n",
            "Global step: 541,loss: 0.03111887\n",
            "\n",
            "Global step: 542,loss: 0.034489892\n",
            "\n",
            "Global step: 543,loss: 0.050484546\n",
            "\n",
            "Global step: 544,loss: 0.02647552\n",
            "\n",
            "Global step: 545,loss: 0.031476375\n",
            "\n",
            "Global step: 546,loss: 0.028507851\n",
            "\n",
            "Global step: 547,loss: 0.034184825\n",
            "\n",
            "Global step: 548,loss: 0.031945795\n",
            "\n",
            "Global step: 549,loss: 0.032290757\n",
            "\n",
            "Global step: 550,loss: 0.030968066\n",
            "\n",
            "Global step: 551,loss: 0.041367274\n",
            "\n",
            "Global step: 552,loss: 0.03017841\n",
            "\n",
            "Global step: 553,loss: 0.03608191\n",
            "\n",
            "Global step: 554,loss: 0.04064508\n",
            "\n",
            "Global step: 555,loss: 0.03710197\n",
            "\n",
            "Global step: 556,loss: 0.02758788\n",
            "\n",
            "Global step: 557,loss: 0.02574525\n",
            "\n",
            "Global step: 558,loss: 0.025370732\n",
            "\n",
            "Global step: 559,loss: 0.031817015\n",
            "\n",
            "Global step: 560,loss: 0.028857257\n",
            "\n",
            "Global step: 561,loss: 0.028494298\n",
            "\n",
            "Global step: 562,loss: 0.036885828\n",
            "\n",
            "Global step: 563,loss: 0.030012656\n",
            "\n",
            "Global step: 564,loss: 0.028669212\n",
            "\n",
            "Global step: 565,loss: 0.037242886\n",
            "\n",
            "Global step: 566,loss: 0.033123266\n",
            "\n",
            "Global step: 567,loss: 0.02875511\n",
            "\n",
            "Global step: 568,loss: 0.055510532\n",
            "\n",
            "Global step: 569,loss: 0.034882326\n",
            "\n",
            "Global step: 570,loss: 0.027240442\n",
            "\n",
            "Global step: 571,loss: 0.03189151\n",
            "\n",
            "Global step: 572,loss: 0.025656983\n",
            "\n",
            "Global step: 573,loss: 0.03202716\n",
            "\n",
            "Global step: 574,loss: 0.029105235\n",
            "\n",
            "Global step: 575,loss: 0.026310483\n",
            "\n",
            "Global step: 576,loss: 0.03625093\n",
            "\n",
            "Global step: 577,loss: 0.035920557\n",
            "\n",
            "Global step: 578,loss: 0.02686886\n",
            "\n",
            "Global step: 579,loss: 0.027506236\n",
            "\n",
            "Global step: 580,loss: 0.038116276\n",
            "\n",
            "Global step: 581,loss: 0.027852705\n",
            "\n",
            "Global step: 582,loss: 0.027056057\n",
            "\n",
            "Global step: 583,loss: 0.032628305\n",
            "\n",
            "Global step: 584,loss: 0.027651764\n",
            "\n",
            "Global step: 585,loss: 0.030158022\n",
            "\n",
            "Global step: 586,loss: 0.022271909\n",
            "\n",
            "Global step: 587,loss: 0.040836714\n",
            "\n",
            "Global step: 588,loss: 0.025596313\n",
            "\n",
            "Global step: 589,loss: 0.022780947\n",
            "\n",
            "Global step: 590,loss: 0.03184529\n",
            "\n",
            "Global step: 591,loss: 0.040363554\n",
            "\n",
            "Global step: 592,loss: 0.025669416\n",
            "\n",
            "Global step: 593,loss: 0.02875571\n",
            "\n",
            "Global step: 594,loss: 0.031146007\n",
            "\n",
            "Global step: 595,loss: 0.034358572\n",
            "\n",
            "Global step: 596,loss: 0.024426583\n",
            "\n",
            "Global step: 597,loss: 0.02884246\n",
            "\n",
            "Global step: 598,loss: 0.031986777\n",
            "\n",
            "Global step: 599,loss: 0.03784714\n",
            "\n",
            "Global step: 600,loss: 0.028397854\n",
            "\n",
            "Global step: 601,loss: 0.029645655\n",
            "\n",
            "Global step: 602,loss: 0.021496598\n",
            "\n",
            "Global step: 603,loss: 0.030253075\n",
            "\n",
            "Global step: 604,loss: 0.027254608\n",
            "\n",
            "Global step: 605,loss: 0.031218838\n",
            "\n",
            "Global step: 606,loss: 0.033125922\n",
            "\n",
            "Global step: 607,loss: 0.030362278\n",
            "\n",
            "Global step: 608,loss: 0.02674707\n",
            "\n",
            "Global step: 609,loss: 0.031607054\n",
            "\n",
            "Global step: 610,loss: 0.022576421\n",
            "\n",
            "Global step: 611,loss: 0.024778936\n",
            "\n",
            "Global step: 612,loss: 0.03902477\n",
            "\n",
            "Global step: 613,loss: 0.04596251\n",
            "\n",
            "Global step: 614,loss: 0.04067227\n",
            "\n",
            "Global step: 615,loss: 0.039948728\n",
            "\n",
            "Global step: 616,loss: 0.037925906\n",
            "\n",
            "Global step: 617,loss: 0.030547842\n",
            "\n",
            "Global step: 618,loss: 0.037384357\n",
            "\n",
            "Global step: 619,loss: 0.045649875\n",
            "\n",
            "Global step: 620,loss: 0.023156745\n",
            "\n",
            "Global step: 621,loss: 0.036283694\n",
            "\n",
            "Global step: 622,loss: 0.034562238\n",
            "\n",
            "Global step: 623,loss: 0.035362616\n",
            "\n",
            "Global step: 624,loss: 0.048612893\n",
            "\n",
            "Global step: 625,loss: 0.03468438\n",
            "\n",
            "Global step: 626,loss: 0.02415621\n",
            "\n",
            "Global step: 627,loss: 0.04838014\n",
            "\n",
            "Global step: 628,loss: 0.029087624\n",
            "\n",
            "Global step: 629,loss: 0.03611461\n",
            "\n",
            "Global step: 630,loss: 0.024215043\n",
            "\n",
            "Global step: 631,loss: 0.04506112\n",
            "\n",
            "Global step: 632,loss: 0.035857424\n",
            "\n",
            "Global step: 633,loss: 0.031411283\n",
            "\n",
            "Global step: 634,loss: 0.041042987\n",
            "\n",
            "Global step: 635,loss: 0.023159618\n",
            "\n",
            "Global step: 636,loss: 0.03461194\n",
            "\n",
            "Global step: 637,loss: 0.023780216\n",
            "\n",
            "Global step: 638,loss: 0.022967719\n",
            "\n",
            "Global step: 639,loss: 0.0331192\n",
            "\n",
            "Global step: 640,loss: 0.023505863\n",
            "\n",
            "Global step: 641,loss: 0.030098135\n",
            "\n",
            "Global step: 642,loss: 0.031236395\n",
            "\n",
            "Global step: 643,loss: 0.033321086\n",
            "\n",
            "Global step: 644,loss: 0.030660387\n",
            "\n",
            "Global step: 645,loss: 0.021352232\n",
            "\n",
            "Global step: 646,loss: 0.034499902\n",
            "\n",
            "Global step: 647,loss: 0.029590387\n",
            "\n",
            "Global step: 648,loss: 0.028282624\n",
            "\n",
            "Global step: 649,loss: 0.029408354\n",
            "\n",
            "Global step: 650,loss: 0.03128131\n",
            "\n",
            "Global step: 651,loss: 0.033717718\n",
            "\n",
            "Global step: 652,loss: 0.0242767\n",
            "\n",
            "Global step: 653,loss: 0.035060927\n",
            "\n",
            "Global step: 654,loss: 0.02955316\n",
            "\n",
            "Global step: 655,loss: 0.023139898\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.8251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:57:20.668302 139635366639360 supervisor.py:1099] global_step/sec: 2.8251\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 656.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:57:20.676483 139637785016064 supervisor.py:1050] Recording summary at step 656.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 656,loss: 0.026201095\n",
            "\n",
            "Global step: 657,loss: 0.032817557\n",
            "\n",
            "Global step: 658,loss: 0.03050876\n",
            "\n",
            "Global step: 659,loss: 0.037615493\n",
            "\n",
            "Global step: 660,loss: 0.032819986\n",
            "\n",
            "Global step: 661,loss: 0.02557367\n",
            "\n",
            "Global step: 662,loss: 0.029568497\n",
            "\n",
            "Global step: 663,loss: 0.02966449\n",
            "\n",
            "Global step: 664,loss: 0.030850234\n",
            "\n",
            "Global step: 665,loss: 0.029964127\n",
            "\n",
            "Global step: 666,loss: 0.031637713\n",
            "\n",
            "Global step: 667,loss: 0.020695515\n",
            "\n",
            "Global step: 668,loss: 0.02728917\n",
            "\n",
            "Global step: 669,loss: 0.027874522\n",
            "\n",
            "Global step: 670,loss: 0.03550493\n",
            "\n",
            "Global step: 671,loss: 0.025433403\n",
            "\n",
            "Global step: 672,loss: 0.031163584\n",
            "\n",
            "Global step: 673,loss: 0.029579846\n",
            "\n",
            "Global step: 674,loss: 0.039482392\n",
            "\n",
            "Global step: 675,loss: 0.03550586\n",
            "\n",
            "Global step: 676,loss: 0.021846198\n",
            "\n",
            "Global step: 677,loss: 0.02521938\n",
            "\n",
            "Global step: 678,loss: 0.02464177\n",
            "\n",
            "Global step: 679,loss: 0.031630665\n",
            "\n",
            "Global step: 680,loss: 0.020964144\n",
            "\n",
            "Global step: 681,loss: 0.039143972\n",
            "\n",
            "Global step: 682,loss: 0.02473377\n",
            "\n",
            "Global step: 683,loss: 0.029209543\n",
            "\n",
            "Global step: 684,loss: 0.03515604\n",
            "\n",
            "Global step: 685,loss: 0.039677855\n",
            "\n",
            "Global step: 686,loss: 0.022310931\n",
            "\n",
            "Global step: 687,loss: 0.03402665\n",
            "\n",
            "Global step: 688,loss: 0.027348649\n",
            "\n",
            "Global step: 689,loss: 0.03180578\n",
            "\n",
            "Global step: 690,loss: 0.025564846\n",
            "\n",
            "Global step: 691,loss: 0.03318422\n",
            "\n",
            "Global step: 692,loss: 0.025975868\n",
            "\n",
            "Global step: 693,loss: 0.04825845\n",
            "\n",
            "Global step: 694,loss: 0.02802887\n",
            "\n",
            "Global step: 695,loss: 0.024292894\n",
            "\n",
            "Global step: 696,loss: 0.03567422\n",
            "\n",
            "Global step: 697,loss: 0.02621017\n",
            "\n",
            "Global step: 698,loss: 0.027216371\n",
            "\n",
            "Global step: 699,loss: 0.026991282\n",
            "\n",
            "Global step: 700,loss: 0.021651136\n",
            "\n",
            "Global step: 701,loss: 0.026808685\n",
            "\n",
            "Global step: 702,loss: 0.029468745\n",
            "\n",
            "Global step: 703,loss: 0.02257425\n",
            "\n",
            "Global step: 704,loss: 0.033386257\n",
            "\n",
            "Global step: 705,loss: 0.026402414\n",
            "\n",
            "Global step: 706,loss: 0.02923456\n",
            "\n",
            "Global step: 707,loss: 0.03261534\n",
            "\n",
            "Global step: 708,loss: 0.030658815\n",
            "\n",
            "Global step: 709,loss: 0.032936502\n",
            "\n",
            "Global step: 710,loss: 0.046280954\n",
            "\n",
            "Global step: 711,loss: 0.028742462\n",
            "\n",
            "Global step: 712,loss: 0.024499174\n",
            "\n",
            "Global step: 713,loss: 0.037002698\n",
            "\n",
            "Global step: 714,loss: 0.03477618\n",
            "\n",
            "Global step: 715,loss: 0.029833626\n",
            "\n",
            "Global step: 716,loss: 0.028349362\n",
            "\n",
            "Global step: 717,loss: 0.032999877\n",
            "\n",
            "Global step: 718,loss: 0.025358133\n",
            "\n",
            "Global step: 719,loss: 0.02189925\n",
            "\n",
            "Global step: 720,loss: 0.030217031\n",
            "\n",
            "Global step: 721,loss: 0.03895516\n",
            "\n",
            "Global step: 722,loss: 0.027636027\n",
            "\n",
            "Global step: 723,loss: 0.03271425\n",
            "\n",
            "Global step: 724,loss: 0.026056774\n",
            "\n",
            "Global step: 725,loss: 0.024219465\n",
            "\n",
            "Global step: 726,loss: 0.027532395\n",
            "\n",
            "Global step: 727,loss: 0.026820753\n",
            "\n",
            "Global step: 728,loss: 0.024523716\n",
            "\n",
            "Global step: 729,loss: 0.023878984\n",
            "\n",
            "Global step: 730,loss: 0.027797556\n",
            "\n",
            "Global step: 731,loss: 0.035518333\n",
            "\n",
            "Global step: 732,loss: 0.031087317\n",
            "\n",
            "Global step: 733,loss: 0.036292974\n",
            "\n",
            "Global step: 734,loss: 0.02904243\n",
            "\n",
            "Global step: 735,loss: 0.028047008\n",
            "\n",
            "Global step: 736,loss: 0.03043396\n",
            "\n",
            "Global step: 737,loss: 0.030133069\n",
            "\n",
            "Global step: 738,loss: 0.033722516\n",
            "\n",
            "Global step: 739,loss: 0.024696905\n",
            "\n",
            "Global step: 740,loss: 0.022376005\n",
            "\n",
            "Global step: 741,loss: 0.02989272\n",
            "\n",
            "Global step: 742,loss: 0.023480844\n",
            "\n",
            "Global step: 743,loss: 0.039180845\n",
            "\n",
            "Global step: 744,loss: 0.025156226\n",
            "\n",
            "Global step: 745,loss: 0.023999868\n",
            "\n",
            "Global step: 746,loss: 0.03131854\n",
            "\n",
            "Global step: 747,loss: 0.027210686\n",
            "\n",
            "Global step: 748,loss: 0.029915635\n",
            "\n",
            "Global step: 749,loss: 0.023702327\n",
            "\n",
            "Global step: 750,loss: 0.035244394\n",
            "\n",
            "Global step: 751,loss: 0.027401127\n",
            "\n",
            "Global step: 752,loss: 0.024532584\n",
            "\n",
            "Global step: 753,loss: 0.030908938\n",
            "\n",
            "Global step: 754,loss: 0.03232001\n",
            "\n",
            "Global step: 755,loss: 0.0274846\n",
            "\n",
            "Global step: 756,loss: 0.025620125\n",
            "\n",
            "Global step: 757,loss: 0.036601573\n",
            "\n",
            "Global step: 758,loss: 0.023046702\n",
            "\n",
            "Global step: 759,loss: 0.022893175\n",
            "\n",
            "Global step: 760,loss: 0.029129721\n",
            "\n",
            "Global step: 761,loss: 0.021694193\n",
            "\n",
            "Global step: 762,loss: 0.040691964\n",
            "\n",
            "Global step: 763,loss: 0.026450608\n",
            "\n",
            "Global step: 764,loss: 0.024603564\n",
            "\n",
            "Global step: 765,loss: 0.044046953\n",
            "\n",
            "Global step: 766,loss: 0.019540388\n",
            "\n",
            "Global step: 767,loss: 0.028616272\n",
            "\n",
            "Global step: 768,loss: 0.022727061\n",
            "\n",
            "Global step: 769,loss: 0.029683575\n",
            "\n",
            "Global step: 770,loss: 0.021609277\n",
            "\n",
            "Global step: 771,loss: 0.022694983\n",
            "\n",
            "Global step: 772,loss: 0.02067626\n",
            "\n",
            "Global step: 773,loss: 0.03598754\n",
            "\n",
            "Global step: 774,loss: 0.038961828\n",
            "\n",
            "Global step: 775,loss: 0.026677884\n",
            "\n",
            "Global step: 776,loss: 0.02355829\n",
            "\n",
            "Global step: 777,loss: 0.030054297\n",
            "\n",
            "Global step: 778,loss: 0.023773137\n",
            "\n",
            "Global step: 779,loss: 0.027013142\n",
            "\n",
            "Global step: 780,loss: 0.024514511\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 780,Val_Loss: 0.02458951681947861,  Val_acc: 0.9934895833333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 15:58:18.136898 139641102628736 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/20:\n",
            "Global step: 781,loss: 0.025031561\n",
            "\n",
            "Global step: 782,loss: 0.022255246\n",
            "\n",
            "Global step: 783,loss: 0.0284127\n",
            "\n",
            "Global step: 784,loss: 0.022891637\n",
            "\n",
            "Global step: 785,loss: 0.029726457\n",
            "\n",
            "Global step: 786,loss: 0.022843087\n",
            "\n",
            "Global step: 787,loss: 0.024246091\n",
            "\n",
            "Global step: 788,loss: 0.026266128\n",
            "\n",
            "Global step: 789,loss: 0.022925824\n",
            "\n",
            "Global step: 790,loss: 0.022441052\n",
            "\n",
            "Global step: 791,loss: 0.024613854\n",
            "\n",
            "Global step: 792,loss: 0.034302026\n",
            "\n",
            "Global step: 793,loss: 0.019858617\n",
            "\n",
            "Global step: 794,loss: 0.023623014\n",
            "\n",
            "Global step: 795,loss: 0.019922707\n",
            "\n",
            "Global step: 796,loss: 0.021547839\n",
            "\n",
            "Global step: 797,loss: 0.051873747\n",
            "\n",
            "Global step: 798,loss: 0.024254614\n",
            "\n",
            "Global step: 799,loss: 0.032585166\n",
            "\n",
            "Global step: 800,loss: 0.020704929\n",
            "\n",
            "Global step: 801,loss: 0.02000839\n",
            "\n",
            "Global step: 802,loss: 0.030437294\n",
            "\n",
            "Global step: 803,loss: 0.01992298\n",
            "\n",
            "Global step: 804,loss: 0.022009099\n",
            "\n",
            "Global step: 805,loss: 0.022339512\n",
            "\n",
            "Global step: 806,loss: 0.024980064\n",
            "\n",
            "Global step: 807,loss: 0.027024802\n",
            "\n",
            "Global step: 808,loss: 0.022735205\n",
            "\n",
            "Global step: 809,loss: 0.038860336\n",
            "\n",
            "Global step: 810,loss: 0.024406048\n",
            "\n",
            "Global step: 811,loss: 0.01944075\n",
            "\n",
            "Global step: 812,loss: 0.028434612\n",
            "\n",
            "Global step: 813,loss: 0.032337803\n",
            "\n",
            "Global step: 814,loss: 0.024418695\n",
            "\n",
            "Global step: 815,loss: 0.030413814\n",
            "\n",
            "Global step: 816,loss: 0.03334619\n",
            "\n",
            "Global step: 817,loss: 0.017565576\n",
            "\n",
            "Global step: 818,loss: 0.025924316\n",
            "\n",
            "Global step: 819,loss: 0.03271622\n",
            "\n",
            "Global step: 820,loss: 0.023068279\n",
            "\n",
            "Global step: 821,loss: 0.020800415\n",
            "\n",
            "Global step: 822,loss: 0.020354513\n",
            "\n",
            "Global step: 823,loss: 0.024868462\n",
            "\n",
            "Global step: 824,loss: 0.03161548\n",
            "\n",
            "Global step: 825,loss: 0.024349097\n",
            "\n",
            "Global step: 826,loss: 0.028029015\n",
            "\n",
            "Global step: 827,loss: 0.029792707\n",
            "\n",
            "Global step: 828,loss: 0.023793591\n",
            "\n",
            "Global step: 829,loss: 0.029497603\n",
            "\n",
            "Global step: 830,loss: 0.03007422\n",
            "\n",
            "Global step: 831,loss: 0.030669611\n",
            "\n",
            "Global step: 832,loss: 0.01960888\n",
            "\n",
            "Global step: 833,loss: 0.030741893\n",
            "\n",
            "Global step: 834,loss: 0.02204579\n",
            "\n",
            "Global step: 835,loss: 0.029546471\n",
            "\n",
            "Global step: 836,loss: 0.020332776\n",
            "\n",
            "Global step: 837,loss: 0.020178324\n",
            "\n",
            "Global step: 838,loss: 0.023064055\n",
            "\n",
            "Global step: 839,loss: 0.024697877\n",
            "\n",
            "Global step: 840,loss: 0.020551868\n",
            "\n",
            "Global step: 841,loss: 0.026222136\n",
            "\n",
            "Global step: 842,loss: 0.025599115\n",
            "\n",
            "Global step: 843,loss: 0.027591988\n",
            "\n",
            "Global step: 844,loss: 0.023006873\n",
            "\n",
            "Global step: 845,loss: 0.026324114\n",
            "\n",
            "Global step: 846,loss: 0.021226738\n",
            "\n",
            "Global step: 847,loss: 0.026888933\n",
            "\n",
            "Global step: 848,loss: 0.019938368\n",
            "\n",
            "Global step: 849,loss: 0.021263683\n",
            "\n",
            "Global step: 850,loss: 0.018701762\n",
            "\n",
            "Global step: 851,loss: 0.03533024\n",
            "\n",
            "Global step: 852,loss: 0.019117914\n",
            "\n",
            "Global step: 853,loss: 0.020224564\n",
            "\n",
            "Global step: 854,loss: 0.02524573\n",
            "\n",
            "Global step: 855,loss: 0.03287563\n",
            "\n",
            "Global step: 856,loss: 0.031093774\n",
            "\n",
            "Global step: 857,loss: 0.034707606\n",
            "\n",
            "Global step: 858,loss: 0.029166088\n",
            "\n",
            "Global step: 859,loss: 0.018725155\n",
            "\n",
            "Global step: 860,loss: 0.029364351\n",
            "\n",
            "Global step: 861,loss: 0.022706164\n",
            "\n",
            "Global step: 862,loss: 0.03273934\n",
            "\n",
            "Global step: 863,loss: 0.020941809\n",
            "\n",
            "Global step: 864,loss: 0.022818957\n",
            "\n",
            "Global step: 865,loss: 0.02474072\n",
            "\n",
            "Global step: 866,loss: 0.022341445\n",
            "\n",
            "Global step: 867,loss: 0.022027424\n",
            "\n",
            "Global step: 868,loss: 0.020186571\n",
            "\n",
            "Global step: 869,loss: 0.0270628\n",
            "\n",
            "Global step: 870,loss: 0.021469036\n",
            "\n",
            "Global step: 871,loss: 0.029762648\n",
            "\n",
            "Global step: 872,loss: 0.019936046\n",
            "\n",
            "Global step: 873,loss: 0.017881164\n",
            "\n",
            "Global step: 874,loss: 0.024765076\n",
            "\n",
            "Global step: 875,loss: 0.033273034\n",
            "\n",
            "Global step: 876,loss: 0.029153595\n",
            "\n",
            "Global step: 877,loss: 0.02007805\n",
            "\n",
            "Global step: 878,loss: 0.024414623\n",
            "\n",
            "Global step: 879,loss: 0.029828994\n",
            "\n",
            "Global step: 880,loss: 0.028952038\n",
            "\n",
            "Global step: 881,loss: 0.028980164\n",
            "\n",
            "Global step: 882,loss: 0.025676612\n",
            "\n",
            "Global step: 883,loss: 0.01886426\n",
            "\n",
            "Global step: 884,loss: 0.024662554\n",
            "\n",
            "Global step: 885,loss: 0.030724699\n",
            "\n",
            "Global step: 886,loss: 0.025528248\n",
            "\n",
            "Global step: 887,loss: 0.023545654\n",
            "\n",
            "Global step: 888,loss: 0.018621432\n",
            "\n",
            "Global step: 889,loss: 0.022924915\n",
            "\n",
            "Global step: 890,loss: 0.02475093\n",
            "\n",
            "Global step: 891,loss: 0.0376665\n",
            "\n",
            "Global step: 892,loss: 0.021469768\n",
            "\n",
            "Global step: 893,loss: 0.0188232\n",
            "\n",
            "Global step: 894,loss: 0.026563458\n",
            "\n",
            "Global step: 895,loss: 0.023767032\n",
            "\n",
            "Global step: 896,loss: 0.02988664\n",
            "\n",
            "Global step: 897,loss: 0.032721724\n",
            "\n",
            "Global step: 898,loss: 0.016365368\n",
            "\n",
            "Global step: 899,loss: 0.021069331\n",
            "\n",
            "Global step: 900,loss: 0.03767422\n",
            "\n",
            "Global step: 901,loss: 0.028935552\n",
            "\n",
            "Global step: 902,loss: 0.028435979\n",
            "\n",
            "Global step: 903,loss: 0.028102115\n",
            "\n",
            "Global step: 904,loss: 0.032137103\n",
            "\n",
            "Global step: 905,loss: 0.02629509\n",
            "\n",
            "Global step: 906,loss: 0.025005806\n",
            "\n",
            "Global step: 907,loss: 0.019962076\n",
            "\n",
            "Global step: 908,loss: 0.037793793\n",
            "\n",
            "Global step: 909,loss: 0.019951286\n",
            "\n",
            "Global step: 910,loss: 0.023270953\n",
            "\n",
            "Global step: 911,loss: 0.025758572\n",
            "\n",
            "Global step: 912,loss: 0.024357602\n",
            "\n",
            "Global step: 913,loss: 0.024720678\n",
            "\n",
            "Global step: 914,loss: 0.020690039\n",
            "\n",
            "Global step: 915,loss: 0.02199547\n",
            "\n",
            "Global step: 916,loss: 0.022224758\n",
            "\n",
            "Global step: 917,loss: 0.019106712\n",
            "\n",
            "Global step: 918,loss: 0.03198822\n",
            "\n",
            "Global step: 919,loss: 0.023681201\n",
            "\n",
            "Global step: 920,loss: 0.020074598\n",
            "\n",
            "Global step: 921,loss: 0.01842025\n",
            "\n",
            "Global step: 922,loss: 0.024878088\n",
            "\n",
            "Global step: 923,loss: 0.021370597\n",
            "\n",
            "Global step: 924,loss: 0.035640027\n",
            "\n",
            "Global step: 925,loss: 0.018193534\n",
            "\n",
            "Global step: 926,loss: 0.032950215\n",
            "\n",
            "Global step: 927,loss: 0.02377692\n",
            "\n",
            "Global step: 928,loss: 0.026770556\n",
            "\n",
            "Global step: 929,loss: 0.03462023\n",
            "\n",
            "Global step: 930,loss: 0.022915535\n",
            "\n",
            "Global step: 931,loss: 0.023652213\n",
            "\n",
            "Global step: 932,loss: 0.023728624\n",
            "\n",
            "Global step: 933,loss: 0.02747829\n",
            "\n",
            "Global step: 934,loss: 0.018496715\n",
            "\n",
            "Global step: 935,loss: 0.019504953\n",
            "\n",
            "Global step: 936,loss: 0.0183857\n",
            "\n",
            "Global step: 937,loss: 0.023079589\n",
            "\n",
            "Global step: 938,loss: 0.03128306\n",
            "\n",
            "Global step: 939,loss: 0.031010931\n",
            "\n",
            "Global step: 940,loss: 0.033896875\n",
            "\n",
            "Global step: 941,loss: 0.025962986\n",
            "\n",
            "Global step: 942,loss: 0.026789743\n",
            "\n",
            "Global step: 943,loss: 0.026656516\n",
            "\n",
            "Global step: 944,loss: 0.032463137\n",
            "\n",
            "Global step: 945,loss: 0.024403894\n",
            "\n",
            "Global step: 946,loss: 0.019390434\n",
            "\n",
            "Global step: 947,loss: 0.022597069\n",
            "\n",
            "Global step: 948,loss: 0.026340276\n",
            "\n",
            "Global step: 949,loss: 0.021174736\n",
            "\n",
            "Global step: 950,loss: 0.024224285\n",
            "\n",
            "Global step: 951,loss: 0.02834867\n",
            "\n",
            "Global step: 952,loss: 0.023386726\n",
            "\n",
            "Global step: 953,loss: 0.025333442\n",
            "\n",
            "Global step: 954,loss: 0.029305166\n",
            "\n",
            "Global step: 955,loss: 0.025557589\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 956.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:59:20.582361 139637785016064 supervisor.py:1050] Recording summary at step 956.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.50154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 15:59:20.594257 139635366639360 supervisor.py:1099] global_step/sec: 2.50154\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 956,loss: 0.025052026\n",
            "\n",
            "Global step: 957,loss: 0.017566623\n",
            "\n",
            "Global step: 958,loss: 0.016582021\n",
            "\n",
            "Global step: 959,loss: 0.022661144\n",
            "\n",
            "Global step: 960,loss: 0.02221844\n",
            "\n",
            "Global step: 961,loss: 0.021885108\n",
            "\n",
            "Global step: 962,loss: 0.016408771\n",
            "\n",
            "Global step: 963,loss: 0.017311035\n",
            "\n",
            "Global step: 964,loss: 0.018590702\n",
            "\n",
            "Global step: 965,loss: 0.019693166\n",
            "\n",
            "Global step: 966,loss: 0.02152815\n",
            "\n",
            "Global step: 967,loss: 0.029278513\n",
            "\n",
            "Global step: 968,loss: 0.023055278\n",
            "\n",
            "Global step: 969,loss: 0.019865092\n",
            "\n",
            "Global step: 970,loss: 0.022643011\n",
            "\n",
            "Global step: 971,loss: 0.020374127\n",
            "\n",
            "Global step: 972,loss: 0.021876603\n",
            "\n",
            "Global step: 973,loss: 0.019125856\n",
            "\n",
            "Global step: 974,loss: 0.016974572\n",
            "\n",
            "Global step: 975,loss: 0.035617236\n",
            "\n",
            "Global step: 976,loss: 0.024584109\n",
            "\n",
            "Global step: 977,loss: 0.018206552\n",
            "\n",
            "Global step: 978,loss: 0.028043445\n",
            "\n",
            "Global step: 979,loss: 0.025262997\n",
            "\n",
            "Global step: 980,loss: 0.0263978\n",
            "\n",
            "Global step: 981,loss: 0.048229627\n",
            "\n",
            "Global step: 982,loss: 0.026354805\n",
            "\n",
            "Global step: 983,loss: 0.019050565\n",
            "\n",
            "Global step: 984,loss: 0.029220833\n",
            "\n",
            "Global step: 985,loss: 0.024766307\n",
            "\n",
            "Global step: 986,loss: 0.024541236\n",
            "\n",
            "Global step: 987,loss: 0.024729785\n",
            "\n",
            "Global step: 988,loss: 0.023071475\n",
            "\n",
            "Global step: 989,loss: 0.027426105\n",
            "\n",
            "Global step: 990,loss: 0.028850371\n",
            "\n",
            "Global step: 991,loss: 0.035749264\n",
            "\n",
            "Global step: 992,loss: 0.02718407\n",
            "\n",
            "Global step: 993,loss: 0.025714587\n",
            "\n",
            "Global step: 994,loss: 0.021623518\n",
            "\n",
            "Global step: 995,loss: 0.032265212\n",
            "\n",
            "Global step: 996,loss: 0.031506166\n",
            "\n",
            "Global step: 997,loss: 0.019803902\n",
            "\n",
            "Global step: 998,loss: 0.023482691\n",
            "\n",
            "Global step: 999,loss: 0.017075233\n",
            "\n",
            "Global step: 1000,loss: 0.015042275\n",
            "\n",
            "Global step: 1001,loss: 0.023144443\n",
            "\n",
            "Global step: 1002,loss: 0.021398328\n",
            "\n",
            "Global step: 1003,loss: 0.02066911\n",
            "\n",
            "Global step: 1004,loss: 0.022803696\n",
            "\n",
            "Global step: 1005,loss: 0.028707422\n",
            "\n",
            "Global step: 1006,loss: 0.019457683\n",
            "\n",
            "Global step: 1007,loss: 0.020685619\n",
            "\n",
            "Global step: 1008,loss: 0.021749951\n",
            "\n",
            "Global step: 1009,loss: 0.016585672\n",
            "\n",
            "Global step: 1010,loss: 0.018359696\n",
            "\n",
            "Global step: 1011,loss: 0.029854931\n",
            "\n",
            "Global step: 1012,loss: 0.02388837\n",
            "\n",
            "Global step: 1013,loss: 0.027601907\n",
            "\n",
            "Global step: 1014,loss: 0.021649849\n",
            "\n",
            "Global step: 1015,loss: 0.01763197\n",
            "\n",
            "Global step: 1016,loss: 0.01848909\n",
            "\n",
            "Global step: 1017,loss: 0.019387558\n",
            "\n",
            "Global step: 1018,loss: 0.024901498\n",
            "\n",
            "Global step: 1019,loss: 0.031834982\n",
            "\n",
            "Global step: 1020,loss: 0.017246678\n",
            "\n",
            "Global step: 1021,loss: 0.02934453\n",
            "\n",
            "Global step: 1022,loss: 0.023348324\n",
            "\n",
            "Global step: 1023,loss: 0.02966053\n",
            "\n",
            "Global step: 1024,loss: 0.019058317\n",
            "\n",
            "Global step: 1025,loss: 0.019257784\n",
            "\n",
            "Global step: 1026,loss: 0.01938278\n",
            "\n",
            "Global step: 1027,loss: 0.02011211\n",
            "\n",
            "Global step: 1028,loss: 0.024154441\n",
            "\n",
            "Global step: 1029,loss: 0.020858545\n",
            "\n",
            "Global step: 1030,loss: 0.024796147\n",
            "\n",
            "Global step: 1031,loss: 0.031871483\n",
            "\n",
            "Global step: 1032,loss: 0.02786861\n",
            "\n",
            "Global step: 1033,loss: 0.022300683\n",
            "\n",
            "Global step: 1034,loss: 0.01846574\n",
            "\n",
            "Global step: 1035,loss: 0.019536955\n",
            "\n",
            "Global step: 1036,loss: 0.034826763\n",
            "\n",
            "Global step: 1037,loss: 0.02028852\n",
            "\n",
            "Global step: 1038,loss: 0.020824336\n",
            "\n",
            "Global step: 1039,loss: 0.024345132\n",
            "\n",
            "Global step: 1040,loss: 0.020242939\n",
            "\n",
            "Global step: 1041,loss: 0.018083135\n",
            "\n",
            "Global step: 1042,loss: 0.024741702\n",
            "\n",
            "Global step: 1043,loss: 0.020026837\n",
            "\n",
            "Global step: 1044,loss: 0.03191229\n",
            "\n",
            "Global step: 1045,loss: 0.0320224\n",
            "\n",
            "Global step: 1046,loss: 0.025052931\n",
            "\n",
            "Global step: 1047,loss: 0.026125602\n",
            "\n",
            "Global step: 1048,loss: 0.02180988\n",
            "\n",
            "Global step: 1049,loss: 0.024685249\n",
            "\n",
            "Global step: 1050,loss: 0.01749484\n",
            "\n",
            "Global step: 1051,loss: 0.019509427\n",
            "\n",
            "Global step: 1052,loss: 0.024972655\n",
            "\n",
            "Global step: 1053,loss: 0.029105853\n",
            "\n",
            "Global step: 1054,loss: 0.02205303\n",
            "\n",
            "Global step: 1055,loss: 0.03420158\n",
            "\n",
            "Global step: 1056,loss: 0.020097718\n",
            "\n",
            "Global step: 1057,loss: 0.021433128\n",
            "\n",
            "Global step: 1058,loss: 0.01868926\n",
            "\n",
            "Global step: 1059,loss: 0.034556847\n",
            "\n",
            "Global step: 1060,loss: 0.026823245\n",
            "\n",
            "Global step: 1061,loss: 0.022870354\n",
            "\n",
            "Global step: 1062,loss: 0.022243306\n",
            "\n",
            "Global step: 1063,loss: 0.028514853\n",
            "\n",
            "Global step: 1064,loss: 0.019556005\n",
            "\n",
            "Global step: 1065,loss: 0.019041639\n",
            "\n",
            "Global step: 1066,loss: 0.028039336\n",
            "\n",
            "Global step: 1067,loss: 0.018796828\n",
            "\n",
            "Global step: 1068,loss: 0.0196667\n",
            "\n",
            "Global step: 1069,loss: 0.030179482\n",
            "\n",
            "Global step: 1070,loss: 0.024044715\n",
            "\n",
            "Global step: 1071,loss: 0.02470701\n",
            "\n",
            "Global step: 1072,loss: 0.02885973\n",
            "\n",
            "Global step: 1073,loss: 0.019781781\n",
            "\n",
            "Global step: 1074,loss: 0.020418942\n",
            "\n",
            "Global step: 1075,loss: 0.021324128\n",
            "\n",
            "Global step: 1076,loss: 0.02099103\n",
            "\n",
            "Global step: 1077,loss: 0.019525392\n",
            "\n",
            "Global step: 1078,loss: 0.020094182\n",
            "\n",
            "Global step: 1079,loss: 0.020231208\n",
            "\n",
            "Global step: 1080,loss: 0.026181292\n",
            "\n",
            "Global step: 1081,loss: 0.022414166\n",
            "\n",
            "Global step: 1082,loss: 0.026245821\n",
            "\n",
            "Global step: 1083,loss: 0.023322232\n",
            "\n",
            "Global step: 1084,loss: 0.018528052\n",
            "\n",
            "Global step: 1085,loss: 0.023637889\n",
            "\n",
            "Global step: 1086,loss: 0.020557718\n",
            "\n",
            "Global step: 1087,loss: 0.019458286\n",
            "\n",
            "Global step: 1088,loss: 0.023211654\n",
            "\n",
            "Global step: 1089,loss: 0.015703097\n",
            "\n",
            "Global step: 1090,loss: 0.01862638\n",
            "\n",
            "Global step: 1091,loss: 0.020778552\n",
            "\n",
            "Global step: 1092,loss: 0.021026602\n",
            "\n",
            "Global step: 1093,loss: 0.024344958\n",
            "\n",
            "Global step: 1094,loss: 0.039525833\n",
            "\n",
            "Global step: 1095,loss: 0.024093103\n",
            "\n",
            "Global step: 1096,loss: 0.022636075\n",
            "\n",
            "Global step: 1097,loss: 0.027962651\n",
            "\n",
            "Global step: 1098,loss: 0.029691124\n",
            "\n",
            "Global step: 1099,loss: 0.02044329\n",
            "\n",
            "Global step: 1100,loss: 0.016008073\n",
            "\n",
            "Global step: 1101,loss: 0.017036205\n",
            "\n",
            "Global step: 1102,loss: 0.027528293\n",
            "\n",
            "Global step: 1103,loss: 0.016508676\n",
            "\n",
            "Global step: 1104,loss: 0.020256583\n",
            "\n",
            "Global step: 1105,loss: 0.01860992\n",
            "\n",
            "Global step: 1106,loss: 0.021480918\n",
            "\n",
            "Global step: 1107,loss: 0.030465646\n",
            "\n",
            "Global step: 1108,loss: 0.022291385\n",
            "\n",
            "Global step: 1109,loss: 0.025835793\n",
            "\n",
            "Global step: 1110,loss: 0.025928907\n",
            "\n",
            "Global step: 1111,loss: 0.027442435\n",
            "\n",
            "Global step: 1112,loss: 0.02298582\n",
            "\n",
            "Global step: 1113,loss: 0.023237042\n",
            "\n",
            "Global step: 1114,loss: 0.023232339\n",
            "\n",
            "Global step: 1115,loss: 0.028925356\n",
            "\n",
            "Global step: 1116,loss: 0.021926343\n",
            "\n",
            "Global step: 1117,loss: 0.029293906\n",
            "\n",
            "Global step: 1118,loss: 0.019919291\n",
            "\n",
            "Global step: 1119,loss: 0.018475631\n",
            "\n",
            "Global step: 1120,loss: 0.023621023\n",
            "\n",
            "Global step: 1121,loss: 0.018300043\n",
            "\n",
            "Global step: 1122,loss: 0.02553716\n",
            "\n",
            "Global step: 1123,loss: 0.019956237\n",
            "\n",
            "Global step: 1124,loss: 0.02249855\n",
            "\n",
            "Global step: 1125,loss: 0.021690335\n",
            "\n",
            "Global step: 1126,loss: 0.022206832\n",
            "\n",
            "Global step: 1127,loss: 0.019920561\n",
            "\n",
            "Global step: 1128,loss: 0.028710023\n",
            "\n",
            "Global step: 1129,loss: 0.020546066\n",
            "\n",
            "Global step: 1130,loss: 0.020960372\n",
            "\n",
            "Global step: 1131,loss: 0.034049857\n",
            "\n",
            "Global step: 1132,loss: 0.019078588\n",
            "\n",
            "Global step: 1133,loss: 0.02395089\n",
            "\n",
            "Global step: 1134,loss: 0.021707894\n",
            "\n",
            "Global step: 1135,loss: 0.018695723\n",
            "\n",
            "Global step: 1136,loss: 0.022137016\n",
            "\n",
            "Global step: 1137,loss: 0.019396206\n",
            "\n",
            "Global step: 1138,loss: 0.022865327\n",
            "\n",
            "Global step: 1139,loss: 0.020864896\n",
            "\n",
            "Global step: 1140,loss: 0.016207661\n",
            "\n",
            "Global step: 1141,loss: 0.020627247\n",
            "\n",
            "Global step: 1142,loss: 0.025665388\n",
            "\n",
            "Global step: 1143,loss: 0.018588556\n",
            "\n",
            "Global step: 1144,loss: 0.02320172\n",
            "\n",
            "Global step: 1145,loss: 0.026130848\n",
            "\n",
            "Global step: 1146,loss: 0.019931383\n",
            "\n",
            "Global step: 1147,loss: 0.016112482\n",
            "\n",
            "Global step: 1148,loss: 0.02022409\n",
            "\n",
            "Global step: 1149,loss: 0.025507271\n",
            "\n",
            "Global step: 1150,loss: 0.014861136\n",
            "\n",
            "Global step: 1151,loss: 0.01649588\n",
            "\n",
            "Global step: 1152,loss: 0.027849792\n",
            "\n",
            "Global step: 1153,loss: 0.02663681\n",
            "\n",
            "Global step: 1154,loss: 0.021979444\n",
            "\n",
            "Global step: 1155,loss: 0.020176303\n",
            "\n",
            "Global step: 1156,loss: 0.01793087\n",
            "\n",
            "Global step: 1157,loss: 0.015517125\n",
            "\n",
            "Global step: 1158,loss: 0.033373363\n",
            "\n",
            "Global step: 1159,loss: 0.023985866\n",
            "\n",
            "Global step: 1160,loss: 0.022560386\n",
            "\n",
            "Global step: 1161,loss: 0.018291984\n",
            "\n",
            "Global step: 1162,loss: 0.021478713\n",
            "\n",
            "Global step: 1163,loss: 0.03642291\n",
            "\n",
            "Global step: 1164,loss: 0.025496695\n",
            "\n",
            "Global step: 1165,loss: 0.027259648\n",
            "\n",
            "Global step: 1166,loss: 0.024500899\n",
            "\n",
            "Global step: 1167,loss: 0.027706038\n",
            "\n",
            "Global step: 1168,loss: 0.017094474\n",
            "\n",
            "Global step: 1169,loss: 0.027603546\n",
            "\n",
            "Global step: 1170,loss: 0.01852826\n",
            "\n",
            "Global step: 1171,loss: 0.023367938\n",
            "\n",
            "Global step: 1172,loss: 0.026107227\n",
            "\n",
            "Global step: 1173,loss: 0.018838566\n",
            "\n",
            "Global step: 1174,loss: 0.028006949\n",
            "\n",
            "Global step: 1175,loss: 0.02663108\n",
            "\n",
            "Global step: 1176,loss: 0.02060172\n",
            "\n",
            "Global step: 1177,loss: 0.024759904\n",
            "\n",
            "Global step: 1178,loss: 0.029964684\n",
            "\n",
            "Global step: 1179,loss: 0.03486335\n",
            "\n",
            "Global step: 1180,loss: 0.018259844\n",
            "\n",
            "Global step: 1181,loss: 0.020870931\n",
            "\n",
            "Global step: 1182,loss: 0.023202999\n",
            "\n",
            "Global step: 1183,loss: 0.02236177\n",
            "\n",
            "Global step: 1184,loss: 0.038307335\n",
            "\n",
            "Global step: 1185,loss: 0.01777485\n",
            "\n",
            "Global step: 1186,loss: 0.028795281\n",
            "\n",
            "Global step: 1187,loss: 0.015062239\n",
            "\n",
            "Global step: 1188,loss: 0.02711641\n",
            "\n",
            "Global step: 1189,loss: 0.028129216\n",
            "\n",
            "Global step: 1190,loss: 0.02726698\n",
            "\n",
            "Global step: 1191,loss: 0.019185133\n",
            "\n",
            "Global step: 1192,loss: 0.018756637\n",
            "\n",
            "Global step: 1193,loss: 0.024368498\n",
            "\n",
            "Global step: 1194,loss: 0.027519632\n",
            "\n",
            "Global step: 1195,loss: 0.024355631\n",
            "\n",
            "Global step: 1196,loss: 0.018271929\n",
            "\n",
            "Global step: 1197,loss: 0.023872921\n",
            "\n",
            "Global step: 1198,loss: 0.028224017\n",
            "\n",
            "Global step: 1199,loss: 0.019819558\n",
            "\n",
            "Global step: 1200,loss: 0.019860882\n",
            "\n",
            "Global step: 1201,loss: 0.018260218\n",
            "\n",
            "Global step: 1202,loss: 0.022178989\n",
            "\n",
            "Global step: 1203,loss: 0.015468538\n",
            "\n",
            "Global step: 1204,loss: 0.019315464\n",
            "\n",
            "Global step: 1205,loss: 0.022583457\n",
            "\n",
            "Global step: 1206,loss: 0.023192596\n",
            "\n",
            "Global step: 1207,loss: 0.042409994\n",
            "\n",
            "Global step: 1208,loss: 0.016624458\n",
            "\n",
            "Global step: 1209,loss: 0.028427918\n",
            "\n",
            "Global step: 1210,loss: 0.028474664\n",
            "\n",
            "Global step: 1211,loss: 0.03675937\n",
            "\n",
            "Global step: 1212,loss: 0.02901254\n",
            "\n",
            "Global step: 1213,loss: 0.01957558\n",
            "\n",
            "Global step: 1214,loss: 0.023234393\n",
            "\n",
            "Global step: 1215,loss: 0.031918548\n",
            "\n",
            "Global step: 1216,loss: 0.039741904\n",
            "\n",
            "Global step: 1217,loss: 0.018024035\n",
            "\n",
            "Global step: 1218,loss: 0.027340379\n",
            "\n",
            "Global step: 1219,loss: 0.021462245\n",
            "\n",
            "Global step: 1220,loss: 0.036895502\n",
            "\n",
            "Global step: 1221,loss: 0.03372209\n",
            "\n",
            "Global step: 1222,loss: 0.019640569\n",
            "\n",
            "Global step: 1223,loss: 0.025052104\n",
            "\n",
            "Global step: 1224,loss: 0.034611605\n",
            "\n",
            "Global step: 1225,loss: 0.027678438\n",
            "\n",
            "Global step: 1226,loss: 0.016743207\n",
            "\n",
            "Global step: 1227,loss: 0.02404312\n",
            "\n",
            "Global step: 1228,loss: 0.026199836\n",
            "\n",
            "Global step: 1229,loss: 0.022796806\n",
            "\n",
            "Global step: 1230,loss: 0.023003139\n",
            "\n",
            "Global step: 1231,loss: 0.027037796\n",
            "\n",
            "Global step: 1232,loss: 0.030747972\n",
            "\n",
            "Global step: 1233,loss: 0.03120668\n",
            "\n",
            "Global step: 1234,loss: 0.020065226\n",
            "\n",
            "Global step: 1235,loss: 0.01750008\n",
            "\n",
            "Global step: 1236,loss: 0.038736995\n",
            "\n",
            "Global step: 1237,loss: 0.030894738\n",
            "\n",
            "Global step: 1238,loss: 0.03118032\n",
            "\n",
            "Global step: 1239,loss: 0.019723566\n",
            "\n",
            "Global step: 1240,loss: 0.033779316\n",
            "\n",
            "Global step: 1241,loss: 0.021743651\n",
            "\n",
            "Global step: 1242,loss: 0.01946404\n",
            "\n",
            "Global step: 1243,loss: 0.025386183\n",
            "\n",
            "Global step: 1244,loss: 0.026316606\n",
            "\n",
            "Global step: 1245,loss: 0.027102876\n",
            "\n",
            "Global step: 1246,loss: 0.022366676\n",
            "\n",
            "Global step: 1247,loss: 0.03452706\n",
            "\n",
            "Global step: 1248,loss: 0.020230848\n",
            "\n",
            "Global step: 1249,loss: 0.021831853\n",
            "\n",
            "Global step: 1250,loss: 0.029000223\n",
            "\n",
            "Global step: 1251,loss: 0.019164314\n",
            "\n",
            "Global step: 1252,loss: 0.02510648\n",
            "\n",
            "Global step: 1253,loss: 0.023406208\n",
            "\n",
            "Global step: 1254,loss: 0.016738635\n",
            "\n",
            "Global step: 1255,loss: 0.025296226\n",
            "\n",
            "Global step: 1256,loss: 0.017962106\n",
            "\n",
            "Global step: 1257,loss: 0.0195769\n",
            "\n",
            "Global step: 1258,loss: 0.022923127\n",
            "\n",
            "Global step: 1259,loss: 0.01742009\n",
            "\n",
            "Global step: 1260,loss: 0.023900524\n",
            "\n",
            "Global step: 1261,loss: 0.019369226\n",
            "\n",
            "Global step: 1262,loss: 0.014768057\n",
            "\n",
            "Global step: 1263,loss: 0.01758049\n",
            "\n",
            "Global step: 1264,loss: 0.024499707\n",
            "\n",
            "Global step: 1265,loss: 0.02389168\n",
            "\n",
            "Global step: 1266,loss: 0.01764725\n",
            "\n",
            "Global step: 1267,loss: 0.021937072\n",
            "\n",
            "Global step: 1268,loss: 0.016697325\n",
            "\n",
            "Global step: 1269,loss: 0.015722252\n",
            "\n",
            "Global step: 1270,loss: 0.017416496\n",
            "\n",
            "Global step: 1271,loss: 0.020200953\n",
            "\n",
            "Global step: 1272,loss: 0.018789554\n",
            "\n",
            "Global step: 1273,loss: 0.024512738\n",
            "\n",
            "Global step: 1274,loss: 0.018117514\n",
            "\n",
            "Global step: 1275,loss: 0.022413494\n",
            "\n",
            "Global step: 1276,loss: 0.02342229\n",
            "\n",
            "Global step: 1277,loss: 0.019645065\n",
            "\n",
            "Global step: 1278,loss: 0.035750527\n",
            "\n",
            "Global step: 1279,loss: 0.024879988\n",
            "\n",
            "Global step: 1280,loss: 0.022231903\n",
            "\n",
            "Global step: 1281,loss: 0.021010209\n",
            "\n",
            "Global step: 1282,loss: 0.017581448\n",
            "\n",
            "Global step: 1283,loss: 0.020583967\n",
            "\n",
            "Global step: 1284,loss: 0.025887307\n",
            "\n",
            "Global step: 1285,loss: 0.014793813\n",
            "\n",
            "Global step: 1286,loss: 0.021811906\n",
            "\n",
            "Global step: 1287,loss: 0.024627313\n",
            "\n",
            "Global step: 1288,loss: 0.01721767\n",
            "\n",
            "Global step: 1289,loss: 0.022404227\n",
            "\n",
            "Global step: 1290,loss: 0.018457167\n",
            "\n",
            "Global step: 1291,loss: 0.027148988\n",
            "\n",
            "Global step: 1292,loss: 0.018928658\n",
            "\n",
            "Global step: 1293,loss: 0.02424813\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.81621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:01:20.613764 139635366639360 supervisor.py:1099] global_step/sec: 2.81621\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1294,loss: 0.02626137\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1295.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:01:20.801787 139637785016064 supervisor.py:1050] Recording summary at step 1295.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1295,loss: 0.024219353\n",
            "\n",
            "Global step: 1296,loss: 0.020594707\n",
            "\n",
            "Global step: 1297,loss: 0.018029453\n",
            "\n",
            "Global step: 1298,loss: 0.023629792\n",
            "\n",
            "Global step: 1299,loss: 0.024933118\n",
            "\n",
            "Global step: 1300,loss: 0.019704517\n",
            "\n",
            "Global step: 1301,loss: 0.016414762\n",
            "\n",
            "Global step: 1302,loss: 0.018867884\n",
            "\n",
            "Global step: 1303,loss: 0.020067416\n",
            "\n",
            "Global step: 1304,loss: 0.01943897\n",
            "\n",
            "Global step: 1305,loss: 0.017559115\n",
            "\n",
            "Global step: 1306,loss: 0.028987646\n",
            "\n",
            "Global step: 1307,loss: 0.03472773\n",
            "\n",
            "Global step: 1308,loss: 0.026070597\n",
            "\n",
            "Global step: 1309,loss: 0.018495351\n",
            "\n",
            "Global step: 1310,loss: 0.0295301\n",
            "\n",
            "Global step: 1311,loss: 0.029180568\n",
            "\n",
            "Global step: 1312,loss: 0.019462861\n",
            "\n",
            "Global step: 1313,loss: 0.023654968\n",
            "\n",
            "Global step: 1314,loss: 0.017895514\n",
            "\n",
            "Global step: 1315,loss: 0.02047092\n",
            "\n",
            "Global step: 1316,loss: 0.035018675\n",
            "\n",
            "Global step: 1317,loss: 0.029818688\n",
            "\n",
            "Global step: 1318,loss: 0.025041606\n",
            "\n",
            "Global step: 1319,loss: 0.0261482\n",
            "\n",
            "Global step: 1320,loss: 0.0329979\n",
            "\n",
            "Global step: 1321,loss: 0.026774652\n",
            "\n",
            "Global step: 1322,loss: 0.03205826\n",
            "\n",
            "Global step: 1323,loss: 0.022017527\n",
            "\n",
            "Global step: 1324,loss: 0.018664224\n",
            "\n",
            "Global step: 1325,loss: 0.020033348\n",
            "\n",
            "Global step: 1326,loss: 0.031384442\n",
            "\n",
            "Global step: 1327,loss: 0.028358709\n",
            "\n",
            "Global step: 1328,loss: 0.020476602\n",
            "\n",
            "Global step: 1329,loss: 0.020379417\n",
            "\n",
            "Global step: 1330,loss: 0.032365758\n",
            "\n",
            "Global step: 1331,loss: 0.023720328\n",
            "\n",
            "Global step: 1332,loss: 0.016394224\n",
            "\n",
            "Global step: 1333,loss: 0.0213118\n",
            "\n",
            "Global step: 1334,loss: 0.027664902\n",
            "\n",
            "Global step: 1335,loss: 0.02958078\n",
            "\n",
            "Global step: 1336,loss: 0.031068582\n",
            "\n",
            "Global step: 1337,loss: 0.022156391\n",
            "\n",
            "Global step: 1338,loss: 0.022973007\n",
            "\n",
            "Global step: 1339,loss: 0.02357833\n",
            "\n",
            "Global step: 1340,loss: 0.019764006\n",
            "\n",
            "Global step: 1341,loss: 0.023723673\n",
            "\n",
            "Global step: 1342,loss: 0.018549077\n",
            "\n",
            "Global step: 1343,loss: 0.022733327\n",
            "\n",
            "Global step: 1344,loss: 0.029801538\n",
            "\n",
            "Global step: 1345,loss: 0.027538259\n",
            "\n",
            "Global step: 1346,loss: 0.03345979\n",
            "\n",
            "Global step: 1347,loss: 0.020862725\n",
            "\n",
            "Global step: 1348,loss: 0.016248072\n",
            "\n",
            "Global step: 1349,loss: 0.024708245\n",
            "\n",
            "Global step: 1350,loss: 0.021232009\n",
            "\n",
            "Global step: 1351,loss: 0.0184427\n",
            "\n",
            "Global step: 1352,loss: 0.022975024\n",
            "\n",
            "Global step: 1353,loss: 0.01878462\n",
            "\n",
            "Global step: 1354,loss: 0.028830234\n",
            "\n",
            "Global step: 1355,loss: 0.026649922\n",
            "\n",
            "Global step: 1356,loss: 0.01595418\n",
            "\n",
            "Global step: 1357,loss: 0.019273318\n",
            "\n",
            "Global step: 1358,loss: 0.020949174\n",
            "\n",
            "Global step: 1359,loss: 0.029173983\n",
            "\n",
            "Global step: 1360,loss: 0.015134077\n",
            "\n",
            "Global step: 1361,loss: 0.02691548\n",
            "\n",
            "Global step: 1362,loss: 0.025207913\n",
            "\n",
            "Global step: 1363,loss: 0.017304134\n",
            "\n",
            "Global step: 1364,loss: 0.033618957\n",
            "\n",
            "Global step: 1365,loss: 0.018910892\n",
            "\n",
            "Global step: 1366,loss: 0.015496723\n",
            "\n",
            "Global step: 1367,loss: 0.01676377\n",
            "\n",
            "Global step: 1368,loss: 0.0148371365\n",
            "\n",
            "Global step: 1369,loss: 0.022863723\n",
            "\n",
            "Global step: 1370,loss: 0.019631915\n",
            "\n",
            "Global step: 1371,loss: 0.01586095\n",
            "\n",
            "Global step: 1372,loss: 0.024533512\n",
            "\n",
            "Global step: 1373,loss: 0.024322856\n",
            "\n",
            "Global step: 1374,loss: 0.018190835\n",
            "\n",
            "Global step: 1375,loss: 0.01838488\n",
            "\n",
            "Global step: 1376,loss: 0.01775687\n",
            "\n",
            "Global step: 1377,loss: 0.023286814\n",
            "\n",
            "Global step: 1378,loss: 0.01754265\n",
            "\n",
            "Global step: 1379,loss: 0.02049128\n",
            "\n",
            "Global step: 1380,loss: 0.025067154\n",
            "\n",
            "Global step: 1381,loss: 0.016728364\n",
            "\n",
            "Global step: 1382,loss: 0.023508716\n",
            "\n",
            "Global step: 1383,loss: 0.018830735\n",
            "\n",
            "Global step: 1384,loss: 0.016209532\n",
            "\n",
            "Global step: 1385,loss: 0.019196253\n",
            "\n",
            "Global step: 1386,loss: 0.02025599\n",
            "\n",
            "Global step: 1387,loss: 0.022615332\n",
            "\n",
            "Global step: 1388,loss: 0.02590749\n",
            "\n",
            "Global step: 1389,loss: 0.022633972\n",
            "\n",
            "Global step: 1390,loss: 0.020141162\n",
            "\n",
            "Global step: 1391,loss: 0.021889402\n",
            "\n",
            "Global step: 1392,loss: 0.020404896\n",
            "\n",
            "Global step: 1393,loss: 0.016234659\n",
            "\n",
            "Global step: 1394,loss: 0.01880376\n",
            "\n",
            "Global step: 1395,loss: 0.01878263\n",
            "\n",
            "Global step: 1396,loss: 0.018915894\n",
            "\n",
            "Global step: 1397,loss: 0.015512506\n",
            "\n",
            "Global step: 1398,loss: 0.020578016\n",
            "\n",
            "Global step: 1399,loss: 0.025699943\n",
            "\n",
            "Global step: 1400,loss: 0.020854414\n",
            "\n",
            "Global step: 1401,loss: 0.037517376\n",
            "\n",
            "Global step: 1402,loss: 0.031540938\n",
            "\n",
            "Global step: 1403,loss: 0.023238715\n",
            "\n",
            "Global step: 1404,loss: 0.024903242\n",
            "\n",
            "Global step: 1405,loss: 0.02141809\n",
            "\n",
            "Global step: 1406,loss: 0.03226848\n",
            "\n",
            "Global step: 1407,loss: 0.02567215\n",
            "\n",
            "Global step: 1408,loss: 0.019732472\n",
            "\n",
            "Global step: 1409,loss: 0.018276412\n",
            "\n",
            "Global step: 1410,loss: 0.01690207\n",
            "\n",
            "Global step: 1411,loss: 0.019708112\n",
            "\n",
            "Global step: 1412,loss: 0.019663667\n",
            "\n",
            "Global step: 1413,loss: 0.025239889\n",
            "\n",
            "Global step: 1414,loss: 0.024723079\n",
            "\n",
            "Global step: 1415,loss: 0.025969885\n",
            "\n",
            "Global step: 1416,loss: 0.024240348\n",
            "\n",
            "Global step: 1417,loss: 0.04177488\n",
            "\n",
            "Global step: 1418,loss: 0.020387085\n",
            "\n",
            "Global step: 1419,loss: 0.016670723\n",
            "\n",
            "Global step: 1420,loss: 0.02794487\n",
            "\n",
            "Global step: 1421,loss: 0.018444948\n",
            "\n",
            "Global step: 1422,loss: 0.031888228\n",
            "\n",
            "Global step: 1423,loss: 0.02185421\n",
            "\n",
            "Global step: 1424,loss: 0.018111609\n",
            "\n",
            "Global step: 1425,loss: 0.016847555\n",
            "\n",
            "Global step: 1426,loss: 0.019226339\n",
            "\n",
            "Global step: 1427,loss: 0.024650376\n",
            "\n",
            "Global step: 1428,loss: 0.026743874\n",
            "\n",
            "Global step: 1429,loss: 0.017465262\n",
            "\n",
            "Global step: 1430,loss: 0.016613552\n",
            "\n",
            "Global step: 1431,loss: 0.016676694\n",
            "\n",
            "Global step: 1432,loss: 0.03143995\n",
            "\n",
            "Global step: 1433,loss: 0.027034447\n",
            "\n",
            "Global step: 1434,loss: 0.015176619\n",
            "\n",
            "Global step: 1435,loss: 0.018265376\n",
            "\n",
            "Global step: 1436,loss: 0.028579803\n",
            "\n",
            "Global step: 1437,loss: 0.026308727\n",
            "\n",
            "Global step: 1438,loss: 0.02193011\n",
            "\n",
            "Global step: 1439,loss: 0.031525433\n",
            "\n",
            "Global step: 1440,loss: 0.017485373\n",
            "\n",
            "Global step: 1441,loss: 0.021724008\n",
            "\n",
            "Global step: 1442,loss: 0.019570727\n",
            "\n",
            "Global step: 1443,loss: 0.021259746\n",
            "\n",
            "Global step: 1444,loss: 0.022837648\n",
            "\n",
            "Global step: 1445,loss: 0.024276074\n",
            "\n",
            "Global step: 1446,loss: 0.016285216\n",
            "\n",
            "Global step: 1447,loss: 0.023279127\n",
            "\n",
            "Global step: 1448,loss: 0.0290712\n",
            "\n",
            "Global step: 1449,loss: 0.02130132\n",
            "\n",
            "Global step: 1450,loss: 0.020768557\n",
            "\n",
            "Global step: 1451,loss: 0.03550161\n",
            "\n",
            "Global step: 1452,loss: 0.019800438\n",
            "\n",
            "Global step: 1453,loss: 0.019553836\n",
            "\n",
            "Global step: 1454,loss: 0.02102143\n",
            "\n",
            "Global step: 1455,loss: 0.027016025\n",
            "\n",
            "Global step: 1456,loss: 0.024231426\n",
            "\n",
            "Global step: 1457,loss: 0.028949749\n",
            "\n",
            "Global step: 1458,loss: 0.016334837\n",
            "\n",
            "Global step: 1459,loss: 0.025720464\n",
            "\n",
            "Global step: 1460,loss: 0.021056708\n",
            "\n",
            "Global step: 1461,loss: 0.016811507\n",
            "\n",
            "Global step: 1462,loss: 0.023819426\n",
            "\n",
            "Global step: 1463,loss: 0.0314757\n",
            "\n",
            "Global step: 1464,loss: 0.025922112\n",
            "\n",
            "Global step: 1465,loss: 0.018704461\n",
            "\n",
            "Global step: 1466,loss: 0.029182408\n",
            "\n",
            "Global step: 1467,loss: 0.017439386\n",
            "\n",
            "Global step: 1468,loss: 0.016646428\n",
            "\n",
            "Global step: 1469,loss: 0.021850694\n",
            "\n",
            "Global step: 1470,loss: 0.018548634\n",
            "\n",
            "Global step: 1471,loss: 0.026847525\n",
            "\n",
            "Global step: 1472,loss: 0.01725825\n",
            "\n",
            "Global step: 1473,loss: 0.017951299\n",
            "\n",
            "Global step: 1474,loss: 0.020959932\n",
            "\n",
            "Global step: 1475,loss: 0.020060698\n",
            "\n",
            "Global step: 1476,loss: 0.021955982\n",
            "\n",
            "Global step: 1477,loss: 0.015772052\n",
            "\n",
            "Global step: 1478,loss: 0.018137626\n",
            "\n",
            "Global step: 1479,loss: 0.018834759\n",
            "\n",
            "Global step: 1480,loss: 0.02321516\n",
            "\n",
            "Global step: 1481,loss: 0.02532442\n",
            "\n",
            "Global step: 1482,loss: 0.020342624\n",
            "\n",
            "Global step: 1483,loss: 0.019631527\n",
            "\n",
            "Global step: 1484,loss: 0.017589137\n",
            "\n",
            "Global step: 1485,loss: 0.02900239\n",
            "\n",
            "Global step: 1486,loss: 0.030788656\n",
            "\n",
            "Global step: 1487,loss: 0.023343552\n",
            "\n",
            "Global step: 1488,loss: 0.019648861\n",
            "\n",
            "Global step: 1489,loss: 0.017848719\n",
            "\n",
            "Global step: 1490,loss: 0.019700713\n",
            "\n",
            "Global step: 1491,loss: 0.020067368\n",
            "\n",
            "Global step: 1492,loss: 0.020775678\n",
            "\n",
            "Global step: 1493,loss: 0.016897876\n",
            "\n",
            "Global step: 1494,loss: 0.02331743\n",
            "\n",
            "Global step: 1495,loss: 0.020825146\n",
            "\n",
            "Global step: 1496,loss: 0.02551279\n",
            "\n",
            "Global step: 1497,loss: 0.024705902\n",
            "\n",
            "Global step: 1498,loss: 0.021786269\n",
            "\n",
            "Global step: 1499,loss: 0.018112293\n",
            "\n",
            "Global step: 1500,loss: 0.025997726\n",
            "\n",
            "Global step: 1501,loss: 0.015442626\n",
            "\n",
            "Global step: 1502,loss: 0.021865185\n",
            "\n",
            "Global step: 1503,loss: 0.024549283\n",
            "\n",
            "Global step: 1504,loss: 0.01891705\n",
            "\n",
            "Global step: 1505,loss: 0.025601149\n",
            "\n",
            "Global step: 1506,loss: 0.019331312\n",
            "\n",
            "Global step: 1507,loss: 0.0173649\n",
            "\n",
            "Global step: 1508,loss: 0.022659987\n",
            "\n",
            "Global step: 1509,loss: 0.025660245\n",
            "\n",
            "Global step: 1510,loss: 0.025565032\n",
            "\n",
            "Global step: 1511,loss: 0.018074349\n",
            "\n",
            "Global step: 1512,loss: 0.020560943\n",
            "\n",
            "Global step: 1513,loss: 0.02728125\n",
            "\n",
            "Global step: 1514,loss: 0.016015327\n",
            "\n",
            "Global step: 1515,loss: 0.023406316\n",
            "\n",
            "Global step: 1516,loss: 0.016217172\n",
            "\n",
            "Global step: 1517,loss: 0.016761875\n",
            "\n",
            "Global step: 1518,loss: 0.020614719\n",
            "\n",
            "Global step: 1519,loss: 0.02229574\n",
            "\n",
            "Global step: 1520,loss: 0.019770864\n",
            "\n",
            "Global step: 1521,loss: 0.01835224\n",
            "\n",
            "Global step: 1522,loss: 0.015844353\n",
            "\n",
            "Global step: 1523,loss: 0.015712446\n",
            "\n",
            "Global step: 1524,loss: 0.018889785\n",
            "\n",
            "Global step: 1525,loss: 0.03384451\n",
            "\n",
            "Global step: 1526,loss: 0.023304349\n",
            "\n",
            "Global step: 1527,loss: 0.036178153\n",
            "\n",
            "Global step: 1528,loss: 0.017284047\n",
            "\n",
            "Global step: 1529,loss: 0.019948274\n",
            "\n",
            "Global step: 1530,loss: 0.021526445\n",
            "\n",
            "Global step: 1531,loss: 0.020422447\n",
            "\n",
            "Global step: 1532,loss: 0.015963813\n",
            "\n",
            "Global step: 1533,loss: 0.0211113\n",
            "\n",
            "Global step: 1534,loss: 0.014459938\n",
            "\n",
            "Global step: 1535,loss: 0.028402142\n",
            "\n",
            "Global step: 1536,loss: 0.025666807\n",
            "\n",
            "Global step: 1537,loss: 0.020655273\n",
            "\n",
            "Global step: 1538,loss: 0.021443669\n",
            "\n",
            "Global step: 1539,loss: 0.017921297\n",
            "\n",
            "Global step: 1540,loss: 0.024798706\n",
            "\n",
            "Global step: 1541,loss: 0.01885181\n",
            "\n",
            "Global step: 1542,loss: 0.019845348\n",
            "\n",
            "Global step: 1543,loss: 0.024202047\n",
            "\n",
            "Global step: 1544,loss: 0.020398503\n",
            "\n",
            "Global step: 1545,loss: 0.018089812\n",
            "\n",
            "Global step: 1546,loss: 0.02227576\n",
            "\n",
            "Global step: 1547,loss: 0.017126802\n",
            "\n",
            "Global step: 1548,loss: 0.022201547\n",
            "\n",
            "Global step: 1549,loss: 0.019074261\n",
            "\n",
            "Global step: 1550,loss: 0.016579576\n",
            "\n",
            "Global step: 1551,loss: 0.022695411\n",
            "\n",
            "Global step: 1552,loss: 0.016882874\n",
            "\n",
            "Global step: 1553,loss: 0.02245989\n",
            "\n",
            "Global step: 1554,loss: 0.01309057\n",
            "\n",
            "Global step: 1555,loss: 0.028462354\n",
            "\n",
            "Global step: 1556,loss: 0.013502608\n",
            "\n",
            "Global step: 1557,loss: 0.019856378\n",
            "\n",
            "Global step: 1558,loss: 0.01709354\n",
            "\n",
            "Global step: 1559,loss: 0.014392398\n",
            "\n",
            "Global step: 1560,loss: 0.01277875\n",
            "\n",
            "Global step: 1561,loss: 0.017753042\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1561,Val_Loss: 0.018556307499798443,  Val_acc: 0.99609375 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 16:03:07.269699 139641102628736 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/20:\n",
            "Global step: 1562,loss: 0.013652119\n",
            "\n",
            "Global step: 1563,loss: 0.020529278\n",
            "\n",
            "Global step: 1564,loss: 0.01436769\n",
            "\n",
            "Global step: 1565,loss: 0.016902767\n",
            "\n",
            "Global step: 1566,loss: 0.015406682\n",
            "\n",
            "Global step: 1567,loss: 0.014157138\n",
            "\n",
            "Global step: 1568,loss: 0.018118693\n",
            "\n",
            "Global step: 1569,loss: 0.022064667\n",
            "\n",
            "Global step: 1570,loss: 0.023896994\n",
            "\n",
            "Global step: 1571,loss: 0.021576159\n",
            "\n",
            "Global step: 1572,loss: 0.019047208\n",
            "\n",
            "Global step: 1573,loss: 0.021636946\n",
            "\n",
            "Global step: 1574,loss: 0.015704155\n",
            "\n",
            "Global step: 1575,loss: 0.016518341\n",
            "\n",
            "Global step: 1576,loss: 0.015794218\n",
            "\n",
            "Global step: 1577,loss: 0.016859232\n",
            "\n",
            "Global step: 1578,loss: 0.017270092\n",
            "\n",
            "Global step: 1579,loss: 0.01625244\n",
            "\n",
            "Global step: 1580,loss: 0.014245239\n",
            "\n",
            "Global step: 1581,loss: 0.021388527\n",
            "\n",
            "Global step: 1582,loss: 0.01572426\n",
            "\n",
            "Global step: 1583,loss: 0.021822708\n",
            "\n",
            "Global step: 1584,loss: 0.032971717\n",
            "\n",
            "Global step: 1585,loss: 0.013865121\n",
            "\n",
            "Global step: 1586,loss: 0.020787008\n",
            "\n",
            "Global step: 1587,loss: 0.014242683\n",
            "\n",
            "Global step: 1588,loss: 0.020898798\n",
            "\n",
            "Global step: 1589,loss: 0.015994161\n",
            "\n",
            "Global step: 1590,loss: 0.025969982\n",
            "\n",
            "Global step: 1591,loss: 0.017197035\n",
            "\n",
            "Global step: 1592,loss: 0.018869491\n",
            "\n",
            "Global step: 1593,loss: 0.029913824\n",
            "\n",
            "Global step: 1594,loss: 0.017006012\n",
            "\n",
            "Global step: 1595,loss: 0.028531292\n",
            "\n",
            "Global step: 1596,loss: 0.017990401\n",
            "\n",
            "Global step: 1597,loss: 0.015483253\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.5409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:03:20.649733 139635366639360 supervisor.py:1099] global_step/sec: 2.5409\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1598,loss: 0.013532282\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1599.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:03:20.815150 139637785016064 supervisor.py:1050] Recording summary at step 1599.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1599,loss: 0.013552509\n",
            "\n",
            "Global step: 1600,loss: 0.021288889\n",
            "\n",
            "Global step: 1601,loss: 0.01611643\n",
            "\n",
            "Global step: 1602,loss: 0.020955212\n",
            "\n",
            "Global step: 1603,loss: 0.022245586\n",
            "\n",
            "Global step: 1604,loss: 0.02220802\n",
            "\n",
            "Global step: 1605,loss: 0.017129809\n",
            "\n",
            "Global step: 1606,loss: 0.013729634\n",
            "\n",
            "Global step: 1607,loss: 0.019129869\n",
            "\n",
            "Global step: 1608,loss: 0.013370683\n",
            "\n",
            "Global step: 1609,loss: 0.017195126\n",
            "\n",
            "Global step: 1610,loss: 0.015320448\n",
            "\n",
            "Global step: 1611,loss: 0.01582389\n",
            "\n",
            "Global step: 1612,loss: 0.01912581\n",
            "\n",
            "Global step: 1613,loss: 0.023641396\n",
            "\n",
            "Global step: 1614,loss: 0.01931567\n",
            "\n",
            "Global step: 1615,loss: 0.02064491\n",
            "\n",
            "Global step: 1616,loss: 0.018937754\n",
            "\n",
            "Global step: 1617,loss: 0.021008806\n",
            "\n",
            "Global step: 1618,loss: 0.01625929\n",
            "\n",
            "Global step: 1619,loss: 0.017033163\n",
            "\n",
            "Global step: 1620,loss: 0.017861625\n",
            "\n",
            "Global step: 1621,loss: 0.017242406\n",
            "\n",
            "Global step: 1622,loss: 0.014419533\n",
            "\n",
            "Global step: 1623,loss: 0.02138047\n",
            "\n",
            "Global step: 1624,loss: 0.024940293\n",
            "\n",
            "Global step: 1625,loss: 0.01743623\n",
            "\n",
            "Global step: 1626,loss: 0.020711157\n",
            "\n",
            "Global step: 1627,loss: 0.020566206\n",
            "\n",
            "Global step: 1628,loss: 0.017459124\n",
            "\n",
            "Global step: 1629,loss: 0.014482234\n",
            "\n",
            "Global step: 1630,loss: 0.017086182\n",
            "\n",
            "Global step: 1631,loss: 0.027853407\n",
            "\n",
            "Global step: 1632,loss: 0.013792892\n",
            "\n",
            "Global step: 1633,loss: 0.030845102\n",
            "\n",
            "Global step: 1634,loss: 0.017120508\n",
            "\n",
            "Global step: 1635,loss: 0.015752137\n",
            "\n",
            "Global step: 1636,loss: 0.017052054\n",
            "\n",
            "Global step: 1637,loss: 0.015760709\n",
            "\n",
            "Global step: 1638,loss: 0.013959171\n",
            "\n",
            "Global step: 1639,loss: 0.018425768\n",
            "\n",
            "Global step: 1640,loss: 0.017989231\n",
            "\n",
            "Global step: 1641,loss: 0.018130466\n",
            "\n",
            "Global step: 1642,loss: 0.022943052\n",
            "\n",
            "Global step: 1643,loss: 0.017274292\n",
            "\n",
            "Global step: 1644,loss: 0.025943723\n",
            "\n",
            "Global step: 1645,loss: 0.0130224135\n",
            "\n",
            "Global step: 1646,loss: 0.02044087\n",
            "\n",
            "Global step: 1647,loss: 0.01649474\n",
            "\n",
            "Global step: 1648,loss: 0.020143468\n",
            "\n",
            "Global step: 1649,loss: 0.014752966\n",
            "\n",
            "Global step: 1650,loss: 0.028291252\n",
            "\n",
            "Global step: 1651,loss: 0.015172684\n",
            "\n",
            "Global step: 1652,loss: 0.022124566\n",
            "\n",
            "Global step: 1653,loss: 0.021197107\n",
            "\n",
            "Global step: 1654,loss: 0.017403942\n",
            "\n",
            "Global step: 1655,loss: 0.025401998\n",
            "\n",
            "Global step: 1656,loss: 0.02112506\n",
            "\n",
            "Global step: 1657,loss: 0.016437974\n",
            "\n",
            "Global step: 1658,loss: 0.013673543\n",
            "\n",
            "Global step: 1659,loss: 0.024336781\n",
            "\n",
            "Global step: 1660,loss: 0.014365543\n",
            "\n",
            "Global step: 1661,loss: 0.016720375\n",
            "\n",
            "Global step: 1662,loss: 0.01794686\n",
            "\n",
            "Global step: 1663,loss: 0.020823918\n",
            "\n",
            "Global step: 1664,loss: 0.016262447\n",
            "\n",
            "Global step: 1665,loss: 0.016604986\n",
            "\n",
            "Global step: 1666,loss: 0.022663996\n",
            "\n",
            "Global step: 1667,loss: 0.019678252\n",
            "\n",
            "Global step: 1668,loss: 0.017943406\n",
            "\n",
            "Global step: 1669,loss: 0.02657888\n",
            "\n",
            "Global step: 1670,loss: 0.026259597\n",
            "\n",
            "Global step: 1671,loss: 0.016076438\n",
            "\n",
            "Global step: 1672,loss: 0.019364875\n",
            "\n",
            "Global step: 1673,loss: 0.01763049\n",
            "\n",
            "Global step: 1674,loss: 0.01649078\n",
            "\n",
            "Global step: 1675,loss: 0.024291538\n",
            "\n",
            "Global step: 1676,loss: 0.01736286\n",
            "\n",
            "Global step: 1677,loss: 0.015656183\n",
            "\n",
            "Global step: 1678,loss: 0.023948878\n",
            "\n",
            "Global step: 1679,loss: 0.019492533\n",
            "\n",
            "Global step: 1680,loss: 0.014252173\n",
            "\n",
            "Global step: 1681,loss: 0.014301054\n",
            "\n",
            "Global step: 1682,loss: 0.022673633\n",
            "\n",
            "Global step: 1683,loss: 0.01552837\n",
            "\n",
            "Global step: 1684,loss: 0.015478499\n",
            "\n",
            "Global step: 1685,loss: 0.016348777\n",
            "\n",
            "Global step: 1686,loss: 0.033791095\n",
            "\n",
            "Global step: 1687,loss: 0.017623626\n",
            "\n",
            "Global step: 1688,loss: 0.019890396\n",
            "\n",
            "Global step: 1689,loss: 0.014568856\n",
            "\n",
            "Global step: 1690,loss: 0.020372968\n",
            "\n",
            "Global step: 1691,loss: 0.015434623\n",
            "\n",
            "Global step: 1692,loss: 0.017169412\n",
            "\n",
            "Global step: 1693,loss: 0.018497005\n",
            "\n",
            "Global step: 1694,loss: 0.024072967\n",
            "\n",
            "Global step: 1695,loss: 0.021467518\n",
            "\n",
            "Global step: 1696,loss: 0.017419042\n",
            "\n",
            "Global step: 1697,loss: 0.016857568\n",
            "\n",
            "Global step: 1698,loss: 0.01697721\n",
            "\n",
            "Global step: 1699,loss: 0.013826645\n",
            "\n",
            "Global step: 1700,loss: 0.016588653\n",
            "\n",
            "Global step: 1701,loss: 0.016989466\n",
            "\n",
            "Global step: 1702,loss: 0.014047212\n",
            "\n",
            "Global step: 1703,loss: 0.019473389\n",
            "\n",
            "Global step: 1704,loss: 0.021272037\n",
            "\n",
            "Global step: 1705,loss: 0.016110703\n",
            "\n",
            "Global step: 1706,loss: 0.015489516\n",
            "\n",
            "Global step: 1707,loss: 0.018346097\n",
            "\n",
            "Global step: 1708,loss: 0.016208136\n",
            "\n",
            "Global step: 1709,loss: 0.013694368\n",
            "\n",
            "Global step: 1710,loss: 0.02459255\n",
            "\n",
            "Global step: 1711,loss: 0.014443237\n",
            "\n",
            "Global step: 1712,loss: 0.017382078\n",
            "\n",
            "Global step: 1713,loss: 0.018675955\n",
            "\n",
            "Global step: 1714,loss: 0.015538488\n",
            "\n",
            "Global step: 1715,loss: 0.01575465\n",
            "\n",
            "Global step: 1716,loss: 0.024757225\n",
            "\n",
            "Global step: 1717,loss: 0.0149101475\n",
            "\n",
            "Global step: 1718,loss: 0.018054172\n",
            "\n",
            "Global step: 1719,loss: 0.015256379\n",
            "\n",
            "Global step: 1720,loss: 0.016882906\n",
            "\n",
            "Global step: 1721,loss: 0.025971498\n",
            "\n",
            "Global step: 1722,loss: 0.015405487\n",
            "\n",
            "Global step: 1723,loss: 0.014439959\n",
            "\n",
            "Global step: 1724,loss: 0.020003434\n",
            "\n",
            "Global step: 1725,loss: 0.015493834\n",
            "\n",
            "Global step: 1726,loss: 0.017952263\n",
            "\n",
            "Global step: 1727,loss: 0.027946085\n",
            "\n",
            "Global step: 1728,loss: 0.018143324\n",
            "\n",
            "Global step: 1729,loss: 0.020281857\n",
            "\n",
            "Global step: 1730,loss: 0.024024803\n",
            "\n",
            "Global step: 1731,loss: 0.03000236\n",
            "\n",
            "Global step: 1732,loss: 0.020286875\n",
            "\n",
            "Global step: 1733,loss: 0.015113054\n",
            "\n",
            "Global step: 1734,loss: 0.021041311\n",
            "\n",
            "Global step: 1735,loss: 0.013755057\n",
            "\n",
            "Global step: 1736,loss: 0.021855671\n",
            "\n",
            "Global step: 1737,loss: 0.016786952\n",
            "\n",
            "Global step: 1738,loss: 0.019208912\n",
            "\n",
            "Global step: 1739,loss: 0.017213631\n",
            "\n",
            "Global step: 1740,loss: 0.015221907\n",
            "\n",
            "Global step: 1741,loss: 0.014059332\n",
            "\n",
            "Global step: 1742,loss: 0.014954331\n",
            "\n",
            "Global step: 1743,loss: 0.020833848\n",
            "\n",
            "Global step: 1744,loss: 0.020189568\n",
            "\n",
            "Global step: 1745,loss: 0.019443933\n",
            "\n",
            "Global step: 1746,loss: 0.016217237\n",
            "\n",
            "Global step: 1747,loss: 0.023925293\n",
            "\n",
            "Global step: 1748,loss: 0.021842014\n",
            "\n",
            "Global step: 1749,loss: 0.014847137\n",
            "\n",
            "Global step: 1750,loss: 0.016030561\n",
            "\n",
            "Global step: 1751,loss: 0.014593273\n",
            "\n",
            "Global step: 1752,loss: 0.015871571\n",
            "\n",
            "Global step: 1753,loss: 0.016426751\n",
            "\n",
            "Global step: 1754,loss: 0.01865244\n",
            "\n",
            "Global step: 1755,loss: 0.014368426\n",
            "\n",
            "Global step: 1756,loss: 0.0245446\n",
            "\n",
            "Global step: 1757,loss: 0.01539989\n",
            "\n",
            "Global step: 1758,loss: 0.017831486\n",
            "\n",
            "Global step: 1759,loss: 0.020132612\n",
            "\n",
            "Global step: 1760,loss: 0.01921853\n",
            "\n",
            "Global step: 1761,loss: 0.019030564\n",
            "\n",
            "Global step: 1762,loss: 0.015472356\n",
            "\n",
            "Global step: 1763,loss: 0.015619383\n",
            "\n",
            "Global step: 1764,loss: 0.015539816\n",
            "\n",
            "Global step: 1765,loss: 0.02263043\n",
            "\n",
            "Global step: 1766,loss: 0.017856255\n",
            "\n",
            "Global step: 1767,loss: 0.019333227\n",
            "\n",
            "Global step: 1768,loss: 0.020028243\n",
            "\n",
            "Global step: 1769,loss: 0.015225445\n",
            "\n",
            "Global step: 1770,loss: 0.015792126\n",
            "\n",
            "Global step: 1771,loss: 0.029068723\n",
            "\n",
            "Global step: 1772,loss: 0.019979404\n",
            "\n",
            "Global step: 1773,loss: 0.01606772\n",
            "\n",
            "Global step: 1774,loss: 0.015404538\n",
            "\n",
            "Global step: 1775,loss: 0.018165216\n",
            "\n",
            "Global step: 1776,loss: 0.016436215\n",
            "\n",
            "Global step: 1777,loss: 0.016134901\n",
            "\n",
            "Global step: 1778,loss: 0.019606002\n",
            "\n",
            "Global step: 1779,loss: 0.017218228\n",
            "\n",
            "Global step: 1780,loss: 0.014459103\n",
            "\n",
            "Global step: 1781,loss: 0.017224368\n",
            "\n",
            "Global step: 1782,loss: 0.016853979\n",
            "\n",
            "Global step: 1783,loss: 0.021631038\n",
            "\n",
            "Global step: 1784,loss: 0.01441749\n",
            "\n",
            "Global step: 1785,loss: 0.016258689\n",
            "\n",
            "Global step: 1786,loss: 0.020547535\n",
            "\n",
            "Global step: 1787,loss: 0.015233904\n",
            "\n",
            "Global step: 1788,loss: 0.015648982\n",
            "\n",
            "Global step: 1789,loss: 0.017601456\n",
            "\n",
            "Global step: 1790,loss: 0.014672645\n",
            "\n",
            "Global step: 1791,loss: 0.02476646\n",
            "\n",
            "Global step: 1792,loss: 0.018816054\n",
            "\n",
            "Global step: 1793,loss: 0.015800927\n",
            "\n",
            "Global step: 1794,loss: 0.01426118\n",
            "\n",
            "Global step: 1795,loss: 0.013855051\n",
            "\n",
            "Global step: 1796,loss: 0.017352253\n",
            "\n",
            "Global step: 1797,loss: 0.013648092\n",
            "\n",
            "Global step: 1798,loss: 0.025634026\n",
            "\n",
            "Global step: 1799,loss: 0.0196319\n",
            "\n",
            "Global step: 1800,loss: 0.01660061\n",
            "\n",
            "Global step: 1801,loss: 0.014428965\n",
            "\n",
            "Global step: 1802,loss: 0.014493366\n",
            "\n",
            "Global step: 1803,loss: 0.014023665\n",
            "\n",
            "Global step: 1804,loss: 0.014568819\n",
            "\n",
            "Global step: 1805,loss: 0.017538399\n",
            "\n",
            "Global step: 1806,loss: 0.020131331\n",
            "\n",
            "Global step: 1807,loss: 0.022648081\n",
            "\n",
            "Global step: 1808,loss: 0.015740316\n",
            "\n",
            "Global step: 1809,loss: 0.016942935\n",
            "\n",
            "Global step: 1810,loss: 0.025946543\n",
            "\n",
            "Global step: 1811,loss: 0.018424062\n",
            "\n",
            "Global step: 1812,loss: 0.019015996\n",
            "\n",
            "Global step: 1813,loss: 0.016571686\n",
            "\n",
            "Global step: 1814,loss: 0.022931367\n",
            "\n",
            "Global step: 1815,loss: 0.02280182\n",
            "\n",
            "Global step: 1816,loss: 0.017924843\n",
            "\n",
            "Global step: 1817,loss: 0.014990133\n",
            "\n",
            "Global step: 1818,loss: 0.022370623\n",
            "\n",
            "Global step: 1819,loss: 0.015902113\n",
            "\n",
            "Global step: 1820,loss: 0.023406133\n",
            "\n",
            "Global step: 1821,loss: 0.026202202\n",
            "\n",
            "Global step: 1822,loss: 0.016923433\n",
            "\n",
            "Global step: 1823,loss: 0.013056838\n",
            "\n",
            "Global step: 1824,loss: 0.018395426\n",
            "\n",
            "Global step: 1825,loss: 0.021906953\n",
            "\n",
            "Global step: 1826,loss: 0.017428135\n",
            "\n",
            "Global step: 1827,loss: 0.01633503\n",
            "\n",
            "Global step: 1828,loss: 0.0152194835\n",
            "\n",
            "Global step: 1829,loss: 0.016024644\n",
            "\n",
            "Global step: 1830,loss: 0.020622894\n",
            "\n",
            "Global step: 1831,loss: 0.017285356\n",
            "\n",
            "Global step: 1832,loss: 0.013529431\n",
            "\n",
            "Global step: 1833,loss: 0.015945382\n",
            "\n",
            "Global step: 1834,loss: 0.015703335\n",
            "\n",
            "Global step: 1835,loss: 0.019360736\n",
            "\n",
            "Global step: 1836,loss: 0.013061589\n",
            "\n",
            "Global step: 1837,loss: 0.01457906\n",
            "\n",
            "Global step: 1838,loss: 0.017104283\n",
            "\n",
            "Global step: 1839,loss: 0.015126942\n",
            "\n",
            "Global step: 1840,loss: 0.027778357\n",
            "\n",
            "Global step: 1841,loss: 0.016212873\n",
            "\n",
            "Global step: 1842,loss: 0.016258236\n",
            "\n",
            "Global step: 1843,loss: 0.015841093\n",
            "\n",
            "Global step: 1844,loss: 0.014590363\n",
            "\n",
            "Global step: 1845,loss: 0.020111535\n",
            "\n",
            "Global step: 1846,loss: 0.015294251\n",
            "\n",
            "Global step: 1847,loss: 0.014080901\n",
            "\n",
            "Global step: 1848,loss: 0.015451525\n",
            "\n",
            "Global step: 1849,loss: 0.020716308\n",
            "\n",
            "Global step: 1850,loss: 0.014360611\n",
            "\n",
            "Global step: 1851,loss: 0.024430327\n",
            "\n",
            "Global step: 1852,loss: 0.014365314\n",
            "\n",
            "Global step: 1853,loss: 0.019741876\n",
            "\n",
            "Global step: 1854,loss: 0.014336193\n",
            "\n",
            "Global step: 1855,loss: 0.018751044\n",
            "\n",
            "Global step: 1856,loss: 0.014564657\n",
            "\n",
            "Global step: 1857,loss: 0.01516456\n",
            "\n",
            "Global step: 1858,loss: 0.016142214\n",
            "\n",
            "Global step: 1859,loss: 0.014494002\n",
            "\n",
            "Global step: 1860,loss: 0.03466535\n",
            "\n",
            "Global step: 1861,loss: 0.016407564\n",
            "\n",
            "Global step: 1862,loss: 0.0175062\n",
            "\n",
            "Global step: 1863,loss: 0.014487609\n",
            "\n",
            "Global step: 1864,loss: 0.022347592\n",
            "\n",
            "Global step: 1865,loss: 0.0224624\n",
            "\n",
            "Global step: 1866,loss: 0.013427089\n",
            "\n",
            "Global step: 1867,loss: 0.014257068\n",
            "\n",
            "Global step: 1868,loss: 0.026210507\n",
            "\n",
            "Global step: 1869,loss: 0.020076912\n",
            "\n",
            "Global step: 1870,loss: 0.018621635\n",
            "\n",
            "Global step: 1871,loss: 0.02004742\n",
            "\n",
            "Global step: 1872,loss: 0.018131165\n",
            "\n",
            "Global step: 1873,loss: 0.018180456\n",
            "\n",
            "Global step: 1874,loss: 0.019456979\n",
            "\n",
            "Global step: 1875,loss: 0.014970653\n",
            "\n",
            "Global step: 1876,loss: 0.023994178\n",
            "\n",
            "Global step: 1877,loss: 0.021985078\n",
            "\n",
            "Global step: 1878,loss: 0.01332673\n",
            "\n",
            "Global step: 1879,loss: 0.015775822\n",
            "\n",
            "Global step: 1880,loss: 0.020032702\n",
            "\n",
            "Global step: 1881,loss: 0.01754557\n",
            "\n",
            "Global step: 1882,loss: 0.012815662\n",
            "\n",
            "Global step: 1883,loss: 0.016642323\n",
            "\n",
            "Global step: 1884,loss: 0.012784613\n",
            "\n",
            "Global step: 1885,loss: 0.016944773\n",
            "\n",
            "Global step: 1886,loss: 0.012885503\n",
            "\n",
            "Global step: 1887,loss: 0.01920475\n",
            "\n",
            "Global step: 1888,loss: 0.017406125\n",
            "\n",
            "Global step: 1889,loss: 0.016198244\n",
            "\n",
            "Global step: 1890,loss: 0.013884131\n",
            "\n",
            "Global step: 1891,loss: 0.016398326\n",
            "\n",
            "Global step: 1892,loss: 0.014072423\n",
            "\n",
            "Global step: 1893,loss: 0.01899521\n",
            "\n",
            "Global step: 1894,loss: 0.013875417\n",
            "\n",
            "Global step: 1895,loss: 0.023526128\n",
            "\n",
            "Global step: 1896,loss: 0.013039463\n",
            "\n",
            "Global step: 1897,loss: 0.021862619\n",
            "\n",
            "Global step: 1898,loss: 0.01770182\n",
            "\n",
            "Global step: 1899,loss: 0.0205868\n",
            "\n",
            "Global step: 1900,loss: 0.018824546\n",
            "\n",
            "Global step: 1901,loss: 0.014908532\n",
            "\n",
            "Global step: 1902,loss: 0.01658691\n",
            "\n",
            "Global step: 1903,loss: 0.014098271\n",
            "\n",
            "Global step: 1904,loss: 0.016191967\n",
            "\n",
            "Global step: 1905,loss: 0.018980617\n",
            "\n",
            "Global step: 1906,loss: 0.019722225\n",
            "\n",
            "Global step: 1907,loss: 0.022874955\n",
            "\n",
            "Global step: 1908,loss: 0.014548335\n",
            "\n",
            "Global step: 1909,loss: 0.016228152\n",
            "\n",
            "Global step: 1910,loss: 0.021423386\n",
            "\n",
            "Global step: 1911,loss: 0.013844905\n",
            "\n",
            "Global step: 1912,loss: 0.017906442\n",
            "\n",
            "Global step: 1913,loss: 0.026013382\n",
            "\n",
            "Global step: 1914,loss: 0.012671085\n",
            "\n",
            "Global step: 1915,loss: 0.02339188\n",
            "\n",
            "Global step: 1916,loss: 0.015331851\n",
            "\n",
            "Global step: 1917,loss: 0.016520392\n",
            "\n",
            "Global step: 1918,loss: 0.022801694\n",
            "\n",
            "Global step: 1919,loss: 0.020302765\n",
            "\n",
            "Global step: 1920,loss: 0.023527795\n",
            "\n",
            "Global step: 1921,loss: 0.019667123\n",
            "\n",
            "Global step: 1922,loss: 0.01718019\n",
            "\n",
            "Global step: 1923,loss: 0.018812599\n",
            "\n",
            "Global step: 1924,loss: 0.014190532\n",
            "\n",
            "Global step: 1925,loss: 0.0136918835\n",
            "\n",
            "Global step: 1926,loss: 0.017785288\n",
            "\n",
            "Global step: 1927,loss: 0.01948019\n",
            "\n",
            "Global step: 1928,loss: 0.019793648\n",
            "\n",
            "Global step: 1929,loss: 0.020462729\n",
            "\n",
            "Global step: 1930,loss: 0.024222694\n",
            "\n",
            "Global step: 1931,loss: 0.018827127\n",
            "\n",
            "Global step: 1932,loss: 0.016559297\n",
            "\n",
            "Global step: 1933,loss: 0.01756161\n",
            "\n",
            "Global step: 1934,loss: 0.015602786\n",
            "\n",
            "Global step: 1935,loss: 0.015638253\n",
            "\n",
            "Global step: 1936,loss: 0.016982283\n",
            "\n",
            "Global step: 1937,loss: 0.017246632\n",
            "INFO:tensorflow:global_step/sec: 2.82532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:05:20.636128 139635366639360 supervisor.py:1099] global_step/sec: 2.82532\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:tensorflow:Recording summary at step 1938.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:05:20.820401 139637785016064 supervisor.py:1050] Recording summary at step 1938.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1938,loss: 0.018693209\n",
            "\n",
            "Global step: 1939,loss: 0.013780594\n",
            "\n",
            "Global step: 1940,loss: 0.021803223\n",
            "\n",
            "Global step: 1941,loss: 0.018515468\n",
            "\n",
            "Global step: 1942,loss: 0.032121487\n",
            "\n",
            "Global step: 1943,loss: 0.013956451\n",
            "\n",
            "Global step: 1944,loss: 0.019009152\n",
            "\n",
            "Global step: 1945,loss: 0.015560122\n",
            "\n",
            "Global step: 1946,loss: 0.016941296\n",
            "\n",
            "Global step: 1947,loss: 0.019694524\n",
            "\n",
            "Global step: 1948,loss: 0.015660912\n",
            "\n",
            "Global step: 1949,loss: 0.016740158\n",
            "\n",
            "Global step: 1950,loss: 0.01725252\n",
            "\n",
            "Global step: 1951,loss: 0.014941949\n",
            "\n",
            "Global step: 1952,loss: 0.025715223\n",
            "\n",
            "Global step: 1953,loss: 0.014726093\n",
            "\n",
            "Global step: 1954,loss: 0.015766125\n",
            "\n",
            "Global step: 1955,loss: 0.016502967\n",
            "\n",
            "Global step: 1956,loss: 0.021183938\n",
            "\n",
            "Global step: 1957,loss: 0.016043128\n",
            "\n",
            "Global step: 1958,loss: 0.014560168\n",
            "\n",
            "Global step: 1959,loss: 0.016128689\n",
            "\n",
            "Global step: 1960,loss: 0.015614628\n",
            "\n",
            "Global step: 1961,loss: 0.016888317\n",
            "\n",
            "Global step: 1962,loss: 0.016447492\n",
            "\n",
            "Global step: 1963,loss: 0.013883622\n",
            "\n",
            "Global step: 1964,loss: 0.0155380815\n",
            "\n",
            "Global step: 1965,loss: 0.013488408\n",
            "\n",
            "Global step: 1966,loss: 0.021899134\n",
            "\n",
            "Global step: 1967,loss: 0.014215095\n",
            "\n",
            "Global step: 1968,loss: 0.013578064\n",
            "\n",
            "Global step: 1969,loss: 0.014439934\n",
            "\n",
            "Global step: 1970,loss: 0.021712806\n",
            "\n",
            "Global step: 1971,loss: 0.018286152\n",
            "\n",
            "Global step: 1972,loss: 0.025486365\n",
            "\n",
            "Global step: 1973,loss: 0.01354481\n",
            "\n",
            "Global step: 1974,loss: 0.01684051\n",
            "\n",
            "Global step: 1975,loss: 0.016303835\n",
            "\n",
            "Global step: 1976,loss: 0.016914088\n",
            "\n",
            "Global step: 1977,loss: 0.024819061\n",
            "\n",
            "Global step: 1978,loss: 0.017626118\n",
            "\n",
            "Global step: 1979,loss: 0.03055238\n",
            "\n",
            "Global step: 1980,loss: 0.016721845\n",
            "\n",
            "Global step: 1981,loss: 0.0224409\n",
            "\n",
            "Global step: 1982,loss: 0.015723947\n",
            "\n",
            "Global step: 1983,loss: 0.015339296\n",
            "\n",
            "Global step: 1984,loss: 0.023259085\n",
            "\n",
            "Global step: 1985,loss: 0.017535934\n",
            "\n",
            "Global step: 1986,loss: 0.016783614\n",
            "\n",
            "Global step: 1987,loss: 0.016845325\n",
            "\n",
            "Global step: 1988,loss: 0.016587999\n",
            "\n",
            "Global step: 1989,loss: 0.023025779\n",
            "\n",
            "Global step: 1990,loss: 0.012963488\n",
            "\n",
            "Global step: 1991,loss: 0.016943347\n",
            "\n",
            "Global step: 1992,loss: 0.012515434\n",
            "\n",
            "Global step: 1993,loss: 0.024235707\n",
            "\n",
            "Global step: 1994,loss: 0.014753899\n",
            "\n",
            "Global step: 1995,loss: 0.013915273\n",
            "\n",
            "Global step: 1996,loss: 0.016159387\n",
            "\n",
            "Global step: 1997,loss: 0.015756054\n",
            "\n",
            "Global step: 1998,loss: 0.014867586\n",
            "\n",
            "Global step: 1999,loss: 0.019599237\n",
            "\n",
            "Global step: 2000,loss: 0.013657702\n",
            "\n",
            "Global step: 2001,loss: 0.018290281\n",
            "\n",
            "Global step: 2002,loss: 0.020702226\n",
            "\n",
            "Global step: 2003,loss: 0.017860288\n",
            "\n",
            "Global step: 2004,loss: 0.015491366\n",
            "\n",
            "Global step: 2005,loss: 0.0297945\n",
            "\n",
            "Global step: 2006,loss: 0.012991875\n",
            "\n",
            "Global step: 2007,loss: 0.01723017\n",
            "\n",
            "Global step: 2008,loss: 0.015904188\n",
            "\n",
            "Global step: 2009,loss: 0.016254196\n",
            "\n",
            "Global step: 2010,loss: 0.017703747\n",
            "\n",
            "Global step: 2011,loss: 0.014877424\n",
            "\n",
            "Global step: 2012,loss: 0.016587624\n",
            "\n",
            "Global step: 2013,loss: 0.017115192\n",
            "\n",
            "Global step: 2014,loss: 0.014595622\n",
            "\n",
            "Global step: 2015,loss: 0.013668558\n",
            "\n",
            "Global step: 2016,loss: 0.016616212\n",
            "\n",
            "Global step: 2017,loss: 0.015127884\n",
            "\n",
            "Global step: 2018,loss: 0.02051618\n",
            "\n",
            "Global step: 2019,loss: 0.012492584\n",
            "\n",
            "Global step: 2020,loss: 0.01210555\n",
            "\n",
            "Global step: 2021,loss: 0.015553366\n",
            "\n",
            "Global step: 2022,loss: 0.018722426\n",
            "\n",
            "Global step: 2023,loss: 0.015885383\n",
            "\n",
            "Global step: 2024,loss: 0.01177094\n",
            "\n",
            "Global step: 2025,loss: 0.013406582\n",
            "\n",
            "Global step: 2026,loss: 0.012961857\n",
            "\n",
            "Global step: 2027,loss: 0.012525407\n",
            "\n",
            "Global step: 2028,loss: 0.02990413\n",
            "\n",
            "Global step: 2029,loss: 0.018527053\n",
            "\n",
            "Global step: 2030,loss: 0.013573807\n",
            "\n",
            "Global step: 2031,loss: 0.033262283\n",
            "\n",
            "Global step: 2032,loss: 0.020387923\n",
            "\n",
            "Global step: 2033,loss: 0.016774077\n",
            "\n",
            "Global step: 2034,loss: 0.01771772\n",
            "\n",
            "Global step: 2035,loss: 0.014385706\n",
            "\n",
            "Global step: 2036,loss: 0.013105823\n",
            "\n",
            "Global step: 2037,loss: 0.01896277\n",
            "\n",
            "Global step: 2038,loss: 0.018270995\n",
            "\n",
            "Global step: 2039,loss: 0.017509213\n",
            "\n",
            "Global step: 2040,loss: 0.013525585\n",
            "\n",
            "Global step: 2041,loss: 0.018081296\n",
            "\n",
            "Global step: 2042,loss: 0.01603103\n",
            "\n",
            "Global step: 2043,loss: 0.0171559\n",
            "\n",
            "Global step: 2044,loss: 0.016533481\n",
            "\n",
            "Global step: 2045,loss: 0.017176576\n",
            "\n",
            "Global step: 2046,loss: 0.015057668\n",
            "\n",
            "Global step: 2047,loss: 0.015550664\n",
            "\n",
            "Global step: 2048,loss: 0.015478609\n",
            "\n",
            "Global step: 2049,loss: 0.012882602\n",
            "\n",
            "Global step: 2050,loss: 0.01665761\n",
            "\n",
            "Global step: 2051,loss: 0.01520258\n",
            "\n",
            "Global step: 2052,loss: 0.020021994\n",
            "\n",
            "Global step: 2053,loss: 0.013407158\n",
            "\n",
            "Global step: 2054,loss: 0.022448184\n",
            "\n",
            "Global step: 2055,loss: 0.014389852\n",
            "\n",
            "Global step: 2056,loss: 0.014939108\n",
            "\n",
            "Global step: 2057,loss: 0.018615473\n",
            "\n",
            "Global step: 2058,loss: 0.020639293\n",
            "\n",
            "Global step: 2059,loss: 0.013392534\n",
            "\n",
            "Global step: 2060,loss: 0.013599858\n",
            "\n",
            "Global step: 2061,loss: 0.0224281\n",
            "\n",
            "Global step: 2062,loss: 0.03432799\n",
            "\n",
            "Global step: 2063,loss: 0.019607272\n",
            "\n",
            "Global step: 2064,loss: 0.02816119\n",
            "\n",
            "Global step: 2065,loss: 0.015463348\n",
            "\n",
            "Global step: 2066,loss: 0.015794357\n",
            "\n",
            "Global step: 2067,loss: 0.015327821\n",
            "\n",
            "Global step: 2068,loss: 0.023444846\n",
            "\n",
            "Global step: 2069,loss: 0.018891025\n",
            "\n",
            "Global step: 2070,loss: 0.016007405\n",
            "\n",
            "Global step: 2071,loss: 0.01883843\n",
            "\n",
            "Global step: 2072,loss: 0.0161729\n",
            "\n",
            "Global step: 2073,loss: 0.016098354\n",
            "\n",
            "Global step: 2074,loss: 0.013006121\n",
            "\n",
            "Global step: 2075,loss: 0.020374928\n",
            "\n",
            "Global step: 2076,loss: 0.017351527\n",
            "\n",
            "Global step: 2077,loss: 0.015198886\n",
            "\n",
            "Global step: 2078,loss: 0.017242739\n",
            "\n",
            "Global step: 2079,loss: 0.0130158495\n",
            "\n",
            "Global step: 2080,loss: 0.013677715\n",
            "\n",
            "Global step: 2081,loss: 0.012486234\n",
            "\n",
            "Global step: 2082,loss: 0.017969972\n",
            "\n",
            "Global step: 2083,loss: 0.016159514\n",
            "\n",
            "Global step: 2084,loss: 0.013999918\n",
            "\n",
            "Global step: 2085,loss: 0.012770713\n",
            "\n",
            "Global step: 2086,loss: 0.013905405\n",
            "\n",
            "Global step: 2087,loss: 0.01933616\n",
            "\n",
            "Global step: 2088,loss: 0.019472545\n",
            "\n",
            "Global step: 2089,loss: 0.01396445\n",
            "\n",
            "Global step: 2090,loss: 0.018281672\n",
            "\n",
            "Global step: 2091,loss: 0.014798282\n",
            "\n",
            "Global step: 2092,loss: 0.017346147\n",
            "\n",
            "Global step: 2093,loss: 0.019897386\n",
            "\n",
            "Global step: 2094,loss: 0.014843212\n",
            "\n",
            "Global step: 2095,loss: 0.015027979\n",
            "\n",
            "Global step: 2096,loss: 0.012949584\n",
            "\n",
            "Global step: 2097,loss: 0.013137929\n",
            "\n",
            "Global step: 2098,loss: 0.015459818\n",
            "\n",
            "Global step: 2099,loss: 0.01648127\n",
            "\n",
            "Global step: 2100,loss: 0.032248374\n",
            "\n",
            "Global step: 2101,loss: 0.015300723\n",
            "\n",
            "Global step: 2102,loss: 0.024253815\n",
            "\n",
            "Global step: 2103,loss: 0.013562174\n",
            "\n",
            "Global step: 2104,loss: 0.022993525\n",
            "\n",
            "Global step: 2105,loss: 0.016168684\n",
            "\n",
            "Global step: 2106,loss: 0.014520477\n",
            "\n",
            "Global step: 2107,loss: 0.016564276\n",
            "\n",
            "Global step: 2108,loss: 0.020705562\n",
            "\n",
            "Global step: 2109,loss: 0.016574347\n",
            "\n",
            "Global step: 2110,loss: 0.013839368\n",
            "\n",
            "Global step: 2111,loss: 0.014681886\n",
            "\n",
            "Global step: 2112,loss: 0.013249064\n",
            "\n",
            "Global step: 2113,loss: 0.021315178\n",
            "\n",
            "Global step: 2114,loss: 0.016205173\n",
            "\n",
            "Global step: 2115,loss: 0.014658093\n",
            "\n",
            "Global step: 2116,loss: 0.0197841\n",
            "\n",
            "Global step: 2117,loss: 0.014099484\n",
            "\n",
            "Global step: 2118,loss: 0.01869486\n",
            "\n",
            "Global step: 2119,loss: 0.036297075\n",
            "\n",
            "Global step: 2120,loss: 0.027014758\n",
            "\n",
            "Global step: 2121,loss: 0.021325368\n",
            "\n",
            "Global step: 2122,loss: 0.018814836\n",
            "\n",
            "Global step: 2123,loss: 0.016874913\n",
            "\n",
            "Global step: 2124,loss: 0.012975817\n",
            "\n",
            "Global step: 2125,loss: 0.018298442\n",
            "\n",
            "Global step: 2126,loss: 0.019613711\n",
            "\n",
            "Global step: 2127,loss: 0.011858165\n",
            "\n",
            "Global step: 2128,loss: 0.015189991\n",
            "\n",
            "Global step: 2129,loss: 0.021336224\n",
            "\n",
            "Global step: 2130,loss: 0.014535237\n",
            "\n",
            "Global step: 2131,loss: 0.021794338\n",
            "\n",
            "Global step: 2132,loss: 0.032632083\n",
            "\n",
            "Global step: 2133,loss: 0.017116725\n",
            "\n",
            "Global step: 2134,loss: 0.023826052\n",
            "\n",
            "Global step: 2135,loss: 0.01743915\n",
            "\n",
            "Global step: 2136,loss: 0.016693195\n",
            "\n",
            "Global step: 2137,loss: 0.01585914\n",
            "\n",
            "Global step: 2138,loss: 0.016600348\n",
            "\n",
            "Global step: 2139,loss: 0.012622263\n",
            "\n",
            "Global step: 2140,loss: 0.018114213\n",
            "\n",
            "Global step: 2141,loss: 0.021735061\n",
            "\n",
            "Global step: 2142,loss: 0.023399413\n",
            "\n",
            "Global step: 2143,loss: 0.019209867\n",
            "\n",
            "Global step: 2144,loss: 0.013326764\n",
            "\n",
            "Global step: 2145,loss: 0.020703837\n",
            "\n",
            "Global step: 2146,loss: 0.01487978\n",
            "\n",
            "Global step: 2147,loss: 0.0133468285\n",
            "\n",
            "Global step: 2148,loss: 0.016284328\n",
            "\n",
            "Global step: 2149,loss: 0.016155567\n",
            "\n",
            "Global step: 2150,loss: 0.013840251\n",
            "\n",
            "Global step: 2151,loss: 0.013497114\n",
            "\n",
            "Global step: 2152,loss: 0.02004213\n",
            "\n",
            "Global step: 2153,loss: 0.01477573\n",
            "\n",
            "Global step: 2154,loss: 0.015671209\n",
            "\n",
            "Global step: 2155,loss: 0.015045462\n",
            "\n",
            "Global step: 2156,loss: 0.012367699\n",
            "\n",
            "Global step: 2157,loss: 0.022777248\n",
            "\n",
            "Global step: 2158,loss: 0.016303051\n",
            "\n",
            "Global step: 2159,loss: 0.012591171\n",
            "\n",
            "Global step: 2160,loss: 0.014260849\n",
            "\n",
            "Global step: 2161,loss: 0.017501367\n",
            "\n",
            "Global step: 2162,loss: 0.028020876\n",
            "\n",
            "Global step: 2163,loss: 0.01941457\n",
            "\n",
            "Global step: 2164,loss: 0.020129414\n",
            "\n",
            "Global step: 2165,loss: 0.014118687\n",
            "\n",
            "Global step: 2166,loss: 0.017566193\n",
            "\n",
            "Global step: 2167,loss: 0.017331636\n",
            "\n",
            "Global step: 2168,loss: 0.0151856765\n",
            "\n",
            "Global step: 2169,loss: 0.012790277\n",
            "\n",
            "Global step: 2170,loss: 0.020167265\n",
            "\n",
            "Global step: 2171,loss: 0.018498568\n",
            "\n",
            "Global step: 2172,loss: 0.022461768\n",
            "\n",
            "Global step: 2173,loss: 0.017176574\n",
            "\n",
            "Global step: 2174,loss: 0.022051405\n",
            "\n",
            "Global step: 2175,loss: 0.0144283045\n",
            "\n",
            "Global step: 2176,loss: 0.014957709\n",
            "\n",
            "Global step: 2177,loss: 0.018682152\n",
            "\n",
            "Global step: 2178,loss: 0.01637117\n",
            "\n",
            "Global step: 2179,loss: 0.016110662\n",
            "\n",
            "Global step: 2180,loss: 0.021824824\n",
            "\n",
            "Global step: 2181,loss: 0.016937032\n",
            "\n",
            "Global step: 2182,loss: 0.014358097\n",
            "\n",
            "Global step: 2183,loss: 0.015026759\n",
            "\n",
            "Global step: 2184,loss: 0.013037236\n",
            "\n",
            "Global step: 2185,loss: 0.012588873\n",
            "\n",
            "Global step: 2186,loss: 0.013158549\n",
            "\n",
            "Global step: 2187,loss: 0.024860244\n",
            "\n",
            "Global step: 2188,loss: 0.012577931\n",
            "\n",
            "Global step: 2189,loss: 0.018489443\n",
            "\n",
            "Global step: 2190,loss: 0.02180818\n",
            "\n",
            "Global step: 2191,loss: 0.021533728\n",
            "\n",
            "Global step: 2192,loss: 0.015131338\n",
            "\n",
            "Global step: 2193,loss: 0.01886673\n",
            "\n",
            "Global step: 2194,loss: 0.01686488\n",
            "\n",
            "Global step: 2195,loss: 0.017014602\n",
            "\n",
            "Global step: 2196,loss: 0.013638568\n",
            "\n",
            "Global step: 2197,loss: 0.019619752\n",
            "\n",
            "Global step: 2198,loss: 0.015680913\n",
            "\n",
            "Global step: 2199,loss: 0.017445635\n",
            "\n",
            "Global step: 2200,loss: 0.018736076\n",
            "\n",
            "Global step: 2201,loss: 0.016964402\n",
            "\n",
            "Global step: 2202,loss: 0.01336482\n",
            "\n",
            "Global step: 2203,loss: 0.015595941\n",
            "\n",
            "Global step: 2204,loss: 0.013383141\n",
            "\n",
            "Global step: 2205,loss: 0.016340101\n",
            "\n",
            "Global step: 2206,loss: 0.01393131\n",
            "\n",
            "Global step: 2207,loss: 0.016802099\n",
            "\n",
            "Global step: 2208,loss: 0.014267772\n",
            "\n",
            "Global step: 2209,loss: 0.017887324\n",
            "\n",
            "Global step: 2210,loss: 0.016038168\n",
            "\n",
            "Global step: 2211,loss: 0.013350612\n",
            "\n",
            "Global step: 2212,loss: 0.025795046\n",
            "\n",
            "Global step: 2213,loss: 0.013019703\n",
            "\n",
            "Global step: 2214,loss: 0.019074744\n",
            "\n",
            "Global step: 2215,loss: 0.024753856\n",
            "\n",
            "Global step: 2216,loss: 0.014376417\n",
            "\n",
            "Global step: 2217,loss: 0.013780555\n",
            "\n",
            "Global step: 2218,loss: 0.01584208\n",
            "\n",
            "Global step: 2219,loss: 0.014259284\n",
            "\n",
            "Global step: 2220,loss: 0.01505953\n",
            "\n",
            "Global step: 2221,loss: 0.023734683\n",
            "\n",
            "Global step: 2222,loss: 0.019774295\n",
            "\n",
            "Global step: 2223,loss: 0.016854374\n",
            "\n",
            "Global step: 2224,loss: 0.015773334\n",
            "\n",
            "Global step: 2225,loss: 0.015598541\n",
            "\n",
            "Global step: 2226,loss: 0.019509755\n",
            "\n",
            "Global step: 2227,loss: 0.014237623\n",
            "\n",
            "Global step: 2228,loss: 0.014614342\n",
            "\n",
            "Global step: 2229,loss: 0.022782888\n",
            "\n",
            "Global step: 2230,loss: 0.016169216\n",
            "\n",
            "Global step: 2231,loss: 0.014379357\n",
            "\n",
            "Global step: 2232,loss: 0.020980235\n",
            "\n",
            "Global step: 2233,loss: 0.013903313\n",
            "\n",
            "Global step: 2234,loss: 0.02148014\n",
            "\n",
            "Global step: 2235,loss: 0.014538508\n",
            "\n",
            "Global step: 2236,loss: 0.024480741\n",
            "\n",
            "Global step: 2237,loss: 0.017770542\n",
            "\n",
            "Global step: 2238,loss: 0.018035883\n",
            "\n",
            "Global step: 2239,loss: 0.012703542\n",
            "\n",
            "Global step: 2240,loss: 0.015530345\n",
            "\n",
            "Global step: 2241,loss: 0.021901201\n",
            "\n",
            "Global step: 2242,loss: 0.017284706\n",
            "\n",
            "Global step: 2243,loss: 0.020658769\n",
            "\n",
            "Global step: 2244,loss: 0.02164633\n",
            "\n",
            "Global step: 2245,loss: 0.01846184\n",
            "\n",
            "Global step: 2246,loss: 0.018085262\n",
            "\n",
            "Global step: 2247,loss: 0.013435088\n",
            "\n",
            "Global step: 2248,loss: 0.013110147\n",
            "\n",
            "Global step: 2249,loss: 0.014914224\n",
            "\n",
            "Global step: 2250,loss: 0.019924441\n",
            "\n",
            "Global step: 2251,loss: 0.020698301\n",
            "\n",
            "Global step: 2252,loss: 0.0210015\n",
            "\n",
            "Global step: 2253,loss: 0.028171113\n",
            "\n",
            "Global step: 2254,loss: 0.013417438\n",
            "\n",
            "Global step: 2255,loss: 0.01809612\n",
            "\n",
            "Global step: 2256,loss: 0.017721498\n",
            "\n",
            "Global step: 2257,loss: 0.014334785\n",
            "\n",
            "Global step: 2258,loss: 0.017923241\n",
            "\n",
            "Global step: 2259,loss: 0.01634287\n",
            "\n",
            "Global step: 2260,loss: 0.014340486\n",
            "\n",
            "Global step: 2261,loss: 0.018593978\n",
            "\n",
            "Global step: 2262,loss: 0.013353683\n",
            "\n",
            "Global step: 2263,loss: 0.017446257\n",
            "\n",
            "Global step: 2264,loss: 0.014524346\n",
            "\n",
            "Global step: 2265,loss: 0.01853314\n",
            "\n",
            "Global step: 2266,loss: 0.016306838\n",
            "\n",
            "Global step: 2267,loss: 0.013363278\n",
            "\n",
            "Global step: 2268,loss: 0.02624176\n",
            "\n",
            "Global step: 2269,loss: 0.017249096\n",
            "\n",
            "Global step: 2270,loss: 0.025454683\n",
            "\n",
            "Global step: 2271,loss: 0.023718968\n",
            "\n",
            "Global step: 2272,loss: 0.022793658\n",
            "\n",
            "Global step: 2273,loss: 0.015734036\n",
            "\n",
            "Global step: 2274,loss: 0.012784779\n",
            "\n",
            "Global step: 2275,loss: 0.015446789\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2276.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:07:20.570006 139637785016064 supervisor.py:1050] Recording summary at step 2276.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.81765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:07:20.594456 139635366639360 supervisor.py:1099] global_step/sec: 2.81765\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2276,loss: 0.018822271\n",
            "\n",
            "Global step: 2277,loss: 0.021212075\n",
            "\n",
            "Global step: 2278,loss: 0.017911244\n",
            "\n",
            "Global step: 2279,loss: 0.023883\n",
            "\n",
            "Global step: 2280,loss: 0.018226419\n",
            "\n",
            "Global step: 2281,loss: 0.013493275\n",
            "\n",
            "Global step: 2282,loss: 0.014571605\n",
            "\n",
            "Global step: 2283,loss: 0.023377527\n",
            "\n",
            "Global step: 2284,loss: 0.017455833\n",
            "\n",
            "Global step: 2285,loss: 0.021841515\n",
            "\n",
            "Global step: 2286,loss: 0.01875728\n",
            "\n",
            "Global step: 2287,loss: 0.017155573\n",
            "\n",
            "Global step: 2288,loss: 0.014258932\n",
            "\n",
            "Global step: 2289,loss: 0.0180271\n",
            "\n",
            "Global step: 2290,loss: 0.021047942\n",
            "\n",
            "Global step: 2291,loss: 0.02056272\n",
            "\n",
            "Global step: 2292,loss: 0.01891506\n",
            "\n",
            "Global step: 2293,loss: 0.018330647\n",
            "\n",
            "Global step: 2294,loss: 0.012795018\n",
            "\n",
            "Global step: 2295,loss: 0.014392369\n",
            "\n",
            "Global step: 2296,loss: 0.021953652\n",
            "\n",
            "Global step: 2297,loss: 0.013516676\n",
            "\n",
            "Global step: 2298,loss: 0.017702721\n",
            "\n",
            "Global step: 2299,loss: 0.01809331\n",
            "\n",
            "Global step: 2300,loss: 0.023559246\n",
            "\n",
            "Global step: 2301,loss: 0.016211776\n",
            "\n",
            "Global step: 2302,loss: 0.013237161\n",
            "\n",
            "Global step: 2303,loss: 0.017497484\n",
            "\n",
            "Global step: 2304,loss: 0.024894498\n",
            "\n",
            "Global step: 2305,loss: 0.02925742\n",
            "\n",
            "Global step: 2306,loss: 0.015440553\n",
            "\n",
            "Global step: 2307,loss: 0.017593121\n",
            "\n",
            "Global step: 2308,loss: 0.020514637\n",
            "\n",
            "Global step: 2309,loss: 0.024009263\n",
            "\n",
            "Global step: 2310,loss: 0.015318699\n",
            "\n",
            "Global step: 2311,loss: 0.02337725\n",
            "\n",
            "Global step: 2312,loss: 0.020629771\n",
            "\n",
            "Global step: 2313,loss: 0.015002359\n",
            "\n",
            "Global step: 2314,loss: 0.015729133\n",
            "\n",
            "Global step: 2315,loss: 0.013331428\n",
            "\n",
            "Global step: 2316,loss: 0.013902133\n",
            "\n",
            "Global step: 2317,loss: 0.013021508\n",
            "\n",
            "Global step: 2318,loss: 0.01433678\n",
            "\n",
            "Global step: 2319,loss: 0.016275724\n",
            "\n",
            "Global step: 2320,loss: 0.022227755\n",
            "\n",
            "Global step: 2321,loss: 0.023438755\n",
            "\n",
            "Global step: 2322,loss: 0.013824821\n",
            "\n",
            "Global step: 2323,loss: 0.014313165\n",
            "\n",
            "Global step: 2324,loss: 0.018521179\n",
            "\n",
            "Global step: 2325,loss: 0.017746164\n",
            "\n",
            "Global step: 2326,loss: 0.014392551\n",
            "\n",
            "Global step: 2327,loss: 0.014642377\n",
            "\n",
            "Global step: 2328,loss: 0.014098898\n",
            "\n",
            "Global step: 2329,loss: 0.016542526\n",
            "\n",
            "Global step: 2330,loss: 0.013254456\n",
            "\n",
            "Global step: 2331,loss: 0.013856846\n",
            "\n",
            "Global step: 2332,loss: 0.014141701\n",
            "\n",
            "Global step: 2333,loss: 0.014122312\n",
            "\n",
            "Global step: 2334,loss: 0.014309671\n",
            "\n",
            "Global step: 2335,loss: 0.012345645\n",
            "\n",
            "Global step: 2336,loss: 0.01794302\n",
            "\n",
            "Global step: 2337,loss: 0.015637694\n",
            "\n",
            "Global step: 2338,loss: 0.018729145\n",
            "\n",
            "Global step: 2339,loss: 0.013222622\n",
            "\n",
            "Global step: 2340,loss: 0.020651676\n",
            "\n",
            "Global step: 2341,loss: 0.014385404\n",
            "\n",
            "Global step: 2342,loss: 0.0148127545\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2342,Val_Loss: 0.016627503570933372,  Val_acc: 0.9970953525641025 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 16:07:56.590039 139641102628736 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/20:\n",
            "Global step: 2343,loss: 0.014102925\n",
            "\n",
            "Global step: 2344,loss: 0.015788822\n",
            "\n",
            "Global step: 2345,loss: 0.01379751\n",
            "\n",
            "Global step: 2346,loss: 0.013133299\n",
            "\n",
            "Global step: 2347,loss: 0.013442149\n",
            "\n",
            "Global step: 2348,loss: 0.014885786\n",
            "\n",
            "Global step: 2349,loss: 0.0123670865\n",
            "\n",
            "Global step: 2350,loss: 0.025301177\n",
            "\n",
            "Global step: 2351,loss: 0.013568471\n",
            "\n",
            "Global step: 2352,loss: 0.016074123\n",
            "\n",
            "Global step: 2353,loss: 0.017560663\n",
            "\n",
            "Global step: 2354,loss: 0.01407015\n",
            "\n",
            "Global step: 2355,loss: 0.021313276\n",
            "\n",
            "Global step: 2356,loss: 0.019496141\n",
            "\n",
            "Global step: 2357,loss: 0.01307404\n",
            "\n",
            "Global step: 2358,loss: 0.013174263\n",
            "\n",
            "Global step: 2359,loss: 0.014991341\n",
            "\n",
            "Global step: 2360,loss: 0.022980276\n",
            "\n",
            "Global step: 2361,loss: 0.022267867\n",
            "\n",
            "Global step: 2362,loss: 0.015896147\n",
            "\n",
            "Global step: 2363,loss: 0.0210721\n",
            "\n",
            "Global step: 2364,loss: 0.013709025\n",
            "\n",
            "Global step: 2365,loss: 0.013909753\n",
            "\n",
            "Global step: 2366,loss: 0.013161395\n",
            "\n",
            "Global step: 2367,loss: 0.022429798\n",
            "\n",
            "Global step: 2368,loss: 0.01907834\n",
            "\n",
            "Global step: 2369,loss: 0.01680573\n",
            "\n",
            "Global step: 2370,loss: 0.01291495\n",
            "\n",
            "Global step: 2371,loss: 0.022668447\n",
            "\n",
            "Global step: 2372,loss: 0.0149717275\n",
            "\n",
            "Global step: 2373,loss: 0.018585283\n",
            "\n",
            "Global step: 2374,loss: 0.018098759\n",
            "\n",
            "Global step: 2375,loss: 0.014679203\n",
            "\n",
            "Global step: 2376,loss: 0.014577972\n",
            "\n",
            "Global step: 2377,loss: 0.012367712\n",
            "\n",
            "Global step: 2378,loss: 0.0134407785\n",
            "\n",
            "Global step: 2379,loss: 0.016925499\n",
            "\n",
            "Global step: 2380,loss: 0.014246389\n",
            "\n",
            "Global step: 2381,loss: 0.014386751\n",
            "\n",
            "Global step: 2382,loss: 0.016856793\n",
            "\n",
            "Global step: 2383,loss: 0.019491747\n",
            "\n",
            "Global step: 2384,loss: 0.015680868\n",
            "\n",
            "Global step: 2385,loss: 0.012839137\n",
            "\n",
            "Global step: 2386,loss: 0.014529941\n",
            "\n",
            "Global step: 2387,loss: 0.014839773\n",
            "\n",
            "Global step: 2388,loss: 0.012213904\n",
            "\n",
            "Global step: 2389,loss: 0.015284281\n",
            "\n",
            "Global step: 2390,loss: 0.02355496\n",
            "\n",
            "Global step: 2391,loss: 0.01805842\n",
            "\n",
            "Global step: 2392,loss: 0.019658502\n",
            "\n",
            "Global step: 2393,loss: 0.016267445\n",
            "\n",
            "Global step: 2394,loss: 0.015181673\n",
            "\n",
            "Global step: 2395,loss: 0.016358955\n",
            "\n",
            "Global step: 2396,loss: 0.012625213\n",
            "\n",
            "Global step: 2397,loss: 0.016966894\n",
            "\n",
            "Global step: 2398,loss: 0.012282757\n",
            "\n",
            "Global step: 2399,loss: 0.015494112\n",
            "\n",
            "Global step: 2400,loss: 0.013652144\n",
            "\n",
            "Global step: 2401,loss: 0.012847685\n",
            "\n",
            "Global step: 2402,loss: 0.011481495\n",
            "\n",
            "Global step: 2403,loss: 0.017083248\n",
            "\n",
            "Global step: 2404,loss: 0.012347974\n",
            "\n",
            "Global step: 2405,loss: 0.017989589\n",
            "\n",
            "Global step: 2406,loss: 0.024710976\n",
            "\n",
            "Global step: 2407,loss: 0.011971117\n",
            "\n",
            "Global step: 2408,loss: 0.025296519\n",
            "\n",
            "Global step: 2409,loss: 0.017754612\n",
            "\n",
            "Global step: 2410,loss: 0.016317392\n",
            "\n",
            "Global step: 2411,loss: 0.015223017\n",
            "\n",
            "Global step: 2412,loss: 0.015385553\n",
            "\n",
            "Global step: 2413,loss: 0.015366246\n",
            "\n",
            "Global step: 2414,loss: 0.022969145\n",
            "\n",
            "Global step: 2415,loss: 0.012904247\n",
            "\n",
            "Global step: 2416,loss: 0.016291536\n",
            "\n",
            "Global step: 2417,loss: 0.0147408545\n",
            "\n",
            "Global step: 2418,loss: 0.018228898\n",
            "\n",
            "Global step: 2419,loss: 0.017263144\n",
            "\n",
            "Global step: 2420,loss: 0.011306968\n",
            "\n",
            "Global step: 2421,loss: 0.013723045\n",
            "\n",
            "Global step: 2422,loss: 0.012412824\n",
            "\n",
            "Global step: 2423,loss: 0.015298029\n",
            "\n",
            "Global step: 2424,loss: 0.016230164\n",
            "\n",
            "Global step: 2425,loss: 0.020763645\n",
            "\n",
            "Global step: 2426,loss: 0.012966428\n",
            "\n",
            "Global step: 2427,loss: 0.014616948\n",
            "\n",
            "Global step: 2428,loss: 0.012864882\n",
            "\n",
            "Global step: 2429,loss: 0.013567631\n",
            "\n",
            "Global step: 2430,loss: 0.025814526\n",
            "\n",
            "Global step: 2431,loss: 0.019510275\n",
            "\n",
            "Global step: 2432,loss: 0.015792597\n",
            "\n",
            "Global step: 2433,loss: 0.01497448\n",
            "\n",
            "Global step: 2434,loss: 0.013262259\n",
            "\n",
            "Global step: 2435,loss: 0.016082207\n",
            "\n",
            "Global step: 2436,loss: 0.013650232\n",
            "\n",
            "Global step: 2437,loss: 0.011776806\n",
            "\n",
            "Global step: 2438,loss: 0.017896289\n",
            "\n",
            "Global step: 2439,loss: 0.016187944\n",
            "\n",
            "Global step: 2440,loss: 0.012587558\n",
            "\n",
            "Global step: 2441,loss: 0.015554038\n",
            "\n",
            "Global step: 2442,loss: 0.014768932\n",
            "\n",
            "Global step: 2443,loss: 0.01477855\n",
            "\n",
            "Global step: 2444,loss: 0.013070261\n",
            "\n",
            "Global step: 2445,loss: 0.018526834\n",
            "\n",
            "Global step: 2446,loss: 0.013283374\n",
            "\n",
            "Global step: 2447,loss: 0.021220602\n",
            "\n",
            "Global step: 2448,loss: 0.0246428\n",
            "\n",
            "Global step: 2449,loss: 0.013527921\n",
            "\n",
            "Global step: 2450,loss: 0.016573416\n",
            "\n",
            "Global step: 2451,loss: 0.021302193\n",
            "\n",
            "Global step: 2452,loss: 0.012173438\n",
            "\n",
            "Global step: 2453,loss: 0.013788436\n",
            "\n",
            "Global step: 2454,loss: 0.0121481465\n",
            "\n",
            "Global step: 2455,loss: 0.013135198\n",
            "\n",
            "Global step: 2456,loss: 0.012847335\n",
            "\n",
            "Global step: 2457,loss: 0.012542991\n",
            "\n",
            "Global step: 2458,loss: 0.0166783\n",
            "\n",
            "Global step: 2459,loss: 0.012216191\n",
            "\n",
            "Global step: 2460,loss: 0.01730979\n",
            "\n",
            "Global step: 2461,loss: 0.01285035\n",
            "\n",
            "Global step: 2462,loss: 0.013196929\n",
            "\n",
            "Global step: 2463,loss: 0.017712623\n",
            "\n",
            "Global step: 2464,loss: 0.01477218\n",
            "\n",
            "Global step: 2465,loss: 0.016450614\n",
            "\n",
            "Global step: 2466,loss: 0.012093964\n",
            "\n",
            "Global step: 2467,loss: 0.011596166\n",
            "\n",
            "Global step: 2468,loss: 0.015053371\n",
            "\n",
            "Global step: 2469,loss: 0.012364354\n",
            "\n",
            "Global step: 2470,loss: 0.012126023\n",
            "\n",
            "Global step: 2471,loss: 0.014016784\n",
            "\n",
            "Global step: 2472,loss: 0.015181996\n",
            "\n",
            "Global step: 2473,loss: 0.01145627\n",
            "\n",
            "Global step: 2474,loss: 0.011623699\n",
            "\n",
            "Global step: 2475,loss: 0.015990606\n",
            "\n",
            "Global step: 2476,loss: 0.017017612\n",
            "\n",
            "Global step: 2477,loss: 0.011456345\n",
            "\n",
            "Global step: 2478,loss: 0.015383049\n",
            "\n",
            "Global step: 2479,loss: 0.011779749\n",
            "\n",
            "Global step: 2480,loss: 0.019165993\n",
            "\n",
            "Global step: 2481,loss: 0.014698561\n",
            "\n",
            "Global step: 2482,loss: 0.013458406\n",
            "\n",
            "Global step: 2483,loss: 0.011607213\n",
            "\n",
            "Global step: 2484,loss: 0.012287773\n",
            "\n",
            "Global step: 2485,loss: 0.013173884\n",
            "\n",
            "Global step: 2486,loss: 0.015556034\n",
            "\n",
            "Global step: 2487,loss: 0.015116614\n",
            "\n",
            "Global step: 2488,loss: 0.015737966\n",
            "\n",
            "Global step: 2489,loss: 0.011941618\n",
            "\n",
            "Global step: 2490,loss: 0.013517902\n",
            "\n",
            "Global step: 2491,loss: 0.01789745\n",
            "\n",
            "Global step: 2492,loss: 0.01240856\n",
            "\n",
            "Global step: 2493,loss: 0.01353266\n",
            "\n",
            "Global step: 2494,loss: 0.011765746\n",
            "\n",
            "Global step: 2495,loss: 0.012029465\n",
            "\n",
            "Global step: 2496,loss: 0.01285181\n",
            "\n",
            "Global step: 2497,loss: 0.014137655\n",
            "\n",
            "Global step: 2498,loss: 0.014412822\n",
            "\n",
            "Global step: 2499,loss: 0.01700557\n",
            "\n",
            "Global step: 2500,loss: 0.012634983\n",
            "\n",
            "Global step: 2501,loss: 0.01739214\n",
            "\n",
            "Global step: 2502,loss: 0.01742009\n",
            "\n",
            "Global step: 2503,loss: 0.012432402\n",
            "\n",
            "Global step: 2504,loss: 0.020950105\n",
            "\n",
            "Global step: 2505,loss: 0.012997357\n",
            "\n",
            "Global step: 2506,loss: 0.015428995\n",
            "\n",
            "Global step: 2507,loss: 0.020940043\n",
            "\n",
            "Global step: 2508,loss: 0.012841849\n",
            "\n",
            "Global step: 2509,loss: 0.015185596\n",
            "\n",
            "Global step: 2510,loss: 0.012764327\n",
            "\n",
            "Global step: 2511,loss: 0.01316181\n",
            "\n",
            "Global step: 2512,loss: 0.011123764\n",
            "\n",
            "Global step: 2513,loss: 0.0115355775\n",
            "\n",
            "Global step: 2514,loss: 0.012674244\n",
            "\n",
            "Global step: 2515,loss: 0.012480256\n",
            "\n",
            "Global step: 2516,loss: 0.014858576\n",
            "\n",
            "Global step: 2517,loss: 0.015363589\n",
            "\n",
            "Global step: 2518,loss: 0.01947396\n",
            "\n",
            "Global step: 2519,loss: 0.020289216\n",
            "\n",
            "Global step: 2520,loss: 0.014559305\n",
            "\n",
            "Global step: 2521,loss: 0.012456017\n",
            "\n",
            "Global step: 2522,loss: 0.013099609\n",
            "\n",
            "Global step: 2523,loss: 0.016914032\n",
            "\n",
            "Global step: 2524,loss: 0.012295224\n",
            "\n",
            "Global step: 2525,loss: 0.012814177\n",
            "\n",
            "Global step: 2526,loss: 0.012626559\n",
            "\n",
            "Global step: 2527,loss: 0.021319374\n",
            "\n",
            "Global step: 2528,loss: 0.013747216\n",
            "\n",
            "Global step: 2529,loss: 0.023005955\n",
            "\n",
            "Global step: 2530,loss: 0.013158646\n",
            "\n",
            "Global step: 2531,loss: 0.012949571\n",
            "\n",
            "Global step: 2532,loss: 0.012057444\n",
            "\n",
            "Global step: 2533,loss: 0.018888228\n",
            "\n",
            "Global step: 2534,loss: 0.0111662485\n",
            "\n",
            "Global step: 2535,loss: 0.015464431\n",
            "\n",
            "Global step: 2536,loss: 0.021080306\n",
            "\n",
            "Global step: 2537,loss: 0.011856373\n",
            "\n",
            "Global step: 2538,loss: 0.011527321\n",
            "\n",
            "Global step: 2539,loss: 0.020688806\n",
            "\n",
            "Global step: 2540,loss: 0.011382294\n",
            "\n",
            "Global step: 2541,loss: 0.014206433\n",
            "\n",
            "Global step: 2542,loss: 0.018320765\n",
            "\n",
            "Global step: 2543,loss: 0.012667917\n",
            "\n",
            "Global step: 2544,loss: 0.012896338\n",
            "\n",
            "Global step: 2545,loss: 0.015463658\n",
            "\n",
            "Global step: 2546,loss: 0.011783094\n",
            "\n",
            "Global step: 2547,loss: 0.012309214\n",
            "\n",
            "Global step: 2548,loss: 0.012740439\n",
            "\n",
            "Global step: 2549,loss: 0.019275144\n",
            "\n",
            "Global step: 2550,loss: 0.014963736\n",
            "\n",
            "Global step: 2551,loss: 0.015935592\n",
            "\n",
            "Global step: 2552,loss: 0.020859256\n",
            "\n",
            "Global step: 2553,loss: 0.010840921\n",
            "\n",
            "Global step: 2554,loss: 0.021771707\n",
            "\n",
            "Global step: 2555,loss: 0.015793838\n",
            "\n",
            "Global step: 2556,loss: 0.015083976\n",
            "\n",
            "Global step: 2557,loss: 0.014623295\n",
            "\n",
            "Global step: 2558,loss: 0.012232354\n",
            "\n",
            "Global step: 2559,loss: 0.012146698\n",
            "\n",
            "Global step: 2560,loss: 0.01364726\n",
            "\n",
            "Global step: 2561,loss: 0.012760411\n",
            "\n",
            "Global step: 2562,loss: 0.0114568\n",
            "\n",
            "Global step: 2563,loss: 0.014949209\n",
            "\n",
            "Global step: 2564,loss: 0.021785952\n",
            "\n",
            "Global step: 2565,loss: 0.013675025\n",
            "\n",
            "Global step: 2566,loss: 0.017733157\n",
            "\n",
            "Global step: 2567,loss: 0.01472911\n",
            "\n",
            "Global step: 2568,loss: 0.012275341\n",
            "\n",
            "Global step: 2569,loss: 0.01839815\n",
            "\n",
            "Global step: 2570,loss: 0.01658763\n",
            "\n",
            "Global step: 2571,loss: 0.015612372\n",
            "\n",
            "Global step: 2572,loss: 0.014497551\n",
            "\n",
            "Global step: 2573,loss: 0.020430509\n",
            "\n",
            "Global step: 2574,loss: 0.011884313\n",
            "\n",
            "Global step: 2575,loss: 0.011777237\n",
            "\n",
            "Global step: 2576,loss: 0.02179528\n",
            "\n",
            "Global step: 2577,loss: 0.01275541\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.51451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:09:20.697700 139635366639360 supervisor.py:1099] global_step/sec: 2.51451\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2578.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:09:20.709651 139637785016064 supervisor.py:1050] Recording summary at step 2578.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2578,loss: 0.016578726\n",
            "\n",
            "Global step: 2579,loss: 0.018102156\n",
            "\n",
            "Global step: 2580,loss: 0.011824769\n",
            "\n",
            "Global step: 2581,loss: 0.015052353\n",
            "\n",
            "Global step: 2582,loss: 0.021031067\n",
            "\n",
            "Global step: 2583,loss: 0.015301375\n",
            "\n",
            "Global step: 2584,loss: 0.015260709\n",
            "\n",
            "Global step: 2585,loss: 0.015688857\n",
            "\n",
            "Global step: 2586,loss: 0.012905981\n",
            "\n",
            "Global step: 2587,loss: 0.01384397\n",
            "\n",
            "Global step: 2588,loss: 0.016098756\n",
            "\n",
            "Global step: 2589,loss: 0.0127571905\n",
            "\n",
            "Global step: 2590,loss: 0.0163043\n",
            "\n",
            "Global step: 2591,loss: 0.013033931\n",
            "\n",
            "Global step: 2592,loss: 0.01171566\n",
            "\n",
            "Global step: 2593,loss: 0.022401635\n",
            "\n",
            "Global step: 2594,loss: 0.014841606\n",
            "\n",
            "Global step: 2595,loss: 0.013275167\n",
            "\n",
            "Global step: 2596,loss: 0.01842266\n",
            "\n",
            "Global step: 2597,loss: 0.013821679\n",
            "\n",
            "Global step: 2598,loss: 0.01339443\n",
            "\n",
            "Global step: 2599,loss: 0.028074058\n",
            "\n",
            "Global step: 2600,loss: 0.015913252\n",
            "\n",
            "Global step: 2601,loss: 0.011820103\n",
            "\n",
            "Global step: 2602,loss: 0.013047387\n",
            "\n",
            "Global step: 2603,loss: 0.011394927\n",
            "\n",
            "Global step: 2604,loss: 0.012609372\n",
            "\n",
            "Global step: 2605,loss: 0.012692404\n",
            "\n",
            "Global step: 2606,loss: 0.01298746\n",
            "\n",
            "Global step: 2607,loss: 0.012809148\n",
            "\n",
            "Global step: 2608,loss: 0.01448166\n",
            "\n",
            "Global step: 2609,loss: 0.016332416\n",
            "\n",
            "Global step: 2610,loss: 0.013689671\n",
            "\n",
            "Global step: 2611,loss: 0.012243993\n",
            "\n",
            "Global step: 2612,loss: 0.01068893\n",
            "\n",
            "Global step: 2613,loss: 0.016380537\n",
            "\n",
            "Global step: 2614,loss: 0.014588188\n",
            "\n",
            "Global step: 2615,loss: 0.014375586\n",
            "\n",
            "Global step: 2616,loss: 0.016875856\n",
            "\n",
            "Global step: 2617,loss: 0.012911981\n",
            "\n",
            "Global step: 2618,loss: 0.014055834\n",
            "\n",
            "Global step: 2619,loss: 0.012187004\n",
            "\n",
            "Global step: 2620,loss: 0.011739714\n",
            "\n",
            "Global step: 2621,loss: 0.023712642\n",
            "\n",
            "Global step: 2622,loss: 0.012689459\n",
            "\n",
            "Global step: 2623,loss: 0.010252932\n",
            "\n",
            "Global step: 2624,loss: 0.01384558\n",
            "\n",
            "Global step: 2625,loss: 0.021718293\n",
            "\n",
            "Global step: 2626,loss: 0.011887553\n",
            "\n",
            "Global step: 2627,loss: 0.011495813\n",
            "\n",
            "Global step: 2628,loss: 0.011192342\n",
            "\n",
            "Global step: 2629,loss: 0.013698385\n",
            "\n",
            "Global step: 2630,loss: 0.013539705\n",
            "\n",
            "Global step: 2631,loss: 0.017592568\n",
            "\n",
            "Global step: 2632,loss: 0.01139194\n",
            "\n",
            "Global step: 2633,loss: 0.012223493\n",
            "\n",
            "Global step: 2634,loss: 0.015836626\n",
            "\n",
            "Global step: 2635,loss: 0.01182202\n",
            "\n",
            "Global step: 2636,loss: 0.014363095\n",
            "\n",
            "Global step: 2637,loss: 0.017451845\n",
            "\n",
            "Global step: 2638,loss: 0.0107951965\n",
            "\n",
            "Global step: 2639,loss: 0.015028224\n",
            "\n",
            "Global step: 2640,loss: 0.011687441\n",
            "\n",
            "Global step: 2641,loss: 0.014900755\n",
            "\n",
            "Global step: 2642,loss: 0.0146005545\n",
            "\n",
            "Global step: 2643,loss: 0.013508538\n",
            "\n",
            "Global step: 2644,loss: 0.014626576\n",
            "\n",
            "Global step: 2645,loss: 0.01771728\n",
            "\n",
            "Global step: 2646,loss: 0.011794759\n",
            "\n",
            "Global step: 2647,loss: 0.013413279\n",
            "\n",
            "Global step: 2648,loss: 0.010682444\n",
            "\n",
            "Global step: 2649,loss: 0.01536526\n",
            "\n",
            "Global step: 2650,loss: 0.018629704\n",
            "\n",
            "Global step: 2651,loss: 0.015855689\n",
            "\n",
            "Global step: 2652,loss: 0.016705815\n",
            "\n",
            "Global step: 2653,loss: 0.013129154\n",
            "\n",
            "Global step: 2654,loss: 0.017198276\n",
            "\n",
            "Global step: 2655,loss: 0.011502516\n",
            "\n",
            "Global step: 2656,loss: 0.011404421\n",
            "\n",
            "Global step: 2657,loss: 0.012510879\n",
            "\n",
            "Global step: 2658,loss: 0.013318522\n",
            "\n",
            "Global step: 2659,loss: 0.014559191\n",
            "\n",
            "Global step: 2660,loss: 0.013967641\n",
            "\n",
            "Global step: 2661,loss: 0.016521126\n",
            "\n",
            "Global step: 2662,loss: 0.012176489\n",
            "\n",
            "Global step: 2663,loss: 0.017272495\n",
            "\n",
            "Global step: 2664,loss: 0.012174252\n",
            "\n",
            "Global step: 2665,loss: 0.015899003\n",
            "\n",
            "Global step: 2666,loss: 0.014374949\n",
            "\n",
            "Global step: 2667,loss: 0.013700537\n",
            "\n",
            "Global step: 2668,loss: 0.010880006\n",
            "\n",
            "Global step: 2669,loss: 0.015867926\n",
            "\n",
            "Global step: 2670,loss: 0.012160025\n",
            "\n",
            "Global step: 2671,loss: 0.012857276\n",
            "\n",
            "Global step: 2672,loss: 0.014860887\n",
            "\n",
            "Global step: 2673,loss: 0.0147740785\n",
            "\n",
            "Global step: 2674,loss: 0.0133138085\n",
            "\n",
            "Global step: 2675,loss: 0.012243533\n",
            "\n",
            "Global step: 2676,loss: 0.012821904\n",
            "\n",
            "Global step: 2677,loss: 0.015121541\n",
            "\n",
            "Global step: 2678,loss: 0.03178765\n",
            "\n",
            "Global step: 2679,loss: 0.015892468\n",
            "\n",
            "Global step: 2680,loss: 0.013183668\n",
            "\n",
            "Global step: 2681,loss: 0.012413003\n",
            "\n",
            "Global step: 2682,loss: 0.013219768\n",
            "\n",
            "Global step: 2683,loss: 0.015512614\n",
            "\n",
            "Global step: 2684,loss: 0.01969999\n",
            "\n",
            "Global step: 2685,loss: 0.011580735\n",
            "\n",
            "Global step: 2686,loss: 0.020714376\n",
            "\n",
            "Global step: 2687,loss: 0.012225459\n",
            "\n",
            "Global step: 2688,loss: 0.01726407\n",
            "\n",
            "Global step: 2689,loss: 0.012008343\n",
            "\n",
            "Global step: 2690,loss: 0.016577328\n",
            "\n",
            "Global step: 2691,loss: 0.014463736\n",
            "\n",
            "Global step: 2692,loss: 0.012103232\n",
            "\n",
            "Global step: 2693,loss: 0.010590969\n",
            "\n",
            "Global step: 2694,loss: 0.016612196\n",
            "\n",
            "Global step: 2695,loss: 0.013676869\n",
            "\n",
            "Global step: 2696,loss: 0.011332304\n",
            "\n",
            "Global step: 2697,loss: 0.011748306\n",
            "\n",
            "Global step: 2698,loss: 0.010732309\n",
            "\n",
            "Global step: 2699,loss: 0.010417611\n",
            "\n",
            "Global step: 2700,loss: 0.013185864\n",
            "\n",
            "Global step: 2701,loss: 0.011733815\n",
            "\n",
            "Global step: 2702,loss: 0.0139291445\n",
            "\n",
            "Global step: 2703,loss: 0.01238046\n",
            "\n",
            "Global step: 2704,loss: 0.018226685\n",
            "\n",
            "Global step: 2705,loss: 0.016471943\n",
            "\n",
            "Global step: 2706,loss: 0.011268637\n",
            "\n",
            "Global step: 2707,loss: 0.017976537\n",
            "\n",
            "Global step: 2708,loss: 0.023539962\n",
            "\n",
            "Global step: 2709,loss: 0.01184963\n",
            "\n",
            "Global step: 2710,loss: 0.0123687545\n",
            "\n",
            "Global step: 2711,loss: 0.014851483\n",
            "\n",
            "Global step: 2712,loss: 0.012468884\n",
            "\n",
            "Global step: 2713,loss: 0.014170875\n",
            "\n",
            "Global step: 2714,loss: 0.012272578\n",
            "\n",
            "Global step: 2715,loss: 0.011861164\n",
            "\n",
            "Global step: 2716,loss: 0.012332823\n",
            "\n",
            "Global step: 2717,loss: 0.0127949165\n",
            "\n",
            "Global step: 2718,loss: 0.014660113\n",
            "\n",
            "Global step: 2719,loss: 0.016533606\n",
            "\n",
            "Global step: 2720,loss: 0.015915176\n",
            "\n",
            "Global step: 2721,loss: 0.019819714\n",
            "\n",
            "Global step: 2722,loss: 0.013120264\n",
            "\n",
            "Global step: 2723,loss: 0.013830638\n",
            "\n",
            "Global step: 2724,loss: 0.015879177\n",
            "\n",
            "Global step: 2725,loss: 0.015880572\n",
            "\n",
            "Global step: 2726,loss: 0.011174469\n",
            "\n",
            "Global step: 2727,loss: 0.0154304225\n",
            "\n",
            "Global step: 2728,loss: 0.013939063\n",
            "\n",
            "Global step: 2729,loss: 0.012743867\n",
            "\n",
            "Global step: 2730,loss: 0.014777008\n",
            "\n",
            "Global step: 2731,loss: 0.01707685\n",
            "\n",
            "Global step: 2732,loss: 0.013214046\n",
            "\n",
            "Global step: 2733,loss: 0.012729008\n",
            "\n",
            "Global step: 2734,loss: 0.012549546\n",
            "\n",
            "Global step: 2735,loss: 0.01620422\n",
            "\n",
            "Global step: 2736,loss: 0.018383265\n",
            "\n",
            "Global step: 2737,loss: 0.010782432\n",
            "\n",
            "Global step: 2738,loss: 0.01796646\n",
            "\n",
            "Global step: 2739,loss: 0.010929317\n",
            "\n",
            "Global step: 2740,loss: 0.0152293015\n",
            "\n",
            "Global step: 2741,loss: 0.02409601\n",
            "\n",
            "Global step: 2742,loss: 0.015042577\n",
            "\n",
            "Global step: 2743,loss: 0.015700584\n",
            "\n",
            "Global step: 2744,loss: 0.012032454\n",
            "\n",
            "Global step: 2745,loss: 0.011617931\n",
            "\n",
            "Global step: 2746,loss: 0.014122365\n",
            "\n",
            "Global step: 2747,loss: 0.013867835\n",
            "\n",
            "Global step: 2748,loss: 0.013105545\n",
            "\n",
            "Global step: 2749,loss: 0.012095925\n",
            "\n",
            "Global step: 2750,loss: 0.011796759\n",
            "\n",
            "Global step: 2751,loss: 0.010788733\n",
            "\n",
            "Global step: 2752,loss: 0.011073765\n",
            "\n",
            "Global step: 2753,loss: 0.013430089\n",
            "\n",
            "Global step: 2754,loss: 0.010755325\n",
            "\n",
            "Global step: 2755,loss: 0.012389156\n",
            "\n",
            "Global step: 2756,loss: 0.013024734\n",
            "\n",
            "Global step: 2757,loss: 0.013608895\n",
            "\n",
            "Global step: 2758,loss: 0.010975056\n",
            "\n",
            "Global step: 2759,loss: 0.012762402\n",
            "\n",
            "Global step: 2760,loss: 0.0125168115\n",
            "\n",
            "Global step: 2761,loss: 0.0121161835\n",
            "\n",
            "Global step: 2762,loss: 0.012328722\n",
            "\n",
            "Global step: 2763,loss: 0.024231864\n",
            "\n",
            "Global step: 2764,loss: 0.010802159\n",
            "\n",
            "Global step: 2765,loss: 0.01198059\n",
            "\n",
            "Global step: 2766,loss: 0.011478247\n",
            "\n",
            "Global step: 2767,loss: 0.011858561\n",
            "\n",
            "Global step: 2768,loss: 0.015533909\n",
            "\n",
            "Global step: 2769,loss: 0.019090775\n",
            "\n",
            "Global step: 2770,loss: 0.011167599\n",
            "\n",
            "Global step: 2771,loss: 0.019411217\n",
            "\n",
            "Global step: 2772,loss: 0.01365951\n",
            "\n",
            "Global step: 2773,loss: 0.016822107\n",
            "\n",
            "Global step: 2774,loss: 0.017788932\n",
            "\n",
            "Global step: 2775,loss: 0.01846354\n",
            "\n",
            "Global step: 2776,loss: 0.013278846\n",
            "\n",
            "Global step: 2777,loss: 0.011992732\n",
            "\n",
            "Global step: 2778,loss: 0.021476038\n",
            "\n",
            "Global step: 2779,loss: 0.017083954\n",
            "\n",
            "Global step: 2780,loss: 0.01091434\n",
            "\n",
            "Global step: 2781,loss: 0.0144038815\n",
            "\n",
            "Global step: 2782,loss: 0.010131863\n",
            "\n",
            "Global step: 2783,loss: 0.01477119\n",
            "\n",
            "Global step: 2784,loss: 0.0126964385\n",
            "\n",
            "Global step: 2785,loss: 0.01571721\n",
            "\n",
            "Global step: 2786,loss: 0.021830495\n",
            "\n",
            "Global step: 2787,loss: 0.015910693\n",
            "\n",
            "Global step: 2788,loss: 0.012723254\n",
            "\n",
            "Global step: 2789,loss: 0.012125129\n",
            "\n",
            "Global step: 2790,loss: 0.01123256\n",
            "\n",
            "Global step: 2791,loss: 0.014255142\n",
            "\n",
            "Global step: 2792,loss: 0.021035124\n",
            "\n",
            "Global step: 2793,loss: 0.0131428065\n",
            "\n",
            "Global step: 2794,loss: 0.012517121\n",
            "\n",
            "Global step: 2795,loss: 0.011823055\n",
            "\n",
            "Global step: 2796,loss: 0.011565076\n",
            "\n",
            "Global step: 2797,loss: 0.01178469\n",
            "\n",
            "Global step: 2798,loss: 0.01376691\n",
            "\n",
            "Global step: 2799,loss: 0.019257996\n",
            "\n",
            "Global step: 2800,loss: 0.011391204\n",
            "\n",
            "Global step: 2801,loss: 0.0112185\n",
            "\n",
            "Global step: 2802,loss: 0.01610741\n",
            "\n",
            "Global step: 2803,loss: 0.01122613\n",
            "\n",
            "Global step: 2804,loss: 0.012163572\n",
            "\n",
            "Global step: 2805,loss: 0.014469655\n",
            "\n",
            "Global step: 2806,loss: 0.012285352\n",
            "\n",
            "Global step: 2807,loss: 0.012606459\n",
            "\n",
            "Global step: 2808,loss: 0.011087909\n",
            "\n",
            "Global step: 2809,loss: 0.011450866\n",
            "\n",
            "Global step: 2810,loss: 0.011048756\n",
            "\n",
            "Global step: 2811,loss: 0.013765263\n",
            "\n",
            "Global step: 2812,loss: 0.012863146\n",
            "\n",
            "Global step: 2813,loss: 0.010519926\n",
            "\n",
            "Global step: 2814,loss: 0.0112775015\n",
            "\n",
            "Global step: 2815,loss: 0.013632504\n",
            "\n",
            "Global step: 2816,loss: 0.016494175\n",
            "\n",
            "Global step: 2817,loss: 0.01179526\n",
            "\n",
            "Global step: 2818,loss: 0.016254501\n",
            "\n",
            "Global step: 2819,loss: 0.017801715\n",
            "\n",
            "Global step: 2820,loss: 0.014062405\n",
            "\n",
            "Global step: 2821,loss: 0.014671873\n",
            "\n",
            "Global step: 2822,loss: 0.019111775\n",
            "\n",
            "Global step: 2823,loss: 0.018105838\n",
            "\n",
            "Global step: 2824,loss: 0.0230678\n",
            "\n",
            "Global step: 2825,loss: 0.0121844895\n",
            "\n",
            "Global step: 2826,loss: 0.019677924\n",
            "\n",
            "Global step: 2827,loss: 0.012279021\n",
            "\n",
            "Global step: 2828,loss: 0.01090798\n",
            "\n",
            "Global step: 2829,loss: 0.014955263\n",
            "\n",
            "Global step: 2830,loss: 0.01670388\n",
            "\n",
            "Global step: 2831,loss: 0.015484579\n",
            "\n",
            "Global step: 2832,loss: 0.015431451\n",
            "\n",
            "Global step: 2833,loss: 0.015098283\n",
            "\n",
            "Global step: 2834,loss: 0.01233626\n",
            "\n",
            "Global step: 2835,loss: 0.0116991345\n",
            "\n",
            "Global step: 2836,loss: 0.011087106\n",
            "\n",
            "Global step: 2837,loss: 0.012207491\n",
            "\n",
            "Global step: 2838,loss: 0.013771143\n",
            "\n",
            "Global step: 2839,loss: 0.012120681\n",
            "\n",
            "Global step: 2840,loss: 0.011022296\n",
            "\n",
            "Global step: 2841,loss: 0.011204385\n",
            "\n",
            "Global step: 2842,loss: 0.0137017425\n",
            "\n",
            "Global step: 2843,loss: 0.009860939\n",
            "\n",
            "Global step: 2844,loss: 0.0133148385\n",
            "\n",
            "Global step: 2845,loss: 0.01586112\n",
            "\n",
            "Global step: 2846,loss: 0.011699696\n",
            "\n",
            "Global step: 2847,loss: 0.012054943\n",
            "\n",
            "Global step: 2848,loss: 0.016593426\n",
            "\n",
            "Global step: 2849,loss: 0.016884768\n",
            "\n",
            "Global step: 2850,loss: 0.031239003\n",
            "\n",
            "Global step: 2851,loss: 0.012122583\n",
            "\n",
            "Global step: 2852,loss: 0.011147203\n",
            "\n",
            "Global step: 2853,loss: 0.017661948\n",
            "\n",
            "Global step: 2854,loss: 0.012181994\n",
            "\n",
            "Global step: 2855,loss: 0.013201407\n",
            "\n",
            "Global step: 2856,loss: 0.01655077\n",
            "\n",
            "Global step: 2857,loss: 0.012578738\n",
            "\n",
            "Global step: 2858,loss: 0.011432318\n",
            "\n",
            "Global step: 2859,loss: 0.018626068\n",
            "\n",
            "Global step: 2860,loss: 0.014660858\n",
            "\n",
            "Global step: 2861,loss: 0.015523109\n",
            "\n",
            "Global step: 2862,loss: 0.022732664\n",
            "\n",
            "Global step: 2863,loss: 0.010889621\n",
            "\n",
            "Global step: 2864,loss: 0.012728658\n",
            "\n",
            "Global step: 2865,loss: 0.012382743\n",
            "\n",
            "Global step: 2866,loss: 0.018947609\n",
            "\n",
            "Global step: 2867,loss: 0.011639095\n",
            "\n",
            "Global step: 2868,loss: 0.011081893\n",
            "\n",
            "Global step: 2869,loss: 0.01375995\n",
            "\n",
            "Global step: 2870,loss: 0.015486266\n",
            "\n",
            "Global step: 2871,loss: 0.014202756\n",
            "\n",
            "Global step: 2872,loss: 0.015111864\n",
            "\n",
            "Global step: 2873,loss: 0.015797697\n",
            "\n",
            "Global step: 2874,loss: 0.01820271\n",
            "\n",
            "Global step: 2875,loss: 0.020007439\n",
            "\n",
            "Global step: 2876,loss: 0.012248393\n",
            "\n",
            "Global step: 2877,loss: 0.012560809\n",
            "\n",
            "Global step: 2878,loss: 0.014128409\n",
            "\n",
            "Global step: 2879,loss: 0.016203027\n",
            "\n",
            "Global step: 2880,loss: 0.014032302\n",
            "\n",
            "Global step: 2881,loss: 0.013620715\n",
            "\n",
            "Global step: 2882,loss: 0.01499991\n",
            "\n",
            "Global step: 2883,loss: 0.015296151\n",
            "\n",
            "Global step: 2884,loss: 0.014028434\n",
            "\n",
            "Global step: 2885,loss: 0.01299301\n",
            "\n",
            "Global step: 2886,loss: 0.010528485\n",
            "\n",
            "Global step: 2887,loss: 0.013149497\n",
            "\n",
            "Global step: 2888,loss: 0.01287762\n",
            "\n",
            "Global step: 2889,loss: 0.010617434\n",
            "\n",
            "Global step: 2890,loss: 0.012692835\n",
            "\n",
            "Global step: 2891,loss: 0.013061902\n",
            "\n",
            "Global step: 2892,loss: 0.01702145\n",
            "\n",
            "Global step: 2893,loss: 0.01114196\n",
            "\n",
            "Global step: 2894,loss: 0.012321214\n",
            "\n",
            "Global step: 2895,loss: 0.011255023\n",
            "\n",
            "Global step: 2896,loss: 0.010839245\n",
            "\n",
            "Global step: 2897,loss: 0.012667865\n",
            "\n",
            "Global step: 2898,loss: 0.022896327\n",
            "\n",
            "Global step: 2899,loss: 0.01208149\n",
            "\n",
            "Global step: 2900,loss: 0.013289438\n",
            "\n",
            "Global step: 2901,loss: 0.011798281\n",
            "\n",
            "Global step: 2902,loss: 0.012212641\n",
            "\n",
            "Global step: 2903,loss: 0.014926797\n",
            "\n",
            "Global step: 2904,loss: 0.013253072\n",
            "\n",
            "Global step: 2905,loss: 0.013157154\n",
            "\n",
            "Global step: 2906,loss: 0.012616514\n",
            "\n",
            "Global step: 2907,loss: 0.011192427\n",
            "\n",
            "Global step: 2908,loss: 0.014513531\n",
            "\n",
            "Global step: 2909,loss: 0.020738404\n",
            "\n",
            "Global step: 2910,loss: 0.014176057\n",
            "\n",
            "Global step: 2911,loss: 0.01210872\n",
            "\n",
            "Global step: 2912,loss: 0.012198567\n",
            "\n",
            "Global step: 2913,loss: 0.014686062\n",
            "\n",
            "Global step: 2914,loss: 0.014890946\n",
            "\n",
            "Global step: 2915,loss: 0.013803036\n",
            "\n",
            "Global step: 2916,loss: 0.011720503\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 2.82745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:11:20.593724 139635366639360 supervisor.py:1099] global_step/sec: 2.82745\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2917.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:11:20.770729 139637785016064 supervisor.py:1050] Recording summary at step 2917.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2917,loss: 0.010540607\n",
            "\n",
            "Global step: 2918,loss: 0.01638467\n",
            "\n",
            "Global step: 2919,loss: 0.011793488\n",
            "\n",
            "Global step: 2920,loss: 0.017844763\n",
            "\n",
            "Global step: 2921,loss: 0.01552044\n",
            "\n",
            "Global step: 2922,loss: 0.013844219\n",
            "\n",
            "Global step: 2923,loss: 0.015722688\n",
            "\n",
            "Global step: 2924,loss: 0.011394627\n",
            "\n",
            "Global step: 2925,loss: 0.011511455\n",
            "\n",
            "Global step: 2926,loss: 0.017915295\n",
            "\n",
            "Global step: 2927,loss: 0.010210185\n",
            "\n",
            "Global step: 2928,loss: 0.02523071\n",
            "\n",
            "Global step: 2929,loss: 0.02469153\n",
            "\n",
            "Global step: 2930,loss: 0.015578756\n",
            "\n",
            "Global step: 2931,loss: 0.011326025\n",
            "\n",
            "Global step: 2932,loss: 0.01175045\n",
            "\n",
            "Global step: 2933,loss: 0.011549206\n",
            "\n",
            "Global step: 2934,loss: 0.013347566\n",
            "\n",
            "Global step: 2935,loss: 0.011622288\n",
            "\n",
            "Global step: 2936,loss: 0.013476955\n",
            "\n",
            "Global step: 2937,loss: 0.016029287\n",
            "\n",
            "Global step: 2938,loss: 0.01472369\n",
            "\n",
            "Global step: 2939,loss: 0.011016973\n",
            "\n",
            "Global step: 2940,loss: 0.0118235145\n",
            "\n",
            "Global step: 2941,loss: 0.011572856\n",
            "\n",
            "Global step: 2942,loss: 0.011514086\n",
            "\n",
            "Global step: 2943,loss: 0.014086202\n",
            "\n",
            "Global step: 2944,loss: 0.010915413\n",
            "\n",
            "Global step: 2945,loss: 0.011298588\n",
            "\n",
            "Global step: 2946,loss: 0.017551158\n",
            "\n",
            "Global step: 2947,loss: 0.014277018\n",
            "\n",
            "Global step: 2948,loss: 0.0102951955\n",
            "\n",
            "Global step: 2949,loss: 0.011397134\n",
            "\n",
            "Global step: 2950,loss: 0.017716128\n",
            "\n",
            "Global step: 2951,loss: 0.016725449\n",
            "\n",
            "Global step: 2952,loss: 0.0151165705\n",
            "\n",
            "Global step: 2953,loss: 0.018676538\n",
            "\n",
            "Global step: 2954,loss: 0.021376962\n",
            "\n",
            "Global step: 2955,loss: 0.0164239\n",
            "\n",
            "Global step: 2956,loss: 0.012406442\n",
            "\n",
            "Global step: 2957,loss: 0.012113562\n",
            "\n",
            "Global step: 2958,loss: 0.014212923\n",
            "\n",
            "Global step: 2959,loss: 0.021059837\n",
            "\n",
            "Global step: 2960,loss: 0.011842589\n",
            "\n",
            "Global step: 2961,loss: 0.012536262\n",
            "\n",
            "Global step: 2962,loss: 0.031367984\n",
            "\n",
            "Global step: 2963,loss: 0.01117078\n",
            "\n",
            "Global step: 2964,loss: 0.014546467\n",
            "\n",
            "Global step: 2965,loss: 0.011286855\n",
            "\n",
            "Global step: 2966,loss: 0.011770936\n",
            "\n",
            "Global step: 2967,loss: 0.011752432\n",
            "\n",
            "Global step: 2968,loss: 0.020586409\n",
            "\n",
            "Global step: 2969,loss: 0.0108384825\n",
            "\n",
            "Global step: 2970,loss: 0.010655238\n",
            "\n",
            "Global step: 2971,loss: 0.012441212\n",
            "\n",
            "Global step: 2972,loss: 0.014292557\n",
            "\n",
            "Global step: 2973,loss: 0.023219336\n",
            "\n",
            "Global step: 2974,loss: 0.021992505\n",
            "\n",
            "Global step: 2975,loss: 0.016458157\n",
            "\n",
            "Global step: 2976,loss: 0.01308826\n",
            "\n",
            "Global step: 2977,loss: 0.012158919\n",
            "\n",
            "Global step: 2978,loss: 0.016108869\n",
            "\n",
            "Global step: 2979,loss: 0.010833398\n",
            "\n",
            "Global step: 2980,loss: 0.017335717\n",
            "\n",
            "Global step: 2981,loss: 0.012210662\n",
            "\n",
            "Global step: 2982,loss: 0.020081664\n",
            "\n",
            "Global step: 2983,loss: 0.02116146\n",
            "\n",
            "Global step: 2984,loss: 0.01659311\n",
            "\n",
            "Global step: 2985,loss: 0.013971814\n",
            "\n",
            "Global step: 2986,loss: 0.0115156295\n",
            "\n",
            "Global step: 2987,loss: 0.017795209\n",
            "\n",
            "Global step: 2988,loss: 0.014257585\n",
            "\n",
            "Global step: 2989,loss: 0.012567364\n",
            "\n",
            "Global step: 2990,loss: 0.016185362\n",
            "\n",
            "Global step: 2991,loss: 0.013745798\n",
            "\n",
            "Global step: 2992,loss: 0.012784843\n",
            "\n",
            "Global step: 2993,loss: 0.015270213\n",
            "\n",
            "Global step: 2994,loss: 0.017598394\n",
            "\n",
            "Global step: 2995,loss: 0.011976975\n",
            "\n",
            "Global step: 2996,loss: 0.015699133\n",
            "\n",
            "Global step: 2997,loss: 0.0126119945\n",
            "\n",
            "Global step: 2998,loss: 0.011090908\n",
            "\n",
            "Global step: 2999,loss: 0.019745769\n",
            "\n",
            "Global step: 3000,loss: 0.016045773\n",
            "\n",
            "Global step: 3001,loss: 0.013687927\n",
            "\n",
            "Global step: 3002,loss: 0.016950367\n",
            "\n",
            "Global step: 3003,loss: 0.013904689\n",
            "\n",
            "Global step: 3004,loss: 0.018834252\n",
            "\n",
            "Global step: 3005,loss: 0.018649418\n",
            "\n",
            "Global step: 3006,loss: 0.015161589\n",
            "\n",
            "Global step: 3007,loss: 0.021905556\n",
            "\n",
            "Global step: 3008,loss: 0.010601497\n",
            "\n",
            "Global step: 3009,loss: 0.0147732925\n",
            "\n",
            "Global step: 3010,loss: 0.014118575\n",
            "\n",
            "Global step: 3011,loss: 0.012805749\n",
            "\n",
            "Global step: 3012,loss: 0.012152152\n",
            "\n",
            "Global step: 3013,loss: 0.011486944\n",
            "\n",
            "Global step: 3014,loss: 0.012748319\n",
            "\n",
            "Global step: 3015,loss: 0.0145220645\n",
            "\n",
            "Global step: 3016,loss: 0.014500901\n",
            "\n",
            "Global step: 3017,loss: 0.012189976\n",
            "\n",
            "Global step: 3018,loss: 0.018229187\n",
            "\n",
            "Global step: 3019,loss: 0.011996519\n",
            "\n",
            "Global step: 3020,loss: 0.014557546\n",
            "\n",
            "Global step: 3021,loss: 0.016106237\n",
            "\n",
            "Global step: 3022,loss: 0.019498112\n",
            "\n",
            "Global step: 3023,loss: 0.020567968\n",
            "\n",
            "Global step: 3024,loss: 0.0137354415\n",
            "\n",
            "Global step: 3025,loss: 0.014521213\n",
            "\n",
            "Global step: 3026,loss: 0.013842661\n",
            "\n",
            "Global step: 3027,loss: 0.017539313\n",
            "\n",
            "Global step: 3028,loss: 0.018071616\n",
            "\n",
            "Global step: 3029,loss: 0.010309336\n",
            "\n",
            "Global step: 3030,loss: 0.019063758\n",
            "\n",
            "Global step: 3031,loss: 0.013060316\n",
            "\n",
            "Global step: 3032,loss: 0.010962889\n",
            "\n",
            "Global step: 3033,loss: 0.018981483\n",
            "\n",
            "Global step: 3034,loss: 0.014507931\n",
            "\n",
            "Global step: 3035,loss: 0.014853668\n",
            "\n",
            "Global step: 3036,loss: 0.013104305\n",
            "\n",
            "Global step: 3037,loss: 0.014536971\n",
            "\n",
            "Global step: 3038,loss: 0.015691888\n",
            "\n",
            "Global step: 3039,loss: 0.013295298\n",
            "\n",
            "Global step: 3040,loss: 0.012305577\n",
            "\n",
            "Global step: 3041,loss: 0.013384449\n",
            "\n",
            "Global step: 3042,loss: 0.014319612\n",
            "\n",
            "Global step: 3043,loss: 0.020053204\n",
            "\n",
            "Global step: 3044,loss: 0.016878061\n",
            "\n",
            "Global step: 3045,loss: 0.012150614\n",
            "\n",
            "Global step: 3046,loss: 0.011233576\n",
            "\n",
            "Global step: 3047,loss: 0.011855284\n",
            "\n",
            "Global step: 3048,loss: 0.016258901\n",
            "\n",
            "Global step: 3049,loss: 0.01899125\n",
            "\n",
            "Global step: 3050,loss: 0.016667696\n",
            "\n",
            "Global step: 3051,loss: 0.014917769\n",
            "\n",
            "Global step: 3052,loss: 0.021677144\n",
            "\n",
            "Global step: 3053,loss: 0.011747982\n",
            "\n",
            "Global step: 3054,loss: 0.012993808\n",
            "\n",
            "Global step: 3055,loss: 0.011323342\n",
            "\n",
            "Global step: 3056,loss: 0.012189692\n",
            "\n",
            "Global step: 3057,loss: 0.013409075\n",
            "\n",
            "Global step: 3058,loss: 0.011133347\n",
            "\n",
            "Global step: 3059,loss: 0.012300255\n",
            "\n",
            "Global step: 3060,loss: 0.012789713\n",
            "\n",
            "Global step: 3061,loss: 0.012024192\n",
            "\n",
            "Global step: 3062,loss: 0.012311477\n",
            "\n",
            "Global step: 3063,loss: 0.011574502\n",
            "\n",
            "Global step: 3064,loss: 0.019407783\n",
            "\n",
            "Global step: 3065,loss: 0.012022992\n",
            "\n",
            "Global step: 3066,loss: 0.011725892\n",
            "\n",
            "Global step: 3067,loss: 0.013719808\n",
            "\n",
            "Global step: 3068,loss: 0.0134259835\n",
            "\n",
            "Global step: 3069,loss: 0.011279276\n",
            "\n",
            "Global step: 3070,loss: 0.011459411\n",
            "\n",
            "Global step: 3071,loss: 0.012226165\n",
            "\n",
            "Global step: 3072,loss: 0.011478268\n",
            "\n",
            "Global step: 3073,loss: 0.0115824165\n",
            "\n",
            "Global step: 3074,loss: 0.011631356\n",
            "\n",
            "Global step: 3075,loss: 0.011504097\n",
            "\n",
            "Global step: 3076,loss: 0.017477047\n",
            "\n",
            "Global step: 3077,loss: 0.017448101\n",
            "\n",
            "Global step: 3078,loss: 0.013602048\n",
            "\n",
            "Global step: 3079,loss: 0.01276043\n",
            "\n",
            "Global step: 3080,loss: 0.010999812\n",
            "\n",
            "Global step: 3081,loss: 0.012756929\n",
            "\n",
            "Global step: 3082,loss: 0.014026741\n",
            "\n",
            "Global step: 3083,loss: 0.011653205\n",
            "\n",
            "Global step: 3084,loss: 0.013104219\n",
            "\n",
            "Global step: 3085,loss: 0.014681258\n",
            "\n",
            "Global step: 3086,loss: 0.009725241\n",
            "\n",
            "Global step: 3087,loss: 0.013748485\n",
            "\n",
            "Global step: 3088,loss: 0.013882216\n",
            "\n",
            "Global step: 3089,loss: 0.010502116\n",
            "\n",
            "Global step: 3090,loss: 0.011592957\n",
            "\n",
            "Global step: 3091,loss: 0.012118405\n",
            "\n",
            "Global step: 3092,loss: 0.012476627\n",
            "\n",
            "Global step: 3093,loss: 0.013016635\n",
            "\n",
            "Global step: 3094,loss: 0.011044186\n",
            "\n",
            "Global step: 3095,loss: 0.012040997\n",
            "\n",
            "Global step: 3096,loss: 0.015389419\n",
            "\n",
            "Global step: 3097,loss: 0.013611076\n",
            "\n",
            "Global step: 3098,loss: 0.011401892\n",
            "\n",
            "Global step: 3099,loss: 0.012539273\n",
            "\n",
            "Global step: 3100,loss: 0.0125470115\n",
            "\n",
            "Global step: 3101,loss: 0.010413157\n",
            "\n",
            "Global step: 3102,loss: 0.013993889\n",
            "\n",
            "Global step: 3103,loss: 0.014797752\n",
            "\n",
            "Global step: 3104,loss: 0.011906107\n",
            "\n",
            "Global step: 3105,loss: 0.020105386\n",
            "\n",
            "Global step: 3106,loss: 0.010445514\n",
            "\n",
            "Global step: 3107,loss: 0.01055416\n",
            "\n",
            "Global step: 3108,loss: 0.013058396\n",
            "\n",
            "Global step: 3109,loss: 0.011732037\n",
            "\n",
            "Global step: 3110,loss: 0.012380653\n",
            "\n",
            "Global step: 3111,loss: 0.013129054\n",
            "\n",
            "Global step: 3112,loss: 0.019440202\n",
            "\n",
            "Global step: 3113,loss: 0.017229326\n",
            "\n",
            "Global step: 3114,loss: 0.011975324\n",
            "\n",
            "Global step: 3115,loss: 0.01455234\n",
            "\n",
            "Global step: 3116,loss: 0.01194473\n",
            "\n",
            "Global step: 3117,loss: 0.012994459\n",
            "\n",
            "Global step: 3118,loss: 0.011223702\n",
            "\n",
            "Global step: 3119,loss: 0.010748653\n",
            "\n",
            "Global step: 3120,loss: 0.010526973\n",
            "\n",
            "Global step: 3121,loss: 0.011308363\n",
            "\n",
            "Global step: 3122,loss: 0.010639161\n",
            "\n",
            "Global step: 3123,loss: 0.01644912\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3123,Val_Loss: 0.014114382640960125,  Val_acc: 0.9981971153846154 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 16:12:46.436544 139641102628736 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/20:\n",
            "Global step: 3124,loss: 0.011590205\n",
            "\n",
            "Global step: 3125,loss: 0.015758894\n",
            "\n",
            "Global step: 3126,loss: 0.010817743\n",
            "\n",
            "Global step: 3127,loss: 0.012719614\n",
            "\n",
            "Global step: 3128,loss: 0.012056718\n",
            "\n",
            "Global step: 3129,loss: 0.013233893\n",
            "\n",
            "Global step: 3130,loss: 0.016013408\n",
            "\n",
            "Global step: 3131,loss: 0.013880709\n",
            "\n",
            "Global step: 3132,loss: 0.013174843\n",
            "\n",
            "Global step: 3133,loss: 0.013656276\n",
            "\n",
            "Global step: 3134,loss: 0.018450987\n",
            "\n",
            "Global step: 3135,loss: 0.010210142\n",
            "\n",
            "Global step: 3136,loss: 0.011197865\n",
            "\n",
            "Global step: 3137,loss: 0.0131318215\n",
            "\n",
            "Global step: 3138,loss: 0.010921607\n",
            "\n",
            "Global step: 3139,loss: 0.01040476\n",
            "\n",
            "Global step: 3140,loss: 0.011177184\n",
            "\n",
            "Global step: 3141,loss: 0.013108268\n",
            "\n",
            "Global step: 3142,loss: 0.011589229\n",
            "\n",
            "Global step: 3143,loss: 0.011127634\n",
            "\n",
            "Global step: 3144,loss: 0.013601146\n",
            "\n",
            "Global step: 3145,loss: 0.012055562\n",
            "\n",
            "Global step: 3146,loss: 0.019144783\n",
            "\n",
            "Global step: 3147,loss: 0.012207023\n",
            "\n",
            "Global step: 3148,loss: 0.011632412\n",
            "\n",
            "Global step: 3149,loss: 0.02144865\n",
            "\n",
            "Global step: 3150,loss: 0.012085455\n",
            "\n",
            "Global step: 3151,loss: 0.013355916\n",
            "\n",
            "Global step: 3152,loss: 0.011196058\n",
            "\n",
            "Global step: 3153,loss: 0.011081992\n",
            "\n",
            "Global step: 3154,loss: 0.011172513\n",
            "\n",
            "Global step: 3155,loss: 0.011413757\n",
            "\n",
            "Global step: 3156,loss: 0.015492053\n",
            "\n",
            "Global step: 3157,loss: 0.014432516\n",
            "\n",
            "Global step: 3158,loss: 0.01114223\n",
            "\n",
            "Global step: 3159,loss: 0.010111106\n",
            "\n",
            "Global step: 3160,loss: 0.0124096135\n",
            "\n",
            "Global step: 3161,loss: 0.014328987\n",
            "\n",
            "Global step: 3162,loss: 0.011914352\n",
            "\n",
            "Global step: 3163,loss: 0.0114779845\n",
            "\n",
            "Global step: 3164,loss: 0.011559772\n",
            "\n",
            "Global step: 3165,loss: 0.016804617\n",
            "\n",
            "Global step: 3166,loss: 0.010734943\n",
            "\n",
            "Global step: 3167,loss: 0.012161254\n",
            "\n",
            "Global step: 3168,loss: 0.01223344\n",
            "\n",
            "Global step: 3169,loss: 0.014356955\n",
            "\n",
            "Global step: 3170,loss: 0.012902306\n",
            "\n",
            "Global step: 3171,loss: 0.012741621\n",
            "\n",
            "Global step: 3172,loss: 0.0136050675\n",
            "\n",
            "Global step: 3173,loss: 0.0103613585\n",
            "\n",
            "Global step: 3174,loss: 0.012256488\n",
            "\n",
            "Global step: 3175,loss: 0.011154479\n",
            "\n",
            "Global step: 3176,loss: 0.012413892\n",
            "\n",
            "Global step: 3177,loss: 0.012992747\n",
            "\n",
            "Global step: 3178,loss: 0.010352595\n",
            "\n",
            "Global step: 3179,loss: 0.010006819\n",
            "\n",
            "Global step: 3180,loss: 0.012197356\n",
            "\n",
            "Global step: 3181,loss: 0.011056036\n",
            "\n",
            "Global step: 3182,loss: 0.010526635\n",
            "\n",
            "Global step: 3183,loss: 0.016663164\n",
            "\n",
            "Global step: 3184,loss: 0.01008827\n",
            "\n",
            "Global step: 3185,loss: 0.015526129\n",
            "\n",
            "Global step: 3186,loss: 0.01126094\n",
            "\n",
            "Global step: 3187,loss: 0.010973773\n",
            "\n",
            "Global step: 3188,loss: 0.010144609\n",
            "\n",
            "Global step: 3189,loss: 0.0107896235\n",
            "\n",
            "Global step: 3190,loss: 0.014215315\n",
            "\n",
            "Global step: 3191,loss: 0.009981592\n",
            "\n",
            "Global step: 3192,loss: 0.009992781\n",
            "\n",
            "Global step: 3193,loss: 0.01140704\n",
            "\n",
            "Global step: 3194,loss: 0.010458839\n",
            "\n",
            "Global step: 3195,loss: 0.012182077\n",
            "\n",
            "Global step: 3196,loss: 0.012507106\n",
            "\n",
            "Global step: 3197,loss: 0.011365302\n",
            "\n",
            "Global step: 3198,loss: 0.014372002\n",
            "\n",
            "Global step: 3199,loss: 0.012406226\n",
            "\n",
            "Global step: 3200,loss: 0.010156028\n",
            "\n",
            "Global step: 3201,loss: 0.013184126\n",
            "\n",
            "Global step: 3202,loss: 0.0134025095\n",
            "\n",
            "Global step: 3203,loss: 0.010913942\n",
            "\n",
            "Global step: 3204,loss: 0.011235151\n",
            "\n",
            "Global step: 3205,loss: 0.0106116645\n",
            "\n",
            "Global step: 3206,loss: 0.010582103\n",
            "\n",
            "Global step: 3207,loss: 0.010336944\n",
            "\n",
            "Global step: 3208,loss: 0.0101554375\n",
            "\n",
            "Global step: 3209,loss: 0.01232646\n",
            "\n",
            "Global step: 3210,loss: 0.010249108\n",
            "\n",
            "Global step: 3211,loss: 0.010678365\n",
            "\n",
            "Global step: 3212,loss: 0.013524172\n",
            "\n",
            "Global step: 3213,loss: 0.010808299\n",
            "\n",
            "Global step: 3214,loss: 0.010469417\n",
            "\n",
            "Global step: 3215,loss: 0.010027524\n",
            "\n",
            "Global step: 3216,loss: 0.011360976\n",
            "\n",
            "Global step: 3217,loss: 0.010513254\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3218.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:13:20.796315 139637785016064 supervisor.py:1050] Recording summary at step 3218.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3218,loss: 0.01440559\n",
            "\n",
            "Global step: 3219,loss: 0.01010258\n",
            "\n",
            "Global step: 3220,loss: 0.018536676\n",
            "\n",
            "Global step: 3221,loss: 0.009896271\n",
            "\n",
            "Global step: 3222,loss: 0.019654945\n",
            "\n",
            "Global step: 3223,loss: 0.010718723\n",
            "\n",
            "Global step: 3224,loss: 0.012999159\n",
            "\n",
            "Global step: 3225,loss: 0.011516923\n",
            "\n",
            "Global step: 3226,loss: 0.014025527\n",
            "\n",
            "Global step: 3227,loss: 0.010467473\n",
            "\n",
            "Global step: 3228,loss: 0.010901842\n",
            "\n",
            "Global step: 3229,loss: 0.01012235\n",
            "\n",
            "Global step: 3230,loss: 0.011743125\n",
            "\n",
            "Global step: 3231,loss: 0.012453386\n",
            "\n",
            "Global step: 3232,loss: 0.013764089\n",
            "\n",
            "Global step: 3233,loss: 0.0099441055\n",
            "\n",
            "Global step: 3234,loss: 0.013719964\n",
            "\n",
            "Global step: 3235,loss: 0.012416717\n",
            "\n",
            "Global step: 3236,loss: 0.009748078\n",
            "\n",
            "Global step: 3237,loss: 0.012097591\n",
            "\n",
            "Global step: 3238,loss: 0.01257172\n",
            "\n",
            "Global step: 3239,loss: 0.012212185\n",
            "\n",
            "Global step: 3240,loss: 0.010523882\n",
            "\n",
            "Global step: 3241,loss: 0.015293859\n",
            "\n",
            "Global step: 3242,loss: 0.011507554\n",
            "\n",
            "Global step: 3243,loss: 0.010739037\n",
            "\n",
            "Global step: 3244,loss: 0.015674533\n",
            "\n",
            "Global step: 3245,loss: 0.011481952\n",
            "\n",
            "Global step: 3246,loss: 0.0113725\n",
            "\n",
            "Global step: 3247,loss: 0.012202949\n",
            "\n",
            "Global step: 3248,loss: 0.012607511\n",
            "\n",
            "Global step: 3249,loss: 0.012219304\n",
            "\n",
            "Global step: 3250,loss: 0.012607857\n",
            "\n",
            "Global step: 3251,loss: 0.014536409\n",
            "\n",
            "Global step: 3252,loss: 0.011446238\n",
            "\n",
            "Global step: 3253,loss: 0.025399929\n",
            "\n",
            "Global step: 3254,loss: 0.016094401\n",
            "\n",
            "Global step: 3255,loss: 0.015675474\n",
            "\n",
            "Global step: 3256,loss: 0.010893373\n",
            "\n",
            "Global step: 3257,loss: 0.011799226\n",
            "\n",
            "Global step: 3258,loss: 0.017146947\n",
            "\n",
            "Global step: 3259,loss: 0.010155903\n",
            "\n",
            "Global step: 3260,loss: 0.013370177\n",
            "\n",
            "Global step: 3261,loss: 0.011026008\n",
            "\n",
            "Global step: 3262,loss: 0.011086947\n",
            "\n",
            "Global step: 3263,loss: 0.011997327\n",
            "\n",
            "Global step: 3264,loss: 0.011411613\n",
            "\n",
            "Global step: 3265,loss: 0.010358796\n",
            "\n",
            "Global step: 3266,loss: 0.011212812\n",
            "\n",
            "Global step: 3267,loss: 0.011928255\n",
            "\n",
            "Global step: 3268,loss: 0.009766933\n",
            "\n",
            "Global step: 3269,loss: 0.011884065\n",
            "\n",
            "Global step: 3270,loss: 0.013096979\n",
            "\n",
            "Global step: 3271,loss: 0.01053144\n",
            "\n",
            "Global step: 3272,loss: 0.01299395\n",
            "\n",
            "Global step: 3273,loss: 0.010223247\n",
            "\n",
            "Global step: 3274,loss: 0.010392957\n",
            "\n",
            "Global step: 3275,loss: 0.0115900375\n",
            "\n",
            "Global step: 3276,loss: 0.015351226\n",
            "\n",
            "Global step: 3277,loss: 0.0100087365\n",
            "\n",
            "Global step: 3278,loss: 0.010130945\n",
            "\n",
            "Global step: 3279,loss: 0.009712957\n",
            "\n",
            "Global step: 3280,loss: 0.011207115\n",
            "\n",
            "Global step: 3281,loss: 0.011248473\n",
            "\n",
            "Global step: 3282,loss: 0.011266261\n",
            "\n",
            "Global step: 3283,loss: 0.01050069\n",
            "\n",
            "Global step: 3284,loss: 0.015272241\n",
            "\n",
            "Global step: 3285,loss: 0.011394616\n",
            "\n",
            "Global step: 3286,loss: 0.011448579\n",
            "\n",
            "Global step: 3287,loss: 0.012122631\n",
            "\n",
            "Global step: 3288,loss: 0.016730202\n",
            "\n",
            "Global step: 3289,loss: 0.012619023\n",
            "\n",
            "Global step: 3290,loss: 0.010649939\n",
            "\n",
            "Global step: 3291,loss: 0.011155239\n",
            "\n",
            "Global step: 3292,loss: 0.016430266\n",
            "\n",
            "Global step: 3293,loss: 0.013905518\n",
            "\n",
            "Global step: 3294,loss: 0.020020936\n",
            "\n",
            "Global step: 3295,loss: 0.010274395\n",
            "\n",
            "Global step: 3296,loss: 0.009888053\n",
            "\n",
            "Global step: 3297,loss: 0.0110247815\n",
            "\n",
            "Global step: 3298,loss: 0.020631257\n",
            "\n",
            "Global step: 3299,loss: 0.011520347\n",
            "\n",
            "Global step: 3300,loss: 0.01311447\n",
            "\n",
            "Global step: 3301,loss: 0.010513483\n",
            "\n",
            "Global step: 3302,loss: 0.013466165\n",
            "\n",
            "Global step: 3303,loss: 0.014504761\n",
            "\n",
            "Global step: 3304,loss: 0.010602599\n",
            "\n",
            "Global step: 3305,loss: 0.013718223\n",
            "\n",
            "Global step: 3306,loss: 0.009911762\n",
            "\n",
            "Global step: 3307,loss: 0.0107336845\n",
            "\n",
            "Global step: 3308,loss: 0.011973908\n",
            "\n",
            "Global step: 3309,loss: 0.018138938\n",
            "\n",
            "Global step: 3310,loss: 0.010532548\n",
            "\n",
            "Global step: 3311,loss: 0.019369729\n",
            "\n",
            "Global step: 3312,loss: 0.013079847\n",
            "\n",
            "Global step: 3313,loss: 0.010293833\n",
            "\n",
            "Global step: 3314,loss: 0.0102996\n",
            "\n",
            "Global step: 3315,loss: 0.014224797\n",
            "\n",
            "Global step: 3316,loss: 0.012357933\n",
            "\n",
            "Global step: 3317,loss: 0.01046753\n",
            "\n",
            "Global step: 3318,loss: 0.011645745\n",
            "\n",
            "Global step: 3319,loss: 0.016532306\n",
            "\n",
            "Global step: 3320,loss: 0.010620204\n",
            "\n",
            "Global step: 3321,loss: 0.012154983\n",
            "\n",
            "Global step: 3322,loss: 0.010599405\n",
            "\n",
            "Global step: 3323,loss: 0.009830428\n",
            "\n",
            "Global step: 3324,loss: 0.01599218\n",
            "\n",
            "Global step: 3325,loss: 0.010367396\n",
            "\n",
            "Global step: 3326,loss: 0.010348719\n",
            "\n",
            "Global step: 3327,loss: 0.011067481\n",
            "\n",
            "Global step: 3328,loss: 0.011088077\n",
            "\n",
            "Global step: 3329,loss: 0.0097249225\n",
            "\n",
            "Global step: 3330,loss: 0.010104859\n",
            "\n",
            "Global step: 3331,loss: 0.014165502\n",
            "\n",
            "Global step: 3332,loss: 0.010289222\n",
            "\n",
            "Global step: 3333,loss: 0.014846301\n",
            "\n",
            "Global step: 3334,loss: 0.011470167\n",
            "\n",
            "Global step: 3335,loss: 0.014968439\n",
            "\n",
            "Global step: 3336,loss: 0.011191764\n",
            "\n",
            "Global step: 3337,loss: 0.010883267\n",
            "\n",
            "Global step: 3338,loss: 0.011185314\n",
            "\n",
            "Global step: 3339,loss: 0.009206145\n",
            "\n",
            "Global step: 3340,loss: 0.01002005\n",
            "\n",
            "Global step: 3341,loss: 0.0094497\n",
            "\n",
            "Global step: 3342,loss: 0.01247071\n",
            "\n",
            "Global step: 3343,loss: 0.010674296\n",
            "\n",
            "Global step: 3344,loss: 0.01228912\n",
            "\n",
            "Global step: 3345,loss: 0.010090432\n",
            "\n",
            "Global step: 3346,loss: 0.010329368\n",
            "\n",
            "Global step: 3347,loss: 0.009964367\n",
            "\n",
            "Global step: 3348,loss: 0.010913215\n",
            "\n",
            "Global step: 3349,loss: 0.01131476\n",
            "\n",
            "Global step: 3350,loss: 0.011533941\n",
            "\n",
            "Global step: 3351,loss: 0.010472391\n",
            "\n",
            "Global step: 3352,loss: 0.012833951\n",
            "\n",
            "Global step: 3353,loss: 0.010472817\n",
            "\n",
            "Global step: 3354,loss: 0.010202197\n",
            "\n",
            "Global step: 3355,loss: 0.010722408\n",
            "\n",
            "Global step: 3356,loss: 0.009929618\n",
            "\n",
            "Global step: 3357,loss: 0.010478219\n",
            "\n",
            "Global step: 3358,loss: 0.010581099\n",
            "\n",
            "Global step: 3359,loss: 0.012226401\n",
            "\n",
            "Global step: 3360,loss: 0.011766146\n",
            "\n",
            "Global step: 3361,loss: 0.02123588\n",
            "\n",
            "Global step: 3362,loss: 0.010259906\n",
            "\n",
            "Global step: 3363,loss: 0.010581339\n",
            "\n",
            "Global step: 3364,loss: 0.014653124\n",
            "\n",
            "Global step: 3365,loss: 0.012094252\n",
            "\n",
            "Global step: 3366,loss: 0.01751699\n",
            "\n",
            "Global step: 3367,loss: 0.0141841555\n",
            "\n",
            "Global step: 3368,loss: 0.009642684\n",
            "\n",
            "Global step: 3369,loss: 0.011678828\n",
            "\n",
            "Global step: 3370,loss: 0.010552645\n",
            "\n",
            "Global step: 3371,loss: 0.010871988\n",
            "\n",
            "Global step: 3372,loss: 0.01153322\n",
            "\n",
            "Global step: 3373,loss: 0.011003156\n",
            "\n",
            "Global step: 3374,loss: 0.010222589\n",
            "\n",
            "Global step: 3375,loss: 0.012777135\n",
            "\n",
            "Global step: 3376,loss: 0.013046794\n",
            "\n",
            "Global step: 3377,loss: 0.01201682\n",
            "\n",
            "Global step: 3378,loss: 0.01253079\n",
            "\n",
            "Global step: 3379,loss: 0.014064714\n",
            "\n",
            "Global step: 3380,loss: 0.011146199\n",
            "\n",
            "Global step: 3381,loss: 0.009971844\n",
            "\n",
            "Global step: 3382,loss: 0.009771927\n",
            "\n",
            "Global step: 3383,loss: 0.011944205\n",
            "\n",
            "Global step: 3384,loss: 0.011855561\n",
            "\n",
            "Global step: 3385,loss: 0.010080662\n",
            "\n",
            "Global step: 3386,loss: 0.010606656\n",
            "\n",
            "Global step: 3387,loss: 0.011153091\n",
            "\n",
            "Global step: 3388,loss: 0.012403797\n",
            "\n",
            "Global step: 3389,loss: 0.014378476\n",
            "\n",
            "Global step: 3390,loss: 0.009751141\n",
            "\n",
            "Global step: 3391,loss: 0.012863096\n",
            "\n",
            "Global step: 3392,loss: 0.009855869\n",
            "\n",
            "Global step: 3393,loss: 0.014879145\n",
            "\n",
            "Global step: 3394,loss: 0.011816073\n",
            "\n",
            "Global step: 3395,loss: 0.009784726\n",
            "\n",
            "Global step: 3396,loss: 0.01058976\n",
            "\n",
            "Global step: 3397,loss: 0.010662656\n",
            "\n",
            "Global step: 3398,loss: 0.01473919\n",
            "\n",
            "Global step: 3399,loss: 0.0101862\n",
            "\n",
            "Global step: 3400,loss: 0.012061312\n",
            "\n",
            "Global step: 3401,loss: 0.0114538\n",
            "\n",
            "Global step: 3402,loss: 0.012242183\n",
            "\n",
            "Global step: 3403,loss: 0.010892952\n",
            "\n",
            "Global step: 3404,loss: 0.014999576\n",
            "\n",
            "Global step: 3405,loss: 0.010196834\n",
            "\n",
            "Global step: 3406,loss: 0.0104196165\n",
            "\n",
            "Global step: 3407,loss: 0.0128507\n",
            "\n",
            "Global step: 3408,loss: 0.012787855\n",
            "\n",
            "Global step: 3409,loss: 0.011171907\n",
            "\n",
            "Global step: 3410,loss: 0.014235517\n",
            "\n",
            "Global step: 3411,loss: 0.010177844\n",
            "\n",
            "Global step: 3412,loss: 0.013164616\n",
            "\n",
            "Global step: 3413,loss: 0.013091649\n",
            "\n",
            "Global step: 3414,loss: 0.01048858\n",
            "\n",
            "Global step: 3415,loss: 0.009666697\n",
            "\n",
            "Global step: 3416,loss: 0.010837595\n",
            "\n",
            "Global step: 3417,loss: 0.013536043\n",
            "\n",
            "Global step: 3418,loss: 0.010272142\n",
            "\n",
            "Global step: 3419,loss: 0.010508469\n",
            "\n",
            "Global step: 3420,loss: 0.015938425\n",
            "\n",
            "Global step: 3421,loss: 0.011220579\n",
            "\n",
            "Global step: 3422,loss: 0.010778464\n",
            "\n",
            "Global step: 3423,loss: 0.013890414\n",
            "\n",
            "Global step: 3424,loss: 0.010509434\n",
            "\n",
            "Global step: 3425,loss: 0.010130249\n",
            "\n",
            "Global step: 3426,loss: 0.011364492\n",
            "\n",
            "Global step: 3427,loss: 0.012447014\n",
            "\n",
            "Global step: 3428,loss: 0.0103660915\n",
            "\n",
            "Global step: 3429,loss: 0.014204793\n",
            "\n",
            "Global step: 3430,loss: 0.012965266\n",
            "\n",
            "Global step: 3431,loss: 0.009931685\n",
            "\n",
            "Global step: 3432,loss: 0.0127002895\n",
            "\n",
            "Global step: 3433,loss: 0.011902524\n",
            "\n",
            "Global step: 3434,loss: 0.010762749\n",
            "\n",
            "Global step: 3435,loss: 0.014817374\n",
            "\n",
            "Global step: 3436,loss: 0.009938224\n",
            "\n",
            "Global step: 3437,loss: 0.010827379\n",
            "\n",
            "Global step: 3438,loss: 0.010885088\n",
            "\n",
            "Global step: 3439,loss: 0.010883997\n",
            "\n",
            "Global step: 3440,loss: 0.010560364\n",
            "\n",
            "Global step: 3441,loss: 0.011835396\n",
            "\n",
            "Global step: 3442,loss: 0.011794503\n",
            "\n",
            "Global step: 3443,loss: 0.010169314\n",
            "\n",
            "Global step: 3444,loss: 0.010892594\n",
            "\n",
            "Global step: 3445,loss: 0.016670654\n",
            "\n",
            "Global step: 3446,loss: 0.012580149\n",
            "\n",
            "Global step: 3447,loss: 0.009830207\n",
            "\n",
            "Global step: 3448,loss: 0.010948268\n",
            "\n",
            "Global step: 3449,loss: 0.011678876\n",
            "\n",
            "Global step: 3450,loss: 0.010400422\n",
            "\n",
            "Global step: 3451,loss: 0.011133164\n",
            "\n",
            "Global step: 3452,loss: 0.013141049\n",
            "\n",
            "Global step: 3453,loss: 0.010348167\n",
            "\n",
            "Global step: 3454,loss: 0.012871701\n",
            "\n",
            "Global step: 3455,loss: 0.012573417\n",
            "\n",
            "Global step: 3456,loss: 0.010772809\n",
            "\n",
            "Global step: 3457,loss: 0.011398924\n",
            "\n",
            "Global step: 3458,loss: 0.0107277585\n",
            "\n",
            "Global step: 3459,loss: 0.012168419\n",
            "\n",
            "Global step: 3460,loss: 0.013276333\n",
            "\n",
            "Global step: 3461,loss: 0.015216701\n",
            "\n",
            "Global step: 3462,loss: 0.015949853\n",
            "\n",
            "Global step: 3463,loss: 0.012181106\n",
            "\n",
            "Global step: 3464,loss: 0.014061955\n",
            "\n",
            "Global step: 3465,loss: 0.01415107\n",
            "\n",
            "Global step: 3466,loss: 0.010423345\n",
            "\n",
            "Global step: 3467,loss: 0.011132886\n",
            "\n",
            "Global step: 3468,loss: 0.01289245\n",
            "\n",
            "Global step: 3469,loss: 0.019584488\n",
            "\n",
            "Global step: 3470,loss: 0.01006793\n",
            "\n",
            "Global step: 3471,loss: 0.010307848\n",
            "\n",
            "Global step: 3472,loss: 0.0099571\n",
            "\n",
            "Global step: 3473,loss: 0.010221161\n",
            "\n",
            "Global step: 3474,loss: 0.011025585\n",
            "\n",
            "Global step: 3475,loss: 0.011743564\n",
            "\n",
            "Global step: 3476,loss: 0.013137426\n",
            "\n",
            "Global step: 3477,loss: 0.010038613\n",
            "\n",
            "Global step: 3478,loss: 0.010124609\n",
            "\n",
            "Global step: 3479,loss: 0.013650043\n",
            "\n",
            "Global step: 3480,loss: 0.010381963\n",
            "\n",
            "Global step: 3481,loss: 0.012553901\n",
            "\n",
            "Global step: 3482,loss: 0.01179038\n",
            "\n",
            "Global step: 3483,loss: 0.010138904\n",
            "\n",
            "Global step: 3484,loss: 0.011798568\n",
            "\n",
            "Global step: 3485,loss: 0.011051003\n",
            "\n",
            "Global step: 3486,loss: 0.011189943\n",
            "\n",
            "Global step: 3487,loss: 0.011996187\n",
            "\n",
            "Global step: 3488,loss: 0.014543843\n",
            "\n",
            "Global step: 3489,loss: 0.010155994\n",
            "\n",
            "Global step: 3490,loss: 0.019885436\n",
            "\n",
            "Global step: 3491,loss: 0.021077944\n",
            "\n",
            "Global step: 3492,loss: 0.009546128\n",
            "\n",
            "Global step: 3493,loss: 0.01130953\n",
            "\n",
            "Global step: 3494,loss: 0.010372313\n",
            "\n",
            "Global step: 3495,loss: 0.018564403\n",
            "\n",
            "Global step: 3496,loss: 0.01326158\n",
            "\n",
            "Global step: 3497,loss: 0.011591636\n",
            "\n",
            "Global step: 3498,loss: 0.01172718\n",
            "\n",
            "Global step: 3499,loss: 0.011458866\n",
            "\n",
            "Global step: 3500,loss: 0.010588533\n",
            "\n",
            "Global step: 3501,loss: 0.011614008\n",
            "\n",
            "Global step: 3502,loss: 0.013620149\n",
            "\n",
            "Global step: 3503,loss: 0.012106692\n",
            "\n",
            "Global step: 3504,loss: 0.010503071\n",
            "\n",
            "Global step: 3505,loss: 0.010408328\n",
            "\n",
            "Global step: 3506,loss: 0.010689862\n",
            "\n",
            "Global step: 3507,loss: 0.014128286\n",
            "\n",
            "Global step: 3508,loss: 0.01362618\n",
            "\n",
            "Global step: 3509,loss: 0.010943743\n",
            "\n",
            "Global step: 3510,loss: 0.0106727565\n",
            "\n",
            "Global step: 3511,loss: 0.01441264\n",
            "\n",
            "Global step: 3512,loss: 0.011511312\n",
            "\n",
            "Global step: 3513,loss: 0.013872212\n",
            "\n",
            "Global step: 3514,loss: 0.011010352\n",
            "\n",
            "Global step: 3515,loss: 0.011234334\n",
            "\n",
            "Global step: 3516,loss: 0.010498665\n",
            "\n",
            "Global step: 3517,loss: 0.015333487\n",
            "\n",
            "Global step: 3518,loss: 0.011106504\n",
            "\n",
            "Global step: 3519,loss: 0.011533042\n",
            "\n",
            "Global step: 3520,loss: 0.011797442\n",
            "\n",
            "Global step: 3521,loss: 0.010579282\n",
            "\n",
            "Global step: 3522,loss: 0.012688495\n",
            "\n",
            "Global step: 3523,loss: 0.01837859\n",
            "\n",
            "Global step: 3524,loss: 0.009840787\n",
            "\n",
            "Global step: 3525,loss: 0.01095202\n",
            "\n",
            "Global step: 3526,loss: 0.0133619625\n",
            "\n",
            "Global step: 3527,loss: 0.020339323\n",
            "\n",
            "Global step: 3528,loss: 0.013811161\n",
            "\n",
            "Global step: 3529,loss: 0.010062304\n",
            "\n",
            "Global step: 3530,loss: 0.011187659\n",
            "\n",
            "Global step: 3531,loss: 0.011406205\n",
            "\n",
            "Global step: 3532,loss: 0.009648657\n",
            "\n",
            "Global step: 3533,loss: 0.016513713\n",
            "\n",
            "Global step: 3534,loss: 0.013928623\n",
            "\n",
            "Global step: 3535,loss: 0.0103102\n",
            "\n",
            "Global step: 3536,loss: 0.0113047315\n",
            "\n",
            "Global step: 3537,loss: 0.010252022\n",
            "\n",
            "Global step: 3538,loss: 0.013371419\n",
            "\n",
            "Global step: 3539,loss: 0.013473924\n",
            "\n",
            "Global step: 3540,loss: 0.018765863\n",
            "\n",
            "Global step: 3541,loss: 0.014906639\n",
            "\n",
            "Global step: 3542,loss: 0.014328806\n",
            "\n",
            "Global step: 3543,loss: 0.01242009\n",
            "\n",
            "Global step: 3544,loss: 0.013095552\n",
            "\n",
            "Global step: 3545,loss: 0.016047437\n",
            "\n",
            "Global step: 3546,loss: 0.012394252\n",
            "\n",
            "Global step: 3547,loss: 0.01128372\n",
            "\n",
            "Global step: 3548,loss: 0.010349743\n",
            "\n",
            "Global step: 3549,loss: 0.012098948\n",
            "\n",
            "Global step: 3550,loss: 0.010160066\n",
            "\n",
            "Global step: 3551,loss: 0.011744205\n",
            "\n",
            "Global step: 3552,loss: 0.013081516\n",
            "\n",
            "Global step: 3553,loss: 0.010814462\n",
            "\n",
            "Global step: 3554,loss: 0.010898162\n",
            "\n",
            "Global step: 3555,loss: 0.012331461\n",
            "\n",
            "Global step: 3556,loss: 0.010960171\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3557.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:15:20.747808 139637785016064 supervisor.py:1050] Recording summary at step 3557.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3557,loss: 0.011975791\n",
            "\n",
            "Global step: 3558,loss: 0.011009913\n",
            "\n",
            "Global step: 3559,loss: 0.010167287\n",
            "\n",
            "Global step: 3560,loss: 0.009683015\n",
            "\n",
            "Global step: 3561,loss: 0.011399775\n",
            "\n",
            "Global step: 3562,loss: 0.012528276\n",
            "\n",
            "Global step: 3563,loss: 0.015052134\n",
            "\n",
            "Global step: 3564,loss: 0.010344012\n",
            "\n",
            "Global step: 3565,loss: 0.012225056\n",
            "\n",
            "Global step: 3566,loss: 0.010382199\n",
            "\n",
            "Global step: 3567,loss: 0.010969642\n",
            "\n",
            "Global step: 3568,loss: 0.015454818\n",
            "\n",
            "Global step: 3569,loss: 0.02367413\n",
            "\n",
            "Global step: 3570,loss: 0.010192552\n",
            "\n",
            "Global step: 3571,loss: 0.009976134\n",
            "\n",
            "Global step: 3572,loss: 0.013458971\n",
            "\n",
            "Global step: 3573,loss: 0.013026329\n",
            "\n",
            "Global step: 3574,loss: 0.016423257\n",
            "\n",
            "Global step: 3575,loss: 0.016631417\n",
            "\n",
            "Global step: 3576,loss: 0.019198477\n",
            "\n",
            "Global step: 3577,loss: 0.010592899\n",
            "\n",
            "Global step: 3578,loss: 0.011292652\n",
            "\n",
            "Global step: 3579,loss: 0.01159034\n",
            "\n",
            "Global step: 3580,loss: 0.009736142\n",
            "\n",
            "Global step: 3581,loss: 0.014266152\n",
            "\n",
            "Global step: 3582,loss: 0.011065019\n",
            "\n",
            "Global step: 3583,loss: 0.010607579\n",
            "\n",
            "Global step: 3584,loss: 0.011431882\n",
            "\n",
            "Global step: 3585,loss: 0.011811169\n",
            "\n",
            "Global step: 3586,loss: 0.013340583\n",
            "\n",
            "Global step: 3587,loss: 0.013154367\n",
            "\n",
            "Global step: 3588,loss: 0.009782561\n",
            "\n",
            "Global step: 3589,loss: 0.011272273\n",
            "\n",
            "Global step: 3590,loss: 0.011387826\n",
            "\n",
            "Global step: 3591,loss: 0.01194374\n",
            "\n",
            "Global step: 3592,loss: 0.012504735\n",
            "\n",
            "Global step: 3593,loss: 0.013655692\n",
            "\n",
            "Global step: 3594,loss: 0.01049683\n",
            "\n",
            "Global step: 3595,loss: 0.010537369\n",
            "\n",
            "Global step: 3596,loss: 0.009985763\n",
            "\n",
            "Global step: 3597,loss: 0.011360377\n",
            "\n",
            "Global step: 3598,loss: 0.01458233\n",
            "\n",
            "Global step: 3599,loss: 0.010734533\n",
            "\n",
            "Global step: 3600,loss: 0.010442249\n",
            "\n",
            "Global step: 3601,loss: 0.011274638\n",
            "\n",
            "Global step: 3602,loss: 0.01487986\n",
            "\n",
            "Global step: 3603,loss: 0.018900331\n",
            "\n",
            "Global step: 3604,loss: 0.013211891\n",
            "\n",
            "Global step: 3605,loss: 0.010360155\n",
            "\n",
            "Global step: 3606,loss: 0.015252894\n",
            "\n",
            "Global step: 3607,loss: 0.010045065\n",
            "\n",
            "Global step: 3608,loss: 0.010065927\n",
            "\n",
            "Global step: 3609,loss: 0.018162765\n",
            "\n",
            "Global step: 3610,loss: 0.012087854\n",
            "\n",
            "Global step: 3611,loss: 0.01258228\n",
            "\n",
            "Global step: 3612,loss: 0.012614663\n",
            "\n",
            "Global step: 3613,loss: 0.0094951745\n",
            "\n",
            "Global step: 3614,loss: 0.011591357\n",
            "\n",
            "Global step: 3615,loss: 0.019104239\n",
            "\n",
            "Global step: 3616,loss: 0.010369264\n",
            "\n",
            "Global step: 3617,loss: 0.009741418\n",
            "\n",
            "Global step: 3618,loss: 0.011653176\n",
            "\n",
            "Global step: 3619,loss: 0.012518052\n",
            "\n",
            "Global step: 3620,loss: 0.014399389\n",
            "\n",
            "Global step: 3621,loss: 0.011730088\n",
            "\n",
            "Global step: 3622,loss: 0.010425732\n",
            "\n",
            "Global step: 3623,loss: 0.012428204\n",
            "\n",
            "Global step: 3624,loss: 0.0130440965\n",
            "\n",
            "Global step: 3625,loss: 0.012049065\n",
            "\n",
            "Global step: 3626,loss: 0.0154150855\n",
            "\n",
            "Global step: 3627,loss: 0.0103956405\n",
            "\n",
            "Global step: 3628,loss: 0.012474923\n",
            "\n",
            "Global step: 3629,loss: 0.018456463\n",
            "\n",
            "Global step: 3630,loss: 0.009979562\n",
            "\n",
            "Global step: 3631,loss: 0.016809372\n",
            "\n",
            "Global step: 3632,loss: 0.017448664\n",
            "\n",
            "Global step: 3633,loss: 0.011957378\n",
            "\n",
            "Global step: 3634,loss: 0.012332875\n",
            "\n",
            "Global step: 3635,loss: 0.012736787\n",
            "\n",
            "Global step: 3636,loss: 0.012399828\n",
            "\n",
            "Global step: 3637,loss: 0.01648219\n",
            "\n",
            "Global step: 3638,loss: 0.010409213\n",
            "\n",
            "Global step: 3639,loss: 0.010241259\n",
            "\n",
            "Global step: 3640,loss: 0.012599416\n",
            "\n",
            "Global step: 3641,loss: 0.011336711\n",
            "\n",
            "Global step: 3642,loss: 0.0103528835\n",
            "\n",
            "Global step: 3643,loss: 0.010748612\n",
            "\n",
            "Global step: 3644,loss: 0.013347102\n",
            "\n",
            "Global step: 3645,loss: 0.009718617\n",
            "\n",
            "Global step: 3646,loss: 0.011343772\n",
            "\n",
            "Global step: 3647,loss: 0.011048899\n",
            "\n",
            "Global step: 3648,loss: 0.011180928\n",
            "\n",
            "Global step: 3649,loss: 0.009363261\n",
            "\n",
            "Global step: 3650,loss: 0.010593356\n",
            "\n",
            "Global step: 3651,loss: 0.010770474\n",
            "\n",
            "Global step: 3652,loss: 0.015512675\n",
            "\n",
            "Global step: 3653,loss: 0.012749376\n",
            "\n",
            "Global step: 3654,loss: 0.009732088\n",
            "\n",
            "Global step: 3655,loss: 0.01096553\n",
            "\n",
            "Global step: 3656,loss: 0.01145341\n",
            "\n",
            "Global step: 3657,loss: 0.0134809725\n",
            "\n",
            "Global step: 3658,loss: 0.010127765\n",
            "\n",
            "Global step: 3659,loss: 0.015658166\n",
            "\n",
            "Global step: 3660,loss: 0.012056511\n",
            "\n",
            "Global step: 3661,loss: 0.012060302\n",
            "\n",
            "Global step: 3662,loss: 0.01495018\n",
            "\n",
            "Global step: 3663,loss: 0.011249959\n",
            "\n",
            "Global step: 3664,loss: 0.017514367\n",
            "\n",
            "Global step: 3665,loss: 0.010845208\n",
            "\n",
            "Global step: 3666,loss: 0.012353006\n",
            "\n",
            "Global step: 3667,loss: 0.009878413\n",
            "\n",
            "Global step: 3668,loss: 0.011246417\n",
            "\n",
            "Global step: 3669,loss: 0.01033074\n",
            "\n",
            "Global step: 3670,loss: 0.00982249\n",
            "\n",
            "Global step: 3671,loss: 0.010060746\n",
            "\n",
            "Global step: 3672,loss: 0.0130122285\n",
            "\n",
            "Global step: 3673,loss: 0.02039773\n",
            "\n",
            "Global step: 3674,loss: 0.01014767\n",
            "\n",
            "Global step: 3675,loss: 0.017717917\n",
            "\n",
            "Global step: 3676,loss: 0.0105222305\n",
            "\n",
            "Global step: 3677,loss: 0.01083508\n",
            "\n",
            "Global step: 3678,loss: 0.011287879\n",
            "\n",
            "Global step: 3679,loss: 0.010569256\n",
            "\n",
            "Global step: 3680,loss: 0.018267691\n",
            "\n",
            "Global step: 3681,loss: 0.0131782275\n",
            "\n",
            "Global step: 3682,loss: 0.010877181\n",
            "\n",
            "Global step: 3683,loss: 0.009458421\n",
            "\n",
            "Global step: 3684,loss: 0.01841838\n",
            "\n",
            "Global step: 3685,loss: 0.012345877\n",
            "\n",
            "Global step: 3686,loss: 0.009659988\n",
            "\n",
            "Global step: 3687,loss: 0.009313019\n",
            "\n",
            "Global step: 3688,loss: 0.012519676\n",
            "\n",
            "Global step: 3689,loss: 0.009512513\n",
            "\n",
            "Global step: 3690,loss: 0.011137592\n",
            "\n",
            "Global step: 3691,loss: 0.012213592\n",
            "\n",
            "Global step: 3692,loss: 0.011670048\n",
            "\n",
            "Global step: 3693,loss: 0.011877984\n",
            "\n",
            "Global step: 3694,loss: 0.012499511\n",
            "\n",
            "Global step: 3695,loss: 0.020882137\n",
            "\n",
            "Global step: 3696,loss: 0.018835882\n",
            "\n",
            "Global step: 3697,loss: 0.012273651\n",
            "\n",
            "Global step: 3698,loss: 0.016753422\n",
            "\n",
            "Global step: 3699,loss: 0.0142584555\n",
            "\n",
            "Global step: 3700,loss: 0.012428155\n",
            "\n",
            "Global step: 3701,loss: 0.016809177\n",
            "\n",
            "Global step: 3702,loss: 0.010166136\n",
            "\n",
            "Global step: 3703,loss: 0.011030812\n",
            "\n",
            "Global step: 3704,loss: 0.01370093\n",
            "\n",
            "Global step: 3705,loss: 0.012374157\n",
            "\n",
            "Global step: 3706,loss: 0.013177114\n",
            "\n",
            "Global step: 3707,loss: 0.0139203165\n",
            "\n",
            "Global step: 3708,loss: 0.021558989\n",
            "\n",
            "Global step: 3709,loss: 0.010382104\n",
            "\n",
            "Global step: 3710,loss: 0.0151572\n",
            "\n",
            "Global step: 3711,loss: 0.009641123\n",
            "\n",
            "Global step: 3712,loss: 0.010264155\n",
            "\n",
            "Global step: 3713,loss: 0.010848604\n",
            "\n",
            "Global step: 3714,loss: 0.011819964\n",
            "\n",
            "Global step: 3715,loss: 0.012031638\n",
            "\n",
            "Global step: 3716,loss: 0.011320498\n",
            "\n",
            "Global step: 3717,loss: 0.010427839\n",
            "\n",
            "Global step: 3718,loss: 0.0104725035\n",
            "\n",
            "Global step: 3719,loss: 0.012323532\n",
            "\n",
            "Global step: 3720,loss: 0.010707857\n",
            "\n",
            "Global step: 3721,loss: 0.012671245\n",
            "\n",
            "Global step: 3722,loss: 0.010813331\n",
            "\n",
            "Global step: 3723,loss: 0.0102357045\n",
            "\n",
            "Global step: 3724,loss: 0.011358657\n",
            "\n",
            "Global step: 3725,loss: 0.010672265\n",
            "\n",
            "Global step: 3726,loss: 0.012139097\n",
            "\n",
            "Global step: 3727,loss: 0.018892653\n",
            "\n",
            "Global step: 3728,loss: 0.009759661\n",
            "\n",
            "Global step: 3729,loss: 0.016140508\n",
            "\n",
            "Global step: 3730,loss: 0.012763574\n",
            "\n",
            "Global step: 3731,loss: 0.012444412\n",
            "\n",
            "Global step: 3732,loss: 0.010771812\n",
            "\n",
            "Global step: 3733,loss: 0.010174797\n",
            "\n",
            "Global step: 3734,loss: 0.010982095\n",
            "\n",
            "Global step: 3735,loss: 0.010460331\n",
            "\n",
            "Global step: 3736,loss: 0.009717441\n",
            "\n",
            "Global step: 3737,loss: 0.01306087\n",
            "\n",
            "Global step: 3738,loss: 0.020559426\n",
            "\n",
            "Global step: 3739,loss: 0.010134862\n",
            "\n",
            "Global step: 3740,loss: 0.01235011\n",
            "\n",
            "Global step: 3741,loss: 0.012081291\n",
            "\n",
            "Global step: 3742,loss: 0.010260581\n",
            "\n",
            "Global step: 3743,loss: 0.02253206\n",
            "\n",
            "Global step: 3744,loss: 0.010418014\n",
            "\n",
            "Global step: 3745,loss: 0.018251047\n",
            "\n",
            "Global step: 3746,loss: 0.016005449\n",
            "\n",
            "Global step: 3747,loss: 0.012602888\n",
            "\n",
            "Global step: 3748,loss: 0.022398725\n",
            "\n",
            "Global step: 3749,loss: 0.010803062\n",
            "\n",
            "Global step: 3750,loss: 0.013205584\n",
            "\n",
            "Global step: 3751,loss: 0.011463377\n",
            "\n",
            "Global step: 3752,loss: 0.01762582\n",
            "\n",
            "Global step: 3753,loss: 0.014274761\n",
            "\n",
            "Global step: 3754,loss: 0.015748102\n",
            "\n",
            "Global step: 3755,loss: 0.01029144\n",
            "\n",
            "Global step: 3756,loss: 0.01348004\n",
            "\n",
            "Global step: 3757,loss: 0.013993021\n",
            "\n",
            "Global step: 3758,loss: 0.012425929\n",
            "\n",
            "Global step: 3759,loss: 0.013377787\n",
            "\n",
            "Global step: 3760,loss: 0.011114767\n",
            "\n",
            "Global step: 3761,loss: 0.011309496\n",
            "\n",
            "Global step: 3762,loss: 0.0134798745\n",
            "\n",
            "Global step: 3763,loss: 0.009983927\n",
            "\n",
            "Global step: 3764,loss: 0.010412747\n",
            "\n",
            "Global step: 3765,loss: 0.01146687\n",
            "\n",
            "Global step: 3766,loss: 0.016343482\n",
            "\n",
            "Global step: 3767,loss: 0.010538876\n",
            "\n",
            "Global step: 3768,loss: 0.011481217\n",
            "\n",
            "Global step: 3769,loss: 0.013417192\n",
            "\n",
            "Global step: 3770,loss: 0.00985458\n",
            "\n",
            "Global step: 3771,loss: 0.012989757\n",
            "\n",
            "Global step: 3772,loss: 0.011001136\n",
            "\n",
            "Global step: 3773,loss: 0.012038903\n",
            "\n",
            "Global step: 3774,loss: 0.011105758\n",
            "\n",
            "Global step: 3775,loss: 0.012170069\n",
            "\n",
            "Global step: 3776,loss: 0.017570412\n",
            "\n",
            "Global step: 3777,loss: 0.009100591\n",
            "\n",
            "Global step: 3778,loss: 0.012446471\n",
            "\n",
            "Global step: 3779,loss: 0.009455348\n",
            "\n",
            "Global step: 3780,loss: 0.012777174\n",
            "\n",
            "Global step: 3781,loss: 0.011663638\n",
            "\n",
            "Global step: 3782,loss: 0.015419527\n",
            "\n",
            "Global step: 3783,loss: 0.022884838\n",
            "\n",
            "Global step: 3784,loss: 0.010372504\n",
            "\n",
            "Global step: 3785,loss: 0.0155121265\n",
            "\n",
            "Global step: 3786,loss: 0.023321163\n",
            "\n",
            "Global step: 3787,loss: 0.010450681\n",
            "\n",
            "Global step: 3788,loss: 0.0114566125\n",
            "\n",
            "Global step: 3789,loss: 0.011044778\n",
            "\n",
            "Global step: 3790,loss: 0.0130806025\n",
            "\n",
            "Global step: 3791,loss: 0.011316326\n",
            "\n",
            "Global step: 3792,loss: 0.01223877\n",
            "\n",
            "Global step: 3793,loss: 0.010958951\n",
            "\n",
            "Global step: 3794,loss: 0.010612833\n",
            "\n",
            "Global step: 3795,loss: 0.021198604\n",
            "\n",
            "Global step: 3796,loss: 0.009410336\n",
            "\n",
            "Global step: 3797,loss: 0.010663658\n",
            "\n",
            "Global step: 3798,loss: 0.02062925\n",
            "\n",
            "Global step: 3799,loss: 0.013331207\n",
            "\n",
            "Global step: 3800,loss: 0.011656512\n",
            "\n",
            "Global step: 3801,loss: 0.010547558\n",
            "\n",
            "Global step: 3802,loss: 0.013247393\n",
            "\n",
            "Global step: 3803,loss: 0.012184385\n",
            "\n",
            "Global step: 3804,loss: 0.010337142\n",
            "\n",
            "Global step: 3805,loss: 0.010636757\n",
            "\n",
            "Global step: 3806,loss: 0.01012857\n",
            "\n",
            "Global step: 3807,loss: 0.011330623\n",
            "\n",
            "Global step: 3808,loss: 0.009426887\n",
            "\n",
            "Global step: 3809,loss: 0.010255154\n",
            "\n",
            "Global step: 3810,loss: 0.011527097\n",
            "\n",
            "Global step: 3811,loss: 0.0092090145\n",
            "\n",
            "Global step: 3812,loss: 0.010095599\n",
            "\n",
            "Global step: 3813,loss: 0.013887476\n",
            "\n",
            "Global step: 3814,loss: 0.011217808\n",
            "\n",
            "Global step: 3815,loss: 0.010044858\n",
            "\n",
            "Global step: 3816,loss: 0.019565754\n",
            "\n",
            "Global step: 3817,loss: 0.010494072\n",
            "\n",
            "Global step: 3818,loss: 0.011102002\n",
            "\n",
            "Global step: 3819,loss: 0.010049908\n",
            "\n",
            "Global step: 3820,loss: 0.011116456\n",
            "\n",
            "Global step: 3821,loss: 0.011073577\n",
            "\n",
            "Global step: 3822,loss: 0.012611735\n",
            "\n",
            "Global step: 3823,loss: 0.014791773\n",
            "\n",
            "Global step: 3824,loss: 0.010766475\n",
            "\n",
            "Global step: 3825,loss: 0.009518722\n",
            "\n",
            "Global step: 3826,loss: 0.01423323\n",
            "\n",
            "Global step: 3827,loss: 0.010042034\n",
            "\n",
            "Global step: 3828,loss: 0.011819277\n",
            "\n",
            "Global step: 3829,loss: 0.012654365\n",
            "\n",
            "Global step: 3830,loss: 0.0143504925\n",
            "\n",
            "Global step: 3831,loss: 0.009346879\n",
            "\n",
            "Global step: 3832,loss: 0.0109473895\n",
            "\n",
            "Global step: 3833,loss: 0.011206921\n",
            "\n",
            "Global step: 3834,loss: 0.010383572\n",
            "\n",
            "Global step: 3835,loss: 0.011324851\n",
            "\n",
            "Global step: 3836,loss: 0.012074906\n",
            "\n",
            "Global step: 3837,loss: 0.009843697\n",
            "\n",
            "Global step: 3838,loss: 0.010106819\n",
            "\n",
            "Global step: 3839,loss: 0.011218156\n",
            "\n",
            "Global step: 3840,loss: 0.011328451\n",
            "\n",
            "Global step: 3841,loss: 0.010243758\n",
            "\n",
            "Global step: 3842,loss: 0.009874\n",
            "\n",
            "Global step: 3843,loss: 0.010018397\n",
            "\n",
            "Global step: 3844,loss: 0.010002831\n",
            "\n",
            "Global step: 3845,loss: 0.010161011\n",
            "\n",
            "Global step: 3846,loss: 0.0092062075\n",
            "\n",
            "Global step: 3847,loss: 0.015169502\n",
            "\n",
            "Global step: 3848,loss: 0.011741081\n",
            "\n",
            "Global step: 3849,loss: 0.010417376\n",
            "\n",
            "Global step: 3850,loss: 0.009309269\n",
            "\n",
            "Global step: 3851,loss: 0.009346386\n",
            "\n",
            "Global step: 3852,loss: 0.010443334\n",
            "\n",
            "Global step: 3853,loss: 0.013008407\n",
            "\n",
            "Global step: 3854,loss: 0.009073481\n",
            "\n",
            "Global step: 3855,loss: 0.009403943\n",
            "\n",
            "Global step: 3856,loss: 0.011122031\n",
            "\n",
            "Global step: 3857,loss: 0.009392237\n",
            "\n",
            "Global step: 3858,loss: 0.009684719\n",
            "\n",
            "Global step: 3859,loss: 0.015628338\n",
            "\n",
            "Global step: 3860,loss: 0.011204277\n",
            "\n",
            "Global step: 3861,loss: 0.012391348\n",
            "\n",
            "Global step: 3862,loss: 0.009478342\n",
            "\n",
            "Global step: 3863,loss: 0.012281088\n",
            "\n",
            "Global step: 3864,loss: 0.011258541\n",
            "\n",
            "Global step: 3865,loss: 0.010101145\n",
            "\n",
            "Global step: 3866,loss: 0.009552871\n",
            "\n",
            "Global step: 3867,loss: 0.0128875\n",
            "\n",
            "Global step: 3868,loss: 0.010544916\n",
            "\n",
            "Global step: 3869,loss: 0.009741195\n",
            "\n",
            "Global step: 3870,loss: 0.012433825\n",
            "\n",
            "Global step: 3871,loss: 0.009391272\n",
            "\n",
            "Global step: 3872,loss: 0.01021212\n",
            "\n",
            "Global step: 3873,loss: 0.009114774\n",
            "\n",
            "Global step: 3874,loss: 0.011255639\n",
            "\n",
            "Global step: 3875,loss: 0.010564225\n",
            "\n",
            "Global step: 3876,loss: 0.009384034\n",
            "\n",
            "Global step: 3877,loss: 0.013628694\n",
            "\n",
            "Global step: 3878,loss: 0.013731894\n",
            "\n",
            "Global step: 3879,loss: 0.010858882\n",
            "\n",
            "Global step: 3880,loss: 0.009963102\n",
            "\n",
            "Global step: 3881,loss: 0.011018118\n",
            "\n",
            "Global step: 3882,loss: 0.016626053\n",
            "\n",
            "Global step: 3883,loss: 0.0111953765\n",
            "\n",
            "Global step: 3884,loss: 0.0106168175\n",
            "\n",
            "Global step: 3885,loss: 0.009557435\n",
            "\n",
            "Global step: 3886,loss: 0.010430927\n",
            "\n",
            "Global step: 3887,loss: 0.0118489005\n",
            "\n",
            "Global step: 3888,loss: 0.010829112\n",
            "\n",
            "Global step: 3889,loss: 0.011338772\n",
            "\n",
            "Global step: 3890,loss: 0.013262562\n",
            "\n",
            "Global step: 3891,loss: 0.0114612505\n",
            "\n",
            "Global step: 3892,loss: 0.011243368\n",
            "\n",
            "Global step: 3893,loss: 0.017263753\n",
            "\n",
            "Global step: 3894,loss: 0.011454981\n",
            "\n",
            "Global step: 3895,loss: 0.019641127\n",
            "\n",
            "Global step: 3896,loss: 0.010854444\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3897.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1117 16:17:20.798354 139637785016064 supervisor.py:1050] Recording summary at step 3897.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3897,loss: 0.012434591\n",
            "\n",
            "Global step: 3898,loss: 0.012543424\n",
            "\n",
            "Global step: 3899,loss: 0.010407003\n",
            "\n",
            "Global step: 3900,loss: 0.014906576\n",
            "\n",
            "Global step: 3901,loss: 0.012247023\n",
            "\n",
            "Global step: 3902,loss: 0.010077432\n",
            "\n",
            "Global step: 3903,loss: 0.012582599\n",
            "\n",
            "Global step: 3904,loss: 0.012916265\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3904,Val_Loss: 0.011942917755685555,  Val_acc: 0.9988982371794872 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1117 16:17:35.878813 139641102628736 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/20:\n",
            "Global step: 3905,loss: 0.009978084\n",
            "\n",
            "Global step: 3906,loss: 0.013158691\n",
            "\n",
            "Global step: 3907,loss: 0.009305319\n",
            "\n",
            "Global step: 3908,loss: 0.010745664\n",
            "\n",
            "Global step: 3909,loss: 0.008810238\n",
            "\n",
            "Global step: 3910,loss: 0.009299971\n",
            "\n",
            "Global step: 3911,loss: 0.00915268\n",
            "\n",
            "Global step: 3912,loss: 0.010801005\n",
            "\n",
            "Global step: 3913,loss: 0.008732555\n",
            "\n",
            "Global step: 3914,loss: 0.0104255425\n",
            "\n",
            "Global step: 3915,loss: 0.010063237\n",
            "\n",
            "Global step: 3916,loss: 0.01130298\n",
            "\n",
            "Global step: 3917,loss: 0.011412919\n",
            "\n",
            "Global step: 3918,loss: 0.009103063\n",
            "\n",
            "Global step: 3919,loss: 0.013644184\n",
            "\n",
            "Global step: 3920,loss: 0.01007131\n",
            "\n",
            "Global step: 3921,loss: 0.010583121\n",
            "\n",
            "Global step: 3922,loss: 0.010423167\n",
            "\n",
            "Global step: 3923,loss: 0.010503259\n",
            "\n",
            "Global step: 3924,loss: 0.010105432\n",
            "\n",
            "Global step: 3925,loss: 0.009350896\n",
            "\n",
            "Global step: 3926,loss: 0.010455856\n",
            "\n",
            "Global step: 3927,loss: 0.009564632\n",
            "\n",
            "Global step: 3928,loss: 0.010252314\n",
            "\n",
            "Global step: 3929,loss: 0.010040817\n",
            "\n",
            "Global step: 3930,loss: 0.012007294\n",
            "\n",
            "Global step: 3931,loss: 0.011036207\n",
            "\n",
            "Global step: 3932,loss: 0.011952996\n",
            "\n",
            "Global step: 3933,loss: 0.009120619\n",
            "\n",
            "Global step: 3934,loss: 0.009474991\n",
            "\n",
            "Global step: 3935,loss: 0.011246551\n",
            "\n",
            "Global step: 3936,loss: 0.010870547\n",
            "\n",
            "Global step: 3937,loss: 0.009997668\n",
            "\n",
            "Global step: 3938,loss: 0.011245516\n",
            "\n",
            "Global step: 3939,loss: 0.009576759\n",
            "\n",
            "Global step: 3940,loss: 0.010055421\n",
            "\n",
            "Global step: 3941,loss: 0.0094178235\n",
            "\n",
            "Global step: 3942,loss: 0.009772066\n",
            "\n",
            "Global step: 3943,loss: 0.009167129\n",
            "\n",
            "Global step: 3944,loss: 0.00954706\n",
            "\n",
            "Global step: 3945,loss: 0.009705203\n",
            "\n",
            "Global step: 3946,loss: 0.009990428\n",
            "\n",
            "Global step: 3947,loss: 0.009025532\n",
            "\n",
            "Global step: 3948,loss: 0.00906583\n",
            "\n",
            "Global step: 3949,loss: 0.011962094\n",
            "\n",
            "Global step: 3950,loss: 0.009687816\n",
            "\n",
            "Global step: 3951,loss: 0.011196228\n",
            "\n",
            "Global step: 3952,loss: 0.010875048\n",
            "\n",
            "Global step: 3953,loss: 0.010189154\n",
            "\n",
            "Global step: 3954,loss: 0.010417197\n",
            "\n",
            "Global step: 3955,loss: 0.009541704\n",
            "\n",
            "Global step: 3956,loss: 0.009059945\n",
            "\n",
            "Global step: 3957,loss: 0.009888496\n",
            "\n",
            "Global step: 3958,loss: 0.011531974\n",
            "\n",
            "Global step: 3959,loss: 0.00926567\n",
            "\n",
            "Global step: 3960,loss: 0.010881451\n",
            "\n",
            "Global step: 3961,loss: 0.00896279\n",
            "\n",
            "Global step: 3962,loss: 0.0086985435\n",
            "\n",
            "Global step: 3963,loss: 0.00976988\n",
            "\n",
            "Global step: 3964,loss: 0.009301732\n",
            "\n",
            "Global step: 3965,loss: 0.009247164\n",
            "\n",
            "Global step: 3966,loss: 0.009564096\n",
            "\n",
            "Global step: 3967,loss: 0.008937222\n",
            "\n",
            "Global step: 3968,loss: 0.0100365775\n",
            "\n",
            "Global step: 3969,loss: 0.009265324\n",
            "\n",
            "Global step: 3970,loss: 0.011456698\n",
            "\n",
            "Global step: 3971,loss: 0.011884864\n",
            "\n",
            "Global step: 3972,loss: 0.00934815\n",
            "\n",
            "Global step: 3973,loss: 0.008698974\n",
            "\n",
            "Global step: 3974,loss: 0.01137477\n",
            "\n",
            "Global step: 3975,loss: 0.009395598\n",
            "\n",
            "Global step: 3976,loss: 0.009091462\n",
            "\n",
            "Global step: 3977,loss: 0.008910164\n",
            "\n",
            "Global step: 3978,loss: 0.010194428\n",
            "\n",
            "Global step: 3979,loss: 0.009032005\n",
            "\n",
            "Global step: 3980,loss: 0.009553717\n",
            "\n",
            "Global step: 3981,loss: 0.01063427\n",
            "\n",
            "Global step: 3982,loss: 0.0093528535\n",
            "\n",
            "Global step: 3983,loss: 0.008908346\n",
            "\n",
            "Global step: 3984,loss: 0.009710696\n",
            "\n",
            "Global step: 3985,loss: 0.009657659\n",
            "\n",
            "Global step: 3986,loss: 0.010986563\n",
            "\n",
            "Global step: 3987,loss: 0.0115950685\n",
            "\n",
            "Global step: 3988,loss: 0.010156547\n",
            "\n",
            "Global step: 3989,loss: 0.011909746\n",
            "\n",
            "Global step: 3990,loss: 0.009188915\n",
            "\n",
            "Global step: 3991,loss: 0.009176724\n",
            "\n",
            "Global step: 3992,loss: 0.008898657\n",
            "\n",
            "Global step: 3993,loss: 0.009593295\n",
            "\n",
            "Global step: 3994,loss: 0.010486473\n",
            "\n",
            "Global step: 3995,loss: 0.011256639\n",
            "\n",
            "Global step: 3996,loss: 0.010800161\n",
            "\n",
            "Global step: 3997,loss: 0.008168699\n",
            "\n",
            "Global step: 3998,loss: 0.008908772\n",
            "\n",
            "Global step: 3999,loss: 0.009263871\n",
            "\n",
            "Global step: 4000,loss: 0.009780355\n",
            "\n",
            "Global step: 4001,loss: 0.009609661\n",
            "\n",
            "Global step: 4002,loss: 0.011666244\n",
            "\n",
            "Global step: 4003,loss: 0.010050638\n",
            "\n",
            "Global step: 4004,loss: 0.009685592\n",
            "\n",
            "Global step: 4005,loss: 0.009735907\n",
            "\n",
            "Global step: 4006,loss: 0.010714437\n",
            "\n",
            "Global step: 4007,loss: 0.010754792\n",
            "\n",
            "Global step: 4008,loss: 0.013383426\n",
            "\n",
            "Global step: 4009,loss: 0.010642137\n",
            "\n",
            "Global step: 4010,loss: 0.0128343655\n",
            "\n",
            "Global step: 4011,loss: 0.009702846\n",
            "\n",
            "Global step: 4012,loss: 0.013106853\n",
            "\n",
            "Global step: 4013,loss: 0.008847661\n",
            "\n",
            "Global step: 4014,loss: 0.010689925\n",
            "\n",
            "Global step: 4015,loss: 0.008961612\n",
            "\n",
            "Global step: 4016,loss: 0.008957287\n",
            "\n",
            "Global step: 4017,loss: 0.009951464\n",
            "\n",
            "Global step: 4018,loss: 0.010179458\n",
            "\n",
            "Global step: 4019,loss: 0.011275781\n",
            "\n",
            "Global step: 4020,loss: 0.009460101\n",
            "\n",
            "Global step: 4021,loss: 0.009502477\n",
            "\n",
            "Global step: 4022,loss: 0.015230839\n",
            "\n",
            "Global step: 4023,loss: 0.00930081\n",
            "\n",
            "Global step: 4024,loss: 0.010194534\n",
            "\n",
            "Global step: 4025,loss: 0.009740055\n",
            "\n",
            "Global step: 4026,loss: 0.010403376\n",
            "\n",
            "Global step: 4027,loss: 0.010019915\n",
            "\n",
            "Global step: 4028,loss: 0.008649676\n",
            "\n",
            "Global step: 4029,loss: 0.009942541\n",
            "\n",
            "Global step: 4030,loss: 0.010212653\n",
            "\n",
            "Global step: 4031,loss: 0.009872941\n",
            "\n",
            "Global step: 4032,loss: 0.010240539\n",
            "\n",
            "Global step: 4033,loss: 0.009577603\n",
            "\n",
            "Global step: 4034,loss: 0.009027921\n",
            "\n",
            "Global step: 4035,loss: 0.009471238\n",
            "\n",
            "Global step: 4036,loss: 0.00938689\n",
            "\n",
            "Global step: 4037,loss: 0.016980514\n",
            "\n",
            "Global step: 4038,loss: 0.010291482\n",
            "\n",
            "Global step: 4039,loss: 0.0092957355\n",
            "\n",
            "Global step: 4040,loss: 0.0108275525\n",
            "\n",
            "Global step: 4041,loss: 0.009358533\n",
            "\n",
            "Global step: 4042,loss: 0.016475022\n",
            "\n",
            "Global step: 4043,loss: 0.012320356\n",
            "\n",
            "Global step: 4044,loss: 0.01370072\n",
            "\n",
            "Global step: 4045,loss: 0.010717298\n",
            "\n",
            "Global step: 4046,loss: 0.019096814\n",
            "\n",
            "Global step: 4047,loss: 0.012389535\n",
            "\n",
            "Global step: 4048,loss: 0.014934931\n",
            "\n",
            "Global step: 4049,loss: 0.009492367\n",
            "\n",
            "Global step: 4050,loss: 0.009429112\n",
            "\n",
            "Global step: 4051,loss: 0.009963936\n",
            "\n",
            "Global step: 4052,loss: 0.011137864\n",
            "\n",
            "Global step: 4053,loss: 0.010092202\n",
            "\n",
            "Global step: 4054,loss: 0.009811962\n",
            "\n",
            "Global step: 4055,loss: 0.009967732\n",
            "\n",
            "Global step: 4056,loss: 0.013573513\n",
            "\n",
            "Global step: 4057,loss: 0.011652837\n",
            "\n",
            "Global step: 4058,loss: 0.0094311\n",
            "\n",
            "Global step: 4059,loss: 0.009570988\n",
            "\n",
            "Global step: 4060,loss: 0.009612145\n",
            "\n",
            "Global step: 4061,loss: 0.009423336\n",
            "\n",
            "Global step: 4062,loss: 0.009303638\n",
            "\n",
            "Global step: 4063,loss: 0.010806797\n",
            "\n",
            "Global step: 4064,loss: 0.010089551\n",
            "\n",
            "Global step: 4065,loss: 0.009368827\n",
            "\n",
            "Global step: 4066,loss: 0.0093012545\n",
            "\n",
            "Global step: 4067,loss: 0.010670038\n",
            "\n",
            "Global step: 4068,loss: 0.010009008\n",
            "\n",
            "Global step: 4069,loss: 0.009554584\n",
            "\n",
            "Global step: 4070,loss: 0.010138587\n",
            "\n",
            "Global step: 4071,loss: 0.012076579\n",
            "\n",
            "Global step: 4072,loss: 0.010936318\n",
            "\n",
            "Global step: 4073,loss: 0.01093411\n",
            "\n",
            "Global step: 4074,loss: 0.009385615\n",
            "\n",
            "Global step: 4075,loss: 0.009889201\n",
            "\n",
            "Global step: 4076,loss: 0.010064308\n",
            "\n",
            "Global step: 4077,loss: 0.011206703\n",
            "\n",
            "Global step: 4078,loss: 0.010351327\n",
            "\n",
            "Global step: 4079,loss: 0.01017394\n",
            "\n",
            "Global step: 4080,loss: 0.0116677275\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "cfg.is_training=False\n",
        "st = time.time()\n",
        "try:\n",
        "  def main(_):\n",
        "\n",
        "\n",
        "\n",
        "      #sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      # print(\"After sv\")\n",
        "      evaluation(model, sv, num_label)\n",
        "      print(\"Completed in: {}s\".format(time.time()-st))\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"Success !!\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}