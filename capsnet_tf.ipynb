{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "4f9017fb-59e3-4b32-80d0-139d572762a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "eaf68c6a-0121-4bd4-94cd-5145616a36bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   data     logdir_try8   tensorboard.ipynb   Try-10\t Try-4\n",
            " capsnet_tf.py\t     logdir   results\t    Try-1\t        Try-3\t try-5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "outputId": "2c628742-b7d0-4501-bfc2-5c6285a954a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "import random\n",
        "import skimage.io\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3O76C7FuuN",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAbq_6IFxbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_aug(images,labels,angel,resize_rate, populate):\n",
        "\n",
        "\n",
        "  new_img = []\n",
        "  new_label = []\n",
        "  print(\"\\nStarting Data Augmentation\")\n",
        "  for img,label in zip(images,labels):\n",
        "\n",
        "    image = img\n",
        "    #label1 = label.reshape(1,1)\n",
        "    #flip = random.randint(0, 1)\n",
        "    size = image.shape[0]\n",
        "\n",
        "    sh = random.random()/2-0.25\n",
        "    rotate_angel = random.random()/180*np.pi*angel\n",
        "    # Create Afine transform\n",
        "    afine_tf = transform.AffineTransform(shear=sh,rotation=rotate_angel)\n",
        "    # Apply transform to image data\n",
        "    image = transform.warp(image, inverse_map=afine_tf,mode='edge')\n",
        "    \n",
        "    new_img.append(image)\n",
        "    new_label.append(label)\n",
        "  \n",
        "  print(\"\\nFinished Augmentation\")\n",
        "  if(populate):\n",
        "\n",
        "    final_trX = np.asarray(images + new_img)\n",
        "    final_labels = np.asarray(labels + new_label)\n",
        "    return final_trX, final_labels\n",
        "  return (np.array(new_img)).astype('float32'), (np.array(labels,dtype='int32').astype('int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "        trX = trainX[:50000] / 255.\n",
        "        trY = trainY[:50000]\n",
        "\n",
        "        trX, trY = data_aug(list(trX),list(trY),angel=5,resize_rate=0.9,populate=False)\n",
        "\n",
        "        valX = trainX[50000:, ] / 255.\n",
        "        valY = trainY[50000:]\n",
        "\n",
        "        num_tr_batch = 50000 // batch_size\n",
        "        num_val_batch = 10000 // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 16, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.392,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "# ############################\n",
        "# #   distributed setting    #\n",
        "# ############################\n",
        "# flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "# flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "# flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                # if ((val_loss < best_val_loss) and (best_val_acc < val_acc) ):\n",
        "                if ((val_loss < best_val_loss) ):\n",
        "                  best_val_loss = val_loss\n",
        "                  # best_val_acc = val_acc\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######NOT SAVING MODEL #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "outputId": "2d7dd08d-70a9-4403-f0f5-fefe674ab2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cfg.is_training=True\n",
        "# try:\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n",
        "\n",
        "# except:\n",
        "#   print(\"\\nBeginning Eval\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:03:57.781072 139738004481920 <ipython-input-10-fd696e1d0d24>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "WARNING:tensorflow:From <ipython-input-5-c01d0c89e12e>:56: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.403683 139738004481920 deprecation.py:323] From <ipython-input-5-c01d0c89e12e>:56: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.605807 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.611527 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.615091 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.619921 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.624349 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-c01d0c89e12e>:61: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.640045 139738004481920 deprecation.py:323] From <ipython-input-5-c01d0c89e12e>:61: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:05.664620 139738004481920 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I1112 10:04:05.926654 139738004481920 utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:06.270354 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:06.499218 139738004481920 deprecation.py:323] From <ipython-input-7-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:06.860172 139738004481920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:07.503783 139738004481920 <ipython-input-7-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:07.506253 139738004481920 <ipython-input-10-fd696e1d0d24>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-fd696e1d0d24>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:04:07.508584 139738004481920 deprecation.py:323] From <ipython-input-10-fd696e1d0d24>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:07.817688 139738004481920 <ipython-input-10-fd696e1d0d24>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:19.829684 139738004481920 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:19.858209 139738004481920 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:51.739310 139738004481920 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:52.336312 139738004481920 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/16:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:54.403638 139735703181056 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:04:59.665842 139735711573760 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.7456157\n",
            "\n",
            "Global step: 1,loss: 0.7329464\n",
            "\n",
            "Global step: 2,loss: 0.76249844\n",
            "\n",
            "Global step: 3,loss: 0.7238721\n",
            "\n",
            "Global step: 4,loss: 0.7169268\n",
            "\n",
            "Global step: 5,loss: 0.71687865\n",
            "\n",
            "Global step: 6,loss: 0.7020121\n",
            "\n",
            "Global step: 7,loss: 0.6712132\n",
            "\n",
            "Global step: 8,loss: 0.64225614\n",
            "\n",
            "Global step: 9,loss: 0.59654343\n",
            "\n",
            "Global step: 10,loss: 0.57098037\n",
            "\n",
            "Global step: 11,loss: 0.5519383\n",
            "\n",
            "Global step: 12,loss: 0.49943924\n",
            "\n",
            "Global step: 13,loss: 0.48433232\n",
            "\n",
            "Global step: 14,loss: 0.429649\n",
            "\n",
            "Global step: 15,loss: 0.41555727\n",
            "\n",
            "Global step: 16,loss: 0.4226393\n",
            "\n",
            "Global step: 17,loss: 0.39483032\n",
            "\n",
            "Global step: 18,loss: 0.40081984\n",
            "\n",
            "Global step: 19,loss: 0.3662163\n",
            "\n",
            "Global step: 20,loss: 0.37258318\n",
            "\n",
            "Global step: 21,loss: 0.3496287\n",
            "\n",
            "Global step: 22,loss: 0.3232719\n",
            "\n",
            "Global step: 23,loss: 0.31327182\n",
            "\n",
            "Global step: 24,loss: 0.33009657\n",
            "\n",
            "Global step: 25,loss: 0.3108474\n",
            "\n",
            "Global step: 26,loss: 0.29507452\n",
            "\n",
            "Global step: 27,loss: 0.27192408\n",
            "\n",
            "Global step: 28,loss: 0.2934436\n",
            "\n",
            "Global step: 29,loss: 0.25020614\n",
            "\n",
            "Global step: 30,loss: 0.31933776\n",
            "\n",
            "Global step: 31,loss: 0.30953678\n",
            "\n",
            "Global step: 32,loss: 0.26285625\n",
            "\n",
            "Global step: 33,loss: 0.26050252\n",
            "\n",
            "Global step: 34,loss: 0.2328917\n",
            "\n",
            "Global step: 35,loss: 0.24461655\n",
            "\n",
            "Global step: 36,loss: 0.23982602\n",
            "\n",
            "Global step: 37,loss: 0.21866886\n",
            "\n",
            "Global step: 38,loss: 0.22794531\n",
            "\n",
            "Global step: 39,loss: 0.22889246\n",
            "\n",
            "Global step: 40,loss: 0.21169643\n",
            "\n",
            "Global step: 41,loss: 0.2338795\n",
            "\n",
            "Global step: 42,loss: 0.20158762\n",
            "\n",
            "Global step: 43,loss: 0.20930392\n",
            "\n",
            "Global step: 44,loss: 0.2014868\n",
            "\n",
            "Global step: 45,loss: 0.19240335\n",
            "\n",
            "Global step: 46,loss: 0.1847844\n",
            "\n",
            "Global step: 47,loss: 0.17469463\n",
            "\n",
            "Global step: 48,loss: 0.19459283\n",
            "\n",
            "Global step: 49,loss: 0.18195027\n",
            "\n",
            "Global step: 50,loss: 0.1785\n",
            "\n",
            "Global step: 51,loss: 0.18669738\n",
            "\n",
            "Global step: 52,loss: 0.17115653\n",
            "\n",
            "Global step: 53,loss: 0.17392106\n",
            "\n",
            "Global step: 54,loss: 0.16619828\n",
            "\n",
            "Global step: 55,loss: 0.15268244\n",
            "\n",
            "Global step: 56,loss: 0.15087327\n",
            "\n",
            "Global step: 57,loss: 0.15109302\n",
            "\n",
            "Global step: 58,loss: 0.163764\n",
            "\n",
            "Global step: 59,loss: 0.17039427\n",
            "\n",
            "Global step: 60,loss: 0.17237295\n",
            "\n",
            "Global step: 61,loss: 0.15146075\n",
            "\n",
            "Global step: 62,loss: 0.13451827\n",
            "\n",
            "Global step: 63,loss: 0.14437136\n",
            "\n",
            "Global step: 64,loss: 0.15625148\n",
            "\n",
            "Global step: 65,loss: 0.14272211\n",
            "\n",
            "Global step: 66,loss: 0.13736486\n",
            "\n",
            "Global step: 67,loss: 0.13053875\n",
            "\n",
            "Global step: 68,loss: 0.14253414\n",
            "\n",
            "Global step: 69,loss: 0.12220586\n",
            "\n",
            "Global step: 70,loss: 0.118523225\n",
            "\n",
            "Global step: 71,loss: 0.13519417\n",
            "\n",
            "Global step: 72,loss: 0.12416653\n",
            "\n",
            "Global step: 73,loss: 0.1247377\n",
            "\n",
            "Global step: 74,loss: 0.12526624\n",
            "\n",
            "Global step: 75,loss: 0.11479973\n",
            "\n",
            "Global step: 76,loss: 0.1082022\n",
            "\n",
            "Global step: 77,loss: 0.10530335\n",
            "\n",
            "Global step: 78,loss: 0.13448533\n",
            "\n",
            "Global step: 79,loss: 0.1160853\n",
            "\n",
            "Global step: 80,loss: 0.1111063\n",
            "\n",
            "Global step: 81,loss: 0.10764778\n",
            "\n",
            "Global step: 82,loss: 0.11112435\n",
            "\n",
            "Global step: 83,loss: 0.13399662\n",
            "\n",
            "Global step: 84,loss: 0.1139149\n",
            "\n",
            "Global step: 85,loss: 0.10609402\n",
            "\n",
            "Global step: 86,loss: 0.1029286\n",
            "\n",
            "Global step: 87,loss: 0.125613\n",
            "\n",
            "Global step: 88,loss: 0.09289708\n",
            "\n",
            "Global step: 89,loss: 0.1147554\n",
            "\n",
            "Global step: 90,loss: 0.09013558\n",
            "\n",
            "Global step: 91,loss: 0.090415224\n",
            "\n",
            "Global step: 92,loss: 0.100103766\n",
            "\n",
            "Global step: 93,loss: 0.0986619\n",
            "\n",
            "Global step: 94,loss: 0.08995594\n",
            "\n",
            "Global step: 95,loss: 0.08643889\n",
            "\n",
            "Global step: 96,loss: 0.082973406\n",
            "\n",
            "Global step: 97,loss: 0.073643975\n",
            "\n",
            "Global step: 98,loss: 0.08660628\n",
            "\n",
            "Global step: 99,loss: 0.0974417\n",
            "\n",
            "Global step: 100,loss: 0.08152682\n",
            "\n",
            "Global step: 101,loss: 0.0810913\n",
            "\n",
            "Global step: 102,loss: 0.07157012\n",
            "\n",
            "Global step: 103,loss: 0.07742211\n",
            "\n",
            "Global step: 104,loss: 0.0828053\n",
            "\n",
            "Global step: 105,loss: 0.08782183\n",
            "\n",
            "Global step: 106,loss: 0.077692315\n",
            "\n",
            "Global step: 107,loss: 0.06799116\n",
            "\n",
            "Global step: 108,loss: 0.07802441\n",
            "\n",
            "Global step: 109,loss: 0.0881519\n",
            "\n",
            "Global step: 110,loss: 0.06917351\n",
            "\n",
            "Global step: 111,loss: 0.09175211\n",
            "\n",
            "Global step: 112,loss: 0.06577888\n",
            "\n",
            "Global step: 113,loss: 0.06666812\n",
            "\n",
            "Global step: 114,loss: 0.071807556\n",
            "\n",
            "Global step: 115,loss: 0.09307867\n",
            "\n",
            "Global step: 116,loss: 0.08022878\n",
            "\n",
            "Global step: 117,loss: 0.10688851\n",
            "\n",
            "Global step: 118,loss: 0.08670673\n",
            "\n",
            "Global step: 119,loss: 0.06484495\n",
            "\n",
            "Global step: 120,loss: 0.09762944\n",
            "\n",
            "Global step: 121,loss: 0.08685063\n",
            "\n",
            "Global step: 122,loss: 0.07396313\n",
            "\n",
            "Global step: 123,loss: 0.07164426\n",
            "\n",
            "Global step: 124,loss: 0.07946758\n",
            "\n",
            "Global step: 125,loss: 0.07530702\n",
            "\n",
            "Global step: 126,loss: 0.08368954\n",
            "\n",
            "Global step: 127,loss: 0.07895457\n",
            "\n",
            "Global step: 128,loss: 0.07430814\n",
            "\n",
            "Global step: 129,loss: 0.068162315\n",
            "\n",
            "Global step: 130,loss: 0.06300099\n",
            "\n",
            "Global step: 131,loss: 0.07046888\n",
            "\n",
            "Global step: 132,loss: 0.073393404\n",
            "\n",
            "Global step: 133,loss: 0.059971943\n",
            "\n",
            "Global step: 134,loss: 0.060593717\n",
            "\n",
            "Global step: 135,loss: 0.07829295\n",
            "\n",
            "Global step: 136,loss: 0.06690782\n",
            "\n",
            "Global step: 137,loss: 0.06319849\n",
            "\n",
            "Global step: 138,loss: 0.08650359\n",
            "\n",
            "Global step: 139,loss: 0.07407879\n",
            "\n",
            "Global step: 140,loss: 0.06295033\n",
            "\n",
            "Global step: 141,loss: 0.07284217\n",
            "\n",
            "Global step: 142,loss: 0.06851802\n",
            "\n",
            "Global step: 143,loss: 0.06978875\n",
            "\n",
            "Global step: 144,loss: 0.08526189\n",
            "\n",
            "Global step: 145,loss: 0.07663071\n",
            "\n",
            "Global step: 146,loss: 0.06989766\n",
            "\n",
            "Global step: 147,loss: 0.08291538\n",
            "\n",
            "Global step: 148,loss: 0.0831623\n",
            "\n",
            "Global step: 149,loss: 0.072445706\n",
            "\n",
            "Global step: 150,loss: 0.05633106\n",
            "\n",
            "Global step: 151,loss: 0.0726558\n",
            "\n",
            "Global step: 152,loss: 0.06819382\n",
            "\n",
            "Global step: 153,loss: 0.05935832\n",
            "\n",
            "Global step: 154,loss: 0.07109385\n",
            "\n",
            "Global step: 155,loss: 0.056861114\n",
            "\n",
            "Global step: 156,loss: 0.07150939\n",
            "\n",
            "Global step: 157,loss: 0.06283053\n",
            "\n",
            "Global step: 158,loss: 0.082688995\n",
            "\n",
            "Global step: 159,loss: 0.048928842\n",
            "\n",
            "Global step: 160,loss: 0.051252387\n",
            "\n",
            "Global step: 161,loss: 0.06805766\n",
            "\n",
            "Global step: 162,loss: 0.054475237\n",
            "\n",
            "Global step: 163,loss: 0.06274617\n",
            "\n",
            "Global step: 164,loss: 0.060970888\n",
            "\n",
            "Global step: 165,loss: 0.06878901\n",
            "\n",
            "Global step: 166,loss: 0.049420517\n",
            "\n",
            "Global step: 167,loss: 0.057934098\n",
            "\n",
            "Global step: 168,loss: 0.05792204\n",
            "\n",
            "Global step: 169,loss: 0.061004594\n",
            "\n",
            "Global step: 170,loss: 0.05495985\n",
            "\n",
            "Global step: 171,loss: 0.053900126\n",
            "\n",
            "Global step: 172,loss: 0.07534833\n",
            "\n",
            "Global step: 173,loss: 0.051135443\n",
            "\n",
            "Global step: 174,loss: 0.064838074\n",
            "\n",
            "Global step: 175,loss: 0.047770843\n",
            "\n",
            "Global step: 176,loss: 0.06260066\n",
            "\n",
            "Global step: 177,loss: 0.038515903\n",
            "\n",
            "Global step: 178,loss: 0.049916316\n",
            "\n",
            "Global step: 179,loss: 0.06410373\n",
            "\n",
            "Global step: 180,loss: 0.060252838\n",
            "\n",
            "Global step: 181,loss: 0.06328179\n",
            "\n",
            "Global step: 182,loss: 0.0492448\n",
            "\n",
            "Global step: 183,loss: 0.056851294\n",
            "\n",
            "Global step: 184,loss: 0.052715957\n",
            "\n",
            "Global step: 185,loss: 0.0658948\n",
            "\n",
            "Global step: 186,loss: 0.04123066\n",
            "\n",
            "Global step: 187,loss: 0.07080768\n",
            "\n",
            "Global step: 188,loss: 0.06087549\n",
            "\n",
            "Global step: 189,loss: 0.054255888\n",
            "\n",
            "Global step: 190,loss: 0.048166808\n",
            "\n",
            "Global step: 191,loss: 0.058023624\n",
            "\n",
            "Global step: 192,loss: 0.051883746\n",
            "\n",
            "Global step: 193,loss: 0.045503184\n",
            "\n",
            "Global step: 194,loss: 0.058304314\n",
            "\n",
            "Global step: 195,loss: 0.054991692\n",
            "\n",
            "Global step: 196,loss: 0.044121772\n",
            "\n",
            "Global step: 197,loss: 0.048584957\n",
            "\n",
            "Global step: 198,loss: 0.052019853\n",
            "\n",
            "Global step: 199,loss: 0.0670114\n",
            "\n",
            "Global step: 200,loss: 0.05041637\n",
            "\n",
            "Global step: 201,loss: 0.051254354\n",
            "\n",
            "Global step: 202,loss: 0.06029467\n",
            "\n",
            "Global step: 203,loss: 0.061469544\n",
            "\n",
            "Global step: 204,loss: 0.063210465\n",
            "\n",
            "Global step: 205,loss: 0.041304782\n",
            "\n",
            "Global step: 206,loss: 0.04367689\n",
            "\n",
            "Global step: 207,loss: 0.04377617\n",
            "\n",
            "Global step: 208,loss: 0.052476466\n",
            "\n",
            "Global step: 209,loss: 0.047448788\n",
            "\n",
            "Global step: 210,loss: 0.056103766\n",
            "\n",
            "Global step: 211,loss: 0.041381877\n",
            "\n",
            "Global step: 212,loss: 0.051063567\n",
            "\n",
            "Global step: 213,loss: 0.04588832\n",
            "\n",
            "Global step: 214,loss: 0.042950645\n",
            "\n",
            "Global step: 215,loss: 0.060130544\n",
            "\n",
            "Global step: 216,loss: 0.05125333\n",
            "\n",
            "Global step: 217,loss: 0.061231844\n",
            "\n",
            "Global step: 218,loss: 0.042865578\n",
            "\n",
            "Global step: 219,loss: 0.050259978\n",
            "\n",
            "Global step: 220,loss: 0.045741722\n",
            "\n",
            "Global step: 221,loss: 0.058798455\n",
            "\n",
            "Global step: 222,loss: 0.04948451\n",
            "\n",
            "Global step: 223,loss: 0.04790184\n",
            "\n",
            "Global step: 224,loss: 0.050880574\n",
            "\n",
            "Global step: 225,loss: 0.047333546\n",
            "\n",
            "Global step: 226,loss: 0.04445026\n",
            "\n",
            "Global step: 227,loss: 0.049898747\n",
            "\n",
            "Global step: 228,loss: 0.045343395\n",
            "\n",
            "Global step: 229,loss: 0.043235138\n",
            "\n",
            "Global step: 230,loss: 0.06890473\n",
            "\n",
            "Global step: 231,loss: 0.043621883\n",
            "\n",
            "Global step: 232,loss: 0.04624275\n",
            "\n",
            "Global step: 233,loss: 0.05458034\n",
            "\n",
            "Global step: 234,loss: 0.05987425\n",
            "\n",
            "Global step: 235,loss: 0.054780573\n",
            "\n",
            "Global step: 236,loss: 0.04591704\n",
            "\n",
            "Global step: 237,loss: 0.060196325\n",
            "\n",
            "Global step: 238,loss: 0.042122044\n",
            "\n",
            "Global step: 239,loss: 0.055165313\n",
            "\n",
            "Global step: 240,loss: 0.047681447\n",
            "\n",
            "Global step: 241,loss: 0.045400634\n",
            "\n",
            "Global step: 242,loss: 0.057785638\n",
            "\n",
            "Global step: 243,loss: 0.03676183\n",
            "\n",
            "Global step: 244,loss: 0.04271556\n",
            "\n",
            "Global step: 245,loss: 0.04918178\n",
            "\n",
            "Global step: 246,loss: 0.04838452\n",
            "\n",
            "Global step: 247,loss: 0.05594805\n",
            "\n",
            "Global step: 248,loss: 0.03970436\n",
            "\n",
            "Global step: 249,loss: 0.046228707\n",
            "\n",
            "Global step: 250,loss: 0.049102437\n",
            "\n",
            "Global step: 251,loss: 0.04842057\n",
            "\n",
            "Global step: 252,loss: 0.05008833\n",
            "\n",
            "Global step: 253,loss: 0.045093432\n",
            "\n",
            "Global step: 254,loss: 0.053873397\n",
            "\n",
            "Global step: 255,loss: 0.045024484\n",
            "\n",
            "Global step: 256,loss: 0.04307628\n",
            "\n",
            "Global step: 257,loss: 0.03533204\n",
            "\n",
            "Global step: 258,loss: 0.046896264\n",
            "\n",
            "Global step: 259,loss: 0.057152152\n",
            "\n",
            "Global step: 260,loss: 0.05038888\n",
            "\n",
            "Global step: 261,loss: 0.09457628\n",
            "\n",
            "Global step: 262,loss: 0.050485373\n",
            "\n",
            "Global step: 263,loss: 0.046583213\n",
            "\n",
            "Global step: 264,loss: 0.036100443\n",
            "\n",
            "Global step: 265,loss: 0.033299375\n",
            "\n",
            "Global step: 266,loss: 0.04065194\n",
            "\n",
            "Global step: 267,loss: 0.042001866\n",
            "\n",
            "Global step: 268,loss: 0.035081252\n",
            "\n",
            "Global step: 269,loss: 0.04154298\n",
            "\n",
            "Global step: 270,loss: 0.035192408\n",
            "\n",
            "Global step: 271,loss: 0.0493775\n",
            "\n",
            "Global step: 272,loss: 0.037236717\n",
            "\n",
            "Global step: 273,loss: 0.052265823\n",
            "\n",
            "Global step: 274,loss: 0.04413756\n",
            "\n",
            "Global step: 275,loss: 0.032673903\n",
            "\n",
            "Global step: 276,loss: 0.050220598\n",
            "\n",
            "Global step: 277,loss: 0.038305663\n",
            "\n",
            "Global step: 278,loss: 0.041213788\n",
            "\n",
            "Global step: 279,loss: 0.046480596\n",
            "\n",
            "Global step: 280,loss: 0.044232015\n",
            "\n",
            "Global step: 281,loss: 0.037294284\n",
            "\n",
            "Global step: 282,loss: 0.034343846\n",
            "\n",
            "Global step: 283,loss: 0.04724566\n",
            "\n",
            "Global step: 284,loss: 0.04173463\n",
            "\n",
            "Global step: 285,loss: 0.033304427\n",
            "\n",
            "Global step: 286,loss: 0.035989\n",
            "\n",
            "Global step: 287,loss: 0.044170864\n",
            "\n",
            "Global step: 288,loss: 0.044873267\n",
            "\n",
            "Global step: 289,loss: 0.04836008\n",
            "\n",
            "Global step: 290,loss: 0.03245545\n",
            "\n",
            "Global step: 291,loss: 0.04578787\n",
            "\n",
            "Global step: 292,loss: 0.04246456\n",
            "\n",
            "Global step: 293,loss: 0.043663986\n",
            "\n",
            "Global step: 294,loss: 0.045443065\n",
            "\n",
            "Global step: 295,loss: 0.036464393\n",
            "\n",
            "Global step: 296,loss: 0.045255467\n",
            "\n",
            "Global step: 297,loss: 0.037112232\n",
            "\n",
            "Global step: 298,loss: 0.048929498\n",
            "\n",
            "Global step: 299,loss: 0.041467406\n",
            "\n",
            "Global step: 300,loss: 0.05025268\n",
            "\n",
            "Global step: 301,loss: 0.03129109\n",
            "\n",
            "Global step: 302,loss: 0.042894535\n",
            "\n",
            "Global step: 303,loss: 0.038223412\n",
            "\n",
            "Global step: 304,loss: 0.037148546\n",
            "\n",
            "Global step: 305,loss: 0.04404041\n",
            "\n",
            "Global step: 306,loss: 0.04992629\n",
            "\n",
            "Global step: 307,loss: 0.046559818\n",
            "\n",
            "Global step: 308,loss: 0.040320914\n",
            "\n",
            "Global step: 309,loss: 0.035960577\n",
            "\n",
            "Global step: 310,loss: 0.0526981\n",
            "\n",
            "Global step: 311,loss: 0.03158188\n",
            "\n",
            "Global step: 312,loss: 0.035722613\n",
            "\n",
            "Global step: 313,loss: 0.045277584\n",
            "\n",
            "Global step: 314,loss: 0.03548307\n",
            "\n",
            "Global step: 315,loss: 0.037784193\n",
            "\n",
            "Global step: 316,loss: 0.035988733\n",
            "\n",
            "Global step: 317,loss: 0.027425325\n",
            "\n",
            "Global step: 318,loss: 0.03791865\n",
            "\n",
            "Global step: 319,loss: 0.03535589\n",
            "\n",
            "Global step: 320,loss: 0.040587313\n",
            "\n",
            "Global step: 321,loss: 0.036246378\n",
            "\n",
            "Global step: 322,loss: 0.043850563\n",
            "\n",
            "Global step: 323,loss: 0.047260206\n",
            "\n",
            "Global step: 324,loss: 0.038385533\n",
            "\n",
            "Global step: 325,loss: 0.040890288\n",
            "\n",
            "Global step: 326,loss: 0.04300937\n",
            "\n",
            "Global step: 327,loss: 0.043102562\n",
            "\n",
            "Global step: 328,loss: 0.035190035\n",
            "\n",
            "Global step: 329,loss: 0.03846723\n",
            "\n",
            "Global step: 330,loss: 0.046659194\n",
            "\n",
            "Global step: 331,loss: 0.055570945\n",
            "\n",
            "Global step: 332,loss: 0.04552198\n",
            "\n",
            "Global step: 333,loss: 0.039697208\n",
            "\n",
            "Global step: 334,loss: 0.039415844\n",
            "\n",
            "Global step: 335,loss: 0.0488197\n",
            "\n",
            "Global step: 336,loss: 0.04574705\n",
            "\n",
            "Global step: 337,loss: 0.04999668\n",
            "\n",
            "Global step: 338,loss: 0.05019232\n",
            "\n",
            "Global step: 339,loss: 0.047554363\n",
            "\n",
            "Global step: 340,loss: 0.038233973\n",
            "\n",
            "Global step: 341,loss: 0.05407726\n",
            "\n",
            "Global step: 342,loss: 0.040078923\n",
            "\n",
            "Global step: 343,loss: 0.043672055\n",
            "\n",
            "Global step: 344,loss: 0.03391169\n",
            "\n",
            "Global step: 345,loss: 0.04444544\n",
            "\n",
            "Global step: 346,loss: 0.047087636\n",
            "\n",
            "Global step: 347,loss: 0.04114031\n",
            "\n",
            "Global step: 348,loss: 0.03615645\n",
            "\n",
            "Global step: 349,loss: 0.050207533\n",
            "\n",
            "Global step: 350,loss: 0.04658985\n",
            "\n",
            "Global step: 351,loss: 0.03732066\n",
            "\n",
            "Global step: 352,loss: 0.044808954\n",
            "\n",
            "Global step: 353,loss: 0.0302847\n",
            "\n",
            "Global step: 354,loss: 0.040708944\n",
            "\n",
            "Global step: 355,loss: 0.030854832\n",
            "\n",
            "Global step: 356,loss: 0.045105662\n",
            "\n",
            "Global step: 357,loss: 0.03367914\n",
            "\n",
            "Global step: 358,loss: 0.034754798\n",
            "\n",
            "Global step: 359,loss: 0.044483844\n",
            "\n",
            "Global step: 360,loss: 0.0369191\n",
            "\n",
            "Global step: 361,loss: 0.04525684\n",
            "\n",
            "Global step: 362,loss: 0.035314\n",
            "\n",
            "Global step: 363,loss: 0.03650018\n",
            "\n",
            "Global step: 364,loss: 0.061067812\n",
            "\n",
            "Global step: 365,loss: 0.046660416\n",
            "\n",
            "Global step: 366,loss: 0.036645487\n",
            "\n",
            "Global step: 367,loss: 0.027693622\n",
            "\n",
            "Global step: 368,loss: 0.038268737\n",
            "\n",
            "Global step: 369,loss: 0.045783363\n",
            "\n",
            "Global step: 370,loss: 0.041487098\n",
            "\n",
            "Global step: 371,loss: 0.0331968\n",
            "\n",
            "Global step: 372,loss: 0.027820937\n",
            "\n",
            "Global step: 373,loss: 0.027018854\n",
            "\n",
            "Global step: 374,loss: 0.036730193\n",
            "\n",
            "Global step: 375,loss: 0.035571575\n",
            "\n",
            "Global step: 376,loss: 0.05280973\n",
            "\n",
            "Global step: 377,loss: 0.029868446\n",
            "\n",
            "Global step: 378,loss: 0.041272555\n",
            "\n",
            "Global step: 379,loss: 0.02494516\n",
            "\n",
            "Global step: 380,loss: 0.03141315\n",
            "\n",
            "Global step: 381,loss: 0.038733967\n",
            "\n",
            "Global step: 382,loss: 0.03656821\n",
            "\n",
            "Global step: 383,loss: 0.03410781\n",
            "\n",
            "Global step: 384,loss: 0.02883469\n",
            "\n",
            "Global step: 385,loss: 0.03633336\n",
            "\n",
            "Global step: 386,loss: 0.02747569\n",
            "\n",
            "Global step: 387,loss: 0.04738325\n",
            "\n",
            "Global step: 388,loss: 0.04021031\n",
            "\n",
            "Global step: 389,loss: 0.030656513\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 389,Val_Loss: 0.03543643813389234,  Val_acc: 0.9896834935897436 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:06:02.134067 139738004481920 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/16:\n",
            "Global step: 390,loss: 0.062191524\n",
            "\n",
            "Global step: 391,loss: 0.03637346\n",
            "\n",
            "Global step: 392,loss: 0.04202199\n",
            "\n",
            "Global step: 393,loss: 0.036985125\n",
            "\n",
            "Global step: 394,loss: 0.05144764\n",
            "\n",
            "Global step: 395,loss: 0.031606846\n",
            "\n",
            "Global step: 396,loss: 0.029419307\n",
            "\n",
            "Global step: 397,loss: 0.039523564\n",
            "\n",
            "Global step: 398,loss: 0.043793775\n",
            "\n",
            "Global step: 399,loss: 0.030159108\n",
            "\n",
            "Global step: 400,loss: 0.04178923\n",
            "\n",
            "Global step: 401,loss: 0.046166696\n",
            "\n",
            "Global step: 402,loss: 0.025694791\n",
            "\n",
            "Global step: 403,loss: 0.044580407\n",
            "\n",
            "Global step: 404,loss: 0.044358697\n",
            "\n",
            "Global step: 405,loss: 0.040714763\n",
            "\n",
            "Global step: 406,loss: 0.040203325\n",
            "\n",
            "Global step: 407,loss: 0.03890744\n",
            "\n",
            "Global step: 408,loss: 0.035562612\n",
            "\n",
            "Global step: 409,loss: 0.042427957\n",
            "\n",
            "Global step: 410,loss: 0.033697944\n",
            "\n",
            "Global step: 411,loss: 0.027469661\n",
            "\n",
            "Global step: 412,loss: 0.04665229\n",
            "\n",
            "Global step: 413,loss: 0.036221705\n",
            "\n",
            "Global step: 414,loss: 0.02742868\n",
            "\n",
            "Global step: 415,loss: 0.036891565\n",
            "\n",
            "Global step: 416,loss: 0.032201037\n",
            "\n",
            "Global step: 417,loss: 0.027727418\n",
            "\n",
            "Global step: 418,loss: 0.03218887\n",
            "\n",
            "Global step: 419,loss: 0.032669477\n",
            "\n",
            "Global step: 420,loss: 0.032415226\n",
            "\n",
            "Global step: 421,loss: 0.03431552\n",
            "\n",
            "Global step: 422,loss: 0.028332323\n",
            "\n",
            "Global step: 423,loss: 0.026400378\n",
            "\n",
            "Global step: 424,loss: 0.05092234\n",
            "\n",
            "Global step: 425,loss: 0.029257407\n",
            "\n",
            "Global step: 426,loss: 0.03805793\n",
            "\n",
            "Global step: 427,loss: 0.042840116\n",
            "\n",
            "Global step: 428,loss: 0.039693244\n",
            "\n",
            "Global step: 429,loss: 0.03932829\n",
            "\n",
            "Global step: 430,loss: 0.02891726\n",
            "\n",
            "Global step: 431,loss: 0.036158286\n",
            "\n",
            "Global step: 432,loss: 0.028342877\n",
            "\n",
            "Global step: 433,loss: 0.030336019\n",
            "\n",
            "Global step: 434,loss: 0.034585968\n",
            "\n",
            "Global step: 435,loss: 0.036460087\n",
            "\n",
            "Global step: 436,loss: 0.032752845\n",
            "\n",
            "Global step: 437,loss: 0.034060307\n",
            "\n",
            "Global step: 438,loss: 0.039937682\n",
            "\n",
            "Global step: 439,loss: 0.029536424\n",
            "\n",
            "Global step: 440,loss: 0.044455357\n",
            "\n",
            "Global step: 441,loss: 0.030828474\n",
            "\n",
            "Global step: 442,loss: 0.035535224\n",
            "\n",
            "Global step: 443,loss: 0.027360212\n",
            "\n",
            "Global step: 444,loss: 0.037266947\n",
            "\n",
            "Global step: 445,loss: 0.027783148\n",
            "\n",
            "Global step: 446,loss: 0.031966537\n",
            "\n",
            "Global step: 447,loss: 0.025087029\n",
            "\n",
            "Global step: 448,loss: 0.031218633\n",
            "\n",
            "Global step: 449,loss: 0.03352787\n",
            "\n",
            "Global step: 450,loss: 0.03381896\n",
            "\n",
            "Global step: 451,loss: 0.041522525\n",
            "\n",
            "Global step: 452,loss: 0.037060425\n",
            "\n",
            "Global step: 453,loss: 0.03592766\n",
            "\n",
            "Global step: 454,loss: 0.038599387\n",
            "\n",
            "Global step: 455,loss: 0.0313937\n",
            "\n",
            "Global step: 456,loss: 0.029800195\n",
            "\n",
            "Global step: 457,loss: 0.036573283\n",
            "\n",
            "Global step: 458,loss: 0.03691469\n",
            "\n",
            "Global step: 459,loss: 0.031727906\n",
            "\n",
            "Global step: 460,loss: 0.042318154\n",
            "\n",
            "Global step: 461,loss: 0.034410626\n",
            "\n",
            "Global step: 462,loss: 0.028711699\n",
            "\n",
            "Global step: 463,loss: 0.05075476\n",
            "\n",
            "Global step: 464,loss: 0.041543104\n",
            "\n",
            "Global step: 465,loss: 0.02998405\n",
            "\n",
            "Global step: 466,loss: 0.028582025\n",
            "\n",
            "Global step: 467,loss: 0.035158567\n",
            "\n",
            "Global step: 468,loss: 0.0349829\n",
            "\n",
            "Global step: 469,loss: 0.033307135\n",
            "\n",
            "Global step: 470,loss: 0.04602488\n",
            "\n",
            "Global step: 471,loss: 0.03655622\n",
            "\n",
            "Global step: 472,loss: 0.034791633\n",
            "\n",
            "Global step: 473,loss: 0.027300274\n",
            "\n",
            "Global step: 474,loss: 0.032790553\n",
            "\n",
            "Global step: 475,loss: 0.03292471\n",
            "\n",
            "Global step: 476,loss: 0.048579346\n",
            "\n",
            "Global step: 477,loss: 0.042104222\n",
            "\n",
            "Global step: 478,loss: 0.032274347\n",
            "\n",
            "Global step: 479,loss: 0.032678008\n",
            "\n",
            "Global step: 480,loss: 0.029211015\n",
            "\n",
            "Global step: 481,loss: 0.038912304\n",
            "\n",
            "Global step: 482,loss: 0.04366944\n",
            "\n",
            "Global step: 483,loss: 0.03041099\n",
            "\n",
            "Global step: 484,loss: 0.033621788\n",
            "\n",
            "Global step: 485,loss: 0.03390406\n",
            "\n",
            "Global step: 486,loss: 0.0276074\n",
            "\n",
            "Global step: 487,loss: 0.039015833\n",
            "\n",
            "Global step: 488,loss: 0.03411127\n",
            "\n",
            "Global step: 489,loss: 0.03534327\n",
            "\n",
            "Global step: 490,loss: 0.026691053\n",
            "\n",
            "Global step: 491,loss: 0.03671515\n",
            "\n",
            "Global step: 492,loss: 0.031811655\n",
            "\n",
            "Global step: 493,loss: 0.025064196\n",
            "\n",
            "Global step: 494,loss: 0.022405483\n",
            "\n",
            "Global step: 495,loss: 0.03467672\n",
            "\n",
            "Global step: 496,loss: 0.025275208\n",
            "\n",
            "Global step: 497,loss: 0.042689323\n",
            "\n",
            "Global step: 498,loss: 0.028985223\n",
            "\n",
            "Global step: 499,loss: 0.025511278\n",
            "\n",
            "Global step: 500,loss: 0.020639889\n",
            "\n",
            "Global step: 501,loss: 0.029781325\n",
            "\n",
            "Global step: 502,loss: 0.03431429\n",
            "\n",
            "Global step: 503,loss: 0.03632934\n",
            "\n",
            "Global step: 504,loss: 0.028604925\n",
            "\n",
            "Global step: 505,loss: 0.048182625\n",
            "\n",
            "Global step: 506,loss: 0.028536923\n",
            "\n",
            "Global step: 507,loss: 0.03546341\n",
            "\n",
            "Global step: 508,loss: 0.02762308\n",
            "\n",
            "Global step: 509,loss: 0.027876582\n",
            "\n",
            "Global step: 510,loss: 0.032004744\n",
            "\n",
            "Global step: 511,loss: 0.029477026\n",
            "\n",
            "Global step: 512,loss: 0.035128567\n",
            "\n",
            "Global step: 513,loss: 0.03728346\n",
            "\n",
            "Global step: 514,loss: 0.031415425\n",
            "\n",
            "Global step: 515,loss: 0.031366237\n",
            "\n",
            "Global step: 516,loss: 0.024177406\n",
            "\n",
            "Global step: 517,loss: 0.032318406\n",
            "\n",
            "Global step: 518,loss: 0.027687697\n",
            "\n",
            "Global step: 519,loss: 0.036160015\n",
            "\n",
            "Global step: 520,loss: 0.0309178\n",
            "\n",
            "Global step: 521,loss: 0.031635765\n",
            "\n",
            "Global step: 522,loss: 0.034706797\n",
            "\n",
            "Global step: 523,loss: 0.035264242\n",
            "\n",
            "Global step: 524,loss: 0.026172757\n",
            "\n",
            "Global step: 525,loss: 0.029236816\n",
            "\n",
            "Global step: 526,loss: 0.020387094\n",
            "\n",
            "Global step: 527,loss: 0.030930188\n",
            "\n",
            "Global step: 528,loss: 0.030150149\n",
            "\n",
            "Global step: 529,loss: 0.02620474\n",
            "\n",
            "Global step: 530,loss: 0.030982459\n",
            "\n",
            "Global step: 531,loss: 0.03698626\n",
            "\n",
            "Global step: 532,loss: 0.0366406\n",
            "\n",
            "Global step: 533,loss: 0.03455005\n",
            "\n",
            "Global step: 534,loss: 0.027552312\n",
            "\n",
            "Global step: 535,loss: 0.036160577\n",
            "\n",
            "Global step: 536,loss: 0.03524401\n",
            "\n",
            "Global step: 537,loss: 0.029072195\n",
            "\n",
            "Global step: 538,loss: 0.023618288\n",
            "\n",
            "Global step: 539,loss: 0.020632308\n",
            "\n",
            "Global step: 540,loss: 0.030968787\n",
            "\n",
            "Global step: 541,loss: 0.028347265\n",
            "\n",
            "Global step: 542,loss: 0.03911404\n",
            "\n",
            "Global step: 543,loss: 0.030900354\n",
            "\n",
            "Global step: 544,loss: 0.03642691\n",
            "\n",
            "Global step: 545,loss: 0.023954555\n",
            "\n",
            "Global step: 546,loss: 0.030734412\n",
            "\n",
            "Global step: 547,loss: 0.029306868\n",
            "\n",
            "Global step: 548,loss: 0.028138675\n",
            "\n",
            "Global step: 549,loss: 0.039062385\n",
            "\n",
            "Global step: 550,loss: 0.03329682\n",
            "\n",
            "Global step: 551,loss: 0.021945726\n",
            "\n",
            "Global step: 552,loss: 0.025379064\n",
            "\n",
            "Global step: 553,loss: 0.029145557\n",
            "\n",
            "Global step: 554,loss: 0.028198434\n",
            "\n",
            "Global step: 555,loss: 0.02850395\n",
            "\n",
            "Global step: 556,loss: 0.030201253\n",
            "\n",
            "Global step: 557,loss: 0.02385306\n",
            "\n",
            "Global step: 558,loss: 0.023141712\n",
            "\n",
            "Global step: 559,loss: 0.033410136\n",
            "\n",
            "Global step: 560,loss: 0.03002831\n",
            "\n",
            "Global step: 561,loss: 0.034871705\n",
            "\n",
            "Global step: 562,loss: 0.027956737\n",
            "\n",
            "Global step: 563,loss: 0.041646995\n",
            "\n",
            "Global step: 564,loss: 0.026685491\n",
            "\n",
            "Global step: 565,loss: 0.03059246\n",
            "\n",
            "Global step: 566,loss: 0.024963632\n",
            "\n",
            "Global step: 567,loss: 0.02633457\n",
            "\n",
            "Global step: 568,loss: 0.028065674\n",
            "\n",
            "Global step: 569,loss: 0.04723916\n",
            "\n",
            "Global step: 570,loss: 0.0380896\n",
            "\n",
            "Global step: 571,loss: 0.03329648\n",
            "\n",
            "Global step: 572,loss: 0.032578602\n",
            "\n",
            "Global step: 573,loss: 0.024971157\n",
            "\n",
            "Global step: 574,loss: 0.027941857\n",
            "\n",
            "Global step: 575,loss: 0.050644968\n",
            "\n",
            "Global step: 576,loss: 0.043958686\n",
            "\n",
            "Global step: 577,loss: 0.03445167\n",
            "\n",
            "Global step: 578,loss: 0.025160514\n",
            "\n",
            "Global step: 579,loss: 0.036693357\n",
            "\n",
            "Global step: 580,loss: 0.04037511\n",
            "\n",
            "Global step: 581,loss: 0.037787534\n",
            "\n",
            "Global step: 582,loss: 0.023082376\n",
            "\n",
            "Global step: 583,loss: 0.036290728\n",
            "\n",
            "Global step: 584,loss: 0.027317546\n",
            "\n",
            "Global step: 585,loss: 0.03673642\n",
            "\n",
            "Global step: 586,loss: 0.023593314\n",
            "\n",
            "Global step: 587,loss: 0.0284954\n",
            "\n",
            "Global step: 588,loss: 0.034956776\n",
            "\n",
            "Global step: 589,loss: 0.03709747\n",
            "\n",
            "Global step: 590,loss: 0.02640991\n",
            "\n",
            "Global step: 591,loss: 0.02846702\n",
            "\n",
            "Global step: 592,loss: 0.02476879\n",
            "\n",
            "Global step: 593,loss: 0.031812396\n",
            "\n",
            "Global step: 594,loss: 0.027226686\n",
            "\n",
            "Global step: 595,loss: 0.026791992\n",
            "\n",
            "Global step: 596,loss: 0.03509579\n",
            "\n",
            "Global step: 597,loss: 0.039737675\n",
            "\n",
            "Global step: 598,loss: 0.031160375\n",
            "\n",
            "Global step: 599,loss: 0.032010134\n",
            "\n",
            "Global step: 600,loss: 0.042474166\n",
            "\n",
            "Global step: 601,loss: 0.026662027\n",
            "\n",
            "Global step: 602,loss: 0.03304957\n",
            "\n",
            "Global step: 603,loss: 0.036629748\n",
            "\n",
            "Global step: 604,loss: 0.025692757\n",
            "\n",
            "Global step: 605,loss: 0.0313858\n",
            "\n",
            "Global step: 606,loss: 0.02764596\n",
            "\n",
            "Global step: 607,loss: 0.034497768\n",
            "\n",
            "Global step: 608,loss: 0.03384766\n",
            "\n",
            "Global step: 609,loss: 0.02137154\n",
            "\n",
            "Global step: 610,loss: 0.032035407\n",
            "\n",
            "Global step: 611,loss: 0.021807067\n",
            "\n",
            "Global step: 612,loss: 0.046668448\n",
            "\n",
            "Global step: 613,loss: 0.03291151\n",
            "\n",
            "Global step: 614,loss: 0.03975631\n",
            "\n",
            "Global step: 615,loss: 0.02504532\n",
            "\n",
            "Global step: 616,loss: 0.025659544\n",
            "\n",
            "Global step: 617,loss: 0.048029736\n",
            "\n",
            "Global step: 618,loss: 0.029613717\n",
            "\n",
            "Global step: 619,loss: 0.02435909\n",
            "\n",
            "Global step: 620,loss: 0.02739854\n",
            "\n",
            "Global step: 621,loss: 0.025143024\n",
            "\n",
            "Global step: 622,loss: 0.02803268\n",
            "\n",
            "Global step: 623,loss: 0.029596832\n",
            "\n",
            "Global step: 624,loss: 0.02660554\n",
            "\n",
            "Global step: 625,loss: 0.030821571\n",
            "\n",
            "Global step: 626,loss: 0.026941864\n",
            "\n",
            "Global step: 627,loss: 0.028454136\n",
            "\n",
            "Global step: 628,loss: 0.02374377\n",
            "\n",
            "Global step: 629,loss: 0.02395903\n",
            "\n",
            "Global step: 630,loss: 0.040445\n",
            "\n",
            "Global step: 631,loss: 0.03295223\n",
            "\n",
            "Global step: 632,loss: 0.020785263\n",
            "\n",
            "Global step: 633,loss: 0.022680221\n",
            "\n",
            "Global step: 634,loss: 0.032255962\n",
            "\n",
            "Global step: 635,loss: 0.02907741\n",
            "\n",
            "Global step: 636,loss: 0.03628224\n",
            "\n",
            "Global step: 637,loss: 0.029863471\n",
            "\n",
            "Global step: 638,loss: 0.026405375\n",
            "\n",
            "Global step: 639,loss: 0.037442554\n",
            "\n",
            "Global step: 640,loss: 0.028028235\n",
            "\n",
            "Global step: 641,loss: 0.032507315\n",
            "\n",
            "Global step: 642,loss: 0.03663546\n",
            "\n",
            "Global step: 643,loss: 0.023887329\n",
            "\n",
            "Global step: 644,loss: 0.038311392\n",
            "\n",
            "Global step: 645,loss: 0.028629381\n",
            "\n",
            "Global step: 646,loss: 0.0279437\n",
            "\n",
            "Global step: 647,loss: 0.023570985\n",
            "\n",
            "Global step: 648,loss: 0.027477972\n",
            "\n",
            "Global step: 649,loss: 0.026688796\n",
            "\n",
            "Global step: 650,loss: 0.031887513\n",
            "\n",
            "Global step: 651,loss: 0.02048099\n",
            "\n",
            "Global step: 652,loss: 0.027582984\n",
            "\n",
            "Global step: 653,loss: 0.019739132\n",
            "\n",
            "Global step: 654,loss: 0.027395654\n",
            "\n",
            "Global step: 655,loss: 0.022807509\n",
            "\n",
            "Global step: 656,loss: 0.040424246\n",
            "\n",
            "Global step: 657,loss: 0.031663936\n",
            "\n",
            "Global step: 658,loss: 0.035386726\n",
            "\n",
            "Global step: 659,loss: 0.026163852\n",
            "\n",
            "Global step: 660,loss: 0.028395912\n",
            "\n",
            "Global step: 661,loss: 0.025054038\n",
            "\n",
            "Global step: 662,loss: 0.025496356\n",
            "\n",
            "Global step: 663,loss: 0.022477375\n",
            "\n",
            "Global step: 664,loss: 0.017857773\n",
            "\n",
            "Global step: 665,loss: 0.019060906\n",
            "\n",
            "Global step: 666,loss: 0.025598757\n",
            "\n",
            "Global step: 667,loss: 0.037142016\n",
            "\n",
            "Global step: 668,loss: 0.02644885\n",
            "\n",
            "Global step: 669,loss: 0.021629136\n",
            "\n",
            "Global step: 670,loss: 0.031517707\n",
            "\n",
            "Global step: 671,loss: 0.025429213\n",
            "\n",
            "Global step: 672,loss: 0.024708562\n",
            "\n",
            "Global step: 673,loss: 0.022842072\n",
            "\n",
            "Global step: 674,loss: 0.03324488\n",
            "\n",
            "Global step: 675,loss: 0.03286098\n",
            "\n",
            "Global step: 676,loss: 0.035117164\n",
            "\n",
            "Global step: 677,loss: 0.034997027\n",
            "\n",
            "Global step: 678,loss: 0.030683555\n",
            "\n",
            "Global step: 679,loss: 0.030658128\n",
            "\n",
            "Global step: 680,loss: 0.038418468\n",
            "\n",
            "Global step: 681,loss: 0.02467103\n",
            "\n",
            "Global step: 682,loss: 0.035059724\n",
            "\n",
            "Global step: 683,loss: 0.023827292\n",
            "\n",
            "Global step: 684,loss: 0.029428637\n",
            "\n",
            "Global step: 685,loss: 0.026929963\n",
            "\n",
            "Global step: 686,loss: 0.033509985\n",
            "\n",
            "Global step: 687,loss: 0.02539619\n",
            "\n",
            "Global step: 688,loss: 0.029151507\n",
            "\n",
            "Global step: 689,loss: 0.024815943\n",
            "\n",
            "Global step: 690,loss: 0.022538982\n",
            "\n",
            "Global step: 691,loss: 0.025879504\n",
            "\n",
            "Global step: 692,loss: 0.031520616\n",
            "\n",
            "Global step: 693,loss: 0.02491435\n",
            "\n",
            "Global step: 694,loss: 0.025968438\n",
            "\n",
            "Global step: 695,loss: 0.02635492\n",
            "\n",
            "Global step: 696,loss: 0.02348474\n",
            "\n",
            "Global step: 697,loss: 0.023941088\n",
            "\n",
            "Global step: 698,loss: 0.029824242\n",
            "\n",
            "Global step: 699,loss: 0.020313732\n",
            "\n",
            "Global step: 700,loss: 0.023506355\n",
            "\n",
            "Global step: 701,loss: 0.027368395\n",
            "\n",
            "Global step: 702,loss: 0.027035218\n",
            "\n",
            "Global step: 703,loss: 0.024978073\n",
            "\n",
            "Global step: 704,loss: 0.019297821\n",
            "\n",
            "Global step: 705,loss: 0.028659988\n",
            "\n",
            "Global step: 706,loss: 0.03705594\n",
            "\n",
            "Global step: 707,loss: 0.037017528\n",
            "\n",
            "Global step: 708,loss: 0.031044275\n",
            "\n",
            "Global step: 709,loss: 0.036832836\n",
            "\n",
            "Global step: 710,loss: 0.021727318\n",
            "\n",
            "Global step: 711,loss: 0.035279635\n",
            "\n",
            "Global step: 712,loss: 0.0236458\n",
            "\n",
            "Global step: 713,loss: 0.025700811\n",
            "\n",
            "Global step: 714,loss: 0.03265614\n",
            "\n",
            "Global step: 715,loss: 0.025321182\n",
            "\n",
            "Global step: 716,loss: 0.025135322\n",
            "\n",
            "Global step: 717,loss: 0.025262414\n",
            "\n",
            "Global step: 718,loss: 0.022790339\n",
            "\n",
            "Global step: 719,loss: 0.019754432\n",
            "\n",
            "Global step: 720,loss: 0.019056916\n",
            "\n",
            "Global step: 721,loss: 0.0214604\n",
            "\n",
            "Global step: 722,loss: 0.021111684\n",
            "\n",
            "Global step: 723,loss: 0.027291954\n",
            "\n",
            "Global step: 724,loss: 0.026367862\n",
            "\n",
            "Global step: 725,loss: 0.03409767\n",
            "\n",
            "Global step: 726,loss: 0.018390054\n",
            "\n",
            "Global step: 727,loss: 0.02702494\n",
            "\n",
            "Global step: 728,loss: 0.027550358\n",
            "\n",
            "Global step: 729,loss: 0.03406734\n",
            "\n",
            "Global step: 730,loss: 0.031504378\n",
            "\n",
            "Global step: 731,loss: 0.027523976\n",
            "\n",
            "Global step: 732,loss: 0.042731926\n",
            "\n",
            "Global step: 733,loss: 0.024615377\n",
            "\n",
            "Global step: 734,loss: 0.02595602\n",
            "\n",
            "Global step: 735,loss: 0.028237272\n",
            "\n",
            "Global step: 736,loss: 0.024173364\n",
            "\n",
            "Global step: 737,loss: 0.022616923\n",
            "\n",
            "Global step: 738,loss: 0.025970936\n",
            "\n",
            "Global step: 739,loss: 0.021364437\n",
            "\n",
            "Global step: 740,loss: 0.019018546\n",
            "\n",
            "Global step: 741,loss: 0.02494348\n",
            "\n",
            "Global step: 742,loss: 0.022771668\n",
            "\n",
            "Global step: 743,loss: 0.037883483\n",
            "\n",
            "Global step: 744,loss: 0.027858958\n",
            "\n",
            "Global step: 745,loss: 0.032695442\n",
            "\n",
            "Global step: 746,loss: 0.02353623\n",
            "\n",
            "Global step: 747,loss: 0.026173688\n",
            "\n",
            "Global step: 748,loss: 0.024173453\n",
            "\n",
            "Global step: 749,loss: 0.028251644\n",
            "\n",
            "Global step: 750,loss: 0.029744267\n",
            "\n",
            "Global step: 751,loss: 0.017180987\n",
            "\n",
            "Global step: 752,loss: 0.02905718\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 753.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:06:52.454085 139735711573760 supervisor.py:1050] Recording summary at step 753.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.37652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:06:52.492884 139735703181056 supervisor.py:1099] global_step/sec: 6.37652\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 753,loss: 0.035930466\n",
            "\n",
            "Global step: 754,loss: 0.036671735\n",
            "\n",
            "Global step: 755,loss: 0.023020223\n",
            "\n",
            "Global step: 756,loss: 0.01967778\n",
            "\n",
            "Global step: 757,loss: 0.028170943\n",
            "\n",
            "Global step: 758,loss: 0.021921948\n",
            "\n",
            "Global step: 759,loss: 0.027651634\n",
            "\n",
            "Global step: 760,loss: 0.021832135\n",
            "\n",
            "Global step: 761,loss: 0.021464191\n",
            "\n",
            "Global step: 762,loss: 0.020042913\n",
            "\n",
            "Global step: 763,loss: 0.035884872\n",
            "\n",
            "Global step: 764,loss: 0.026008748\n",
            "\n",
            "Global step: 765,loss: 0.033689655\n",
            "\n",
            "Global step: 766,loss: 0.038970087\n",
            "\n",
            "Global step: 767,loss: 0.028438691\n",
            "\n",
            "Global step: 768,loss: 0.027564976\n",
            "\n",
            "Global step: 769,loss: 0.027254943\n",
            "\n",
            "Global step: 770,loss: 0.020557992\n",
            "\n",
            "Global step: 771,loss: 0.020002358\n",
            "\n",
            "Global step: 772,loss: 0.02023343\n",
            "\n",
            "Global step: 773,loss: 0.03481988\n",
            "\n",
            "Global step: 774,loss: 0.036071748\n",
            "\n",
            "Global step: 775,loss: 0.032504603\n",
            "\n",
            "Global step: 776,loss: 0.030086989\n",
            "\n",
            "Global step: 777,loss: 0.017626101\n",
            "\n",
            "Global step: 778,loss: 0.025301397\n",
            "\n",
            "Global step: 779,loss: 0.022128288\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 779,Val_Loss: 0.027265070268932063,  Val_acc: 0.9930889423076923 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:07:01.210408 139738004481920 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/16:\n",
            "Global step: 780,loss: 0.022995187\n",
            "\n",
            "Global step: 781,loss: 0.020318247\n",
            "\n",
            "Global step: 782,loss: 0.022683883\n",
            "\n",
            "Global step: 783,loss: 0.04330811\n",
            "\n",
            "Global step: 784,loss: 0.01890262\n",
            "\n",
            "Global step: 785,loss: 0.018343832\n",
            "\n",
            "Global step: 786,loss: 0.017090192\n",
            "\n",
            "Global step: 787,loss: 0.01729724\n",
            "\n",
            "Global step: 788,loss: 0.017937008\n",
            "\n",
            "Global step: 789,loss: 0.019285347\n",
            "\n",
            "Global step: 790,loss: 0.02619103\n",
            "\n",
            "Global step: 791,loss: 0.019239206\n",
            "\n",
            "Global step: 792,loss: 0.019803448\n",
            "\n",
            "Global step: 793,loss: 0.024552211\n",
            "\n",
            "Global step: 794,loss: 0.021682084\n",
            "\n",
            "Global step: 795,loss: 0.02509376\n",
            "\n",
            "Global step: 796,loss: 0.0197649\n",
            "\n",
            "Global step: 797,loss: 0.027787592\n",
            "\n",
            "Global step: 798,loss: 0.019668644\n",
            "\n",
            "Global step: 799,loss: 0.023615582\n",
            "\n",
            "Global step: 800,loss: 0.03281314\n",
            "\n",
            "Global step: 801,loss: 0.02656845\n",
            "\n",
            "Global step: 802,loss: 0.027644113\n",
            "\n",
            "Global step: 803,loss: 0.022786925\n",
            "\n",
            "Global step: 804,loss: 0.022245526\n",
            "\n",
            "Global step: 805,loss: 0.023737118\n",
            "\n",
            "Global step: 806,loss: 0.02767745\n",
            "\n",
            "Global step: 807,loss: 0.031659707\n",
            "\n",
            "Global step: 808,loss: 0.028127544\n",
            "\n",
            "Global step: 809,loss: 0.027286604\n",
            "\n",
            "Global step: 810,loss: 0.021269266\n",
            "\n",
            "Global step: 811,loss: 0.0200197\n",
            "\n",
            "Global step: 812,loss: 0.020734388\n",
            "\n",
            "Global step: 813,loss: 0.01587467\n",
            "\n",
            "Global step: 814,loss: 0.022393662\n",
            "\n",
            "Global step: 815,loss: 0.018599572\n",
            "\n",
            "Global step: 816,loss: 0.02816606\n",
            "\n",
            "Global step: 817,loss: 0.030467406\n",
            "\n",
            "Global step: 818,loss: 0.021396339\n",
            "\n",
            "Global step: 819,loss: 0.024198567\n",
            "\n",
            "Global step: 820,loss: 0.030455299\n",
            "\n",
            "Global step: 821,loss: 0.0165906\n",
            "\n",
            "Global step: 822,loss: 0.028134223\n",
            "\n",
            "Global step: 823,loss: 0.025722336\n",
            "\n",
            "Global step: 824,loss: 0.021215241\n",
            "\n",
            "Global step: 825,loss: 0.029206257\n",
            "\n",
            "Global step: 826,loss: 0.02600472\n",
            "\n",
            "Global step: 827,loss: 0.021732032\n",
            "\n",
            "Global step: 828,loss: 0.030560369\n",
            "\n",
            "Global step: 829,loss: 0.03483914\n",
            "\n",
            "Global step: 830,loss: 0.022313494\n",
            "\n",
            "Global step: 831,loss: 0.018511087\n",
            "\n",
            "Global step: 832,loss: 0.029029332\n",
            "\n",
            "Global step: 833,loss: 0.018692154\n",
            "\n",
            "Global step: 834,loss: 0.019996934\n",
            "\n",
            "Global step: 835,loss: 0.025213797\n",
            "\n",
            "Global step: 836,loss: 0.024045706\n",
            "\n",
            "Global step: 837,loss: 0.02331772\n",
            "\n",
            "Global step: 838,loss: 0.028799988\n",
            "\n",
            "Global step: 839,loss: 0.033068046\n",
            "\n",
            "Global step: 840,loss: 0.0241925\n",
            "\n",
            "Global step: 841,loss: 0.019608347\n",
            "\n",
            "Global step: 842,loss: 0.024050593\n",
            "\n",
            "Global step: 843,loss: 0.02381514\n",
            "\n",
            "Global step: 844,loss: 0.022806797\n",
            "\n",
            "Global step: 845,loss: 0.027555164\n",
            "\n",
            "Global step: 846,loss: 0.022254685\n",
            "\n",
            "Global step: 847,loss: 0.030617088\n",
            "\n",
            "Global step: 848,loss: 0.03174645\n",
            "\n",
            "Global step: 849,loss: 0.02490082\n",
            "\n",
            "Global step: 850,loss: 0.020514952\n",
            "\n",
            "Global step: 851,loss: 0.02347555\n",
            "\n",
            "Global step: 852,loss: 0.03061775\n",
            "\n",
            "Global step: 853,loss: 0.028495427\n",
            "\n",
            "Global step: 854,loss: 0.021666428\n",
            "\n",
            "Global step: 855,loss: 0.03769753\n",
            "\n",
            "Global step: 856,loss: 0.02866228\n",
            "\n",
            "Global step: 857,loss: 0.017813059\n",
            "\n",
            "Global step: 858,loss: 0.021513894\n",
            "\n",
            "Global step: 859,loss: 0.01768427\n",
            "\n",
            "Global step: 860,loss: 0.034009915\n",
            "\n",
            "Global step: 861,loss: 0.027964495\n",
            "\n",
            "Global step: 862,loss: 0.016941125\n",
            "\n",
            "Global step: 863,loss: 0.02344893\n",
            "\n",
            "Global step: 864,loss: 0.044353195\n",
            "\n",
            "Global step: 865,loss: 0.018555552\n",
            "\n",
            "Global step: 866,loss: 0.021023992\n",
            "\n",
            "Global step: 867,loss: 0.024912952\n",
            "\n",
            "Global step: 868,loss: 0.028469848\n",
            "\n",
            "Global step: 869,loss: 0.024390798\n",
            "\n",
            "Global step: 870,loss: 0.029654786\n",
            "\n",
            "Global step: 871,loss: 0.027807716\n",
            "\n",
            "Global step: 872,loss: 0.02283499\n",
            "\n",
            "Global step: 873,loss: 0.032778483\n",
            "\n",
            "Global step: 874,loss: 0.02064244\n",
            "\n",
            "Global step: 875,loss: 0.019087587\n",
            "\n",
            "Global step: 876,loss: 0.024079565\n",
            "\n",
            "Global step: 877,loss: 0.019973211\n",
            "\n",
            "Global step: 878,loss: 0.027509807\n",
            "\n",
            "Global step: 879,loss: 0.028641604\n",
            "\n",
            "Global step: 880,loss: 0.020341575\n",
            "\n",
            "Global step: 881,loss: 0.017771006\n",
            "\n",
            "Global step: 882,loss: 0.021159597\n",
            "\n",
            "Global step: 883,loss: 0.022034137\n",
            "\n",
            "Global step: 884,loss: 0.03279718\n",
            "\n",
            "Global step: 885,loss: 0.02447278\n",
            "\n",
            "Global step: 886,loss: 0.040512186\n",
            "\n",
            "Global step: 887,loss: 0.022366822\n",
            "\n",
            "Global step: 888,loss: 0.025982276\n",
            "\n",
            "Global step: 889,loss: 0.0214694\n",
            "\n",
            "Global step: 890,loss: 0.029652495\n",
            "\n",
            "Global step: 891,loss: 0.018271863\n",
            "\n",
            "Global step: 892,loss: 0.023093756\n",
            "\n",
            "Global step: 893,loss: 0.020506246\n",
            "\n",
            "Global step: 894,loss: 0.021813942\n",
            "\n",
            "Global step: 895,loss: 0.027013151\n",
            "\n",
            "Global step: 896,loss: 0.02004273\n",
            "\n",
            "Global step: 897,loss: 0.02132609\n",
            "\n",
            "Global step: 898,loss: 0.015304051\n",
            "\n",
            "Global step: 899,loss: 0.020318078\n",
            "\n",
            "Global step: 900,loss: 0.027118165\n",
            "\n",
            "Global step: 901,loss: 0.023675472\n",
            "\n",
            "Global step: 902,loss: 0.016802099\n",
            "\n",
            "Global step: 903,loss: 0.019317087\n",
            "\n",
            "Global step: 904,loss: 0.020418912\n",
            "\n",
            "Global step: 905,loss: 0.017072305\n",
            "\n",
            "Global step: 906,loss: 0.019877544\n",
            "\n",
            "Global step: 907,loss: 0.02281773\n",
            "\n",
            "Global step: 908,loss: 0.020837177\n",
            "\n",
            "Global step: 909,loss: 0.017392252\n",
            "\n",
            "Global step: 910,loss: 0.01413814\n",
            "\n",
            "Global step: 911,loss: 0.021703742\n",
            "\n",
            "Global step: 912,loss: 0.03151172\n",
            "\n",
            "Global step: 913,loss: 0.024160048\n",
            "\n",
            "Global step: 914,loss: 0.019885702\n",
            "\n",
            "Global step: 915,loss: 0.024704713\n",
            "\n",
            "Global step: 916,loss: 0.019333187\n",
            "\n",
            "Global step: 917,loss: 0.028669912\n",
            "\n",
            "Global step: 918,loss: 0.0211681\n",
            "\n",
            "Global step: 919,loss: 0.026294589\n",
            "\n",
            "Global step: 920,loss: 0.02556634\n",
            "\n",
            "Global step: 921,loss: 0.020847961\n",
            "\n",
            "Global step: 922,loss: 0.02457779\n",
            "\n",
            "Global step: 923,loss: 0.024755102\n",
            "\n",
            "Global step: 924,loss: 0.02093498\n",
            "\n",
            "Global step: 925,loss: 0.019149728\n",
            "\n",
            "Global step: 926,loss: 0.030246446\n",
            "\n",
            "Global step: 927,loss: 0.023458676\n",
            "\n",
            "Global step: 928,loss: 0.029714664\n",
            "\n",
            "Global step: 929,loss: 0.017870158\n",
            "\n",
            "Global step: 930,loss: 0.030288782\n",
            "\n",
            "Global step: 931,loss: 0.028600575\n",
            "\n",
            "Global step: 932,loss: 0.018402096\n",
            "\n",
            "Global step: 933,loss: 0.015909921\n",
            "\n",
            "Global step: 934,loss: 0.023338832\n",
            "\n",
            "Global step: 935,loss: 0.01752038\n",
            "\n",
            "Global step: 936,loss: 0.024088867\n",
            "\n",
            "Global step: 937,loss: 0.023605121\n",
            "\n",
            "Global step: 938,loss: 0.016439429\n",
            "\n",
            "Global step: 939,loss: 0.018917654\n",
            "\n",
            "Global step: 940,loss: 0.022780392\n",
            "\n",
            "Global step: 941,loss: 0.021787029\n",
            "\n",
            "Global step: 942,loss: 0.029751994\n",
            "\n",
            "Global step: 943,loss: 0.03226097\n",
            "\n",
            "Global step: 944,loss: 0.020266047\n",
            "\n",
            "Global step: 945,loss: 0.015514668\n",
            "\n",
            "Global step: 946,loss: 0.016763426\n",
            "\n",
            "Global step: 947,loss: 0.020826887\n",
            "\n",
            "Global step: 948,loss: 0.022634909\n",
            "\n",
            "Global step: 949,loss: 0.021098167\n",
            "\n",
            "Global step: 950,loss: 0.025806779\n",
            "\n",
            "Global step: 951,loss: 0.015755812\n",
            "\n",
            "Global step: 952,loss: 0.018853346\n",
            "\n",
            "Global step: 953,loss: 0.019710168\n",
            "\n",
            "Global step: 954,loss: 0.020316795\n",
            "\n",
            "Global step: 955,loss: 0.016301472\n",
            "\n",
            "Global step: 956,loss: 0.017764509\n",
            "\n",
            "Global step: 957,loss: 0.019074513\n",
            "\n",
            "Global step: 958,loss: 0.021272212\n",
            "\n",
            "Global step: 959,loss: 0.020183474\n",
            "\n",
            "Global step: 960,loss: 0.021036837\n",
            "\n",
            "Global step: 961,loss: 0.020466931\n",
            "\n",
            "Global step: 962,loss: 0.021223335\n",
            "\n",
            "Global step: 963,loss: 0.020274924\n",
            "\n",
            "Global step: 964,loss: 0.020871475\n",
            "\n",
            "Global step: 965,loss: 0.016522896\n",
            "\n",
            "Global step: 966,loss: 0.02564513\n",
            "\n",
            "Global step: 967,loss: 0.025798565\n",
            "\n",
            "Global step: 968,loss: 0.020981634\n",
            "\n",
            "Global step: 969,loss: 0.019356478\n",
            "\n",
            "Global step: 970,loss: 0.018469531\n",
            "\n",
            "Global step: 971,loss: 0.016174175\n",
            "\n",
            "Global step: 972,loss: 0.021451117\n",
            "\n",
            "Global step: 973,loss: 0.023041634\n",
            "\n",
            "Global step: 974,loss: 0.016446985\n",
            "\n",
            "Global step: 975,loss: 0.028360173\n",
            "\n",
            "Global step: 976,loss: 0.024057485\n",
            "\n",
            "Global step: 977,loss: 0.019905295\n",
            "\n",
            "Global step: 978,loss: 0.02026761\n",
            "\n",
            "Global step: 979,loss: 0.022792336\n",
            "\n",
            "Global step: 980,loss: 0.018631548\n",
            "\n",
            "Global step: 981,loss: 0.017779807\n",
            "\n",
            "Global step: 982,loss: 0.027227852\n",
            "\n",
            "Global step: 983,loss: 0.019921124\n",
            "\n",
            "Global step: 984,loss: 0.019703194\n",
            "\n",
            "Global step: 985,loss: 0.018241482\n",
            "\n",
            "Global step: 986,loss: 0.017002078\n",
            "\n",
            "Global step: 987,loss: 0.028312381\n",
            "\n",
            "Global step: 988,loss: 0.019140476\n",
            "\n",
            "Global step: 989,loss: 0.021697126\n",
            "\n",
            "Global step: 990,loss: 0.01876868\n",
            "\n",
            "Global step: 991,loss: 0.02054834\n",
            "\n",
            "Global step: 992,loss: 0.016423978\n",
            "\n",
            "Global step: 993,loss: 0.029809128\n",
            "\n",
            "Global step: 994,loss: 0.018779177\n",
            "\n",
            "Global step: 995,loss: 0.017045904\n",
            "\n",
            "Global step: 996,loss: 0.022775628\n",
            "\n",
            "Global step: 997,loss: 0.01536061\n",
            "\n",
            "Global step: 998,loss: 0.02048149\n",
            "\n",
            "Global step: 999,loss: 0.016015999\n",
            "\n",
            "Global step: 1000,loss: 0.025410278\n",
            "\n",
            "Global step: 1001,loss: 0.016983848\n",
            "\n",
            "Global step: 1002,loss: 0.043989856\n",
            "\n",
            "Global step: 1003,loss: 0.026530325\n",
            "\n",
            "Global step: 1004,loss: 0.03011182\n",
            "\n",
            "Global step: 1005,loss: 0.022297896\n",
            "\n",
            "Global step: 1006,loss: 0.020086948\n",
            "\n",
            "Global step: 1007,loss: 0.024361461\n",
            "\n",
            "Global step: 1008,loss: 0.027990561\n",
            "\n",
            "Global step: 1009,loss: 0.02622278\n",
            "\n",
            "Global step: 1010,loss: 0.029668298\n",
            "\n",
            "Global step: 1011,loss: 0.020607393\n",
            "\n",
            "Global step: 1012,loss: 0.014271356\n",
            "\n",
            "Global step: 1013,loss: 0.015036644\n",
            "\n",
            "Global step: 1014,loss: 0.026630305\n",
            "\n",
            "Global step: 1015,loss: 0.040351138\n",
            "\n",
            "Global step: 1016,loss: 0.01740121\n",
            "\n",
            "Global step: 1017,loss: 0.023939427\n",
            "\n",
            "Global step: 1018,loss: 0.021236524\n",
            "\n",
            "Global step: 1019,loss: 0.020709805\n",
            "\n",
            "Global step: 1020,loss: 0.01884972\n",
            "\n",
            "Global step: 1021,loss: 0.017201347\n",
            "\n",
            "Global step: 1022,loss: 0.021711517\n",
            "\n",
            "Global step: 1023,loss: 0.019183518\n",
            "\n",
            "Global step: 1024,loss: 0.016717702\n",
            "\n",
            "Global step: 1025,loss: 0.019357633\n",
            "\n",
            "Global step: 1026,loss: 0.023046944\n",
            "\n",
            "Global step: 1027,loss: 0.024077542\n",
            "\n",
            "Global step: 1028,loss: 0.01685638\n",
            "\n",
            "Global step: 1029,loss: 0.030443434\n",
            "\n",
            "Global step: 1030,loss: 0.016461086\n",
            "\n",
            "Global step: 1031,loss: 0.019431997\n",
            "\n",
            "Global step: 1032,loss: 0.025028422\n",
            "\n",
            "Global step: 1033,loss: 0.02167046\n",
            "\n",
            "Global step: 1034,loss: 0.018422417\n",
            "\n",
            "Global step: 1035,loss: 0.02698628\n",
            "\n",
            "Global step: 1036,loss: 0.018687412\n",
            "\n",
            "Global step: 1037,loss: 0.03345437\n",
            "\n",
            "Global step: 1038,loss: 0.018165048\n",
            "\n",
            "Global step: 1039,loss: 0.030135479\n",
            "\n",
            "Global step: 1040,loss: 0.01535091\n",
            "\n",
            "Global step: 1041,loss: 0.024664834\n",
            "\n",
            "Global step: 1042,loss: 0.018818576\n",
            "\n",
            "Global step: 1043,loss: 0.021033498\n",
            "\n",
            "Global step: 1044,loss: 0.015174484\n",
            "\n",
            "Global step: 1045,loss: 0.022019591\n",
            "\n",
            "Global step: 1046,loss: 0.014671878\n",
            "\n",
            "Global step: 1047,loss: 0.017472835\n",
            "\n",
            "Global step: 1048,loss: 0.019129835\n",
            "\n",
            "Global step: 1049,loss: 0.018236741\n",
            "\n",
            "Global step: 1050,loss: 0.02366943\n",
            "\n",
            "Global step: 1051,loss: 0.022540811\n",
            "\n",
            "Global step: 1052,loss: 0.012611276\n",
            "\n",
            "Global step: 1053,loss: 0.020932235\n",
            "\n",
            "Global step: 1054,loss: 0.01753712\n",
            "\n",
            "Global step: 1055,loss: 0.019221354\n",
            "\n",
            "Global step: 1056,loss: 0.026721828\n",
            "\n",
            "Global step: 1057,loss: 0.014775669\n",
            "\n",
            "Global step: 1058,loss: 0.02555802\n",
            "\n",
            "Global step: 1059,loss: 0.027832903\n",
            "\n",
            "Global step: 1060,loss: 0.020078104\n",
            "\n",
            "Global step: 1061,loss: 0.018861793\n",
            "\n",
            "Global step: 1062,loss: 0.015521511\n",
            "\n",
            "Global step: 1063,loss: 0.020292606\n",
            "\n",
            "Global step: 1064,loss: 0.018224642\n",
            "\n",
            "Global step: 1065,loss: 0.01891027\n",
            "\n",
            "Global step: 1066,loss: 0.020243943\n",
            "\n",
            "Global step: 1067,loss: 0.015880747\n",
            "\n",
            "Global step: 1068,loss: 0.026874494\n",
            "\n",
            "Global step: 1069,loss: 0.033063896\n",
            "\n",
            "Global step: 1070,loss: 0.018529035\n",
            "\n",
            "Global step: 1071,loss: 0.01913751\n",
            "\n",
            "Global step: 1072,loss: 0.01820057\n",
            "\n",
            "Global step: 1073,loss: 0.017998952\n",
            "\n",
            "Global step: 1074,loss: 0.0153567605\n",
            "\n",
            "Global step: 1075,loss: 0.029429007\n",
            "\n",
            "Global step: 1076,loss: 0.02276785\n",
            "\n",
            "Global step: 1077,loss: 0.021454401\n",
            "\n",
            "Global step: 1078,loss: 0.027112778\n",
            "\n",
            "Global step: 1079,loss: 0.026717197\n",
            "\n",
            "Global step: 1080,loss: 0.02456861\n",
            "\n",
            "Global step: 1081,loss: 0.014697144\n",
            "\n",
            "Global step: 1082,loss: 0.024388997\n",
            "\n",
            "Global step: 1083,loss: 0.018976472\n",
            "\n",
            "Global step: 1084,loss: 0.015218738\n",
            "\n",
            "Global step: 1085,loss: 0.029811028\n",
            "\n",
            "Global step: 1086,loss: 0.016888592\n",
            "\n",
            "Global step: 1087,loss: 0.01565934\n",
            "\n",
            "Global step: 1088,loss: 0.016985081\n",
            "\n",
            "Global step: 1089,loss: 0.022000797\n",
            "\n",
            "Global step: 1090,loss: 0.023848724\n",
            "\n",
            "Global step: 1091,loss: 0.024992533\n",
            "\n",
            "Global step: 1092,loss: 0.017296635\n",
            "\n",
            "Global step: 1093,loss: 0.023018776\n",
            "\n",
            "Global step: 1094,loss: 0.029142939\n",
            "\n",
            "Global step: 1095,loss: 0.021939013\n",
            "\n",
            "Global step: 1096,loss: 0.020820819\n",
            "\n",
            "Global step: 1097,loss: 0.017919414\n",
            "\n",
            "Global step: 1098,loss: 0.01896972\n",
            "\n",
            "Global step: 1099,loss: 0.029200204\n",
            "\n",
            "Global step: 1100,loss: 0.021422409\n",
            "\n",
            "Global step: 1101,loss: 0.021391217\n",
            "\n",
            "Global step: 1102,loss: 0.020958275\n",
            "\n",
            "Global step: 1103,loss: 0.023116777\n",
            "\n",
            "Global step: 1104,loss: 0.020277942\n",
            "\n",
            "Global step: 1105,loss: 0.022641376\n",
            "\n",
            "Global step: 1106,loss: 0.020425122\n",
            "\n",
            "Global step: 1107,loss: 0.020306949\n",
            "\n",
            "Global step: 1108,loss: 0.017348664\n",
            "\n",
            "Global step: 1109,loss: 0.02227389\n",
            "\n",
            "Global step: 1110,loss: 0.017917536\n",
            "\n",
            "Global step: 1111,loss: 0.025471793\n",
            "\n",
            "Global step: 1112,loss: 0.019529648\n",
            "\n",
            "Global step: 1113,loss: 0.025516778\n",
            "\n",
            "Global step: 1114,loss: 0.023107747\n",
            "\n",
            "Global step: 1115,loss: 0.031555675\n",
            "\n",
            "Global step: 1116,loss: 0.01573004\n",
            "\n",
            "Global step: 1117,loss: 0.01971151\n",
            "\n",
            "Global step: 1118,loss: 0.021847593\n",
            "\n",
            "Global step: 1119,loss: 0.017799592\n",
            "\n",
            "Global step: 1120,loss: 0.017016143\n",
            "\n",
            "Global step: 1121,loss: 0.020174023\n",
            "\n",
            "Global step: 1122,loss: 0.019685134\n",
            "\n",
            "Global step: 1123,loss: 0.015658462\n",
            "\n",
            "Global step: 1124,loss: 0.023612078\n",
            "\n",
            "Global step: 1125,loss: 0.029093964\n",
            "\n",
            "Global step: 1126,loss: 0.01636141\n",
            "\n",
            "Global step: 1127,loss: 0.022406196\n",
            "\n",
            "Global step: 1128,loss: 0.020992236\n",
            "\n",
            "Global step: 1129,loss: 0.022791825\n",
            "\n",
            "Global step: 1130,loss: 0.01666838\n",
            "\n",
            "Global step: 1131,loss: 0.016300973\n",
            "\n",
            "Global step: 1132,loss: 0.017926473\n",
            "\n",
            "Global step: 1133,loss: 0.026410695\n",
            "\n",
            "Global step: 1134,loss: 0.022850243\n",
            "\n",
            "Global step: 1135,loss: 0.017511206\n",
            "\n",
            "Global step: 1136,loss: 0.030315535\n",
            "\n",
            "Global step: 1137,loss: 0.031287447\n",
            "\n",
            "Global step: 1138,loss: 0.032588392\n",
            "\n",
            "Global step: 1139,loss: 0.017103668\n",
            "\n",
            "Global step: 1140,loss: 0.022605967\n",
            "\n",
            "Global step: 1141,loss: 0.02442736\n",
            "\n",
            "Global step: 1142,loss: 0.020828838\n",
            "\n",
            "Global step: 1143,loss: 0.020612359\n",
            "\n",
            "Global step: 1144,loss: 0.02716567\n",
            "\n",
            "Global step: 1145,loss: 0.02386633\n",
            "\n",
            "Global step: 1146,loss: 0.017203648\n",
            "\n",
            "Global step: 1147,loss: 0.017302273\n",
            "\n",
            "Global step: 1148,loss: 0.018596243\n",
            "\n",
            "Global step: 1149,loss: 0.017685212\n",
            "\n",
            "Global step: 1150,loss: 0.02329453\n",
            "\n",
            "Global step: 1151,loss: 0.017523743\n",
            "\n",
            "Global step: 1152,loss: 0.014040287\n",
            "\n",
            "Global step: 1153,loss: 0.020968582\n",
            "\n",
            "Global step: 1154,loss: 0.017213907\n",
            "\n",
            "Global step: 1155,loss: 0.022469416\n",
            "\n",
            "Global step: 1156,loss: 0.019811777\n",
            "\n",
            "Global step: 1157,loss: 0.014881091\n",
            "\n",
            "Global step: 1158,loss: 0.017302005\n",
            "\n",
            "Global step: 1159,loss: 0.01753387\n",
            "\n",
            "Global step: 1160,loss: 0.01837325\n",
            "\n",
            "Global step: 1161,loss: 0.020506524\n",
            "\n",
            "Global step: 1162,loss: 0.019756537\n",
            "\n",
            "Global step: 1163,loss: 0.02024189\n",
            "\n",
            "Global step: 1164,loss: 0.01770822\n",
            "\n",
            "Global step: 1165,loss: 0.017804561\n",
            "\n",
            "Global step: 1166,loss: 0.016037297\n",
            "\n",
            "Global step: 1167,loss: 0.018710848\n",
            "\n",
            "Global step: 1168,loss: 0.01996275\n",
            "\n",
            "Global step: 1169,loss: 0.016370185\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 1169,val_loss: 0.025201645703651965\n",
            "\n",
            "Training for epoch 4/16:\n",
            "Global step: 1170,loss: 0.016969938\n",
            "\n",
            "Global step: 1171,loss: 0.017351422\n",
            "\n",
            "Global step: 1172,loss: 0.015690587\n",
            "\n",
            "Global step: 1173,loss: 0.015339051\n",
            "\n",
            "Global step: 1174,loss: 0.013656521\n",
            "\n",
            "Global step: 1175,loss: 0.018102515\n",
            "\n",
            "Global step: 1176,loss: 0.02183389\n",
            "\n",
            "Global step: 1177,loss: 0.021594588\n",
            "\n",
            "Global step: 1178,loss: 0.019639106\n",
            "\n",
            "Global step: 1179,loss: 0.021480788\n",
            "\n",
            "Global step: 1180,loss: 0.018044183\n",
            "\n",
            "Global step: 1181,loss: 0.014577657\n",
            "\n",
            "Global step: 1182,loss: 0.012957126\n",
            "\n",
            "Global step: 1183,loss: 0.014867976\n",
            "\n",
            "Global step: 1184,loss: 0.01580769\n",
            "\n",
            "Global step: 1185,loss: 0.032181054\n",
            "\n",
            "Global step: 1186,loss: 0.017209148\n",
            "\n",
            "Global step: 1187,loss: 0.021607103\n",
            "\n",
            "Global step: 1188,loss: 0.016190575\n",
            "\n",
            "Global step: 1189,loss: 0.022973433\n",
            "\n",
            "Global step: 1190,loss: 0.01461979\n",
            "\n",
            "Global step: 1191,loss: 0.01273708\n",
            "\n",
            "Global step: 1192,loss: 0.016332796\n",
            "\n",
            "Global step: 1193,loss: 0.017829997\n",
            "\n",
            "Global step: 1194,loss: 0.017876368\n",
            "\n",
            "Global step: 1195,loss: 0.016799524\n",
            "\n",
            "Global step: 1196,loss: 0.0119435955\n",
            "\n",
            "Global step: 1197,loss: 0.018424433\n",
            "\n",
            "Global step: 1198,loss: 0.015227133\n",
            "\n",
            "Global step: 1199,loss: 0.019620053\n",
            "\n",
            "Global step: 1200,loss: 0.024150549\n",
            "\n",
            "Global step: 1201,loss: 0.014102483\n",
            "\n",
            "Global step: 1202,loss: 0.01803909\n",
            "\n",
            "Global step: 1203,loss: 0.017994698\n",
            "\n",
            "Global step: 1204,loss: 0.014751915\n",
            "\n",
            "Global step: 1205,loss: 0.019084744\n",
            "\n",
            "Global step: 1206,loss: 0.016414484\n",
            "\n",
            "Global step: 1207,loss: 0.022328515\n",
            "\n",
            "Global step: 1208,loss: 0.013573812\n",
            "\n",
            "Global step: 1209,loss: 0.018920612\n",
            "\n",
            "Global step: 1210,loss: 0.017946279\n",
            "\n",
            "Global step: 1211,loss: 0.032854084\n",
            "\n",
            "Global step: 1212,loss: 0.015630098\n",
            "\n",
            "Global step: 1213,loss: 0.023813117\n",
            "\n",
            "Global step: 1214,loss: 0.015072638\n",
            "\n",
            "Global step: 1215,loss: 0.014497154\n",
            "\n",
            "Global step: 1216,loss: 0.016110115\n",
            "\n",
            "Global step: 1217,loss: 0.014841379\n",
            "\n",
            "Global step: 1218,loss: 0.015784968\n",
            "\n",
            "Global step: 1219,loss: 0.0320141\n",
            "\n",
            "Global step: 1220,loss: 0.015703116\n",
            "\n",
            "Global step: 1221,loss: 0.014091138\n",
            "\n",
            "Global step: 1222,loss: 0.01734412\n",
            "\n",
            "Global step: 1223,loss: 0.024339225\n",
            "\n",
            "Global step: 1224,loss: 0.019353524\n",
            "\n",
            "Global step: 1225,loss: 0.026008196\n",
            "\n",
            "Global step: 1226,loss: 0.021176089\n",
            "\n",
            "Global step: 1227,loss: 0.025026198\n",
            "\n",
            "Global step: 1228,loss: 0.013108783\n",
            "\n",
            "Global step: 1229,loss: 0.018168952\n",
            "\n",
            "Global step: 1230,loss: 0.014561782\n",
            "\n",
            "Global step: 1231,loss: 0.0194202\n",
            "\n",
            "Global step: 1232,loss: 0.016521817\n",
            "\n",
            "Global step: 1233,loss: 0.015741482\n",
            "\n",
            "Global step: 1234,loss: 0.022908596\n",
            "\n",
            "Global step: 1235,loss: 0.014746195\n",
            "\n",
            "Global step: 1236,loss: 0.020704387\n",
            "\n",
            "Global step: 1237,loss: 0.013017987\n",
            "\n",
            "Global step: 1238,loss: 0.01860844\n",
            "\n",
            "Global step: 1239,loss: 0.024557773\n",
            "\n",
            "Global step: 1240,loss: 0.016797923\n",
            "\n",
            "Global step: 1241,loss: 0.01688454\n",
            "\n",
            "Global step: 1242,loss: 0.019722765\n",
            "\n",
            "Global step: 1243,loss: 0.01283286\n",
            "\n",
            "Global step: 1244,loss: 0.02047264\n",
            "\n",
            "Global step: 1245,loss: 0.012414476\n",
            "\n",
            "Global step: 1246,loss: 0.019441599\n",
            "\n",
            "Global step: 1247,loss: 0.0171902\n",
            "\n",
            "Global step: 1248,loss: 0.01823702\n",
            "\n",
            "Global step: 1249,loss: 0.016753517\n",
            "\n",
            "Global step: 1250,loss: 0.01863302\n",
            "\n",
            "Global step: 1251,loss: 0.017786726\n",
            "\n",
            "Global step: 1252,loss: 0.016475633\n",
            "\n",
            "Global step: 1253,loss: 0.025744908\n",
            "\n",
            "Global step: 1254,loss: 0.015590195\n",
            "\n",
            "Global step: 1255,loss: 0.02338705\n",
            "\n",
            "Global step: 1256,loss: 0.020446736\n",
            "\n",
            "Global step: 1257,loss: 0.016076365\n",
            "\n",
            "Global step: 1258,loss: 0.01722088\n",
            "\n",
            "Global step: 1259,loss: 0.021474626\n",
            "\n",
            "Global step: 1260,loss: 0.024015907\n",
            "\n",
            "Global step: 1261,loss: 0.018228116\n",
            "\n",
            "Global step: 1262,loss: 0.016889708\n",
            "\n",
            "Global step: 1263,loss: 0.024946075\n",
            "\n",
            "Global step: 1264,loss: 0.013905512\n",
            "\n",
            "Global step: 1265,loss: 0.015647607\n",
            "\n",
            "Global step: 1266,loss: 0.013388362\n",
            "\n",
            "Global step: 1267,loss: 0.014550224\n",
            "\n",
            "Global step: 1268,loss: 0.015047482\n",
            "\n",
            "Global step: 1269,loss: 0.018581247\n",
            "\n",
            "Global step: 1270,loss: 0.014602152\n",
            "\n",
            "Global step: 1271,loss: 0.030561203\n",
            "\n",
            "Global step: 1272,loss: 0.012740964\n",
            "\n",
            "Global step: 1273,loss: 0.022654843\n",
            "\n",
            "Global step: 1274,loss: 0.015109913\n",
            "\n",
            "Global step: 1275,loss: 0.019357761\n",
            "\n",
            "Global step: 1276,loss: 0.020911347\n",
            "\n",
            "Global step: 1277,loss: 0.013248345\n",
            "\n",
            "Global step: 1278,loss: 0.01586759\n",
            "\n",
            "Global step: 1279,loss: 0.015175369\n",
            "\n",
            "Global step: 1280,loss: 0.02081531\n",
            "\n",
            "Global step: 1281,loss: 0.015344611\n",
            "\n",
            "Global step: 1282,loss: 0.025624473\n",
            "\n",
            "Global step: 1283,loss: 0.020362977\n",
            "\n",
            "Global step: 1284,loss: 0.021882609\n",
            "\n",
            "Global step: 1285,loss: 0.0143036265\n",
            "\n",
            "Global step: 1286,loss: 0.015609374\n",
            "\n",
            "Global step: 1287,loss: 0.012268248\n",
            "\n",
            "Global step: 1288,loss: 0.01334872\n",
            "\n",
            "Global step: 1289,loss: 0.014444327\n",
            "\n",
            "Global step: 1290,loss: 0.022304045\n",
            "\n",
            "Global step: 1291,loss: 0.0129514625\n",
            "\n",
            "Global step: 1292,loss: 0.012503646\n",
            "\n",
            "Global step: 1293,loss: 0.015740242\n",
            "\n",
            "Global step: 1294,loss: 0.015593925\n",
            "\n",
            "Global step: 1295,loss: 0.014476586\n",
            "\n",
            "Global step: 1296,loss: 0.014226975\n",
            "\n",
            "Global step: 1297,loss: 0.023874592\n",
            "\n",
            "Global step: 1298,loss: 0.013469434\n",
            "\n",
            "Global step: 1299,loss: 0.016644688\n",
            "\n",
            "Global step: 1300,loss: 0.020043302\n",
            "\n",
            "Global step: 1301,loss: 0.022813227\n",
            "\n",
            "Global step: 1302,loss: 0.011964427\n",
            "\n",
            "Global step: 1303,loss: 0.01700579\n",
            "\n",
            "Global step: 1304,loss: 0.014227111\n",
            "\n",
            "Global step: 1305,loss: 0.018716522\n",
            "\n",
            "Global step: 1306,loss: 0.019015877\n",
            "\n",
            "Global step: 1307,loss: 0.013865749\n",
            "\n",
            "Global step: 1308,loss: 0.020154163\n",
            "\n",
            "Global step: 1309,loss: 0.013967559\n",
            "\n",
            "Global step: 1310,loss: 0.018185977\n",
            "\n",
            "Global step: 1311,loss: 0.021390595\n",
            "\n",
            "Global step: 1312,loss: 0.033144575\n",
            "\n",
            "Global step: 1313,loss: 0.026522858\n",
            "\n",
            "Global step: 1314,loss: 0.02129326\n",
            "\n",
            "Global step: 1315,loss: 0.025646351\n",
            "\n",
            "Global step: 1316,loss: 0.022059746\n",
            "\n",
            "Global step: 1317,loss: 0.017039992\n",
            "\n",
            "Global step: 1318,loss: 0.014663996\n",
            "\n",
            "Global step: 1319,loss: 0.013581714\n",
            "\n",
            "Global step: 1320,loss: 0.019477256\n",
            "\n",
            "Global step: 1321,loss: 0.016106237\n",
            "\n",
            "Global step: 1322,loss: 0.017378245\n",
            "\n",
            "Global step: 1323,loss: 0.02056439\n",
            "\n",
            "Global step: 1324,loss: 0.013738268\n",
            "\n",
            "Global step: 1325,loss: 0.024127483\n",
            "\n",
            "Global step: 1326,loss: 0.018399052\n",
            "\n",
            "Global step: 1327,loss: 0.012668025\n",
            "\n",
            "Global step: 1328,loss: 0.015587723\n",
            "\n",
            "Global step: 1329,loss: 0.019220516\n",
            "\n",
            "Global step: 1330,loss: 0.01701904\n",
            "\n",
            "Global step: 1331,loss: 0.012675328\n",
            "\n",
            "Global step: 1332,loss: 0.025154673\n",
            "\n",
            "Global step: 1333,loss: 0.018724062\n",
            "\n",
            "Global step: 1334,loss: 0.023182645\n",
            "\n",
            "Global step: 1335,loss: 0.02029589\n",
            "\n",
            "Global step: 1336,loss: 0.017710008\n",
            "\n",
            "Global step: 1337,loss: 0.021966754\n",
            "\n",
            "Global step: 1338,loss: 0.018745355\n",
            "\n",
            "Global step: 1339,loss: 0.017277319\n",
            "\n",
            "Global step: 1340,loss: 0.017178608\n",
            "\n",
            "Global step: 1341,loss: 0.01594971\n",
            "\n",
            "Global step: 1342,loss: 0.017327283\n",
            "\n",
            "Global step: 1343,loss: 0.020506954\n",
            "\n",
            "Global step: 1344,loss: 0.024157021\n",
            "\n",
            "Global step: 1345,loss: 0.023070982\n",
            "\n",
            "Global step: 1346,loss: 0.014896096\n",
            "\n",
            "Global step: 1347,loss: 0.023704616\n",
            "\n",
            "Global step: 1348,loss: 0.02051666\n",
            "\n",
            "Global step: 1349,loss: 0.01974411\n",
            "\n",
            "Global step: 1350,loss: 0.017349673\n",
            "\n",
            "Global step: 1351,loss: 0.014146571\n",
            "\n",
            "Global step: 1352,loss: 0.020333692\n",
            "\n",
            "Global step: 1353,loss: 0.016716745\n",
            "\n",
            "Global step: 1354,loss: 0.015328926\n",
            "\n",
            "Global step: 1355,loss: 0.0120329615\n",
            "\n",
            "Global step: 1356,loss: 0.02069699\n",
            "\n",
            "Global step: 1357,loss: 0.019587843\n",
            "\n",
            "Global step: 1358,loss: 0.020171281\n",
            "\n",
            "Global step: 1359,loss: 0.013554597\n",
            "\n",
            "Global step: 1360,loss: 0.024340771\n",
            "\n",
            "Global step: 1361,loss: 0.017923782\n",
            "\n",
            "Global step: 1362,loss: 0.015410215\n",
            "\n",
            "Global step: 1363,loss: 0.019425543\n",
            "\n",
            "Global step: 1364,loss: 0.015371949\n",
            "\n",
            "Global step: 1365,loss: 0.016709294\n",
            "\n",
            "Global step: 1366,loss: 0.01819095\n",
            "\n",
            "Global step: 1367,loss: 0.017301118\n",
            "\n",
            "Global step: 1368,loss: 0.013980153\n",
            "\n",
            "Global step: 1369,loss: 0.016603187\n",
            "\n",
            "Global step: 1370,loss: 0.015877407\n",
            "\n",
            "Global step: 1371,loss: 0.0136589\n",
            "\n",
            "Global step: 1372,loss: 0.01616577\n",
            "\n",
            "Global step: 1373,loss: 0.016747246\n",
            "\n",
            "Global step: 1374,loss: 0.01847952\n",
            "\n",
            "Global step: 1375,loss: 0.01668632\n",
            "\n",
            "Global step: 1376,loss: 0.015279878\n",
            "\n",
            "Global step: 1377,loss: 0.01403857\n",
            "\n",
            "Global step: 1378,loss: 0.017848253\n",
            "\n",
            "Global step: 1379,loss: 0.01743278\n",
            "\n",
            "Global step: 1380,loss: 0.013212858\n",
            "\n",
            "Global step: 1381,loss: 0.019629687\n",
            "\n",
            "Global step: 1382,loss: 0.01564594\n",
            "\n",
            "Global step: 1383,loss: 0.016121687\n",
            "\n",
            "Global step: 1384,loss: 0.015507549\n",
            "\n",
            "Global step: 1385,loss: 0.013351709\n",
            "\n",
            "Global step: 1386,loss: 0.018082887\n",
            "\n",
            "Global step: 1387,loss: 0.014345836\n",
            "\n",
            "Global step: 1388,loss: 0.016835868\n",
            "\n",
            "Global step: 1389,loss: 0.01471859\n",
            "\n",
            "Global step: 1390,loss: 0.021695733\n",
            "\n",
            "Global step: 1391,loss: 0.021996658\n",
            "\n",
            "Global step: 1392,loss: 0.01765751\n",
            "\n",
            "Global step: 1393,loss: 0.020895392\n",
            "\n",
            "Global step: 1394,loss: 0.015176219\n",
            "\n",
            "Global step: 1395,loss: 0.023121158\n",
            "\n",
            "Global step: 1396,loss: 0.016230844\n",
            "\n",
            "Global step: 1397,loss: 0.015919931\n",
            "\n",
            "Global step: 1398,loss: 0.014773514\n",
            "\n",
            "Global step: 1399,loss: 0.01849597\n",
            "\n",
            "Global step: 1400,loss: 0.017908271\n",
            "\n",
            "Global step: 1401,loss: 0.016593488\n",
            "\n",
            "Global step: 1402,loss: 0.01776633\n",
            "\n",
            "Global step: 1403,loss: 0.014062611\n",
            "\n",
            "Global step: 1404,loss: 0.01745185\n",
            "\n",
            "Global step: 1405,loss: 0.03309103\n",
            "\n",
            "Global step: 1406,loss: 0.017965171\n",
            "\n",
            "Global step: 1407,loss: 0.015412554\n",
            "\n",
            "Global step: 1408,loss: 0.01946729\n",
            "\n",
            "Global step: 1409,loss: 0.016819473\n",
            "\n",
            "Global step: 1410,loss: 0.020664047\n",
            "\n",
            "Global step: 1411,loss: 0.021332247\n",
            "\n",
            "Global step: 1412,loss: 0.02243054\n",
            "\n",
            "Global step: 1413,loss: 0.016618643\n",
            "\n",
            "Global step: 1414,loss: 0.013099155\n",
            "\n",
            "Global step: 1415,loss: 0.014523419\n",
            "\n",
            "Global step: 1416,loss: 0.021530762\n",
            "\n",
            "Global step: 1417,loss: 0.019275293\n",
            "\n",
            "Global step: 1418,loss: 0.015213326\n",
            "\n",
            "Global step: 1419,loss: 0.024116369\n",
            "\n",
            "Global step: 1420,loss: 0.022491112\n",
            "\n",
            "Global step: 1421,loss: 0.014587196\n",
            "\n",
            "Global step: 1422,loss: 0.021055087\n",
            "\n",
            "Global step: 1423,loss: 0.018070754\n",
            "\n",
            "Global step: 1424,loss: 0.014789677\n",
            "\n",
            "Global step: 1425,loss: 0.011872158\n",
            "\n",
            "Global step: 1426,loss: 0.013478014\n",
            "\n",
            "Global step: 1427,loss: 0.017202433\n",
            "\n",
            "Global step: 1428,loss: 0.020810027\n",
            "\n",
            "Global step: 1429,loss: 0.015174277\n",
            "\n",
            "Global step: 1430,loss: 0.013240103\n",
            "\n",
            "Global step: 1431,loss: 0.013476454\n",
            "\n",
            "Global step: 1432,loss: 0.018297441\n",
            "\n",
            "Global step: 1433,loss: 0.025671104\n",
            "\n",
            "Global step: 1434,loss: 0.020138424\n",
            "\n",
            "Global step: 1435,loss: 0.014142607\n",
            "\n",
            "Global step: 1436,loss: 0.02168065\n",
            "\n",
            "Global step: 1437,loss: 0.017191608\n",
            "\n",
            "Global step: 1438,loss: 0.014733206\n",
            "\n",
            "Global step: 1439,loss: 0.01736715\n",
            "\n",
            "Global step: 1440,loss: 0.0155647695\n",
            "\n",
            "Global step: 1441,loss: 0.023838103\n",
            "\n",
            "Global step: 1442,loss: 0.014620887\n",
            "\n",
            "Global step: 1443,loss: 0.021867134\n",
            "\n",
            "Global step: 1444,loss: 0.016861826\n",
            "\n",
            "Global step: 1445,loss: 0.014073611\n",
            "\n",
            "Global step: 1446,loss: 0.015795428\n",
            "\n",
            "Global step: 1447,loss: 0.023213938\n",
            "\n",
            "Global step: 1448,loss: 0.019651555\n",
            "\n",
            "Global step: 1449,loss: 0.029454038\n",
            "\n",
            "Global step: 1450,loss: 0.025118448\n",
            "\n",
            "Global step: 1451,loss: 0.019358225\n",
            "\n",
            "Global step: 1452,loss: 0.013841155\n",
            "\n",
            "Global step: 1453,loss: 0.019572556\n",
            "\n",
            "Global step: 1454,loss: 0.017245352\n",
            "\n",
            "Global step: 1455,loss: 0.01575767\n",
            "\n",
            "Global step: 1456,loss: 0.026253672\n",
            "\n",
            "Global step: 1457,loss: 0.019642506\n",
            "\n",
            "Global step: 1458,loss: 0.014856845\n",
            "\n",
            "Global step: 1459,loss: 0.016579252\n",
            "\n",
            "Global step: 1460,loss: 0.022104573\n",
            "\n",
            "Global step: 1461,loss: 0.021949936\n",
            "\n",
            "Global step: 1462,loss: 0.020125989\n",
            "\n",
            "Global step: 1463,loss: 0.0122232875\n",
            "\n",
            "Global step: 1464,loss: 0.016542636\n",
            "\n",
            "Global step: 1465,loss: 0.021774285\n",
            "\n",
            "Global step: 1466,loss: 0.020240009\n",
            "\n",
            "Global step: 1467,loss: 0.02215036\n",
            "\n",
            "Global step: 1468,loss: 0.017774105\n",
            "\n",
            "Global step: 1469,loss: 0.014324231\n",
            "\n",
            "Global step: 1470,loss: 0.013532074\n",
            "\n",
            "Global step: 1471,loss: 0.01282124\n",
            "\n",
            "Global step: 1472,loss: 0.014559196\n",
            "\n",
            "Global step: 1473,loss: 0.02879741\n",
            "\n",
            "Global step: 1474,loss: 0.016480362\n",
            "\n",
            "Global step: 1475,loss: 0.022983558\n",
            "\n",
            "Global step: 1476,loss: 0.019234573\n",
            "\n",
            "Global step: 1477,loss: 0.016320996\n",
            "\n",
            "Global step: 1478,loss: 0.014042776\n",
            "\n",
            "Global step: 1479,loss: 0.0136505505\n",
            "\n",
            "Global step: 1480,loss: 0.01588102\n",
            "\n",
            "Global step: 1481,loss: 0.03078311\n",
            "\n",
            "Global step: 1482,loss: 0.020924095\n",
            "\n",
            "Global step: 1483,loss: 0.024684329\n",
            "\n",
            "Global step: 1484,loss: 0.015880093\n",
            "\n",
            "Global step: 1485,loss: 0.013724252\n",
            "\n",
            "Global step: 1486,loss: 0.018902961\n",
            "\n",
            "Global step: 1487,loss: 0.019534066\n",
            "\n",
            "Global step: 1488,loss: 0.020826306\n",
            "\n",
            "Global step: 1489,loss: 0.017022874\n",
            "\n",
            "Global step: 1490,loss: 0.017682526\n",
            "\n",
            "Global step: 1491,loss: 0.021441273\n",
            "\n",
            "Global step: 1492,loss: 0.02126608\n",
            "\n",
            "Global step: 1493,loss: 0.011792856\n",
            "\n",
            "Global step: 1494,loss: 0.013113275\n",
            "\n",
            "Global step: 1495,loss: 0.0150785055\n",
            "\n",
            "Global step: 1496,loss: 0.021752082\n",
            "\n",
            "Global step: 1497,loss: 0.01502708\n",
            "\n",
            "Global step: 1498,loss: 0.022065535\n",
            "\n",
            "Global step: 1499,loss: 0.016636467\n",
            "\n",
            "Global step: 1500,loss: 0.016924957\n",
            "\n",
            "Global step: 1501,loss: 0.023963232\n",
            "\n",
            "Global step: 1502,loss: 0.0142367\n",
            "\n",
            "Global step: 1503,loss: 0.018112\n",
            "\n",
            "Global step: 1504,loss: 0.014309267\n",
            "\n",
            "Global step: 1505,loss: 0.014377885\n",
            "\n",
            "Global step: 1506,loss: 0.012263216\n",
            "\n",
            "Global step: 1507,loss: 0.015137911\n",
            "\n",
            "Global step: 1508,loss: 0.015443467\n",
            "\n",
            "Global step: 1509,loss: 0.01824973\n",
            "\n",
            "Global step: 1510,loss: 0.012406574\n",
            "\n",
            "Global step: 1511,loss: 0.013167301\n",
            "\n",
            "Global step: 1512,loss: 0.028139815\n",
            "\n",
            "Global step: 1513,loss: 0.020258285\n",
            "\n",
            "Global step: 1514,loss: 0.014567334\n",
            "\n",
            "Global step: 1515,loss: 0.014197931\n",
            "\n",
            "Global step: 1516,loss: 0.015294568\n",
            "\n",
            "Global step: 1517,loss: 0.015102569\n",
            "\n",
            "Global step: 1518,loss: 0.013662004\n",
            "\n",
            "Global step: 1519,loss: 0.018757176\n",
            "\n",
            "Global step: 1520,loss: 0.013081031\n",
            "\n",
            "Global step: 1521,loss: 0.015350012\n",
            "\n",
            "Global step: 1522,loss: 0.015569342\n",
            "\n",
            "Global step: 1523,loss: 0.017037228\n",
            "\n",
            "Global step: 1524,loss: 0.012742311\n",
            "\n",
            "Global step: 1525,loss: 0.013303672\n",
            "\n",
            "Global step: 1526,loss: 0.015337765\n",
            "\n",
            "Global step: 1527,loss: 0.01879783\n",
            "\n",
            "Global step: 1528,loss: 0.017963817\n",
            "\n",
            "Global step: 1529,loss: 0.012107365\n",
            "\n",
            "Global step: 1530,loss: 0.022938754\n",
            "\n",
            "Global step: 1531,loss: 0.014420823\n",
            "\n",
            "Global step: 1532,loss: 0.021112636\n",
            "\n",
            "Global step: 1533,loss: 0.013161456\n",
            "\n",
            "Global step: 1534,loss: 0.01757516\n",
            "\n",
            "Global step: 1535,loss: 0.020599987\n",
            "\n",
            "Global step: 1536,loss: 0.012029072\n",
            "\n",
            "Global step: 1537,loss: 0.024940696\n",
            "\n",
            "Global step: 1538,loss: 0.01399692\n",
            "\n",
            "Global step: 1539,loss: 0.026977766\n",
            "\n",
            "Global step: 1540,loss: 0.013750445\n",
            "\n",
            "Global step: 1541,loss: 0.017128399\n",
            "\n",
            "Global step: 1542,loss: 0.01441998\n",
            "\n",
            "Global step: 1543,loss: 0.015860628\n",
            "\n",
            "Global step: 1544,loss: 0.019990869\n",
            "\n",
            "Global step: 1545,loss: 0.013703097\n",
            "\n",
            "Global step: 1546,loss: 0.020048322\n",
            "\n",
            "Global step: 1547,loss: 0.014952291\n",
            "\n",
            "Global step: 1548,loss: 0.017859697\n",
            "\n",
            "Global step: 1549,loss: 0.014678331\n",
            "\n",
            "Global step: 1550,loss: 0.013081471\n",
            "\n",
            "Global step: 1551,loss: 0.016338734\n",
            "\n",
            "Global step: 1552,loss: 0.015714666\n",
            "\n",
            "Global step: 1553,loss: 0.014387594\n",
            "\n",
            "Global step: 1554,loss: 0.014515713\n",
            "\n",
            "Global step: 1555,loss: 0.014096383\n",
            "\n",
            "Global step: 1556,loss: 0.021773236\n",
            "\n",
            "Global step: 1557,loss: 0.013890224\n",
            "\n",
            "Global step: 1558,loss: 0.013779407\n",
            "\n",
            "Global step: 1559,loss: 0.020776127\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.72295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:08:52.529603 139735703181056 supervisor.py:1099] global_step/sec: 6.72295\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 1560.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:08:52.743608 139735711573760 supervisor.py:1050] Recording summary at step 1560.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 1559,val_loss: 0.023575410652810182\n",
            "\n",
            "Training for epoch 5/16:\n",
            "Global step: 1560,loss: 0.015149244\n",
            "\n",
            "Global step: 1561,loss: 0.018834718\n",
            "\n",
            "Global step: 1562,loss: 0.013066475\n",
            "\n",
            "Global step: 1563,loss: 0.020270485\n",
            "\n",
            "Global step: 1564,loss: 0.021424497\n",
            "\n",
            "Global step: 1565,loss: 0.026308978\n",
            "\n",
            "Global step: 1566,loss: 0.013228073\n",
            "\n",
            "Global step: 1567,loss: 0.015093406\n",
            "\n",
            "Global step: 1568,loss: 0.016582172\n",
            "\n",
            "Global step: 1569,loss: 0.017293911\n",
            "\n",
            "Global step: 1570,loss: 0.016358245\n",
            "\n",
            "Global step: 1571,loss: 0.015506373\n",
            "\n",
            "Global step: 1572,loss: 0.017354786\n",
            "\n",
            "Global step: 1573,loss: 0.020884968\n",
            "\n",
            "Global step: 1574,loss: 0.023607707\n",
            "\n",
            "Global step: 1575,loss: 0.01860907\n",
            "\n",
            "Global step: 1576,loss: 0.015956353\n",
            "\n",
            "Global step: 1577,loss: 0.011842972\n",
            "\n",
            "Global step: 1578,loss: 0.013249783\n",
            "\n",
            "Global step: 1579,loss: 0.015949896\n",
            "\n",
            "Global step: 1580,loss: 0.01251437\n",
            "\n",
            "Global step: 1581,loss: 0.012459811\n",
            "\n",
            "Global step: 1582,loss: 0.012776056\n",
            "\n",
            "Global step: 1583,loss: 0.021332735\n",
            "\n",
            "Global step: 1584,loss: 0.015167863\n",
            "\n",
            "Global step: 1585,loss: 0.013161976\n",
            "\n",
            "Global step: 1586,loss: 0.012411697\n",
            "\n",
            "Global step: 1587,loss: 0.013928199\n",
            "\n",
            "Global step: 1588,loss: 0.01370827\n",
            "\n",
            "Global step: 1589,loss: 0.012254084\n",
            "\n",
            "Global step: 1590,loss: 0.014312137\n",
            "\n",
            "Global step: 1591,loss: 0.014373107\n",
            "\n",
            "Global step: 1592,loss: 0.016330376\n",
            "\n",
            "Global step: 1593,loss: 0.017783713\n",
            "\n",
            "Global step: 1594,loss: 0.011586161\n",
            "\n",
            "Global step: 1595,loss: 0.011777341\n",
            "\n",
            "Global step: 1596,loss: 0.013782258\n",
            "\n",
            "Global step: 1597,loss: 0.012492292\n",
            "\n",
            "Global step: 1598,loss: 0.021062486\n",
            "\n",
            "Global step: 1599,loss: 0.012387136\n",
            "\n",
            "Global step: 1600,loss: 0.020130293\n",
            "\n",
            "Global step: 1601,loss: 0.011961006\n",
            "\n",
            "Global step: 1602,loss: 0.013061688\n",
            "\n",
            "Global step: 1603,loss: 0.01560212\n",
            "\n",
            "Global step: 1604,loss: 0.012535275\n",
            "\n",
            "Global step: 1605,loss: 0.011649204\n",
            "\n",
            "Global step: 1606,loss: 0.019494813\n",
            "\n",
            "Global step: 1607,loss: 0.015846368\n",
            "\n",
            "Global step: 1608,loss: 0.015801106\n",
            "\n",
            "Global step: 1609,loss: 0.01801347\n",
            "\n",
            "Global step: 1610,loss: 0.019344067\n",
            "\n",
            "Global step: 1611,loss: 0.017901585\n",
            "\n",
            "Global step: 1612,loss: 0.012517717\n",
            "\n",
            "Global step: 1613,loss: 0.010785628\n",
            "\n",
            "Global step: 1614,loss: 0.012582139\n",
            "\n",
            "Global step: 1615,loss: 0.0120011605\n",
            "\n",
            "Global step: 1616,loss: 0.016513102\n",
            "\n",
            "Global step: 1617,loss: 0.017797623\n",
            "\n",
            "Global step: 1618,loss: 0.0141890235\n",
            "\n",
            "Global step: 1619,loss: 0.013071796\n",
            "\n",
            "Global step: 1620,loss: 0.013895298\n",
            "\n",
            "Global step: 1621,loss: 0.010738397\n",
            "\n",
            "Global step: 1622,loss: 0.012164878\n",
            "\n",
            "Global step: 1623,loss: 0.017873297\n",
            "\n",
            "Global step: 1624,loss: 0.01626945\n",
            "\n",
            "Global step: 1625,loss: 0.017375754\n",
            "\n",
            "Global step: 1626,loss: 0.014311808\n",
            "\n",
            "Global step: 1627,loss: 0.014269989\n",
            "\n",
            "Global step: 1628,loss: 0.0152580105\n",
            "\n",
            "Global step: 1629,loss: 0.011016127\n",
            "\n",
            "Global step: 1630,loss: 0.015090616\n",
            "\n",
            "Global step: 1631,loss: 0.011534605\n",
            "\n",
            "Global step: 1632,loss: 0.012859011\n",
            "\n",
            "Global step: 1633,loss: 0.012889883\n",
            "\n",
            "Global step: 1634,loss: 0.011043301\n",
            "\n",
            "Global step: 1635,loss: 0.011750489\n",
            "\n",
            "Global step: 1636,loss: 0.014255397\n",
            "\n",
            "Global step: 1637,loss: 0.018980924\n",
            "\n",
            "Global step: 1638,loss: 0.013883154\n",
            "\n",
            "Global step: 1639,loss: 0.014469901\n",
            "\n",
            "Global step: 1640,loss: 0.01174334\n",
            "\n",
            "Global step: 1641,loss: 0.013249436\n",
            "\n",
            "Global step: 1642,loss: 0.011574009\n",
            "\n",
            "Global step: 1643,loss: 0.011807095\n",
            "\n",
            "Global step: 1644,loss: 0.02192879\n",
            "\n",
            "Global step: 1645,loss: 0.01123523\n",
            "\n",
            "Global step: 1646,loss: 0.013663477\n",
            "\n",
            "Global step: 1647,loss: 0.0118947625\n",
            "\n",
            "Global step: 1648,loss: 0.013128\n",
            "\n",
            "Global step: 1649,loss: 0.018961653\n",
            "\n",
            "Global step: 1650,loss: 0.011351179\n",
            "\n",
            "Global step: 1651,loss: 0.013015382\n",
            "\n",
            "Global step: 1652,loss: 0.016559718\n",
            "\n",
            "Global step: 1653,loss: 0.013424344\n",
            "\n",
            "Global step: 1654,loss: 0.01893549\n",
            "\n",
            "Global step: 1655,loss: 0.015396606\n",
            "\n",
            "Global step: 1656,loss: 0.017089374\n",
            "\n",
            "Global step: 1657,loss: 0.011236442\n",
            "\n",
            "Global step: 1658,loss: 0.024624936\n",
            "\n",
            "Global step: 1659,loss: 0.010651949\n",
            "\n",
            "Global step: 1660,loss: 0.015792228\n",
            "\n",
            "Global step: 1661,loss: 0.011459359\n",
            "\n",
            "Global step: 1662,loss: 0.012844923\n",
            "\n",
            "Global step: 1663,loss: 0.029405039\n",
            "\n",
            "Global step: 1664,loss: 0.0109832\n",
            "\n",
            "Global step: 1665,loss: 0.012692314\n",
            "\n",
            "Global step: 1666,loss: 0.013167232\n",
            "\n",
            "Global step: 1667,loss: 0.013493945\n",
            "\n",
            "Global step: 1668,loss: 0.014883158\n",
            "\n",
            "Global step: 1669,loss: 0.015165596\n",
            "\n",
            "Global step: 1670,loss: 0.020618761\n",
            "\n",
            "Global step: 1671,loss: 0.016382538\n",
            "\n",
            "Global step: 1672,loss: 0.019398935\n",
            "\n",
            "Global step: 1673,loss: 0.0153012965\n",
            "\n",
            "Global step: 1674,loss: 0.013007447\n",
            "\n",
            "Global step: 1675,loss: 0.014348421\n",
            "\n",
            "Global step: 1676,loss: 0.019639093\n",
            "\n",
            "Global step: 1677,loss: 0.0140865315\n",
            "\n",
            "Global step: 1678,loss: 0.013457784\n",
            "\n",
            "Global step: 1679,loss: 0.010933906\n",
            "\n",
            "Global step: 1680,loss: 0.011742078\n",
            "\n",
            "Global step: 1681,loss: 0.014066479\n",
            "\n",
            "Global step: 1682,loss: 0.019658757\n",
            "\n",
            "Global step: 1683,loss: 0.016008403\n",
            "\n",
            "Global step: 1684,loss: 0.012325946\n",
            "\n",
            "Global step: 1685,loss: 0.014351023\n",
            "\n",
            "Global step: 1686,loss: 0.017672116\n",
            "\n",
            "Global step: 1687,loss: 0.014691866\n",
            "\n",
            "Global step: 1688,loss: 0.025632232\n",
            "\n",
            "Global step: 1689,loss: 0.011984633\n",
            "\n",
            "Global step: 1690,loss: 0.014580135\n",
            "\n",
            "Global step: 1691,loss: 0.015819225\n",
            "\n",
            "Global step: 1692,loss: 0.013931738\n",
            "\n",
            "Global step: 1693,loss: 0.024296936\n",
            "\n",
            "Global step: 1694,loss: 0.011081518\n",
            "\n",
            "Global step: 1695,loss: 0.011210205\n",
            "\n",
            "Global step: 1696,loss: 0.011779498\n",
            "\n",
            "Global step: 1697,loss: 0.012226192\n",
            "\n",
            "Global step: 1698,loss: 0.017358687\n",
            "\n",
            "Global step: 1699,loss: 0.020400275\n",
            "\n",
            "Global step: 1700,loss: 0.014523567\n",
            "\n",
            "Global step: 1701,loss: 0.014117001\n",
            "\n",
            "Global step: 1702,loss: 0.0143205635\n",
            "\n",
            "Global step: 1703,loss: 0.014181484\n",
            "\n",
            "Global step: 1704,loss: 0.012951355\n",
            "\n",
            "Global step: 1705,loss: 0.016017951\n",
            "\n",
            "Global step: 1706,loss: 0.012794204\n",
            "\n",
            "Global step: 1707,loss: 0.01568836\n",
            "\n",
            "Global step: 1708,loss: 0.01632588\n",
            "\n",
            "Global step: 1709,loss: 0.012070408\n",
            "\n",
            "Global step: 1710,loss: 0.016941668\n",
            "\n",
            "Global step: 1711,loss: 0.016591627\n",
            "\n",
            "Global step: 1712,loss: 0.021186443\n",
            "\n",
            "Global step: 1713,loss: 0.026030252\n",
            "\n",
            "Global step: 1714,loss: 0.01145268\n",
            "\n",
            "Global step: 1715,loss: 0.016055863\n",
            "\n",
            "Global step: 1716,loss: 0.021992858\n",
            "\n",
            "Global step: 1717,loss: 0.02712417\n",
            "\n",
            "Global step: 1718,loss: 0.014162361\n",
            "\n",
            "Global step: 1719,loss: 0.017794326\n",
            "\n",
            "Global step: 1720,loss: 0.014787714\n",
            "\n",
            "Global step: 1721,loss: 0.017473724\n",
            "\n",
            "Global step: 1722,loss: 0.01627358\n",
            "\n",
            "Global step: 1723,loss: 0.016661858\n",
            "\n",
            "Global step: 1724,loss: 0.011774343\n",
            "\n",
            "Global step: 1725,loss: 0.019027067\n",
            "\n",
            "Global step: 1726,loss: 0.013267575\n",
            "\n",
            "Global step: 1727,loss: 0.020019308\n",
            "\n",
            "Global step: 1728,loss: 0.020240355\n",
            "\n",
            "Global step: 1729,loss: 0.012493428\n",
            "\n",
            "Global step: 1730,loss: 0.012882791\n",
            "\n",
            "Global step: 1731,loss: 0.017595721\n",
            "\n",
            "Global step: 1732,loss: 0.0128770815\n",
            "\n",
            "Global step: 1733,loss: 0.01979988\n",
            "\n",
            "Global step: 1734,loss: 0.014935014\n",
            "\n",
            "Global step: 1735,loss: 0.013857677\n",
            "\n",
            "Global step: 1736,loss: 0.012501754\n",
            "\n",
            "Global step: 1737,loss: 0.012681815\n",
            "\n",
            "Global step: 1738,loss: 0.014744429\n",
            "\n",
            "Global step: 1739,loss: 0.017006658\n",
            "\n",
            "Global step: 1740,loss: 0.025467496\n",
            "\n",
            "Global step: 1741,loss: 0.013056932\n",
            "\n",
            "Global step: 1742,loss: 0.015001109\n",
            "\n",
            "Global step: 1743,loss: 0.014824393\n",
            "\n",
            "Global step: 1744,loss: 0.015173941\n",
            "\n",
            "Global step: 1745,loss: 0.012427761\n",
            "\n",
            "Global step: 1746,loss: 0.015460456\n",
            "\n",
            "Global step: 1747,loss: 0.015065115\n",
            "\n",
            "Global step: 1748,loss: 0.015225091\n",
            "\n",
            "Global step: 1749,loss: 0.01319417\n",
            "\n",
            "Global step: 1750,loss: 0.01717265\n",
            "\n",
            "Global step: 1751,loss: 0.013667087\n",
            "\n",
            "Global step: 1752,loss: 0.0163135\n",
            "\n",
            "Global step: 1753,loss: 0.011903699\n",
            "\n",
            "Global step: 1754,loss: 0.013341609\n",
            "\n",
            "Global step: 1755,loss: 0.013344829\n",
            "\n",
            "Global step: 1756,loss: 0.013920551\n",
            "\n",
            "Global step: 1757,loss: 0.01425973\n",
            "\n",
            "Global step: 1758,loss: 0.013324969\n",
            "\n",
            "Global step: 1759,loss: 0.010703342\n",
            "\n",
            "Global step: 1760,loss: 0.018014356\n",
            "\n",
            "Global step: 1761,loss: 0.014745643\n",
            "\n",
            "Global step: 1762,loss: 0.011704324\n",
            "\n",
            "Global step: 1763,loss: 0.016004886\n",
            "\n",
            "Global step: 1764,loss: 0.012096034\n",
            "\n",
            "Global step: 1765,loss: 0.01929478\n",
            "\n",
            "Global step: 1766,loss: 0.012983149\n",
            "\n",
            "Global step: 1767,loss: 0.012557109\n",
            "\n",
            "Global step: 1768,loss: 0.013266743\n",
            "\n",
            "Global step: 1769,loss: 0.011923803\n",
            "\n",
            "Global step: 1770,loss: 0.01605105\n",
            "\n",
            "Global step: 1771,loss: 0.016070088\n",
            "\n",
            "Global step: 1772,loss: 0.012888169\n",
            "\n",
            "Global step: 1773,loss: 0.015486739\n",
            "\n",
            "Global step: 1774,loss: 0.014170345\n",
            "\n",
            "Global step: 1775,loss: 0.013098201\n",
            "\n",
            "Global step: 1776,loss: 0.011506066\n",
            "\n",
            "Global step: 1777,loss: 0.011871772\n",
            "\n",
            "Global step: 1778,loss: 0.015724178\n",
            "\n",
            "Global step: 1779,loss: 0.011418085\n",
            "\n",
            "Global step: 1780,loss: 0.012802527\n",
            "\n",
            "Global step: 1781,loss: 0.012220559\n",
            "\n",
            "Global step: 1782,loss: 0.018432636\n",
            "\n",
            "Global step: 1783,loss: 0.011433063\n",
            "\n",
            "Global step: 1784,loss: 0.011420747\n",
            "\n",
            "Global step: 1785,loss: 0.01473372\n",
            "\n",
            "Global step: 1786,loss: 0.01976484\n",
            "\n",
            "Global step: 1787,loss: 0.012162391\n",
            "\n",
            "Global step: 1788,loss: 0.012876002\n",
            "\n",
            "Global step: 1789,loss: 0.017917842\n",
            "\n",
            "Global step: 1790,loss: 0.018769426\n",
            "\n",
            "Global step: 1791,loss: 0.021253858\n",
            "\n",
            "Global step: 1792,loss: 0.016033366\n",
            "\n",
            "Global step: 1793,loss: 0.014645845\n",
            "\n",
            "Global step: 1794,loss: 0.011754964\n",
            "\n",
            "Global step: 1795,loss: 0.012271703\n",
            "\n",
            "Global step: 1796,loss: 0.01226067\n",
            "\n",
            "Global step: 1797,loss: 0.018124923\n",
            "\n",
            "Global step: 1798,loss: 0.012900098\n",
            "\n",
            "Global step: 1799,loss: 0.0153980395\n",
            "\n",
            "Global step: 1800,loss: 0.011900449\n",
            "\n",
            "Global step: 1801,loss: 0.016140627\n",
            "\n",
            "Global step: 1802,loss: 0.015937176\n",
            "\n",
            "Global step: 1803,loss: 0.01237151\n",
            "\n",
            "Global step: 1804,loss: 0.01208921\n",
            "\n",
            "Global step: 1805,loss: 0.017281309\n",
            "\n",
            "Global step: 1806,loss: 0.01133078\n",
            "\n",
            "Global step: 1807,loss: 0.01274325\n",
            "\n",
            "Global step: 1808,loss: 0.014129857\n",
            "\n",
            "Global step: 1809,loss: 0.01508051\n",
            "\n",
            "Global step: 1810,loss: 0.012785485\n",
            "\n",
            "Global step: 1811,loss: 0.014941812\n",
            "\n",
            "Global step: 1812,loss: 0.0128344465\n",
            "\n",
            "Global step: 1813,loss: 0.020741116\n",
            "\n",
            "Global step: 1814,loss: 0.014840261\n",
            "\n",
            "Global step: 1815,loss: 0.013478726\n",
            "\n",
            "Global step: 1816,loss: 0.016069453\n",
            "\n",
            "Global step: 1817,loss: 0.023058599\n",
            "\n",
            "Global step: 1818,loss: 0.020625014\n",
            "\n",
            "Global step: 1819,loss: 0.012443244\n",
            "\n",
            "Global step: 1820,loss: 0.016545087\n",
            "\n",
            "Global step: 1821,loss: 0.028501775\n",
            "\n",
            "Global step: 1822,loss: 0.014367515\n",
            "\n",
            "Global step: 1823,loss: 0.020254869\n",
            "\n",
            "Global step: 1824,loss: 0.016795052\n",
            "\n",
            "Global step: 1825,loss: 0.012818139\n",
            "\n",
            "Global step: 1826,loss: 0.016488373\n",
            "\n",
            "Global step: 1827,loss: 0.019286107\n",
            "\n",
            "Global step: 1828,loss: 0.012479012\n",
            "\n",
            "Global step: 1829,loss: 0.016417038\n",
            "\n",
            "Global step: 1830,loss: 0.013111792\n",
            "\n",
            "Global step: 1831,loss: 0.012145699\n",
            "\n",
            "Global step: 1832,loss: 0.019609898\n",
            "\n",
            "Global step: 1833,loss: 0.011983489\n",
            "\n",
            "Global step: 1834,loss: 0.019199787\n",
            "\n",
            "Global step: 1835,loss: 0.01150113\n",
            "\n",
            "Global step: 1836,loss: 0.012963194\n",
            "\n",
            "Global step: 1837,loss: 0.013928913\n",
            "\n",
            "Global step: 1838,loss: 0.014028447\n",
            "\n",
            "Global step: 1839,loss: 0.015064637\n",
            "\n",
            "Global step: 1840,loss: 0.011570471\n",
            "\n",
            "Global step: 1841,loss: 0.021613128\n",
            "\n",
            "Global step: 1842,loss: 0.019999456\n",
            "\n",
            "Global step: 1843,loss: 0.01572933\n",
            "\n",
            "Global step: 1844,loss: 0.017625755\n",
            "\n",
            "Global step: 1845,loss: 0.012430017\n",
            "\n",
            "Global step: 1846,loss: 0.01808343\n",
            "\n",
            "Global step: 1847,loss: 0.014869085\n",
            "\n",
            "Global step: 1848,loss: 0.0116163585\n",
            "\n",
            "Global step: 1849,loss: 0.014363168\n",
            "\n",
            "Global step: 1850,loss: 0.018291123\n",
            "\n",
            "Global step: 1851,loss: 0.015309684\n",
            "\n",
            "Global step: 1852,loss: 0.014163683\n",
            "\n",
            "Global step: 1853,loss: 0.015452405\n",
            "\n",
            "Global step: 1854,loss: 0.014603322\n",
            "\n",
            "Global step: 1855,loss: 0.0150762815\n",
            "\n",
            "Global step: 1856,loss: 0.01168783\n",
            "\n",
            "Global step: 1857,loss: 0.012829239\n",
            "\n",
            "Global step: 1858,loss: 0.012342903\n",
            "\n",
            "Global step: 1859,loss: 0.017734157\n",
            "\n",
            "Global step: 1860,loss: 0.012816297\n",
            "\n",
            "Global step: 1861,loss: 0.02157947\n",
            "\n",
            "Global step: 1862,loss: 0.01191101\n",
            "\n",
            "Global step: 1863,loss: 0.012582285\n",
            "\n",
            "Global step: 1864,loss: 0.01292526\n",
            "\n",
            "Global step: 1865,loss: 0.014762764\n",
            "\n",
            "Global step: 1866,loss: 0.021509683\n",
            "\n",
            "Global step: 1867,loss: 0.012858928\n",
            "\n",
            "Global step: 1868,loss: 0.0127528235\n",
            "\n",
            "Global step: 1869,loss: 0.012540695\n",
            "\n",
            "Global step: 1870,loss: 0.016110633\n",
            "\n",
            "Global step: 1871,loss: 0.018390976\n",
            "\n",
            "Global step: 1872,loss: 0.02217062\n",
            "\n",
            "Global step: 1873,loss: 0.016083898\n",
            "\n",
            "Global step: 1874,loss: 0.016384592\n",
            "\n",
            "Global step: 1875,loss: 0.010909725\n",
            "\n",
            "Global step: 1876,loss: 0.0123904925\n",
            "\n",
            "Global step: 1877,loss: 0.015843738\n",
            "\n",
            "Global step: 1878,loss: 0.013451453\n",
            "\n",
            "Global step: 1879,loss: 0.011707732\n",
            "\n",
            "Global step: 1880,loss: 0.013290344\n",
            "\n",
            "Global step: 1881,loss: 0.012296103\n",
            "\n",
            "Global step: 1882,loss: 0.01204727\n",
            "\n",
            "Global step: 1883,loss: 0.021550521\n",
            "\n",
            "Global step: 1884,loss: 0.014116298\n",
            "\n",
            "Global step: 1885,loss: 0.026000269\n",
            "\n",
            "Global step: 1886,loss: 0.01646062\n",
            "\n",
            "Global step: 1887,loss: 0.015195226\n",
            "\n",
            "Global step: 1888,loss: 0.012532896\n",
            "\n",
            "Global step: 1889,loss: 0.01686921\n",
            "\n",
            "Global step: 1890,loss: 0.015051215\n",
            "\n",
            "Global step: 1891,loss: 0.026263598\n",
            "\n",
            "Global step: 1892,loss: 0.01152223\n",
            "\n",
            "Global step: 1893,loss: 0.015626198\n",
            "\n",
            "Global step: 1894,loss: 0.013106458\n",
            "\n",
            "Global step: 1895,loss: 0.016950741\n",
            "\n",
            "Global step: 1896,loss: 0.011956882\n",
            "\n",
            "Global step: 1897,loss: 0.01283199\n",
            "\n",
            "Global step: 1898,loss: 0.011313189\n",
            "\n",
            "Global step: 1899,loss: 0.015623218\n",
            "\n",
            "Global step: 1900,loss: 0.011910632\n",
            "\n",
            "Global step: 1901,loss: 0.013069367\n",
            "\n",
            "Global step: 1902,loss: 0.023802405\n",
            "\n",
            "Global step: 1903,loss: 0.011553179\n",
            "\n",
            "Global step: 1904,loss: 0.013681184\n",
            "\n",
            "Global step: 1905,loss: 0.015246578\n",
            "\n",
            "Global step: 1906,loss: 0.019070812\n",
            "\n",
            "Global step: 1907,loss: 0.011633605\n",
            "\n",
            "Global step: 1908,loss: 0.018493997\n",
            "\n",
            "Global step: 1909,loss: 0.015457571\n",
            "\n",
            "Global step: 1910,loss: 0.011911621\n",
            "\n",
            "Global step: 1911,loss: 0.015353518\n",
            "\n",
            "Global step: 1912,loss: 0.011759732\n",
            "\n",
            "Global step: 1913,loss: 0.0112109855\n",
            "\n",
            "Global step: 1914,loss: 0.013922507\n",
            "\n",
            "Global step: 1915,loss: 0.013770826\n",
            "\n",
            "Global step: 1916,loss: 0.0147066135\n",
            "\n",
            "Global step: 1917,loss: 0.010564339\n",
            "\n",
            "Global step: 1918,loss: 0.015538301\n",
            "\n",
            "Global step: 1919,loss: 0.013291998\n",
            "\n",
            "Global step: 1920,loss: 0.015615413\n",
            "\n",
            "Global step: 1921,loss: 0.01271674\n",
            "\n",
            "Global step: 1922,loss: 0.0130037265\n",
            "\n",
            "Global step: 1923,loss: 0.018704064\n",
            "\n",
            "Global step: 1924,loss: 0.015315097\n",
            "\n",
            "Global step: 1925,loss: 0.011272099\n",
            "\n",
            "Global step: 1926,loss: 0.013156608\n",
            "\n",
            "Global step: 1927,loss: 0.015823828\n",
            "\n",
            "Global step: 1928,loss: 0.014828136\n",
            "\n",
            "Global step: 1929,loss: 0.028717563\n",
            "\n",
            "Global step: 1930,loss: 0.012036544\n",
            "\n",
            "Global step: 1931,loss: 0.011083759\n",
            "\n",
            "Global step: 1932,loss: 0.012217948\n",
            "\n",
            "Global step: 1933,loss: 0.013747869\n",
            "\n",
            "Global step: 1934,loss: 0.014269978\n",
            "\n",
            "Global step: 1935,loss: 0.017304849\n",
            "\n",
            "Global step: 1936,loss: 0.022407971\n",
            "\n",
            "Global step: 1937,loss: 0.0153289605\n",
            "\n",
            "Global step: 1938,loss: 0.02010205\n",
            "\n",
            "Global step: 1939,loss: 0.023279157\n",
            "\n",
            "Global step: 1940,loss: 0.018974246\n",
            "\n",
            "Global step: 1941,loss: 0.014294793\n",
            "\n",
            "Global step: 1942,loss: 0.013953114\n",
            "\n",
            "Global step: 1943,loss: 0.013443422\n",
            "\n",
            "Global step: 1944,loss: 0.013054513\n",
            "\n",
            "Global step: 1945,loss: 0.01417643\n",
            "\n",
            "Global step: 1946,loss: 0.0138216475\n",
            "\n",
            "Global step: 1947,loss: 0.01724321\n",
            "\n",
            "Global step: 1948,loss: 0.016714824\n",
            "\n",
            "Global step: 1949,loss: 0.016641382\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1949,Val_Loss: 0.022062967519443005,  Val_acc: 0.9945913461538461 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1112 10:09:54.775118 139738004481920 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/16:\n",
            "Global step: 1950,loss: 0.013942121\n",
            "\n",
            "Global step: 1951,loss: 0.012400796\n",
            "\n",
            "Global step: 1952,loss: 0.015792564\n",
            "\n",
            "Global step: 1953,loss: 0.01174216\n",
            "\n",
            "Global step: 1954,loss: 0.011264063\n",
            "\n",
            "Global step: 1955,loss: 0.012268982\n",
            "\n",
            "Global step: 1956,loss: 0.012306583\n",
            "\n",
            "Global step: 1957,loss: 0.014963305\n",
            "\n",
            "Global step: 1958,loss: 0.02275278\n",
            "\n",
            "Global step: 1959,loss: 0.014679793\n",
            "\n",
            "Global step: 1960,loss: 0.011095032\n",
            "\n",
            "Global step: 1961,loss: 0.013803588\n",
            "\n",
            "Global step: 1962,loss: 0.01750152\n",
            "\n",
            "Global step: 1963,loss: 0.014587991\n",
            "\n",
            "Global step: 1964,loss: 0.013723313\n",
            "\n",
            "Global step: 1965,loss: 0.01537184\n",
            "\n",
            "Global step: 1966,loss: 0.011544228\n",
            "\n",
            "Global step: 1967,loss: 0.012361275\n",
            "\n",
            "Global step: 1968,loss: 0.013454622\n",
            "\n",
            "Global step: 1969,loss: 0.014767087\n",
            "\n",
            "Global step: 1970,loss: 0.01211505\n",
            "\n",
            "Global step: 1971,loss: 0.010974058\n",
            "\n",
            "Global step: 1972,loss: 0.013490918\n",
            "\n",
            "Global step: 1973,loss: 0.009955582\n",
            "\n",
            "Global step: 1974,loss: 0.01381095\n",
            "\n",
            "Global step: 1975,loss: 0.011402229\n",
            "\n",
            "Global step: 1976,loss: 0.009790929\n",
            "\n",
            "Global step: 1977,loss: 0.013941375\n",
            "\n",
            "Global step: 1978,loss: 0.020321123\n",
            "\n",
            "Global step: 1979,loss: 0.011698997\n",
            "\n",
            "Global step: 1980,loss: 0.011577932\n",
            "\n",
            "Global step: 1981,loss: 0.014672125\n",
            "\n",
            "Global step: 1982,loss: 0.012363019\n",
            "\n",
            "Global step: 1983,loss: 0.011928881\n",
            "\n",
            "Global step: 1984,loss: 0.012817353\n",
            "\n",
            "Global step: 1985,loss: 0.010540828\n",
            "\n",
            "Global step: 1986,loss: 0.017845126\n",
            "\n",
            "Global step: 1987,loss: 0.011063973\n",
            "\n",
            "Global step: 1988,loss: 0.012290312\n",
            "\n",
            "Global step: 1989,loss: 0.0143339755\n",
            "\n",
            "Global step: 1990,loss: 0.012703722\n",
            "\n",
            "Global step: 1991,loss: 0.016308904\n",
            "\n",
            "Global step: 1992,loss: 0.011727378\n",
            "\n",
            "Global step: 1993,loss: 0.015679574\n",
            "\n",
            "Global step: 1994,loss: 0.011772805\n",
            "\n",
            "Global step: 1995,loss: 0.011982434\n",
            "\n",
            "Global step: 1996,loss: 0.013399189\n",
            "\n",
            "Global step: 1997,loss: 0.018985063\n",
            "\n",
            "Global step: 1998,loss: 0.011324195\n",
            "\n",
            "Global step: 1999,loss: 0.010706599\n",
            "\n",
            "Global step: 2000,loss: 0.013337506\n",
            "\n",
            "Global step: 2001,loss: 0.010216493\n",
            "\n",
            "Global step: 2002,loss: 0.010674465\n",
            "\n",
            "Global step: 2003,loss: 0.012076896\n",
            "\n",
            "Global step: 2004,loss: 0.020584479\n",
            "\n",
            "Global step: 2005,loss: 0.012138066\n",
            "\n",
            "Global step: 2006,loss: 0.013068738\n",
            "\n",
            "Global step: 2007,loss: 0.014384427\n",
            "\n",
            "Global step: 2008,loss: 0.009797588\n",
            "\n",
            "Global step: 2009,loss: 0.012669838\n",
            "\n",
            "Global step: 2010,loss: 0.014323415\n",
            "\n",
            "Global step: 2011,loss: 0.011301341\n",
            "\n",
            "Global step: 2012,loss: 0.01627072\n",
            "\n",
            "Global step: 2013,loss: 0.011487748\n",
            "\n",
            "Global step: 2014,loss: 0.013320228\n",
            "\n",
            "Global step: 2015,loss: 0.0113545265\n",
            "\n",
            "Global step: 2016,loss: 0.012012723\n",
            "\n",
            "Global step: 2017,loss: 0.013700972\n",
            "\n",
            "Global step: 2018,loss: 0.020214709\n",
            "\n",
            "Global step: 2019,loss: 0.017139163\n",
            "\n",
            "Global step: 2020,loss: 0.010957086\n",
            "\n",
            "Global step: 2021,loss: 0.012606608\n",
            "\n",
            "Global step: 2022,loss: 0.010166878\n",
            "\n",
            "Global step: 2023,loss: 0.0108686\n",
            "\n",
            "Global step: 2024,loss: 0.012026286\n",
            "\n",
            "Global step: 2025,loss: 0.011655301\n",
            "\n",
            "Global step: 2026,loss: 0.009970858\n",
            "\n",
            "Global step: 2027,loss: 0.012217543\n",
            "\n",
            "Global step: 2028,loss: 0.016270854\n",
            "\n",
            "Global step: 2029,loss: 0.016981885\n",
            "\n",
            "Global step: 2030,loss: 0.010617957\n",
            "\n",
            "Global step: 2031,loss: 0.011621885\n",
            "\n",
            "Global step: 2032,loss: 0.016334515\n",
            "\n",
            "Global step: 2033,loss: 0.016491497\n",
            "\n",
            "Global step: 2034,loss: 0.01767875\n",
            "\n",
            "Global step: 2035,loss: 0.012080304\n",
            "\n",
            "Global step: 2036,loss: 0.013863966\n",
            "\n",
            "Global step: 2037,loss: 0.010864479\n",
            "\n",
            "Global step: 2038,loss: 0.011825446\n",
            "\n",
            "Global step: 2039,loss: 0.010604001\n",
            "\n",
            "Global step: 2040,loss: 0.011760484\n",
            "\n",
            "Global step: 2041,loss: 0.016561553\n",
            "\n",
            "Global step: 2042,loss: 0.013132421\n",
            "\n",
            "Global step: 2043,loss: 0.012235268\n",
            "\n",
            "Global step: 2044,loss: 0.013794553\n",
            "\n",
            "Global step: 2045,loss: 0.014423141\n",
            "\n",
            "Global step: 2046,loss: 0.014089604\n",
            "\n",
            "Global step: 2047,loss: 0.010986523\n",
            "\n",
            "Global step: 2048,loss: 0.010205563\n",
            "\n",
            "Global step: 2049,loss: 0.0144668\n",
            "\n",
            "Global step: 2050,loss: 0.011198214\n",
            "\n",
            "Global step: 2051,loss: 0.012397462\n",
            "\n",
            "Global step: 2052,loss: 0.011410108\n",
            "\n",
            "Global step: 2053,loss: 0.019735638\n",
            "\n",
            "Global step: 2054,loss: 0.012303084\n",
            "\n",
            "Global step: 2055,loss: 0.013280908\n",
            "\n",
            "Global step: 2056,loss: 0.013613416\n",
            "\n",
            "Global step: 2057,loss: 0.010851407\n",
            "\n",
            "Global step: 2058,loss: 0.011611279\n",
            "\n",
            "Global step: 2059,loss: 0.011683714\n",
            "\n",
            "Global step: 2060,loss: 0.009838904\n",
            "\n",
            "Global step: 2061,loss: 0.011161137\n",
            "\n",
            "Global step: 2062,loss: 0.010853402\n",
            "\n",
            "Global step: 2063,loss: 0.015216349\n",
            "\n",
            "Global step: 2064,loss: 0.0095653245\n",
            "\n",
            "Global step: 2065,loss: 0.013460826\n",
            "\n",
            "Global step: 2066,loss: 0.012536244\n",
            "\n",
            "Global step: 2067,loss: 0.017214084\n",
            "\n",
            "Global step: 2068,loss: 0.011288235\n",
            "\n",
            "Global step: 2069,loss: 0.010302517\n",
            "\n",
            "Global step: 2070,loss: 0.010329565\n",
            "\n",
            "Global step: 2071,loss: 0.01108885\n",
            "\n",
            "Global step: 2072,loss: 0.011508293\n",
            "\n",
            "Global step: 2073,loss: 0.012155161\n",
            "\n",
            "Global step: 2074,loss: 0.013815653\n",
            "\n",
            "Global step: 2075,loss: 0.010379702\n",
            "\n",
            "Global step: 2076,loss: 0.012145909\n",
            "\n",
            "Global step: 2077,loss: 0.011094254\n",
            "\n",
            "Global step: 2078,loss: 0.015381695\n",
            "\n",
            "Global step: 2079,loss: 0.010063935\n",
            "\n",
            "Global step: 2080,loss: 0.011013719\n",
            "\n",
            "Global step: 2081,loss: 0.015091751\n",
            "\n",
            "Global step: 2082,loss: 0.023523016\n",
            "\n",
            "Global step: 2083,loss: 0.01204692\n",
            "\n",
            "Global step: 2084,loss: 0.012855228\n",
            "\n",
            "Global step: 2085,loss: 0.0135512\n",
            "\n",
            "Global step: 2086,loss: 0.018902004\n",
            "\n",
            "Global step: 2087,loss: 0.015762858\n",
            "\n",
            "Global step: 2088,loss: 0.015415332\n",
            "\n",
            "Global step: 2089,loss: 0.018188518\n",
            "\n",
            "Global step: 2090,loss: 0.018511862\n",
            "\n",
            "Global step: 2091,loss: 0.013307666\n",
            "\n",
            "Global step: 2092,loss: 0.0148732895\n",
            "\n",
            "Global step: 2093,loss: 0.014762187\n",
            "\n",
            "Global step: 2094,loss: 0.017297573\n",
            "\n",
            "Global step: 2095,loss: 0.011200684\n",
            "\n",
            "Global step: 2096,loss: 0.012017035\n",
            "\n",
            "Global step: 2097,loss: 0.011812813\n",
            "\n",
            "Global step: 2098,loss: 0.011148023\n",
            "\n",
            "Global step: 2099,loss: 0.020906959\n",
            "\n",
            "Global step: 2100,loss: 0.011575109\n",
            "\n",
            "Global step: 2101,loss: 0.01203691\n",
            "\n",
            "Global step: 2102,loss: 0.011033971\n",
            "\n",
            "Global step: 2103,loss: 0.011111118\n",
            "\n",
            "Global step: 2104,loss: 0.015964568\n",
            "\n",
            "Global step: 2105,loss: 0.010497024\n",
            "\n",
            "Global step: 2106,loss: 0.012629228\n",
            "\n",
            "Global step: 2107,loss: 0.011791003\n",
            "\n",
            "Global step: 2108,loss: 0.016339146\n",
            "\n",
            "Global step: 2109,loss: 0.013724655\n",
            "\n",
            "Global step: 2110,loss: 0.013015991\n",
            "\n",
            "Global step: 2111,loss: 0.011629096\n",
            "\n",
            "Global step: 2112,loss: 0.011162615\n",
            "\n",
            "Global step: 2113,loss: 0.014338517\n",
            "\n",
            "Global step: 2114,loss: 0.010164231\n",
            "\n",
            "Global step: 2115,loss: 0.009800681\n",
            "\n",
            "Global step: 2116,loss: 0.015275899\n",
            "\n",
            "Global step: 2117,loss: 0.013432544\n",
            "\n",
            "Global step: 2118,loss: 0.012817258\n",
            "\n",
            "Global step: 2119,loss: 0.010446826\n",
            "\n",
            "Global step: 2120,loss: 0.021119392\n",
            "\n",
            "Global step: 2121,loss: 0.012037884\n",
            "\n",
            "Global step: 2122,loss: 0.011188856\n",
            "\n",
            "Global step: 2123,loss: 0.010731987\n",
            "\n",
            "Global step: 2124,loss: 0.019091845\n",
            "\n",
            "Global step: 2125,loss: 0.0107621495\n",
            "\n",
            "Global step: 2126,loss: 0.012057972\n",
            "\n",
            "Global step: 2127,loss: 0.010835089\n",
            "\n",
            "Global step: 2128,loss: 0.021571863\n",
            "\n",
            "Global step: 2129,loss: 0.0120316055\n",
            "\n",
            "Global step: 2130,loss: 0.018860321\n",
            "\n",
            "Global step: 2131,loss: 0.010242035\n",
            "\n",
            "Global step: 2132,loss: 0.012524099\n",
            "\n",
            "Global step: 2133,loss: 0.015979169\n",
            "\n",
            "Global step: 2134,loss: 0.010702761\n",
            "\n",
            "Global step: 2135,loss: 0.011480542\n",
            "\n",
            "Global step: 2136,loss: 0.010440337\n",
            "\n",
            "Global step: 2137,loss: 0.01150702\n",
            "\n",
            "Global step: 2138,loss: 0.013415011\n",
            "\n",
            "Global step: 2139,loss: 0.01135453\n",
            "\n",
            "Global step: 2140,loss: 0.015799321\n",
            "\n",
            "Global step: 2141,loss: 0.009908049\n",
            "\n",
            "Global step: 2142,loss: 0.009985984\n",
            "\n",
            "Global step: 2143,loss: 0.013024392\n",
            "\n",
            "Global step: 2144,loss: 0.011145847\n",
            "\n",
            "Global step: 2145,loss: 0.016876709\n",
            "\n",
            "Global step: 2146,loss: 0.014200535\n",
            "\n",
            "Global step: 2147,loss: 0.0121168345\n",
            "\n",
            "Global step: 2148,loss: 0.014186479\n",
            "\n",
            "Global step: 2149,loss: 0.011304974\n",
            "\n",
            "Global step: 2150,loss: 0.011201453\n",
            "\n",
            "Global step: 2151,loss: 0.010926353\n",
            "\n",
            "Global step: 2152,loss: 0.019034429\n",
            "\n",
            "Global step: 2153,loss: 0.010685186\n",
            "\n",
            "Global step: 2154,loss: 0.0115510225\n",
            "\n",
            "Global step: 2155,loss: 0.015737755\n",
            "\n",
            "Global step: 2156,loss: 0.0101771625\n",
            "\n",
            "Global step: 2157,loss: 0.0102459695\n",
            "\n",
            "Global step: 2158,loss: 0.012079718\n",
            "\n",
            "Global step: 2159,loss: 0.01159259\n",
            "\n",
            "Global step: 2160,loss: 0.011750713\n",
            "\n",
            "Global step: 2161,loss: 0.014438596\n",
            "\n",
            "Global step: 2162,loss: 0.010395333\n",
            "\n",
            "Global step: 2163,loss: 0.010743243\n",
            "\n",
            "Global step: 2164,loss: 0.010561004\n",
            "\n",
            "Global step: 2165,loss: 0.010857845\n",
            "\n",
            "Global step: 2166,loss: 0.011285091\n",
            "\n",
            "Global step: 2167,loss: 0.013556389\n",
            "\n",
            "Global step: 2168,loss: 0.011631057\n",
            "\n",
            "Global step: 2169,loss: 0.013279159\n",
            "\n",
            "Global step: 2170,loss: 0.01473414\n",
            "\n",
            "Global step: 2171,loss: 0.0130161615\n",
            "\n",
            "Global step: 2172,loss: 0.011547156\n",
            "\n",
            "Global step: 2173,loss: 0.010947978\n",
            "\n",
            "Global step: 2174,loss: 0.012997814\n",
            "\n",
            "Global step: 2175,loss: 0.010069679\n",
            "\n",
            "Global step: 2176,loss: 0.012141445\n",
            "\n",
            "Global step: 2177,loss: 0.015508933\n",
            "\n",
            "Global step: 2178,loss: 0.0105521325\n",
            "\n",
            "Global step: 2179,loss: 0.010385441\n",
            "\n",
            "Global step: 2180,loss: 0.0121492585\n",
            "\n",
            "Global step: 2181,loss: 0.011832592\n",
            "\n",
            "Global step: 2182,loss: 0.009643328\n",
            "\n",
            "Global step: 2183,loss: 0.011053363\n",
            "\n",
            "Global step: 2184,loss: 0.014113145\n",
            "\n",
            "Global step: 2185,loss: 0.011597963\n",
            "\n",
            "Global step: 2186,loss: 0.012389971\n",
            "\n",
            "Global step: 2187,loss: 0.00942871\n",
            "\n",
            "Global step: 2188,loss: 0.01350051\n",
            "\n",
            "Global step: 2189,loss: 0.010629193\n",
            "\n",
            "Global step: 2190,loss: 0.010415451\n",
            "\n",
            "Global step: 2191,loss: 0.010389737\n",
            "\n",
            "Global step: 2192,loss: 0.015447529\n",
            "\n",
            "Global step: 2193,loss: 0.01178733\n",
            "\n",
            "Global step: 2194,loss: 0.012747584\n",
            "\n",
            "Global step: 2195,loss: 0.014274179\n",
            "\n",
            "Global step: 2196,loss: 0.015687278\n",
            "\n",
            "Global step: 2197,loss: 0.013274201\n",
            "\n",
            "Global step: 2198,loss: 0.010358318\n",
            "\n",
            "Global step: 2199,loss: 0.016932603\n",
            "\n",
            "Global step: 2200,loss: 0.010392478\n",
            "\n",
            "Global step: 2201,loss: 0.009793668\n",
            "\n",
            "Global step: 2202,loss: 0.018372398\n",
            "\n",
            "Global step: 2203,loss: 0.016883807\n",
            "\n",
            "Global step: 2204,loss: 0.011463176\n",
            "\n",
            "Global step: 2205,loss: 0.01395577\n",
            "\n",
            "Global step: 2206,loss: 0.01193796\n",
            "\n",
            "Global step: 2207,loss: 0.0106976\n",
            "\n",
            "Global step: 2208,loss: 0.010424781\n",
            "\n",
            "Global step: 2209,loss: 0.010726906\n",
            "\n",
            "Global step: 2210,loss: 0.015411984\n",
            "\n",
            "Global step: 2211,loss: 0.016764361\n",
            "\n",
            "Global step: 2212,loss: 0.0106492285\n",
            "\n",
            "Global step: 2213,loss: 0.0107427845\n",
            "\n",
            "Global step: 2214,loss: 0.012552752\n",
            "\n",
            "Global step: 2215,loss: 0.013499815\n",
            "\n",
            "Global step: 2216,loss: 0.012707625\n",
            "\n",
            "Global step: 2217,loss: 0.01456666\n",
            "\n",
            "Global step: 2218,loss: 0.0151917655\n",
            "\n",
            "Global step: 2219,loss: 0.010530461\n",
            "\n",
            "Global step: 2220,loss: 0.012253519\n",
            "\n",
            "Global step: 2221,loss: 0.014753357\n",
            "\n",
            "Global step: 2222,loss: 0.011178451\n",
            "\n",
            "Global step: 2223,loss: 0.011128092\n",
            "\n",
            "Global step: 2224,loss: 0.013927633\n",
            "\n",
            "Global step: 2225,loss: 0.011379495\n",
            "\n",
            "Global step: 2226,loss: 0.010035039\n",
            "\n",
            "Global step: 2227,loss: 0.011788702\n",
            "\n",
            "Global step: 2228,loss: 0.0108430665\n",
            "\n",
            "Global step: 2229,loss: 0.0094202105\n",
            "\n",
            "Global step: 2230,loss: 0.012965451\n",
            "\n",
            "Global step: 2231,loss: 0.014916635\n",
            "\n",
            "Global step: 2232,loss: 0.017783098\n",
            "\n",
            "Global step: 2233,loss: 0.012245448\n",
            "\n",
            "Global step: 2234,loss: 0.01225302\n",
            "\n",
            "Global step: 2235,loss: 0.0108399745\n",
            "\n",
            "Global step: 2236,loss: 0.009717062\n",
            "\n",
            "Global step: 2237,loss: 0.009853816\n",
            "\n",
            "Global step: 2238,loss: 0.013318919\n",
            "\n",
            "Global step: 2239,loss: 0.011030126\n",
            "\n",
            "Global step: 2240,loss: 0.011160685\n",
            "\n",
            "Global step: 2241,loss: 0.011813503\n",
            "\n",
            "Global step: 2242,loss: 0.011027062\n",
            "\n",
            "Global step: 2243,loss: 0.009039685\n",
            "\n",
            "Global step: 2244,loss: 0.013726055\n",
            "\n",
            "Global step: 2245,loss: 0.013884378\n",
            "\n",
            "Global step: 2246,loss: 0.016575793\n",
            "\n",
            "Global step: 2247,loss: 0.011801395\n",
            "\n",
            "Global step: 2248,loss: 0.011940396\n",
            "\n",
            "Global step: 2249,loss: 0.010855401\n",
            "\n",
            "Global step: 2250,loss: 0.010210687\n",
            "\n",
            "Global step: 2251,loss: 0.009469517\n",
            "\n",
            "Global step: 2252,loss: 0.014273837\n",
            "\n",
            "Global step: 2253,loss: 0.013772451\n",
            "\n",
            "Global step: 2254,loss: 0.011230059\n",
            "\n",
            "Global step: 2255,loss: 0.01122794\n",
            "\n",
            "Global step: 2256,loss: 0.017442804\n",
            "\n",
            "Global step: 2257,loss: 0.011251158\n",
            "\n",
            "Global step: 2258,loss: 0.012099378\n",
            "\n",
            "Global step: 2259,loss: 0.021189041\n",
            "\n",
            "Global step: 2260,loss: 0.011030642\n",
            "\n",
            "Global step: 2261,loss: 0.015324266\n",
            "\n",
            "Global step: 2262,loss: 0.012382191\n",
            "\n",
            "Global step: 2263,loss: 0.013623329\n",
            "\n",
            "Global step: 2264,loss: 0.009976393\n",
            "\n",
            "Global step: 2265,loss: 0.014134437\n",
            "\n",
            "Global step: 2266,loss: 0.009503678\n",
            "\n",
            "Global step: 2267,loss: 0.012103433\n",
            "\n",
            "Global step: 2268,loss: 0.011961374\n",
            "\n",
            "Global step: 2269,loss: 0.020707753\n",
            "\n",
            "Global step: 2270,loss: 0.01013264\n",
            "\n",
            "Global step: 2271,loss: 0.015201915\n",
            "\n",
            "Global step: 2272,loss: 0.0118419295\n",
            "\n",
            "Global step: 2273,loss: 0.012688629\n",
            "\n",
            "Global step: 2274,loss: 0.013126154\n",
            "\n",
            "Global step: 2275,loss: 0.015286878\n",
            "\n",
            "Global step: 2276,loss: 0.012311393\n",
            "\n",
            "Global step: 2277,loss: 0.012041941\n",
            "\n",
            "Global step: 2278,loss: 0.011180618\n",
            "\n",
            "Global step: 2279,loss: 0.013156454\n",
            "\n",
            "Global step: 2280,loss: 0.010974672\n",
            "\n",
            "Global step: 2281,loss: 0.011172858\n",
            "\n",
            "Global step: 2282,loss: 0.009796965\n",
            "\n",
            "Global step: 2283,loss: 0.015713995\n",
            "\n",
            "Global step: 2284,loss: 0.010829059\n",
            "\n",
            "Global step: 2285,loss: 0.012549599\n",
            "\n",
            "Global step: 2286,loss: 0.010434002\n",
            "\n",
            "Global step: 2287,loss: 0.011897231\n",
            "\n",
            "Global step: 2288,loss: 0.010239557\n",
            "\n",
            "Global step: 2289,loss: 0.011553207\n",
            "\n",
            "Global step: 2290,loss: 0.010790046\n",
            "\n",
            "Global step: 2291,loss: 0.012143256\n",
            "\n",
            "Global step: 2292,loss: 0.013130562\n",
            "\n",
            "Global step: 2293,loss: 0.009789895\n",
            "\n",
            "Global step: 2294,loss: 0.011462709\n",
            "\n",
            "Global step: 2295,loss: 0.013882997\n",
            "\n",
            "Global step: 2296,loss: 0.009407189\n",
            "\n",
            "Global step: 2297,loss: 0.010371384\n",
            "\n",
            "Global step: 2298,loss: 0.01588124\n",
            "\n",
            "Global step: 2299,loss: 0.010441637\n",
            "\n",
            "Global step: 2300,loss: 0.009251332\n",
            "\n",
            "Global step: 2301,loss: 0.011262151\n",
            "\n",
            "Global step: 2302,loss: 0.01047887\n",
            "\n",
            "Global step: 2303,loss: 0.014223135\n",
            "\n",
            "Global step: 2304,loss: 0.013953126\n",
            "\n",
            "Global step: 2305,loss: 0.012110379\n",
            "\n",
            "Global step: 2306,loss: 0.012138411\n",
            "\n",
            "Global step: 2307,loss: 0.011718952\n",
            "\n",
            "Global step: 2308,loss: 0.010501988\n",
            "\n",
            "Global step: 2309,loss: 0.01079041\n",
            "\n",
            "Global step: 2310,loss: 0.010411456\n",
            "\n",
            "Global step: 2311,loss: 0.0101223495\n",
            "\n",
            "Global step: 2312,loss: 0.012106433\n",
            "\n",
            "Global step: 2313,loss: 0.014617943\n",
            "\n",
            "Global step: 2314,loss: 0.012499841\n",
            "\n",
            "Global step: 2315,loss: 0.010956621\n",
            "\n",
            "Global step: 2316,loss: 0.011024258\n",
            "\n",
            "Global step: 2317,loss: 0.010407572\n",
            "\n",
            "Global step: 2318,loss: 0.011041102\n",
            "\n",
            "Global step: 2319,loss: 0.012079205\n",
            "\n",
            "Global step: 2320,loss: 0.010628032\n",
            "\n",
            "Global step: 2321,loss: 0.013659775\n",
            "\n",
            "Global step: 2322,loss: 0.009568005\n",
            "\n",
            "Global step: 2323,loss: 0.009242907\n",
            "\n",
            "Global step: 2324,loss: 0.01310898\n",
            "\n",
            "Global step: 2325,loss: 0.011261339\n",
            "\n",
            "Global step: 2326,loss: 0.009942171\n",
            "\n",
            "Global step: 2327,loss: 0.012294583\n",
            "\n",
            "Global step: 2328,loss: 0.011444506\n",
            "\n",
            "Global step: 2329,loss: 0.009390522\n",
            "\n",
            "Global step: 2330,loss: 0.009911058\n",
            "\n",
            "Global step: 2331,loss: 0.013818758\n",
            "\n",
            "Global step: 2332,loss: 0.015637465\n",
            "\n",
            "Global step: 2333,loss: 0.01117298\n",
            "\n",
            "Global step: 2334,loss: 0.0129832765\n",
            "\n",
            "Global step: 2335,loss: 0.010680379\n",
            "\n",
            "Global step: 2336,loss: 0.012293749\n",
            "\n",
            "Global step: 2337,loss: 0.0167153\n",
            "\n",
            "Global step: 2338,loss: 0.014679721\n",
            "\n",
            "Global step: 2339,loss: 0.011977838\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2340.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:10:52.408922 139735711573760 supervisor.py:1050] Recording summary at step 2340.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.50065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:10:52.517621 139735703181056 supervisor.py:1099] global_step/sec: 6.50065\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2339,val_loss: 0.022739204972122725\n",
            "\n",
            "Training for epoch 7/16:\n",
            "Global step: 2340,loss: 0.010630598\n",
            "\n",
            "Global step: 2341,loss: 0.012273112\n",
            "\n",
            "Global step: 2342,loss: 0.011762954\n",
            "\n",
            "Global step: 2343,loss: 0.015694771\n",
            "\n",
            "Global step: 2344,loss: 0.011200462\n",
            "\n",
            "Global step: 2345,loss: 0.012138108\n",
            "\n",
            "Global step: 2346,loss: 0.011894616\n",
            "\n",
            "Global step: 2347,loss: 0.010099301\n",
            "\n",
            "Global step: 2348,loss: 0.011609149\n",
            "\n",
            "Global step: 2349,loss: 0.011145353\n",
            "\n",
            "Global step: 2350,loss: 0.011486268\n",
            "\n",
            "Global step: 2351,loss: 0.013307262\n",
            "\n",
            "Global step: 2352,loss: 0.010312446\n",
            "\n",
            "Global step: 2353,loss: 0.009940709\n",
            "\n",
            "Global step: 2354,loss: 0.01175338\n",
            "\n",
            "Global step: 2355,loss: 0.010545569\n",
            "\n",
            "Global step: 2356,loss: 0.011648551\n",
            "\n",
            "Global step: 2357,loss: 0.01307735\n",
            "\n",
            "Global step: 2358,loss: 0.013217742\n",
            "\n",
            "Global step: 2359,loss: 0.01449094\n",
            "\n",
            "Global step: 2360,loss: 0.010532947\n",
            "\n",
            "Global step: 2361,loss: 0.01099764\n",
            "\n",
            "Global step: 2362,loss: 0.009365695\n",
            "\n",
            "Global step: 2363,loss: 0.013005243\n",
            "\n",
            "Global step: 2364,loss: 0.009861635\n",
            "\n",
            "Global step: 2365,loss: 0.008751113\n",
            "\n",
            "Global step: 2366,loss: 0.012547067\n",
            "\n",
            "Global step: 2367,loss: 0.0104807615\n",
            "\n",
            "Global step: 2368,loss: 0.010311917\n",
            "\n",
            "Global step: 2369,loss: 0.01008479\n",
            "\n",
            "Global step: 2370,loss: 0.01113642\n",
            "\n",
            "Global step: 2371,loss: 0.010010906\n",
            "\n",
            "Global step: 2372,loss: 0.010631364\n",
            "\n",
            "Global step: 2373,loss: 0.009794194\n",
            "\n",
            "Global step: 2374,loss: 0.013357765\n",
            "\n",
            "Global step: 2375,loss: 0.010034762\n",
            "\n",
            "Global step: 2376,loss: 0.011235577\n",
            "\n",
            "Global step: 2377,loss: 0.013230886\n",
            "\n",
            "Global step: 2378,loss: 0.016994081\n",
            "\n",
            "Global step: 2379,loss: 0.012416799\n",
            "\n",
            "Global step: 2380,loss: 0.010506917\n",
            "\n",
            "Global step: 2381,loss: 0.010450755\n",
            "\n",
            "Global step: 2382,loss: 0.012320308\n",
            "\n",
            "Global step: 2383,loss: 0.011430147\n",
            "\n",
            "Global step: 2384,loss: 0.010260292\n",
            "\n",
            "Global step: 2385,loss: 0.01430984\n",
            "\n",
            "Global step: 2386,loss: 0.009781047\n",
            "\n",
            "Global step: 2387,loss: 0.009770384\n",
            "\n",
            "Global step: 2388,loss: 0.010205566\n",
            "\n",
            "Global step: 2389,loss: 0.015584237\n",
            "\n",
            "Global step: 2390,loss: 0.010815285\n",
            "\n",
            "Global step: 2391,loss: 0.009477011\n",
            "\n",
            "Global step: 2392,loss: 0.009606396\n",
            "\n",
            "Global step: 2393,loss: 0.01194096\n",
            "\n",
            "Global step: 2394,loss: 0.011182858\n",
            "\n",
            "Global step: 2395,loss: 0.010107891\n",
            "\n",
            "Global step: 2396,loss: 0.010058969\n",
            "\n",
            "Global step: 2397,loss: 0.011364573\n",
            "\n",
            "Global step: 2398,loss: 0.010149354\n",
            "\n",
            "Global step: 2399,loss: 0.01016015\n",
            "\n",
            "Global step: 2400,loss: 0.012407683\n",
            "\n",
            "Global step: 2401,loss: 0.009910199\n",
            "\n",
            "Global step: 2402,loss: 0.010766696\n",
            "\n",
            "Global step: 2403,loss: 0.00983393\n",
            "\n",
            "Global step: 2404,loss: 0.009613594\n",
            "\n",
            "Global step: 2405,loss: 0.009446703\n",
            "\n",
            "Global step: 2406,loss: 0.012527773\n",
            "\n",
            "Global step: 2407,loss: 0.009895185\n",
            "\n",
            "Global step: 2408,loss: 0.01852421\n",
            "\n",
            "Global step: 2409,loss: 0.009987008\n",
            "\n",
            "Global step: 2410,loss: 0.010222081\n",
            "\n",
            "Global step: 2411,loss: 0.010568119\n",
            "\n",
            "Global step: 2412,loss: 0.0127294855\n",
            "\n",
            "Global step: 2413,loss: 0.010377258\n",
            "\n",
            "Global step: 2414,loss: 0.010070295\n",
            "\n",
            "Global step: 2415,loss: 0.010762535\n",
            "\n",
            "Global step: 2416,loss: 0.013183986\n",
            "\n",
            "Global step: 2417,loss: 0.009657277\n",
            "\n",
            "Global step: 2418,loss: 0.009110106\n",
            "\n",
            "Global step: 2419,loss: 0.009359846\n",
            "\n",
            "Global step: 2420,loss: 0.011315938\n",
            "\n",
            "Global step: 2421,loss: 0.009674213\n",
            "\n",
            "Global step: 2422,loss: 0.01326834\n",
            "\n",
            "Global step: 2423,loss: 0.013166472\n",
            "\n",
            "Global step: 2424,loss: 0.009856496\n",
            "\n",
            "Global step: 2425,loss: 0.008902576\n",
            "\n",
            "Global step: 2426,loss: 0.011366024\n",
            "\n",
            "Global step: 2427,loss: 0.009365055\n",
            "\n",
            "Global step: 2428,loss: 0.009992864\n",
            "\n",
            "Global step: 2429,loss: 0.008715477\n",
            "\n",
            "Global step: 2430,loss: 0.010778772\n",
            "\n",
            "Global step: 2431,loss: 0.012020456\n",
            "\n",
            "Global step: 2432,loss: 0.012009617\n",
            "\n",
            "Global step: 2433,loss: 0.010332259\n",
            "\n",
            "Global step: 2434,loss: 0.010236206\n",
            "\n",
            "Global step: 2435,loss: 0.012313113\n",
            "\n",
            "Global step: 2436,loss: 0.011301046\n",
            "\n",
            "Global step: 2437,loss: 0.011072986\n",
            "\n",
            "Global step: 2438,loss: 0.009781965\n",
            "\n",
            "Global step: 2439,loss: 0.0092142625\n",
            "\n",
            "Global step: 2440,loss: 0.00922595\n",
            "\n",
            "Global step: 2441,loss: 0.010100519\n",
            "\n",
            "Global step: 2442,loss: 0.009596486\n",
            "\n",
            "Global step: 2443,loss: 0.008937813\n",
            "\n",
            "Global step: 2444,loss: 0.012741792\n",
            "\n",
            "Global step: 2445,loss: 0.010579697\n",
            "\n",
            "Global step: 2446,loss: 0.010777236\n",
            "\n",
            "Global step: 2447,loss: 0.015149413\n",
            "\n",
            "Global step: 2448,loss: 0.013667002\n",
            "\n",
            "Global step: 2449,loss: 0.010396309\n",
            "\n",
            "Global step: 2450,loss: 0.009699485\n",
            "\n",
            "Global step: 2451,loss: 0.010274034\n",
            "\n",
            "Global step: 2452,loss: 0.012259493\n",
            "\n",
            "Global step: 2453,loss: 0.010509636\n",
            "\n",
            "Global step: 2454,loss: 0.011466004\n",
            "\n",
            "Global step: 2455,loss: 0.009876713\n",
            "\n",
            "Global step: 2456,loss: 0.010129262\n",
            "\n",
            "Global step: 2457,loss: 0.01216564\n",
            "\n",
            "Global step: 2458,loss: 0.009698894\n",
            "\n",
            "Global step: 2459,loss: 0.01041556\n",
            "\n",
            "Global step: 2460,loss: 0.010255901\n",
            "\n",
            "Global step: 2461,loss: 0.010850457\n",
            "\n",
            "Global step: 2462,loss: 0.009214458\n",
            "\n",
            "Global step: 2463,loss: 0.011025263\n",
            "\n",
            "Global step: 2464,loss: 0.010781686\n",
            "\n",
            "Global step: 2465,loss: 0.009581214\n",
            "\n",
            "Global step: 2466,loss: 0.011171149\n",
            "\n",
            "Global step: 2467,loss: 0.009757004\n",
            "\n",
            "Global step: 2468,loss: 0.008985744\n",
            "\n",
            "Global step: 2469,loss: 0.009086353\n",
            "\n",
            "Global step: 2470,loss: 0.01121652\n",
            "\n",
            "Global step: 2471,loss: 0.012262341\n",
            "\n",
            "Global step: 2472,loss: 0.010729888\n",
            "\n",
            "Global step: 2473,loss: 0.011772473\n",
            "\n",
            "Global step: 2474,loss: 0.0109360125\n",
            "\n",
            "Global step: 2475,loss: 0.010410144\n",
            "\n",
            "Global step: 2476,loss: 0.011405539\n",
            "\n",
            "Global step: 2477,loss: 0.011991516\n",
            "\n",
            "Global step: 2478,loss: 0.013557794\n",
            "\n",
            "Global step: 2479,loss: 0.011019336\n",
            "\n",
            "Global step: 2480,loss: 0.013079018\n",
            "\n",
            "Global step: 2481,loss: 0.015907554\n",
            "\n",
            "Global step: 2482,loss: 0.010162042\n",
            "\n",
            "Global step: 2483,loss: 0.011294916\n",
            "\n",
            "Global step: 2484,loss: 0.013179066\n",
            "\n",
            "Global step: 2485,loss: 0.010876348\n",
            "\n",
            "Global step: 2486,loss: 0.017279036\n",
            "\n",
            "Global step: 2487,loss: 0.012761597\n",
            "\n",
            "Global step: 2488,loss: 0.009728698\n",
            "\n",
            "Global step: 2489,loss: 0.011612583\n",
            "\n",
            "Global step: 2490,loss: 0.010062515\n",
            "\n",
            "Global step: 2491,loss: 0.010708303\n",
            "\n",
            "Global step: 2492,loss: 0.010137083\n",
            "\n",
            "Global step: 2493,loss: 0.009200109\n",
            "\n",
            "Global step: 2494,loss: 0.01693127\n",
            "\n",
            "Global step: 2495,loss: 0.0102544585\n",
            "\n",
            "Global step: 2496,loss: 0.009028867\n",
            "\n",
            "Global step: 2497,loss: 0.010375648\n",
            "\n",
            "Global step: 2498,loss: 0.010313236\n",
            "\n",
            "Global step: 2499,loss: 0.011230903\n",
            "\n",
            "Global step: 2500,loss: 0.009360163\n",
            "\n",
            "Global step: 2501,loss: 0.010323762\n",
            "\n",
            "Global step: 2502,loss: 0.010513385\n",
            "\n",
            "Global step: 2503,loss: 0.012337993\n",
            "\n",
            "Global step: 2504,loss: 0.009906627\n",
            "\n",
            "Global step: 2505,loss: 0.014728538\n",
            "\n",
            "Global step: 2506,loss: 0.017765932\n",
            "\n",
            "Global step: 2507,loss: 0.015778547\n",
            "\n",
            "Global step: 2508,loss: 0.010894235\n",
            "\n",
            "Global step: 2509,loss: 0.009645979\n",
            "\n",
            "Global step: 2510,loss: 0.013382422\n",
            "\n",
            "Global step: 2511,loss: 0.0097834\n",
            "\n",
            "Global step: 2512,loss: 0.010396103\n",
            "\n",
            "Global step: 2513,loss: 0.013265168\n",
            "\n",
            "Global step: 2514,loss: 0.009602417\n",
            "\n",
            "Global step: 2515,loss: 0.009191818\n",
            "\n",
            "Global step: 2516,loss: 0.009775312\n",
            "\n",
            "Global step: 2517,loss: 0.00867266\n",
            "\n",
            "Global step: 2518,loss: 0.010746666\n",
            "\n",
            "Global step: 2519,loss: 0.010354577\n",
            "\n",
            "Global step: 2520,loss: 0.010130397\n",
            "\n",
            "Global step: 2521,loss: 0.009525367\n",
            "\n",
            "Global step: 2522,loss: 0.010136771\n",
            "\n",
            "Global step: 2523,loss: 0.011472002\n",
            "\n",
            "Global step: 2524,loss: 0.010203885\n",
            "\n",
            "Global step: 2525,loss: 0.009614451\n",
            "\n",
            "Global step: 2526,loss: 0.009421963\n",
            "\n",
            "Global step: 2527,loss: 0.009218141\n",
            "\n",
            "Global step: 2528,loss: 0.009724615\n",
            "\n",
            "Global step: 2529,loss: 0.0107075395\n",
            "\n",
            "Global step: 2530,loss: 0.014342789\n",
            "\n",
            "Global step: 2531,loss: 0.010499448\n",
            "\n",
            "Global step: 2532,loss: 0.010549281\n",
            "\n",
            "Global step: 2533,loss: 0.01134002\n",
            "\n",
            "Global step: 2534,loss: 0.010166481\n",
            "\n",
            "Global step: 2535,loss: 0.009852767\n",
            "\n",
            "Global step: 2536,loss: 0.013738951\n",
            "\n",
            "Global step: 2537,loss: 0.00971172\n",
            "\n",
            "Global step: 2538,loss: 0.013653632\n",
            "\n",
            "Global step: 2539,loss: 0.015962545\n",
            "\n",
            "Global step: 2540,loss: 0.0133052245\n",
            "\n",
            "Global step: 2541,loss: 0.010983673\n",
            "\n",
            "Global step: 2542,loss: 0.016304355\n",
            "\n",
            "Global step: 2543,loss: 0.0097093135\n",
            "\n",
            "Global step: 2544,loss: 0.010586756\n",
            "\n",
            "Global step: 2545,loss: 0.010515826\n",
            "\n",
            "Global step: 2546,loss: 0.011970577\n",
            "\n",
            "Global step: 2547,loss: 0.012572329\n",
            "\n",
            "Global step: 2548,loss: 0.010456477\n",
            "\n",
            "Global step: 2549,loss: 0.011429093\n",
            "\n",
            "Global step: 2550,loss: 0.009298959\n",
            "\n",
            "Global step: 2551,loss: 0.011179565\n",
            "\n",
            "Global step: 2552,loss: 0.0118604805\n",
            "\n",
            "Global step: 2553,loss: 0.010512411\n",
            "\n",
            "Global step: 2554,loss: 0.015779082\n",
            "\n",
            "Global step: 2555,loss: 0.015809802\n",
            "\n",
            "Global step: 2556,loss: 0.020191781\n",
            "\n",
            "Global step: 2557,loss: 0.010819126\n",
            "\n",
            "Global step: 2558,loss: 0.011451904\n",
            "\n",
            "Global step: 2559,loss: 0.010636423\n",
            "\n",
            "Global step: 2560,loss: 0.013072749\n",
            "\n",
            "Global step: 2561,loss: 0.011986023\n",
            "\n",
            "Global step: 2562,loss: 0.014341662\n",
            "\n",
            "Global step: 2563,loss: 0.011194643\n",
            "\n",
            "Global step: 2564,loss: 0.013743727\n",
            "\n",
            "Global step: 2565,loss: 0.014593461\n",
            "\n",
            "Global step: 2566,loss: 0.01100897\n",
            "\n",
            "Global step: 2567,loss: 0.00986124\n",
            "\n",
            "Global step: 2568,loss: 0.009846712\n",
            "\n",
            "Global step: 2569,loss: 0.013034371\n",
            "\n",
            "Global step: 2570,loss: 0.009864232\n",
            "\n",
            "Global step: 2571,loss: 0.010818021\n",
            "\n",
            "Global step: 2572,loss: 0.018336147\n",
            "\n",
            "Global step: 2573,loss: 0.010446013\n",
            "\n",
            "Global step: 2574,loss: 0.009288431\n",
            "\n",
            "Global step: 2575,loss: 0.0109532\n",
            "\n",
            "Global step: 2576,loss: 0.013091799\n",
            "\n",
            "Global step: 2577,loss: 0.009504685\n",
            "\n",
            "Global step: 2578,loss: 0.010304999\n",
            "\n",
            "Global step: 2579,loss: 0.010643917\n",
            "\n",
            "Global step: 2580,loss: 0.012837769\n",
            "\n",
            "Global step: 2581,loss: 0.010723412\n",
            "\n",
            "Global step: 2582,loss: 0.01123481\n",
            "\n",
            "Global step: 2583,loss: 0.009870261\n",
            "\n",
            "Global step: 2584,loss: 0.008619226\n",
            "\n",
            "Global step: 2585,loss: 0.009223975\n",
            "\n",
            "Global step: 2586,loss: 0.0121932775\n",
            "\n",
            "Global step: 2587,loss: 0.009052951\n",
            "\n",
            "Global step: 2588,loss: 0.008882226\n",
            "\n",
            "Global step: 2589,loss: 0.008830229\n",
            "\n",
            "Global step: 2590,loss: 0.010376966\n",
            "\n",
            "Global step: 2591,loss: 0.01123835\n",
            "\n",
            "Global step: 2592,loss: 0.009675775\n",
            "\n",
            "Global step: 2593,loss: 0.009574131\n",
            "\n",
            "Global step: 2594,loss: 0.009033752\n",
            "\n",
            "Global step: 2595,loss: 0.010455426\n",
            "\n",
            "Global step: 2596,loss: 0.00925804\n",
            "\n",
            "Global step: 2597,loss: 0.011620212\n",
            "\n",
            "Global step: 2598,loss: 0.009005732\n",
            "\n",
            "Global step: 2599,loss: 0.009787478\n",
            "\n",
            "Global step: 2600,loss: 0.0098087145\n",
            "\n",
            "Global step: 2601,loss: 0.009657431\n",
            "\n",
            "Global step: 2602,loss: 0.010754886\n",
            "\n",
            "Global step: 2603,loss: 0.010375427\n",
            "\n",
            "Global step: 2604,loss: 0.01665024\n",
            "\n",
            "Global step: 2605,loss: 0.009559753\n",
            "\n",
            "Global step: 2606,loss: 0.013973764\n",
            "\n",
            "Global step: 2607,loss: 0.014922451\n",
            "\n",
            "Global step: 2608,loss: 0.010190937\n",
            "\n",
            "Global step: 2609,loss: 0.010255529\n",
            "\n",
            "Global step: 2610,loss: 0.01031687\n",
            "\n",
            "Global step: 2611,loss: 0.009417772\n",
            "\n",
            "Global step: 2612,loss: 0.014732766\n",
            "\n",
            "Global step: 2613,loss: 0.011511039\n",
            "\n",
            "Global step: 2614,loss: 0.010800067\n",
            "\n",
            "Global step: 2615,loss: 0.018780418\n",
            "\n",
            "Global step: 2616,loss: 0.013199046\n",
            "\n",
            "Global step: 2617,loss: 0.009758981\n",
            "\n",
            "Global step: 2618,loss: 0.010385521\n",
            "\n",
            "Global step: 2619,loss: 0.014200783\n",
            "\n",
            "Global step: 2620,loss: 0.0093672015\n",
            "\n",
            "Global step: 2621,loss: 0.010795774\n",
            "\n",
            "Global step: 2622,loss: 0.016252583\n",
            "\n",
            "Global step: 2623,loss: 0.016079612\n",
            "\n",
            "Global step: 2624,loss: 0.010188101\n",
            "\n",
            "Global step: 2625,loss: 0.009272929\n",
            "\n",
            "Global step: 2626,loss: 0.012791797\n",
            "\n",
            "Global step: 2627,loss: 0.01009479\n",
            "\n",
            "Global step: 2628,loss: 0.013548775\n",
            "\n",
            "Global step: 2629,loss: 0.010684717\n",
            "\n",
            "Global step: 2630,loss: 0.009659993\n",
            "\n",
            "Global step: 2631,loss: 0.009353265\n",
            "\n",
            "Global step: 2632,loss: 0.0125856325\n",
            "\n",
            "Global step: 2633,loss: 0.012803421\n",
            "\n",
            "Global step: 2634,loss: 0.0108771\n",
            "\n",
            "Global step: 2635,loss: 0.009457475\n",
            "\n",
            "Global step: 2636,loss: 0.011412447\n",
            "\n",
            "Global step: 2637,loss: 0.010999644\n",
            "\n",
            "Global step: 2638,loss: 0.009368949\n",
            "\n",
            "Global step: 2639,loss: 0.009529635\n",
            "\n",
            "Global step: 2640,loss: 0.009533527\n",
            "\n",
            "Global step: 2641,loss: 0.01176765\n",
            "\n",
            "Global step: 2642,loss: 0.009831229\n",
            "\n",
            "Global step: 2643,loss: 0.010061843\n",
            "\n",
            "Global step: 2644,loss: 0.010716324\n",
            "\n",
            "Global step: 2645,loss: 0.010860637\n",
            "\n",
            "Global step: 2646,loss: 0.00999469\n",
            "\n",
            "Global step: 2647,loss: 0.009939728\n",
            "\n",
            "Global step: 2648,loss: 0.009338788\n",
            "\n",
            "Global step: 2649,loss: 0.015502474\n",
            "\n",
            "Global step: 2650,loss: 0.009281077\n",
            "\n",
            "Global step: 2651,loss: 0.012246352\n",
            "\n",
            "Global step: 2652,loss: 0.01333851\n",
            "\n",
            "Global step: 2653,loss: 0.011293263\n",
            "\n",
            "Global step: 2654,loss: 0.010358788\n",
            "\n",
            "Global step: 2655,loss: 0.009404138\n",
            "\n",
            "Global step: 2656,loss: 0.014422693\n",
            "\n",
            "Global step: 2657,loss: 0.009849659\n",
            "\n",
            "Global step: 2658,loss: 0.009888624\n",
            "\n",
            "Global step: 2659,loss: 0.00871214\n",
            "\n",
            "Global step: 2660,loss: 0.00966269\n",
            "\n",
            "Global step: 2661,loss: 0.012776816\n",
            "\n",
            "Global step: 2662,loss: 0.009487611\n",
            "\n",
            "Global step: 2663,loss: 0.0114423605\n",
            "\n",
            "Global step: 2664,loss: 0.011869354\n",
            "\n",
            "Global step: 2665,loss: 0.0105522415\n",
            "\n",
            "Global step: 2666,loss: 0.009551478\n",
            "\n",
            "Global step: 2667,loss: 0.010159861\n",
            "\n",
            "Global step: 2668,loss: 0.011594545\n",
            "\n",
            "Global step: 2669,loss: 0.009404538\n",
            "\n",
            "Global step: 2670,loss: 0.010029374\n",
            "\n",
            "Global step: 2671,loss: 0.010466633\n",
            "\n",
            "Global step: 2672,loss: 0.0102685215\n",
            "\n",
            "Global step: 2673,loss: 0.011604557\n",
            "\n",
            "Global step: 2674,loss: 0.010873424\n",
            "\n",
            "Global step: 2675,loss: 0.011652755\n",
            "\n",
            "Global step: 2676,loss: 0.011711872\n",
            "\n",
            "Global step: 2677,loss: 0.012913175\n",
            "\n",
            "Global step: 2678,loss: 0.009857536\n",
            "\n",
            "Global step: 2679,loss: 0.009865987\n",
            "\n",
            "Global step: 2680,loss: 0.009986794\n",
            "\n",
            "Global step: 2681,loss: 0.017381541\n",
            "\n",
            "Global step: 2682,loss: 0.009489576\n",
            "\n",
            "Global step: 2683,loss: 0.009585987\n",
            "\n",
            "Global step: 2684,loss: 0.010834298\n",
            "\n",
            "Global step: 2685,loss: 0.022207528\n",
            "\n",
            "Global step: 2686,loss: 0.008979126\n",
            "\n",
            "Global step: 2687,loss: 0.010443062\n",
            "\n",
            "Global step: 2688,loss: 0.011196796\n",
            "\n",
            "Global step: 2689,loss: 0.00933857\n",
            "\n",
            "Global step: 2690,loss: 0.010223414\n",
            "\n",
            "Global step: 2691,loss: 0.0095327785\n",
            "\n",
            "Global step: 2692,loss: 0.010404091\n",
            "\n",
            "Global step: 2693,loss: 0.011660042\n",
            "\n",
            "Global step: 2694,loss: 0.0137677\n",
            "\n",
            "Global step: 2695,loss: 0.0113450885\n",
            "\n",
            "Global step: 2696,loss: 0.011681812\n",
            "\n",
            "Global step: 2697,loss: 0.010303076\n",
            "\n",
            "Global step: 2698,loss: 0.010314645\n",
            "\n",
            "Global step: 2699,loss: 0.01053678\n",
            "\n",
            "Global step: 2700,loss: 0.02086835\n",
            "\n",
            "Global step: 2701,loss: 0.011759145\n",
            "\n",
            "Global step: 2702,loss: 0.011493472\n",
            "\n",
            "Global step: 2703,loss: 0.011160251\n",
            "\n",
            "Global step: 2704,loss: 0.009929885\n",
            "\n",
            "Global step: 2705,loss: 0.011117345\n",
            "\n",
            "Global step: 2706,loss: 0.010559264\n",
            "\n",
            "Global step: 2707,loss: 0.009485315\n",
            "\n",
            "Global step: 2708,loss: 0.010964276\n",
            "\n",
            "Global step: 2709,loss: 0.011366342\n",
            "\n",
            "Global step: 2710,loss: 0.009859856\n",
            "\n",
            "Global step: 2711,loss: 0.0108483145\n",
            "\n",
            "Global step: 2712,loss: 0.0107705565\n",
            "\n",
            "Global step: 2713,loss: 0.01092664\n",
            "\n",
            "Global step: 2714,loss: 0.014863303\n",
            "\n",
            "Global step: 2715,loss: 0.010424636\n",
            "\n",
            "Global step: 2716,loss: 0.010656202\n",
            "\n",
            "Global step: 2717,loss: 0.013691951\n",
            "\n",
            "Global step: 2718,loss: 0.012871323\n",
            "\n",
            "Global step: 2719,loss: 0.012896666\n",
            "\n",
            "Global step: 2720,loss: 0.010486714\n",
            "\n",
            "Global step: 2721,loss: 0.009782455\n",
            "\n",
            "Global step: 2722,loss: 0.010446777\n",
            "\n",
            "Global step: 2723,loss: 0.010686006\n",
            "\n",
            "Global step: 2724,loss: 0.009505415\n",
            "\n",
            "Global step: 2725,loss: 0.009714816\n",
            "\n",
            "Global step: 2726,loss: 0.008548965\n",
            "\n",
            "Global step: 2727,loss: 0.009265617\n",
            "\n",
            "Global step: 2728,loss: 0.012695523\n",
            "\n",
            "Global step: 2729,loss: 0.009732309\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 2729,val_loss: 0.02066408263710447\n",
            "\n",
            "Training for epoch 8/16:\n",
            "Global step: 2730,loss: 0.009032168\n",
            "\n",
            "Global step: 2731,loss: 0.012996841\n",
            "\n",
            "Global step: 2732,loss: 0.009040504\n",
            "\n",
            "Global step: 2733,loss: 0.009202975\n",
            "\n",
            "Global step: 2734,loss: 0.009848558\n",
            "\n",
            "Global step: 2735,loss: 0.009139418\n",
            "\n",
            "Global step: 2736,loss: 0.00852211\n",
            "\n",
            "Global step: 2737,loss: 0.008892814\n",
            "\n",
            "Global step: 2738,loss: 0.009935524\n",
            "\n",
            "Global step: 2739,loss: 0.01035963\n",
            "\n",
            "Global step: 2740,loss: 0.009055383\n",
            "\n",
            "Global step: 2741,loss: 0.01109589\n",
            "\n",
            "Global step: 2742,loss: 0.00816867\n",
            "\n",
            "Global step: 2743,loss: 0.008883106\n",
            "\n",
            "Global step: 2744,loss: 0.0090871435\n",
            "\n",
            "Global step: 2745,loss: 0.009078524\n",
            "\n",
            "Global step: 2746,loss: 0.011117315\n",
            "\n",
            "Global step: 2747,loss: 0.010990736\n",
            "\n",
            "Global step: 2748,loss: 0.011280301\n",
            "\n",
            "Global step: 2749,loss: 0.009626785\n",
            "\n",
            "Global step: 2750,loss: 0.010109735\n",
            "\n",
            "Global step: 2751,loss: 0.008500508\n",
            "\n",
            "Global step: 2752,loss: 0.008672488\n",
            "\n",
            "Global step: 2753,loss: 0.010847113\n",
            "\n",
            "Global step: 2754,loss: 0.010117944\n",
            "\n",
            "Global step: 2755,loss: 0.010205926\n",
            "\n",
            "Global step: 2756,loss: 0.009690913\n",
            "\n",
            "Global step: 2757,loss: 0.008473761\n",
            "\n",
            "Global step: 2758,loss: 0.009995937\n",
            "\n",
            "Global step: 2759,loss: 0.008179962\n",
            "\n",
            "Global step: 2760,loss: 0.009007121\n",
            "\n",
            "Global step: 2761,loss: 0.010945823\n",
            "\n",
            "Global step: 2762,loss: 0.008903267\n",
            "\n",
            "Global step: 2763,loss: 0.012663912\n",
            "\n",
            "Global step: 2764,loss: 0.008743682\n",
            "\n",
            "Global step: 2765,loss: 0.010133458\n",
            "\n",
            "Global step: 2766,loss: 0.0089711705\n",
            "\n",
            "Global step: 2767,loss: 0.008607552\n",
            "\n",
            "Global step: 2768,loss: 0.0100304475\n",
            "\n",
            "Global step: 2769,loss: 0.009003165\n",
            "\n",
            "Global step: 2770,loss: 0.009211895\n",
            "\n",
            "Global step: 2771,loss: 0.009917754\n",
            "\n",
            "Global step: 2772,loss: 0.009277298\n",
            "\n",
            "Global step: 2773,loss: 0.0085215755\n",
            "\n",
            "Global step: 2774,loss: 0.012147442\n",
            "\n",
            "Global step: 2775,loss: 0.010929973\n",
            "\n",
            "Global step: 2776,loss: 0.008776163\n",
            "\n",
            "Global step: 2777,loss: 0.009574577\n",
            "\n",
            "Global step: 2778,loss: 0.009918619\n",
            "\n",
            "Global step: 2779,loss: 0.008357844\n",
            "\n",
            "Global step: 2780,loss: 0.009821996\n",
            "\n",
            "Global step: 2781,loss: 0.008777164\n",
            "\n",
            "Global step: 2782,loss: 0.008845246\n",
            "\n",
            "Global step: 2783,loss: 0.008344778\n",
            "\n",
            "Global step: 2784,loss: 0.0082924375\n",
            "\n",
            "Global step: 2785,loss: 0.00930332\n",
            "\n",
            "Global step: 2786,loss: 0.009663058\n",
            "\n",
            "Global step: 2787,loss: 0.009463807\n",
            "\n",
            "Global step: 2788,loss: 0.009210591\n",
            "\n",
            "Global step: 2789,loss: 0.00914449\n",
            "\n",
            "Global step: 2790,loss: 0.00925692\n",
            "\n",
            "Global step: 2791,loss: 0.017319502\n",
            "\n",
            "Global step: 2792,loss: 0.009016772\n",
            "\n",
            "Global step: 2793,loss: 0.008481642\n",
            "\n",
            "Global step: 2794,loss: 0.009808628\n",
            "\n",
            "Global step: 2795,loss: 0.009203275\n",
            "\n",
            "Global step: 2796,loss: 0.012614314\n",
            "\n",
            "Global step: 2797,loss: 0.011098455\n",
            "\n",
            "Global step: 2798,loss: 0.010287348\n",
            "\n",
            "Global step: 2799,loss: 0.01014353\n",
            "\n",
            "Global step: 2800,loss: 0.009015926\n",
            "\n",
            "Global step: 2801,loss: 0.009379926\n",
            "\n",
            "Global step: 2802,loss: 0.009584889\n",
            "\n",
            "Global step: 2803,loss: 0.009665591\n",
            "\n",
            "Global step: 2804,loss: 0.012162892\n",
            "\n",
            "Global step: 2805,loss: 0.009458199\n",
            "\n",
            "Global step: 2806,loss: 0.008900672\n",
            "\n",
            "Global step: 2807,loss: 0.011287458\n",
            "\n",
            "Global step: 2808,loss: 0.013252584\n",
            "\n",
            "Global step: 2809,loss: 0.013303429\n",
            "\n",
            "Global step: 2810,loss: 0.008672319\n",
            "\n",
            "Global step: 2811,loss: 0.008240029\n",
            "\n",
            "Global step: 2812,loss: 0.010213985\n",
            "\n",
            "Global step: 2813,loss: 0.00870135\n",
            "\n",
            "Global step: 2814,loss: 0.008729996\n",
            "\n",
            "Global step: 2815,loss: 0.014610682\n",
            "\n",
            "Global step: 2816,loss: 0.010140349\n",
            "\n",
            "Global step: 2817,loss: 0.009505467\n",
            "\n",
            "Global step: 2818,loss: 0.010544285\n",
            "\n",
            "Global step: 2819,loss: 0.008728941\n",
            "\n",
            "Global step: 2820,loss: 0.008803301\n",
            "\n",
            "Global step: 2821,loss: 0.016261721\n",
            "\n",
            "Global step: 2822,loss: 0.009036098\n",
            "\n",
            "Global step: 2823,loss: 0.009566995\n",
            "\n",
            "Global step: 2824,loss: 0.0109507805\n",
            "\n",
            "Global step: 2825,loss: 0.009158724\n",
            "\n",
            "Global step: 2826,loss: 0.010373028\n",
            "\n",
            "Global step: 2827,loss: 0.010680404\n",
            "\n",
            "Global step: 2828,loss: 0.008803936\n",
            "\n",
            "Global step: 2829,loss: 0.009638494\n",
            "\n",
            "Global step: 2830,loss: 0.008197733\n",
            "\n",
            "Global step: 2831,loss: 0.008600725\n",
            "\n",
            "Global step: 2832,loss: 0.008933446\n",
            "\n",
            "Global step: 2833,loss: 0.008133892\n",
            "\n",
            "Global step: 2834,loss: 0.00859562\n",
            "\n",
            "Global step: 2835,loss: 0.009262072\n",
            "\n",
            "Global step: 2836,loss: 0.009090299\n",
            "\n",
            "Global step: 2837,loss: 0.0113514485\n",
            "\n",
            "Global step: 2838,loss: 0.012261086\n",
            "\n",
            "Global step: 2839,loss: 0.008443248\n",
            "\n",
            "Global step: 2840,loss: 0.008849198\n",
            "\n",
            "Global step: 2841,loss: 0.009192532\n",
            "\n",
            "Global step: 2842,loss: 0.009636983\n",
            "\n",
            "Global step: 2843,loss: 0.009000487\n",
            "\n",
            "Global step: 2844,loss: 0.009619998\n",
            "\n",
            "Global step: 2845,loss: 0.011417731\n",
            "\n",
            "Global step: 2846,loss: 0.00923592\n",
            "\n",
            "Global step: 2847,loss: 0.008615809\n",
            "\n",
            "Global step: 2848,loss: 0.009728181\n",
            "\n",
            "Global step: 2849,loss: 0.00942242\n",
            "\n",
            "Global step: 2850,loss: 0.01249258\n",
            "\n",
            "Global step: 2851,loss: 0.009057164\n",
            "\n",
            "Global step: 2852,loss: 0.008955223\n",
            "\n",
            "Global step: 2853,loss: 0.009465866\n",
            "\n",
            "Global step: 2854,loss: 0.008961965\n",
            "\n",
            "Global step: 2855,loss: 0.0100010475\n",
            "\n",
            "Global step: 2856,loss: 0.008607514\n",
            "\n",
            "Global step: 2857,loss: 0.010094771\n",
            "\n",
            "Global step: 2858,loss: 0.014035587\n",
            "\n",
            "Global step: 2859,loss: 0.009833898\n",
            "\n",
            "Global step: 2860,loss: 0.009040375\n",
            "\n",
            "Global step: 2861,loss: 0.009370536\n",
            "\n",
            "Global step: 2862,loss: 0.008968453\n",
            "\n",
            "Global step: 2863,loss: 0.009542568\n",
            "\n",
            "Global step: 2864,loss: 0.009405568\n",
            "\n",
            "Global step: 2865,loss: 0.010046817\n",
            "\n",
            "Global step: 2866,loss: 0.009126579\n",
            "\n",
            "Global step: 2867,loss: 0.008551299\n",
            "\n",
            "Global step: 2868,loss: 0.0138596855\n",
            "\n",
            "Global step: 2869,loss: 0.009306164\n",
            "\n",
            "Global step: 2870,loss: 0.008526061\n",
            "\n",
            "Global step: 2871,loss: 0.009791949\n",
            "\n",
            "Global step: 2872,loss: 0.010285126\n",
            "\n",
            "Global step: 2873,loss: 0.008963117\n",
            "\n",
            "Global step: 2874,loss: 0.012019124\n",
            "\n",
            "Global step: 2875,loss: 0.0088137975\n",
            "\n",
            "Global step: 2876,loss: 0.010192282\n",
            "\n",
            "Global step: 2877,loss: 0.009216733\n",
            "\n",
            "Global step: 2878,loss: 0.008915395\n",
            "\n",
            "Global step: 2879,loss: 0.011135143\n",
            "\n",
            "Global step: 2880,loss: 0.008737504\n",
            "\n",
            "Global step: 2881,loss: 0.008150611\n",
            "\n",
            "Global step: 2882,loss: 0.008735606\n",
            "\n",
            "Global step: 2883,loss: 0.0092934985\n",
            "\n",
            "Global step: 2884,loss: 0.008896911\n",
            "\n",
            "Global step: 2885,loss: 0.0095695285\n",
            "\n",
            "Global step: 2886,loss: 0.009310754\n",
            "\n",
            "Global step: 2887,loss: 0.00879611\n",
            "\n",
            "Global step: 2888,loss: 0.008379823\n",
            "\n",
            "Global step: 2889,loss: 0.009569902\n",
            "\n",
            "Global step: 2890,loss: 0.009359787\n",
            "\n",
            "Global step: 2891,loss: 0.008599705\n",
            "\n",
            "Global step: 2892,loss: 0.012297351\n",
            "\n",
            "Global step: 2893,loss: 0.013532354\n",
            "\n",
            "Global step: 2894,loss: 0.0098935\n",
            "\n",
            "Global step: 2895,loss: 0.008406647\n",
            "\n",
            "Global step: 2896,loss: 0.009118299\n",
            "\n",
            "Global step: 2897,loss: 0.009615647\n",
            "\n",
            "Global step: 2898,loss: 0.017053476\n",
            "\n",
            "Global step: 2899,loss: 0.010791514\n",
            "\n",
            "Global step: 2900,loss: 0.00998337\n",
            "\n",
            "Global step: 2901,loss: 0.009984527\n",
            "\n",
            "Global step: 2902,loss: 0.00945197\n",
            "\n",
            "Global step: 2903,loss: 0.009106998\n",
            "\n",
            "Global step: 2904,loss: 0.008743806\n",
            "\n",
            "Global step: 2905,loss: 0.009143779\n",
            "\n",
            "Global step: 2906,loss: 0.01069371\n",
            "\n",
            "Global step: 2907,loss: 0.010534763\n",
            "\n",
            "Global step: 2908,loss: 0.009247376\n",
            "\n",
            "Global step: 2909,loss: 0.008969067\n",
            "\n",
            "Global step: 2910,loss: 0.012526358\n",
            "\n",
            "Global step: 2911,loss: 0.008879047\n",
            "\n",
            "Global step: 2912,loss: 0.012161738\n",
            "\n",
            "Global step: 2913,loss: 0.009286062\n",
            "\n",
            "Global step: 2914,loss: 0.008863618\n",
            "\n",
            "Global step: 2915,loss: 0.009192728\n",
            "\n",
            "Global step: 2916,loss: 0.009687845\n",
            "\n",
            "Global step: 2917,loss: 0.008817306\n",
            "\n",
            "Global step: 2918,loss: 0.012861983\n",
            "\n",
            "Global step: 2919,loss: 0.010380669\n",
            "\n",
            "Global step: 2920,loss: 0.008970132\n",
            "\n",
            "Global step: 2921,loss: 0.008766259\n",
            "\n",
            "Global step: 2922,loss: 0.009807661\n",
            "\n",
            "Global step: 2923,loss: 0.0094584115\n",
            "\n",
            "Global step: 2924,loss: 0.009991\n",
            "\n",
            "Global step: 2925,loss: 0.009110872\n",
            "\n",
            "Global step: 2926,loss: 0.009959256\n",
            "\n",
            "Global step: 2927,loss: 0.012075923\n",
            "\n",
            "Global step: 2928,loss: 0.010652242\n",
            "\n",
            "Global step: 2929,loss: 0.010979996\n",
            "\n",
            "Global step: 2930,loss: 0.010275902\n",
            "\n",
            "Global step: 2931,loss: 0.01494625\n",
            "\n",
            "Global step: 2932,loss: 0.008192901\n",
            "\n",
            "Global step: 2933,loss: 0.011691134\n",
            "\n",
            "Global step: 2934,loss: 0.013897703\n",
            "\n",
            "Global step: 2935,loss: 0.011542264\n",
            "\n",
            "Global step: 2936,loss: 0.00971003\n",
            "\n",
            "Global step: 2937,loss: 0.010699015\n",
            "\n",
            "Global step: 2938,loss: 0.019655954\n",
            "\n",
            "Global step: 2939,loss: 0.0101876585\n",
            "\n",
            "Global step: 2940,loss: 0.015308097\n",
            "\n",
            "Global step: 2941,loss: 0.010042931\n",
            "\n",
            "Global step: 2942,loss: 0.01126172\n",
            "\n",
            "Global step: 2943,loss: 0.010808904\n",
            "\n",
            "Global step: 2944,loss: 0.013553728\n",
            "\n",
            "Global step: 2945,loss: 0.010675331\n",
            "\n",
            "Global step: 2946,loss: 0.01210224\n",
            "\n",
            "Global step: 2947,loss: 0.008890164\n",
            "\n",
            "Global step: 2948,loss: 0.009780991\n",
            "\n",
            "Global step: 2949,loss: 0.008242001\n",
            "\n",
            "Global step: 2950,loss: 0.0095168\n",
            "\n",
            "Global step: 2951,loss: 0.009171431\n",
            "\n",
            "Global step: 2952,loss: 0.010152181\n",
            "\n",
            "Global step: 2953,loss: 0.010061528\n",
            "\n",
            "Global step: 2954,loss: 0.010454227\n",
            "\n",
            "Global step: 2955,loss: 0.009782005\n",
            "\n",
            "Global step: 2956,loss: 0.009353802\n",
            "\n",
            "Global step: 2957,loss: 0.009697544\n",
            "\n",
            "Global step: 2958,loss: 0.009801654\n",
            "\n",
            "Global step: 2959,loss: 0.00953764\n",
            "\n",
            "Global step: 2960,loss: 0.008989882\n",
            "\n",
            "Global step: 2961,loss: 0.008768098\n",
            "\n",
            "Global step: 2962,loss: 0.013844188\n",
            "\n",
            "Global step: 2963,loss: 0.008023584\n",
            "\n",
            "Global step: 2964,loss: 0.008517654\n",
            "\n",
            "Global step: 2965,loss: 0.009184141\n",
            "\n",
            "Global step: 2966,loss: 0.009638281\n",
            "\n",
            "Global step: 2967,loss: 0.008243506\n",
            "\n",
            "Global step: 2968,loss: 0.010195403\n",
            "\n",
            "Global step: 2969,loss: 0.009460307\n",
            "\n",
            "Global step: 2970,loss: 0.010432622\n",
            "\n",
            "Global step: 2971,loss: 0.00857742\n",
            "\n",
            "Global step: 2972,loss: 0.014606329\n",
            "\n",
            "Global step: 2973,loss: 0.009572438\n",
            "\n",
            "Global step: 2974,loss: 0.01029598\n",
            "\n",
            "Global step: 2975,loss: 0.009353716\n",
            "\n",
            "Global step: 2976,loss: 0.008274535\n",
            "\n",
            "Global step: 2977,loss: 0.009349943\n",
            "\n",
            "Global step: 2978,loss: 0.009433058\n",
            "\n",
            "Global step: 2979,loss: 0.009988279\n",
            "\n",
            "Global step: 2980,loss: 0.008643586\n",
            "\n",
            "Global step: 2981,loss: 0.009401719\n",
            "\n",
            "Global step: 2982,loss: 0.012977097\n",
            "\n",
            "Global step: 2983,loss: 0.009872872\n",
            "\n",
            "Global step: 2984,loss: 0.011527093\n",
            "\n",
            "Global step: 2985,loss: 0.00943426\n",
            "\n",
            "Global step: 2986,loss: 0.009031841\n",
            "\n",
            "Global step: 2987,loss: 0.014901139\n",
            "\n",
            "Global step: 2988,loss: 0.009112154\n",
            "\n",
            "Global step: 2989,loss: 0.00962061\n",
            "\n",
            "Global step: 2990,loss: 0.009261592\n",
            "\n",
            "Global step: 2991,loss: 0.010262408\n",
            "\n",
            "Global step: 2992,loss: 0.011004895\n",
            "\n",
            "Global step: 2993,loss: 0.010637563\n",
            "\n",
            "Global step: 2994,loss: 0.00934986\n",
            "\n",
            "Global step: 2995,loss: 0.01331416\n",
            "\n",
            "Global step: 2996,loss: 0.009581559\n",
            "\n",
            "Global step: 2997,loss: 0.009211129\n",
            "\n",
            "Global step: 2998,loss: 0.010102486\n",
            "\n",
            "Global step: 2999,loss: 0.011002149\n",
            "\n",
            "Global step: 3000,loss: 0.008911392\n",
            "\n",
            "Global step: 3001,loss: 0.0125516355\n",
            "\n",
            "Global step: 3002,loss: 0.0087012\n",
            "\n",
            "Global step: 3003,loss: 0.009385343\n",
            "\n",
            "Global step: 3004,loss: 0.009580197\n",
            "\n",
            "Global step: 3005,loss: 0.009259596\n",
            "\n",
            "Global step: 3006,loss: 0.0093072755\n",
            "\n",
            "Global step: 3007,loss: 0.015594205\n",
            "\n",
            "Global step: 3008,loss: 0.010545261\n",
            "\n",
            "Global step: 3009,loss: 0.009225456\n",
            "\n",
            "Global step: 3010,loss: 0.009530008\n",
            "\n",
            "Global step: 3011,loss: 0.0089300955\n",
            "\n",
            "Global step: 3012,loss: 0.009272897\n",
            "\n",
            "Global step: 3013,loss: 0.008669323\n",
            "\n",
            "Global step: 3014,loss: 0.011167325\n",
            "\n",
            "Global step: 3015,loss: 0.011535304\n",
            "\n",
            "Global step: 3016,loss: 0.010612932\n",
            "\n",
            "Global step: 3017,loss: 0.009633729\n",
            "\n",
            "Global step: 3018,loss: 0.008568778\n",
            "\n",
            "Global step: 3019,loss: 0.009222122\n",
            "\n",
            "Global step: 3020,loss: 0.012560543\n",
            "\n",
            "Global step: 3021,loss: 0.009308002\n",
            "\n",
            "Global step: 3022,loss: 0.009069637\n",
            "\n",
            "Global step: 3023,loss: 0.009517349\n",
            "\n",
            "Global step: 3024,loss: 0.009667847\n",
            "\n",
            "Global step: 3025,loss: 0.009376229\n",
            "\n",
            "Global step: 3026,loss: 0.011849808\n",
            "\n",
            "Global step: 3027,loss: 0.009436455\n",
            "\n",
            "Global step: 3028,loss: 0.008499203\n",
            "\n",
            "Global step: 3029,loss: 0.009179348\n",
            "\n",
            "Global step: 3030,loss: 0.0089531215\n",
            "\n",
            "Global step: 3031,loss: 0.012087306\n",
            "\n",
            "Global step: 3032,loss: 0.0096464455\n",
            "\n",
            "Global step: 3033,loss: 0.009720323\n",
            "\n",
            "Global step: 3034,loss: 0.01192853\n",
            "\n",
            "Global step: 3035,loss: 0.010183746\n",
            "\n",
            "Global step: 3036,loss: 0.009327388\n",
            "\n",
            "Global step: 3037,loss: 0.013307703\n",
            "\n",
            "Global step: 3038,loss: 0.011006629\n",
            "\n",
            "Global step: 3039,loss: 0.009640924\n",
            "\n",
            "Global step: 3040,loss: 0.009044278\n",
            "\n",
            "Global step: 3041,loss: 0.010365628\n",
            "\n",
            "Global step: 3042,loss: 0.009714551\n",
            "\n",
            "Global step: 3043,loss: 0.011899005\n",
            "\n",
            "Global step: 3044,loss: 0.009652911\n",
            "\n",
            "Global step: 3045,loss: 0.008917649\n",
            "\n",
            "Global step: 3046,loss: 0.009524896\n",
            "\n",
            "Global step: 3047,loss: 0.008845806\n",
            "\n",
            "Global step: 3048,loss: 0.009141075\n",
            "\n",
            "Global step: 3049,loss: 0.009798434\n",
            "\n",
            "Global step: 3050,loss: 0.014759346\n",
            "\n",
            "Global step: 3051,loss: 0.011039465\n",
            "\n",
            "Global step: 3052,loss: 0.009440664\n",
            "\n",
            "Global step: 3053,loss: 0.009006039\n",
            "\n",
            "Global step: 3054,loss: 0.010010691\n",
            "\n",
            "Global step: 3055,loss: 0.0099989725\n",
            "\n",
            "Global step: 3056,loss: 0.011801316\n",
            "\n",
            "Global step: 3057,loss: 0.0103265755\n",
            "\n",
            "Global step: 3058,loss: 0.01372098\n",
            "\n",
            "Global step: 3059,loss: 0.009295395\n",
            "\n",
            "Global step: 3060,loss: 0.009526961\n",
            "\n",
            "Global step: 3061,loss: 0.009567612\n",
            "\n",
            "Global step: 3062,loss: 0.0097608\n",
            "\n",
            "Global step: 3063,loss: 0.01077755\n",
            "\n",
            "Global step: 3064,loss: 0.009288239\n",
            "\n",
            "Global step: 3065,loss: 0.010779256\n",
            "\n",
            "Global step: 3066,loss: 0.008130612\n",
            "\n",
            "Global step: 3067,loss: 0.011461118\n",
            "\n",
            "Global step: 3068,loss: 0.009793526\n",
            "\n",
            "Global step: 3069,loss: 0.010512457\n",
            "\n",
            "Global step: 3070,loss: 0.009359008\n",
            "\n",
            "Global step: 3071,loss: 0.009840278\n",
            "\n",
            "Global step: 3072,loss: 0.012636032\n",
            "\n",
            "Global step: 3073,loss: 0.010683568\n",
            "\n",
            "Global step: 3074,loss: 0.010132671\n",
            "\n",
            "Global step: 3075,loss: 0.010822176\n",
            "\n",
            "Global step: 3076,loss: 0.009963525\n",
            "\n",
            "Global step: 3077,loss: 0.008821383\n",
            "\n",
            "Global step: 3078,loss: 0.009401838\n",
            "\n",
            "Global step: 3079,loss: 0.010956772\n",
            "\n",
            "Global step: 3080,loss: 0.008708072\n",
            "\n",
            "Global step: 3081,loss: 0.009255046\n",
            "\n",
            "Global step: 3082,loss: 0.009061651\n",
            "\n",
            "Global step: 3083,loss: 0.00982887\n",
            "\n",
            "Global step: 3084,loss: 0.010152452\n",
            "\n",
            "Global step: 3085,loss: 0.008135265\n",
            "\n",
            "Global step: 3086,loss: 0.009600116\n",
            "\n",
            "Global step: 3087,loss: 0.008697161\n",
            "\n",
            "Global step: 3088,loss: 0.008733608\n",
            "\n",
            "Global step: 3089,loss: 0.010433054\n",
            "\n",
            "Global step: 3090,loss: 0.0092328815\n",
            "\n",
            "Global step: 3091,loss: 0.009155891\n",
            "\n",
            "Global step: 3092,loss: 0.01193974\n",
            "\n",
            "Global step: 3093,loss: 0.009235833\n",
            "\n",
            "Global step: 3094,loss: 0.00901195\n",
            "\n",
            "Global step: 3095,loss: 0.008979854\n",
            "\n",
            "Global step: 3096,loss: 0.010044322\n",
            "\n",
            "Global step: 3097,loss: 0.01012061\n",
            "\n",
            "Global step: 3098,loss: 0.009278309\n",
            "\n",
            "Global step: 3099,loss: 0.013459176\n",
            "\n",
            "Global step: 3100,loss: 0.008482627\n",
            "\n",
            "Global step: 3101,loss: 0.008598784\n",
            "\n",
            "Global step: 3102,loss: 0.008837708\n",
            "\n",
            "Global step: 3103,loss: 0.010312093\n",
            "\n",
            "Global step: 3104,loss: 0.010204969\n",
            "\n",
            "Global step: 3105,loss: 0.008009966\n",
            "\n",
            "Global step: 3106,loss: 0.010006765\n",
            "\n",
            "Global step: 3107,loss: 0.011307437\n",
            "\n",
            "Global step: 3108,loss: 0.009909794\n",
            "\n",
            "Global step: 3109,loss: 0.0082927495\n",
            "\n",
            "Global step: 3110,loss: 0.011673304\n",
            "\n",
            "Global step: 3111,loss: 0.010897112\n",
            "\n",
            "Global step: 3112,loss: 0.008792284\n",
            "\n",
            "Global step: 3113,loss: 0.012786652\n",
            "\n",
            "Global step: 3114,loss: 0.011831081\n",
            "\n",
            "Global step: 3115,loss: 0.012365606\n",
            "\n",
            "Global step: 3116,loss: 0.01230024\n",
            "\n",
            "Global step: 3117,loss: 0.009970097\n",
            "\n",
            "Global step: 3118,loss: 0.00868438\n",
            "\n",
            "Global step: 3119,loss: 0.009602122\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 3119,val_loss: 0.022569218447479684\n",
            "\n",
            "Training for epoch 9/16:\n",
            "Global step: 3120,loss: 0.009226021\n",
            "\n",
            "Global step: 3121,loss: 0.010765018\n",
            "\n",
            "Global step: 3122,loss: 0.009820745\n",
            "\n",
            "Global step: 3123,loss: 0.009413119\n",
            "\n",
            "Global step: 3124,loss: 0.009442442\n",
            "\n",
            "Global step: 3125,loss: 0.009989036\n",
            "\n",
            "Global step: 3126,loss: 0.008345684\n",
            "\n",
            "Global step: 3127,loss: 0.00827608\n",
            "\n",
            "Global step: 3128,loss: 0.009411074\n",
            "\n",
            "Global step: 3129,loss: 0.008384373\n",
            "\n",
            "Global step: 3130,loss: 0.009334267\n",
            "\n",
            "Global step: 3131,loss: 0.011427179\n",
            "\n",
            "Global step: 3132,loss: 0.011075358\n",
            "\n",
            "Global step: 3133,loss: 0.010578908\n",
            "\n",
            "Global step: 3134,loss: 0.009275104\n",
            "\n",
            "Global step: 3135,loss: 0.00959736\n",
            "\n",
            "Global step: 3136,loss: 0.008955859\n",
            "\n",
            "Global step: 3137,loss: 0.00990549\n",
            "\n",
            "Global step: 3138,loss: 0.010006418\n",
            "\n",
            "Global step: 3139,loss: 0.00882683\n",
            "\n",
            "Global step: 3140,loss: 0.008832388\n",
            "\n",
            "Global step: 3141,loss: 0.008102089\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 6.68459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:12:52.495019 139735703181056 supervisor.py:1099] global_step/sec: 6.68459\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 3142.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:12:52.561490 139735711573760 supervisor.py:1050] Recording summary at step 3142.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3142,loss: 0.008691498\n",
            "\n",
            "Global step: 3143,loss: 0.008641658\n",
            "\n",
            "Global step: 3144,loss: 0.009263372\n",
            "\n",
            "Global step: 3145,loss: 0.008091313\n",
            "\n",
            "Global step: 3146,loss: 0.010716048\n",
            "\n",
            "Global step: 3147,loss: 0.009307895\n",
            "\n",
            "Global step: 3148,loss: 0.007963329\n",
            "\n",
            "Global step: 3149,loss: 0.009013306\n",
            "\n",
            "Global step: 3150,loss: 0.009017847\n",
            "\n",
            "Global step: 3151,loss: 0.007969153\n",
            "\n",
            "Global step: 3152,loss: 0.008570256\n",
            "\n",
            "Global step: 3153,loss: 0.0084498795\n",
            "\n",
            "Global step: 3154,loss: 0.008249876\n",
            "\n",
            "Global step: 3155,loss: 0.009684056\n",
            "\n",
            "Global step: 3156,loss: 0.008369423\n",
            "\n",
            "Global step: 3157,loss: 0.010219715\n",
            "\n",
            "Global step: 3158,loss: 0.008427874\n",
            "\n",
            "Global step: 3159,loss: 0.00765578\n",
            "\n",
            "Global step: 3160,loss: 0.008729548\n",
            "\n",
            "Global step: 3161,loss: 0.008293865\n",
            "\n",
            "Global step: 3162,loss: 0.011285006\n",
            "\n",
            "Global step: 3163,loss: 0.007791181\n",
            "\n",
            "Global step: 3164,loss: 0.008274813\n",
            "\n",
            "Global step: 3165,loss: 0.010555353\n",
            "\n",
            "Global step: 3166,loss: 0.008394234\n",
            "\n",
            "Global step: 3167,loss: 0.008411568\n",
            "\n",
            "Global step: 3168,loss: 0.010434125\n",
            "\n",
            "Global step: 3169,loss: 0.009021422\n",
            "\n",
            "Global step: 3170,loss: 0.008562386\n",
            "\n",
            "Global step: 3171,loss: 0.008080506\n",
            "\n",
            "Global step: 3172,loss: 0.009055086\n",
            "\n",
            "Global step: 3173,loss: 0.007952514\n",
            "\n",
            "Global step: 3174,loss: 0.008498685\n",
            "\n",
            "Global step: 3175,loss: 0.0090208845\n",
            "\n",
            "Global step: 3176,loss: 0.008520898\n",
            "\n",
            "Global step: 3177,loss: 0.012821395\n",
            "\n",
            "Global step: 3178,loss: 0.008555634\n",
            "\n",
            "Global step: 3179,loss: 0.009841652\n",
            "\n",
            "Global step: 3180,loss: 0.0116208885\n",
            "\n",
            "Global step: 3181,loss: 0.010410037\n",
            "\n",
            "Global step: 3182,loss: 0.008735076\n",
            "\n",
            "Global step: 3183,loss: 0.008457055\n",
            "\n",
            "Global step: 3184,loss: 0.009220294\n",
            "\n",
            "Global step: 3185,loss: 0.009881344\n",
            "\n",
            "Global step: 3186,loss: 0.010252686\n",
            "\n",
            "Global step: 3187,loss: 0.010284838\n",
            "\n",
            "Global step: 3188,loss: 0.00785079\n",
            "\n",
            "Global step: 3189,loss: 0.009290779\n",
            "\n",
            "Global step: 3190,loss: 0.013243944\n",
            "\n",
            "Global step: 3191,loss: 0.010174554\n",
            "\n",
            "Global step: 3192,loss: 0.008739633\n",
            "\n",
            "Global step: 3193,loss: 0.010507738\n",
            "\n",
            "Global step: 3194,loss: 0.008984593\n",
            "\n",
            "Global step: 3195,loss: 0.009175665\n",
            "\n",
            "Global step: 3196,loss: 0.008910517\n",
            "\n",
            "Global step: 3197,loss: 0.0114432\n",
            "\n",
            "Global step: 3198,loss: 0.009184204\n",
            "\n",
            "Global step: 3199,loss: 0.010329774\n",
            "\n",
            "Global step: 3200,loss: 0.0084694205\n",
            "\n",
            "Global step: 3201,loss: 0.008677302\n",
            "\n",
            "Global step: 3202,loss: 0.0090084765\n",
            "\n",
            "Global step: 3203,loss: 0.010676884\n",
            "\n",
            "Global step: 3204,loss: 0.010526679\n",
            "\n",
            "Global step: 3205,loss: 0.01065719\n",
            "\n",
            "Global step: 3206,loss: 0.009112299\n",
            "\n",
            "Global step: 3207,loss: 0.0098238075\n",
            "\n",
            "Global step: 3208,loss: 0.008698198\n",
            "\n",
            "Global step: 3209,loss: 0.009252215\n",
            "\n",
            "Global step: 3210,loss: 0.010255478\n",
            "\n",
            "Global step: 3211,loss: 0.008619371\n",
            "\n",
            "Global step: 3212,loss: 0.0103828795\n",
            "\n",
            "Global step: 3213,loss: 0.008621866\n",
            "\n",
            "Global step: 3214,loss: 0.009142123\n",
            "\n",
            "Global step: 3215,loss: 0.009587712\n",
            "\n",
            "Global step: 3216,loss: 0.00861585\n",
            "\n",
            "Global step: 3217,loss: 0.0096785\n",
            "\n",
            "Global step: 3218,loss: 0.012201277\n",
            "\n",
            "Global step: 3219,loss: 0.008402446\n",
            "\n",
            "Global step: 3220,loss: 0.009331099\n",
            "\n",
            "Global step: 3221,loss: 0.008778231\n",
            "\n",
            "Global step: 3222,loss: 0.008251858\n",
            "\n",
            "Global step: 3223,loss: 0.011305084\n",
            "\n",
            "Global step: 3224,loss: 0.009182058\n",
            "\n",
            "Global step: 3225,loss: 0.008357122\n",
            "\n",
            "Global step: 3226,loss: 0.008594848\n",
            "\n",
            "Global step: 3227,loss: 0.008446426\n",
            "\n",
            "Global step: 3228,loss: 0.008878593\n",
            "\n",
            "Global step: 3229,loss: 0.008400412\n",
            "\n",
            "Global step: 3230,loss: 0.007866504\n",
            "\n",
            "Global step: 3231,loss: 0.008520889\n",
            "\n",
            "Global step: 3232,loss: 0.008393721\n",
            "\n",
            "Global step: 3233,loss: 0.008408789\n",
            "\n",
            "Global step: 3234,loss: 0.008087599\n",
            "\n",
            "Global step: 3235,loss: 0.008698927\n",
            "\n",
            "Global step: 3236,loss: 0.008664971\n",
            "\n",
            "Global step: 3237,loss: 0.008818705\n",
            "\n",
            "Global step: 3238,loss: 0.008429681\n",
            "\n",
            "Global step: 3239,loss: 0.008825049\n",
            "\n",
            "Global step: 3240,loss: 0.008179551\n",
            "\n",
            "Global step: 3241,loss: 0.00822771\n",
            "\n",
            "Global step: 3242,loss: 0.008085039\n",
            "\n",
            "Global step: 3243,loss: 0.00782564\n",
            "\n",
            "Global step: 3244,loss: 0.0079148505\n",
            "\n",
            "Global step: 3245,loss: 0.008726697\n",
            "\n",
            "Global step: 3246,loss: 0.012232486\n",
            "\n",
            "Global step: 3247,loss: 0.008770106\n",
            "\n",
            "Global step: 3248,loss: 0.008035798\n",
            "\n",
            "Global step: 3249,loss: 0.0109196445\n",
            "\n",
            "Global step: 3250,loss: 0.00818963\n",
            "\n",
            "Global step: 3251,loss: 0.009353838\n",
            "\n",
            "Global step: 3252,loss: 0.007866545\n",
            "\n",
            "Global step: 3253,loss: 0.008987156\n",
            "\n",
            "Global step: 3254,loss: 0.010028382\n",
            "\n",
            "Global step: 3255,loss: 0.008829508\n",
            "\n",
            "Global step: 3256,loss: 0.008711264\n",
            "\n",
            "Global step: 3257,loss: 0.009055737\n",
            "\n",
            "Global step: 3258,loss: 0.0085588135\n",
            "\n",
            "Global step: 3259,loss: 0.008207071\n",
            "\n",
            "Global step: 3260,loss: 0.009374046\n",
            "\n",
            "Global step: 3261,loss: 0.011167097\n",
            "\n",
            "Global step: 3262,loss: 0.010110732\n",
            "\n",
            "Global step: 3263,loss: 0.00858767\n",
            "\n",
            "Global step: 3264,loss: 0.00833785\n",
            "\n",
            "Global step: 3265,loss: 0.008730468\n",
            "\n",
            "Global step: 3266,loss: 0.008225502\n",
            "\n",
            "Global step: 3267,loss: 0.008073285\n",
            "\n",
            "Global step: 3268,loss: 0.008645496\n",
            "\n",
            "Global step: 3269,loss: 0.009006895\n",
            "\n",
            "Global step: 3270,loss: 0.008238371\n",
            "\n",
            "Global step: 3271,loss: 0.011668155\n",
            "\n",
            "Global step: 3272,loss: 0.009615549\n",
            "\n",
            "Global step: 3273,loss: 0.008370015\n",
            "\n",
            "Global step: 3274,loss: 0.008241529\n",
            "\n",
            "Global step: 3275,loss: 0.0116030285\n",
            "\n",
            "Global step: 3276,loss: 0.008648336\n",
            "\n",
            "Global step: 3277,loss: 0.00884288\n",
            "\n",
            "Global step: 3278,loss: 0.010167686\n",
            "\n",
            "Global step: 3279,loss: 0.008117357\n",
            "\n",
            "Global step: 3280,loss: 0.011286599\n",
            "\n",
            "Global step: 3281,loss: 0.008681281\n",
            "\n",
            "Global step: 3282,loss: 0.009302548\n",
            "\n",
            "Global step: 3283,loss: 0.009710682\n",
            "\n",
            "Global step: 3284,loss: 0.0088402275\n",
            "\n",
            "Global step: 3285,loss: 0.009220776\n",
            "\n",
            "Global step: 3286,loss: 0.012561597\n",
            "\n",
            "Global step: 3287,loss: 0.009087546\n",
            "\n",
            "Global step: 3288,loss: 0.008333221\n",
            "\n",
            "Global step: 3289,loss: 0.008795501\n",
            "\n",
            "Global step: 3290,loss: 0.008936872\n",
            "\n",
            "Global step: 3291,loss: 0.008921076\n",
            "\n",
            "Global step: 3292,loss: 0.008136925\n",
            "\n",
            "Global step: 3293,loss: 0.009058116\n",
            "\n",
            "Global step: 3294,loss: 0.008597701\n",
            "\n",
            "Global step: 3295,loss: 0.009538842\n",
            "\n",
            "Global step: 3296,loss: 0.011886601\n",
            "\n",
            "Global step: 3297,loss: 0.007641536\n",
            "\n",
            "Global step: 3298,loss: 0.009134656\n",
            "\n",
            "Global step: 3299,loss: 0.00827957\n",
            "\n",
            "Global step: 3300,loss: 0.00861316\n",
            "\n",
            "Global step: 3301,loss: 0.008592619\n",
            "\n",
            "Global step: 3302,loss: 0.011973409\n",
            "\n",
            "Global step: 3303,loss: 0.01117182\n",
            "\n",
            "Global step: 3304,loss: 0.008430392\n",
            "\n",
            "Global step: 3305,loss: 0.008642174\n",
            "\n",
            "Global step: 3306,loss: 0.012522146\n",
            "\n",
            "Global step: 3307,loss: 0.0076011666\n",
            "\n",
            "Global step: 3308,loss: 0.008554497\n",
            "\n",
            "Global step: 3309,loss: 0.012041772\n",
            "\n",
            "Global step: 3310,loss: 0.008209577\n",
            "\n",
            "Global step: 3311,loss: 0.008576043\n",
            "\n",
            "Global step: 3312,loss: 0.011428738\n",
            "\n",
            "Global step: 3313,loss: 0.008785749\n",
            "\n",
            "Global step: 3314,loss: 0.008133434\n",
            "\n",
            "Global step: 3315,loss: 0.010089913\n",
            "\n",
            "Global step: 3316,loss: 0.008909466\n",
            "\n",
            "Global step: 3317,loss: 0.00833076\n",
            "\n",
            "Global step: 3318,loss: 0.008428064\n",
            "\n",
            "Global step: 3319,loss: 0.008263902\n",
            "\n",
            "Global step: 3320,loss: 0.009192502\n",
            "\n",
            "Global step: 3321,loss: 0.0083198575\n",
            "\n",
            "Global step: 3322,loss: 0.008884032\n",
            "\n",
            "Global step: 3323,loss: 0.008512647\n",
            "\n",
            "Global step: 3324,loss: 0.009478242\n",
            "\n",
            "Global step: 3325,loss: 0.008572979\n",
            "\n",
            "Global step: 3326,loss: 0.009777725\n",
            "\n",
            "Global step: 3327,loss: 0.009563884\n",
            "\n",
            "Global step: 3328,loss: 0.008821917\n",
            "\n",
            "Global step: 3329,loss: 0.008735255\n",
            "\n",
            "Global step: 3330,loss: 0.0106832655\n",
            "\n",
            "Global step: 3331,loss: 0.008783254\n",
            "\n",
            "Global step: 3332,loss: 0.008951269\n",
            "\n",
            "Global step: 3333,loss: 0.007982791\n",
            "\n",
            "Global step: 3334,loss: 0.008253592\n",
            "\n",
            "Global step: 3335,loss: 0.008613346\n",
            "\n",
            "Global step: 3336,loss: 0.008406901\n",
            "\n",
            "Global step: 3337,loss: 0.007990627\n",
            "\n",
            "Global step: 3338,loss: 0.008365632\n",
            "\n",
            "Global step: 3339,loss: 0.009001625\n",
            "\n",
            "Global step: 3340,loss: 0.009213459\n",
            "\n",
            "Global step: 3341,loss: 0.008127511\n",
            "\n",
            "Global step: 3342,loss: 0.008360202\n",
            "\n",
            "Global step: 3343,loss: 0.00926512\n",
            "\n",
            "Global step: 3344,loss: 0.008038213\n",
            "\n",
            "Global step: 3345,loss: 0.0089916475\n",
            "\n",
            "Global step: 3346,loss: 0.008895189\n",
            "\n",
            "Global step: 3347,loss: 0.008491137\n",
            "\n",
            "Global step: 3348,loss: 0.008032972\n",
            "\n",
            "Global step: 3349,loss: 0.008395681\n",
            "\n",
            "Global step: 3350,loss: 0.008242552\n",
            "\n",
            "Global step: 3351,loss: 0.008138304\n",
            "\n",
            "Global step: 3352,loss: 0.008471857\n",
            "\n",
            "Global step: 3353,loss: 0.008130879\n",
            "\n",
            "Global step: 3354,loss: 0.009869991\n",
            "\n",
            "Global step: 3355,loss: 0.0070857443\n",
            "\n",
            "Global step: 3356,loss: 0.008283072\n",
            "\n",
            "Global step: 3357,loss: 0.0078576\n",
            "\n",
            "Global step: 3358,loss: 0.00807438\n",
            "\n",
            "Global step: 3359,loss: 0.008611008\n",
            "\n",
            "Global step: 3360,loss: 0.008150458\n",
            "\n",
            "Global step: 3361,loss: 0.008112883\n",
            "\n",
            "Global step: 3362,loss: 0.0087759085\n",
            "\n",
            "Global step: 3363,loss: 0.008603465\n",
            "\n",
            "Global step: 3364,loss: 0.0076355175\n",
            "\n",
            "Global step: 3365,loss: 0.008877116\n",
            "\n",
            "Global step: 3366,loss: 0.0110818045\n",
            "\n",
            "Global step: 3367,loss: 0.008382578\n",
            "\n",
            "Global step: 3368,loss: 0.009053094\n",
            "\n",
            "Global step: 3369,loss: 0.009817046\n",
            "\n",
            "Global step: 3370,loss: 0.008416393\n",
            "\n",
            "Global step: 3371,loss: 0.0086051375\n",
            "\n",
            "Global step: 3372,loss: 0.009327122\n",
            "\n",
            "Global step: 3373,loss: 0.015256964\n",
            "\n",
            "Global step: 3374,loss: 0.008867761\n",
            "\n",
            "Global step: 3375,loss: 0.009203315\n",
            "\n",
            "Global step: 3376,loss: 0.008987898\n",
            "\n",
            "Global step: 3377,loss: 0.008453035\n",
            "\n",
            "Global step: 3378,loss: 0.013236858\n",
            "\n",
            "Global step: 3379,loss: 0.009184731\n",
            "\n",
            "Global step: 3380,loss: 0.01198806\n",
            "\n",
            "Global step: 3381,loss: 0.0081437575\n",
            "\n",
            "Global step: 3382,loss: 0.008847544\n",
            "\n",
            "Global step: 3383,loss: 0.007898486\n",
            "\n",
            "Global step: 3384,loss: 0.009955366\n",
            "\n",
            "Global step: 3385,loss: 0.009504866\n",
            "\n",
            "Global step: 3386,loss: 0.010077741\n",
            "\n",
            "Global step: 3387,loss: 0.009196326\n",
            "\n",
            "Global step: 3388,loss: 0.0077349637\n",
            "\n",
            "Global step: 3389,loss: 0.010020967\n",
            "\n",
            "Global step: 3390,loss: 0.008848026\n",
            "\n",
            "Global step: 3391,loss: 0.0085830605\n",
            "\n",
            "Global step: 3392,loss: 0.009882375\n",
            "\n",
            "Global step: 3393,loss: 0.008535653\n",
            "\n",
            "Global step: 3394,loss: 0.009976552\n",
            "\n",
            "Global step: 3395,loss: 0.008263731\n",
            "\n",
            "Global step: 3396,loss: 0.008508805\n",
            "\n",
            "Global step: 3397,loss: 0.008442792\n",
            "\n",
            "Global step: 3398,loss: 0.009195007\n",
            "\n",
            "Global step: 3399,loss: 0.008120304\n",
            "\n",
            "Global step: 3400,loss: 0.008927517\n",
            "\n",
            "Global step: 3401,loss: 0.014904343\n",
            "\n",
            "Global step: 3402,loss: 0.0081980955\n",
            "\n",
            "Global step: 3403,loss: 0.009442622\n",
            "\n",
            "Global step: 3404,loss: 0.008763755\n",
            "\n",
            "Global step: 3405,loss: 0.008210281\n",
            "\n",
            "Global step: 3406,loss: 0.0089774765\n",
            "\n",
            "Global step: 3407,loss: 0.008273045\n",
            "\n",
            "Global step: 3408,loss: 0.008379389\n",
            "\n",
            "Global step: 3409,loss: 0.008795499\n",
            "\n",
            "Global step: 3410,loss: 0.011841135\n",
            "\n",
            "Global step: 3411,loss: 0.008162306\n",
            "\n",
            "Global step: 3412,loss: 0.007942897\n",
            "\n",
            "Global step: 3413,loss: 0.008950619\n",
            "\n",
            "Global step: 3414,loss: 0.0079328865\n",
            "\n",
            "Global step: 3415,loss: 0.008786087\n",
            "\n",
            "Global step: 3416,loss: 0.008905638\n",
            "\n",
            "Global step: 3417,loss: 0.008742481\n",
            "\n",
            "Global step: 3418,loss: 0.009061703\n",
            "\n",
            "Global step: 3419,loss: 0.010283299\n",
            "\n",
            "Global step: 3420,loss: 0.0080312155\n",
            "\n",
            "Global step: 3421,loss: 0.0081271045\n",
            "\n",
            "Global step: 3422,loss: 0.009164879\n",
            "\n",
            "Global step: 3423,loss: 0.008057672\n",
            "\n",
            "Global step: 3424,loss: 0.0077432035\n",
            "\n",
            "Global step: 3425,loss: 0.008383271\n",
            "\n",
            "Global step: 3426,loss: 0.013086105\n",
            "\n",
            "Global step: 3427,loss: 0.008429794\n",
            "\n",
            "Global step: 3428,loss: 0.00843001\n",
            "\n",
            "Global step: 3429,loss: 0.008625497\n",
            "\n",
            "Global step: 3430,loss: 0.009114821\n",
            "\n",
            "Global step: 3431,loss: 0.008733189\n",
            "\n",
            "Global step: 3432,loss: 0.010707945\n",
            "\n",
            "Global step: 3433,loss: 0.008435718\n",
            "\n",
            "Global step: 3434,loss: 0.008221049\n",
            "\n",
            "Global step: 3435,loss: 0.008727068\n",
            "\n",
            "Global step: 3436,loss: 0.008451076\n",
            "\n",
            "Global step: 3437,loss: 0.008740292\n",
            "\n",
            "Global step: 3438,loss: 0.011684896\n",
            "\n",
            "Global step: 3439,loss: 0.008343067\n",
            "\n",
            "Global step: 3440,loss: 0.00881912\n",
            "\n",
            "Global step: 3441,loss: 0.009881841\n",
            "\n",
            "Global step: 3442,loss: 0.0102455355\n",
            "\n",
            "Global step: 3443,loss: 0.008714678\n",
            "\n",
            "Global step: 3444,loss: 0.008414924\n",
            "\n",
            "Global step: 3445,loss: 0.008258014\n",
            "\n",
            "Global step: 3446,loss: 0.008765233\n",
            "\n",
            "Global step: 3447,loss: 0.008727655\n",
            "\n",
            "Global step: 3448,loss: 0.0086844275\n",
            "\n",
            "Global step: 3449,loss: 0.008189853\n",
            "\n",
            "Global step: 3450,loss: 0.008802462\n",
            "\n",
            "Global step: 3451,loss: 0.008387212\n",
            "\n",
            "Global step: 3452,loss: 0.008675836\n",
            "\n",
            "Global step: 3453,loss: 0.007623873\n",
            "\n",
            "Global step: 3454,loss: 0.011785637\n",
            "\n",
            "Global step: 3455,loss: 0.008238245\n",
            "\n",
            "Global step: 3456,loss: 0.008008245\n",
            "\n",
            "Global step: 3457,loss: 0.008085574\n",
            "\n",
            "Global step: 3458,loss: 0.007919345\n",
            "\n",
            "Global step: 3459,loss: 0.0080726035\n",
            "\n",
            "Global step: 3460,loss: 0.0080485\n",
            "\n",
            "Global step: 3461,loss: 0.009018165\n",
            "\n",
            "Global step: 3462,loss: 0.0076272\n",
            "\n",
            "Global step: 3463,loss: 0.008978877\n",
            "\n",
            "Global step: 3464,loss: 0.015438791\n",
            "\n",
            "Global step: 3465,loss: 0.009659161\n",
            "\n",
            "Global step: 3466,loss: 0.015219893\n",
            "\n",
            "Global step: 3467,loss: 0.008689004\n",
            "\n",
            "Global step: 3468,loss: 0.008851888\n",
            "\n",
            "Global step: 3469,loss: 0.009189494\n",
            "\n",
            "Global step: 3470,loss: 0.007921363\n",
            "\n",
            "Global step: 3471,loss: 0.009546528\n",
            "\n",
            "Global step: 3472,loss: 0.00976502\n",
            "\n",
            "Global step: 3473,loss: 0.008859675\n",
            "\n",
            "Global step: 3474,loss: 0.008801974\n",
            "\n",
            "Global step: 3475,loss: 0.008822099\n",
            "\n",
            "Global step: 3476,loss: 0.009518163\n",
            "\n",
            "Global step: 3477,loss: 0.01043384\n",
            "\n",
            "Global step: 3478,loss: 0.015796969\n",
            "\n",
            "Global step: 3479,loss: 0.013340836\n",
            "\n",
            "Global step: 3480,loss: 0.00835911\n",
            "\n",
            "Global step: 3481,loss: 0.009228937\n",
            "\n",
            "Global step: 3482,loss: 0.008024661\n",
            "\n",
            "Global step: 3483,loss: 0.009470789\n",
            "\n",
            "Global step: 3484,loss: 0.008854292\n",
            "\n",
            "Global step: 3485,loss: 0.0085942615\n",
            "\n",
            "Global step: 3486,loss: 0.009374499\n",
            "\n",
            "Global step: 3487,loss: 0.009427932\n",
            "\n",
            "Global step: 3488,loss: 0.008738022\n",
            "\n",
            "Global step: 3489,loss: 0.008013015\n",
            "\n",
            "Global step: 3490,loss: 0.008076691\n",
            "\n",
            "Global step: 3491,loss: 0.009348993\n",
            "\n",
            "Global step: 3492,loss: 0.008718145\n",
            "\n",
            "Global step: 3493,loss: 0.009350136\n",
            "\n",
            "Global step: 3494,loss: 0.009688867\n",
            "\n",
            "Global step: 3495,loss: 0.009259976\n",
            "\n",
            "Global step: 3496,loss: 0.008382691\n",
            "\n",
            "Global step: 3497,loss: 0.009465072\n",
            "\n",
            "Global step: 3498,loss: 0.009995133\n",
            "\n",
            "Global step: 3499,loss: 0.011180313\n",
            "\n",
            "Global step: 3500,loss: 0.0084538665\n",
            "\n",
            "Global step: 3501,loss: 0.0090853395\n",
            "\n",
            "Global step: 3502,loss: 0.0095365895\n",
            "\n",
            "Global step: 3503,loss: 0.0077011036\n",
            "\n",
            "Global step: 3504,loss: 0.008846707\n",
            "\n",
            "Global step: 3505,loss: 0.007991334\n",
            "\n",
            "Global step: 3506,loss: 0.008648111\n",
            "\n",
            "Global step: 3507,loss: 0.008854498\n",
            "\n",
            "Global step: 3508,loss: 0.009034918\n",
            "\n",
            "Global step: 3509,loss: 0.008369459\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 3509,val_loss: 0.020273202313826635\n",
            "\n",
            "Training for epoch 10/16:\n",
            "Global step: 3510,loss: 0.009257451\n",
            "\n",
            "Global step: 3511,loss: 0.012896219\n",
            "\n",
            "Global step: 3512,loss: 0.009777645\n",
            "\n",
            "Global step: 3513,loss: 0.009997254\n",
            "\n",
            "Global step: 3514,loss: 0.012953191\n",
            "\n",
            "Global step: 3515,loss: 0.008904238\n",
            "\n",
            "Global step: 3516,loss: 0.008947072\n",
            "\n",
            "Global step: 3517,loss: 0.008480974\n",
            "\n",
            "Global step: 3518,loss: 0.008944648\n",
            "\n",
            "Global step: 3519,loss: 0.008498427\n",
            "\n",
            "Global step: 3520,loss: 0.009939092\n",
            "\n",
            "Global step: 3521,loss: 0.008336769\n",
            "\n",
            "Global step: 3522,loss: 0.008777881\n",
            "\n",
            "Global step: 3523,loss: 0.00843691\n",
            "\n",
            "Global step: 3524,loss: 0.013844371\n",
            "\n",
            "Global step: 3525,loss: 0.008118155\n",
            "\n",
            "Global step: 3526,loss: 0.0076659615\n",
            "\n",
            "Global step: 3527,loss: 0.008242783\n",
            "\n",
            "Global step: 3528,loss: 0.008442278\n",
            "\n",
            "Global step: 3529,loss: 0.009093594\n",
            "\n",
            "Global step: 3530,loss: 0.012392104\n",
            "\n",
            "Global step: 3531,loss: 0.008231907\n",
            "\n",
            "Global step: 3532,loss: 0.00931926\n",
            "\n",
            "Global step: 3533,loss: 0.009672279\n",
            "\n",
            "Global step: 3534,loss: 0.008502999\n",
            "\n",
            "Global step: 3535,loss: 0.0076793935\n",
            "\n",
            "Global step: 3536,loss: 0.008589876\n",
            "\n",
            "Global step: 3537,loss: 0.008713254\n",
            "\n",
            "Global step: 3538,loss: 0.008108331\n",
            "\n",
            "Global step: 3539,loss: 0.008473997\n",
            "\n",
            "Global step: 3540,loss: 0.012416905\n",
            "\n",
            "Global step: 3541,loss: 0.009036939\n",
            "\n",
            "Global step: 3542,loss: 0.007462866\n",
            "\n",
            "Global step: 3543,loss: 0.008441264\n",
            "\n",
            "Global step: 3544,loss: 0.008491565\n",
            "\n",
            "Global step: 3545,loss: 0.008811081\n",
            "\n",
            "Global step: 3546,loss: 0.008395742\n",
            "\n",
            "Global step: 3547,loss: 0.008145704\n",
            "\n",
            "Global step: 3548,loss: 0.008362857\n",
            "\n",
            "Global step: 3549,loss: 0.010690136\n",
            "\n",
            "Global step: 3550,loss: 0.007871611\n",
            "\n",
            "Global step: 3551,loss: 0.007921883\n",
            "\n",
            "Global step: 3552,loss: 0.009419724\n",
            "\n",
            "Global step: 3553,loss: 0.0074924594\n",
            "\n",
            "Global step: 3554,loss: 0.008062275\n",
            "\n",
            "Global step: 3555,loss: 0.0076846182\n",
            "\n",
            "Global step: 3556,loss: 0.008328948\n",
            "\n",
            "Global step: 3557,loss: 0.008206285\n",
            "\n",
            "Global step: 3558,loss: 0.007930319\n",
            "\n",
            "Global step: 3559,loss: 0.008291491\n",
            "\n",
            "Global step: 3560,loss: 0.008111086\n",
            "\n",
            "Global step: 3561,loss: 0.0083316285\n",
            "\n",
            "Global step: 3562,loss: 0.010900542\n",
            "\n",
            "Global step: 3563,loss: 0.008255556\n",
            "\n",
            "Global step: 3564,loss: 0.008107651\n",
            "\n",
            "Global step: 3565,loss: 0.009226381\n",
            "\n",
            "Global step: 3566,loss: 0.007827753\n",
            "\n",
            "Global step: 3567,loss: 0.00809995\n",
            "\n",
            "Global step: 3568,loss: 0.008513188\n",
            "\n",
            "Global step: 3569,loss: 0.009014415\n",
            "\n",
            "Global step: 3570,loss: 0.00864299\n",
            "\n",
            "Global step: 3571,loss: 0.008550281\n",
            "\n",
            "Global step: 3572,loss: 0.010721305\n",
            "\n",
            "Global step: 3573,loss: 0.008381348\n",
            "\n",
            "Global step: 3574,loss: 0.009292767\n",
            "\n",
            "Global step: 3575,loss: 0.009228307\n",
            "\n",
            "Global step: 3576,loss: 0.008038208\n",
            "\n",
            "Global step: 3577,loss: 0.008342899\n",
            "\n",
            "Global step: 3578,loss: 0.008347819\n",
            "\n",
            "Global step: 3579,loss: 0.010103037\n",
            "\n",
            "Global step: 3580,loss: 0.0077573415\n",
            "\n",
            "Global step: 3581,loss: 0.008090477\n",
            "\n",
            "Global step: 3582,loss: 0.00837971\n",
            "\n",
            "Global step: 3583,loss: 0.0077242926\n",
            "\n",
            "Global step: 3584,loss: 0.007511291\n",
            "\n",
            "Global step: 3585,loss: 0.007589011\n",
            "\n",
            "Global step: 3586,loss: 0.0074285255\n",
            "\n",
            "Global step: 3587,loss: 0.008068551\n",
            "\n",
            "Global step: 3588,loss: 0.008158344\n",
            "\n",
            "Global step: 3589,loss: 0.008120799\n",
            "\n",
            "Global step: 3590,loss: 0.012756374\n",
            "\n",
            "Global step: 3591,loss: 0.007875026\n",
            "\n",
            "Global step: 3592,loss: 0.007895783\n",
            "\n",
            "Global step: 3593,loss: 0.008282516\n",
            "\n",
            "Global step: 3594,loss: 0.010346183\n",
            "\n",
            "Global step: 3595,loss: 0.0077925944\n",
            "\n",
            "Global step: 3596,loss: 0.007848839\n",
            "\n",
            "Global step: 3597,loss: 0.0077524097\n",
            "\n",
            "Global step: 3598,loss: 0.008462452\n",
            "\n",
            "Global step: 3599,loss: 0.008655767\n",
            "\n",
            "Global step: 3600,loss: 0.010562029\n",
            "\n",
            "Global step: 3601,loss: 0.008040398\n",
            "\n",
            "Global step: 3602,loss: 0.007287996\n",
            "\n",
            "Global step: 3603,loss: 0.008862551\n",
            "\n",
            "Global step: 3604,loss: 0.00850286\n",
            "\n",
            "Global step: 3605,loss: 0.009153506\n",
            "\n",
            "Global step: 3606,loss: 0.008177483\n",
            "\n",
            "Global step: 3607,loss: 0.00783669\n",
            "\n",
            "Global step: 3608,loss: 0.007780687\n",
            "\n",
            "Global step: 3609,loss: 0.008888384\n",
            "\n",
            "Global step: 3610,loss: 0.007688018\n",
            "\n",
            "Global step: 3611,loss: 0.0075101512\n",
            "\n",
            "Global step: 3612,loss: 0.008884073\n",
            "\n",
            "Global step: 3613,loss: 0.007882067\n",
            "\n",
            "Global step: 3614,loss: 0.008076577\n",
            "\n",
            "Global step: 3615,loss: 0.008105327\n",
            "\n",
            "Global step: 3616,loss: 0.008217337\n",
            "\n",
            "Global step: 3617,loss: 0.0077628978\n",
            "\n",
            "Global step: 3618,loss: 0.009555009\n",
            "\n",
            "Global step: 3619,loss: 0.008257177\n",
            "\n",
            "Global step: 3620,loss: 0.007900036\n",
            "\n",
            "Global step: 3621,loss: 0.008202977\n",
            "\n",
            "Global step: 3622,loss: 0.007574735\n",
            "\n",
            "Global step: 3623,loss: 0.008689515\n",
            "\n",
            "Global step: 3624,loss: 0.0077713514\n",
            "\n",
            "Global step: 3625,loss: 0.007482956\n",
            "\n",
            "Global step: 3626,loss: 0.0077378904\n",
            "\n",
            "Global step: 3627,loss: 0.007766947\n",
            "\n",
            "Global step: 3628,loss: 0.007056865\n",
            "\n",
            "Global step: 3629,loss: 0.007643798\n",
            "\n",
            "Global step: 3630,loss: 0.008624257\n",
            "\n",
            "Global step: 3631,loss: 0.0076316725\n",
            "\n",
            "Global step: 3632,loss: 0.007869383\n",
            "\n",
            "Global step: 3633,loss: 0.008256875\n",
            "\n",
            "Global step: 3634,loss: 0.007343344\n",
            "\n",
            "Global step: 3635,loss: 0.014116182\n",
            "\n",
            "Global step: 3636,loss: 0.0076501817\n",
            "\n",
            "Global step: 3637,loss: 0.007873672\n",
            "\n",
            "Global step: 3638,loss: 0.008598873\n",
            "\n",
            "Global step: 3639,loss: 0.008432671\n",
            "\n",
            "Global step: 3640,loss: 0.008652184\n",
            "\n",
            "Global step: 3641,loss: 0.0074546575\n",
            "\n",
            "Global step: 3642,loss: 0.008360235\n",
            "\n",
            "Global step: 3643,loss: 0.00843258\n",
            "\n",
            "Global step: 3644,loss: 0.008284596\n",
            "\n",
            "Global step: 3645,loss: 0.008013546\n",
            "\n",
            "Global step: 3646,loss: 0.009460568\n",
            "\n",
            "Global step: 3647,loss: 0.008094629\n",
            "\n",
            "Global step: 3648,loss: 0.0074301097\n",
            "\n",
            "Global step: 3649,loss: 0.007932004\n",
            "\n",
            "Global step: 3650,loss: 0.0085003935\n",
            "\n",
            "Global step: 3651,loss: 0.007913381\n",
            "\n",
            "Global step: 3652,loss: 0.007882853\n",
            "\n",
            "Global step: 3653,loss: 0.007925488\n",
            "\n",
            "Global step: 3654,loss: 0.008149481\n",
            "\n",
            "Global step: 3655,loss: 0.007857536\n",
            "\n",
            "Global step: 3656,loss: 0.0074072564\n",
            "\n",
            "Global step: 3657,loss: 0.0074019902\n",
            "\n",
            "Global step: 3658,loss: 0.00903311\n",
            "\n",
            "Global step: 3659,loss: 0.008221564\n",
            "\n",
            "Global step: 3660,loss: 0.009742444\n",
            "\n",
            "Global step: 3661,loss: 0.008265836\n",
            "\n",
            "Global step: 3662,loss: 0.007827816\n",
            "\n",
            "Global step: 3663,loss: 0.007697025\n",
            "\n",
            "Global step: 3664,loss: 0.008885248\n",
            "\n",
            "Global step: 3665,loss: 0.008416011\n",
            "\n",
            "Global step: 3666,loss: 0.008189198\n",
            "\n",
            "Global step: 3667,loss: 0.0076409676\n",
            "\n",
            "Global step: 3668,loss: 0.008105866\n",
            "\n",
            "Global step: 3669,loss: 0.008116456\n",
            "\n",
            "Global step: 3670,loss: 0.00899979\n",
            "\n",
            "Global step: 3671,loss: 0.007717692\n",
            "\n",
            "Global step: 3672,loss: 0.008247348\n",
            "\n",
            "Global step: 3673,loss: 0.0072806445\n",
            "\n",
            "Global step: 3674,loss: 0.007606449\n",
            "\n",
            "Global step: 3675,loss: 0.007986113\n",
            "\n",
            "Global step: 3676,loss: 0.00864241\n",
            "\n",
            "Global step: 3677,loss: 0.0077994755\n",
            "\n",
            "Global step: 3678,loss: 0.008872616\n",
            "\n",
            "Global step: 3679,loss: 0.008095226\n",
            "\n",
            "Global step: 3680,loss: 0.007725893\n",
            "\n",
            "Global step: 3681,loss: 0.008123258\n",
            "\n",
            "Global step: 3682,loss: 0.0077848425\n",
            "\n",
            "Global step: 3683,loss: 0.007487392\n",
            "\n",
            "Global step: 3684,loss: 0.007465441\n",
            "\n",
            "Global step: 3685,loss: 0.0077187694\n",
            "\n",
            "Global step: 3686,loss: 0.00797728\n",
            "\n",
            "Global step: 3687,loss: 0.008581463\n",
            "\n",
            "Global step: 3688,loss: 0.0073590702\n",
            "\n",
            "Global step: 3689,loss: 0.0072447704\n",
            "\n",
            "Global step: 3690,loss: 0.0079964595\n",
            "\n",
            "Global step: 3691,loss: 0.011402534\n",
            "\n",
            "Global step: 3692,loss: 0.008833948\n",
            "\n",
            "Global step: 3693,loss: 0.008675157\n",
            "\n",
            "Global step: 3694,loss: 0.008058235\n",
            "\n",
            "Global step: 3695,loss: 0.007293476\n",
            "\n",
            "Global step: 3696,loss: 0.008443219\n",
            "\n",
            "Global step: 3697,loss: 0.008312437\n",
            "\n",
            "Global step: 3698,loss: 0.01439436\n",
            "\n",
            "Global step: 3699,loss: 0.008493391\n",
            "\n",
            "Global step: 3700,loss: 0.008218761\n",
            "\n",
            "Global step: 3701,loss: 0.007996118\n",
            "\n",
            "Global step: 3702,loss: 0.00781253\n",
            "\n",
            "Global step: 3703,loss: 0.008562919\n",
            "\n",
            "Global step: 3704,loss: 0.00805805\n",
            "\n",
            "Global step: 3705,loss: 0.008395189\n",
            "\n",
            "Global step: 3706,loss: 0.00871639\n",
            "\n",
            "Global step: 3707,loss: 0.008100663\n",
            "\n",
            "Global step: 3708,loss: 0.0075789015\n",
            "\n",
            "Global step: 3709,loss: 0.008191973\n",
            "\n",
            "Global step: 3710,loss: 0.007431568\n",
            "\n",
            "Global step: 3711,loss: 0.008789564\n",
            "\n",
            "Global step: 3712,loss: 0.0083784\n",
            "\n",
            "Global step: 3713,loss: 0.008475007\n",
            "\n",
            "Global step: 3714,loss: 0.0077563925\n",
            "\n",
            "Global step: 3715,loss: 0.00813624\n",
            "\n",
            "Global step: 3716,loss: 0.007976149\n",
            "\n",
            "Global step: 3717,loss: 0.008657693\n",
            "\n",
            "Global step: 3718,loss: 0.007699359\n",
            "\n",
            "Global step: 3719,loss: 0.008271786\n",
            "\n",
            "Global step: 3720,loss: 0.007834908\n",
            "\n",
            "Global step: 3721,loss: 0.008145226\n",
            "\n",
            "Global step: 3722,loss: 0.008051056\n",
            "\n",
            "Global step: 3723,loss: 0.008043909\n",
            "\n",
            "Global step: 3724,loss: 0.011037787\n",
            "\n",
            "Global step: 3725,loss: 0.007522539\n",
            "\n",
            "Global step: 3726,loss: 0.008058081\n",
            "\n",
            "Global step: 3727,loss: 0.0072947117\n",
            "\n",
            "Global step: 3728,loss: 0.0076067476\n",
            "\n",
            "Global step: 3729,loss: 0.008583294\n",
            "\n",
            "Global step: 3730,loss: 0.0075021023\n",
            "\n",
            "Global step: 3731,loss: 0.008363599\n",
            "\n",
            "Global step: 3732,loss: 0.007484616\n",
            "\n",
            "Global step: 3733,loss: 0.009062685\n",
            "\n",
            "Global step: 3734,loss: 0.0081950035\n",
            "\n",
            "Global step: 3735,loss: 0.0077458597\n",
            "\n",
            "Global step: 3736,loss: 0.008002781\n",
            "\n",
            "Global step: 3737,loss: 0.007973265\n",
            "\n",
            "Global step: 3738,loss: 0.0077960733\n",
            "\n",
            "Global step: 3739,loss: 0.007824667\n",
            "\n",
            "Global step: 3740,loss: 0.008081676\n",
            "\n",
            "Global step: 3741,loss: 0.0075643035\n",
            "\n",
            "Global step: 3742,loss: 0.011417301\n",
            "\n",
            "Global step: 3743,loss: 0.007924994\n",
            "\n",
            "Global step: 3744,loss: 0.008299003\n",
            "\n",
            "Global step: 3745,loss: 0.007919397\n",
            "\n",
            "Global step: 3746,loss: 0.007950948\n",
            "\n",
            "Global step: 3747,loss: 0.00886138\n",
            "\n",
            "Global step: 3748,loss: 0.0078112446\n",
            "\n",
            "Global step: 3749,loss: 0.0077041597\n",
            "\n",
            "Global step: 3750,loss: 0.008145355\n",
            "\n",
            "Global step: 3751,loss: 0.008642269\n",
            "\n",
            "Global step: 3752,loss: 0.007838894\n",
            "\n",
            "Global step: 3753,loss: 0.007984037\n",
            "\n",
            "Global step: 3754,loss: 0.014743956\n",
            "\n",
            "Global step: 3755,loss: 0.008273799\n",
            "\n",
            "Global step: 3756,loss: 0.00940207\n",
            "\n",
            "Global step: 3757,loss: 0.008597883\n",
            "\n",
            "Global step: 3758,loss: 0.008466333\n",
            "\n",
            "Global step: 3759,loss: 0.009300179\n",
            "\n",
            "Global step: 3760,loss: 0.0076518753\n",
            "\n",
            "Global step: 3761,loss: 0.009758661\n",
            "\n",
            "Global step: 3762,loss: 0.007821568\n",
            "\n",
            "Global step: 3763,loss: 0.008063077\n",
            "\n",
            "Global step: 3764,loss: 0.007889722\n",
            "\n",
            "Global step: 3765,loss: 0.0077555715\n",
            "\n",
            "Global step: 3766,loss: 0.0073910076\n",
            "\n",
            "Global step: 3767,loss: 0.008161233\n",
            "\n",
            "Global step: 3768,loss: 0.00739985\n",
            "\n",
            "Global step: 3769,loss: 0.008517595\n",
            "\n",
            "Global step: 3770,loss: 0.008640417\n",
            "\n",
            "Global step: 3771,loss: 0.007835454\n",
            "\n",
            "Global step: 3772,loss: 0.008493802\n",
            "\n",
            "Global step: 3773,loss: 0.008165257\n",
            "\n",
            "Global step: 3774,loss: 0.008182354\n",
            "\n",
            "Global step: 3775,loss: 0.007454902\n",
            "\n",
            "Global step: 3776,loss: 0.007832728\n",
            "\n",
            "Global step: 3777,loss: 0.008606063\n",
            "\n",
            "Global step: 3778,loss: 0.008329231\n",
            "\n",
            "Global step: 3779,loss: 0.007881718\n",
            "\n",
            "Global step: 3780,loss: 0.008113907\n",
            "\n",
            "Global step: 3781,loss: 0.008168996\n",
            "\n",
            "Global step: 3782,loss: 0.008218199\n",
            "\n",
            "Global step: 3783,loss: 0.008549272\n",
            "\n",
            "Global step: 3784,loss: 0.008845272\n",
            "\n",
            "Global step: 3785,loss: 0.009593372\n",
            "\n",
            "Global step: 3786,loss: 0.007543286\n",
            "\n",
            "Global step: 3787,loss: 0.007918658\n",
            "\n",
            "Global step: 3788,loss: 0.009718634\n",
            "\n",
            "Global step: 3789,loss: 0.007819434\n",
            "\n",
            "Global step: 3790,loss: 0.008926712\n",
            "\n",
            "Global step: 3791,loss: 0.007339504\n",
            "\n",
            "Global step: 3792,loss: 0.008803016\n",
            "\n",
            "Global step: 3793,loss: 0.007154918\n",
            "\n",
            "Global step: 3794,loss: 0.008142232\n",
            "\n",
            "Global step: 3795,loss: 0.0081551885\n",
            "\n",
            "Global step: 3796,loss: 0.0076819453\n",
            "\n",
            "Global step: 3797,loss: 0.008212654\n",
            "\n",
            "Global step: 3798,loss: 0.008466063\n",
            "\n",
            "Global step: 3799,loss: 0.008900351\n",
            "\n",
            "Global step: 3800,loss: 0.011506182\n",
            "\n",
            "Global step: 3801,loss: 0.012585308\n",
            "\n",
            "Global step: 3802,loss: 0.0076828594\n",
            "\n",
            "Global step: 3803,loss: 0.010207357\n",
            "\n",
            "Global step: 3804,loss: 0.008397641\n",
            "\n",
            "Global step: 3805,loss: 0.008916843\n",
            "\n",
            "Global step: 3806,loss: 0.008485408\n",
            "\n",
            "Global step: 3807,loss: 0.0110629555\n",
            "\n",
            "Global step: 3808,loss: 0.008185079\n",
            "\n",
            "Global step: 3809,loss: 0.0076765614\n",
            "\n",
            "Global step: 3810,loss: 0.008228712\n",
            "\n",
            "Global step: 3811,loss: 0.009075285\n",
            "\n",
            "Global step: 3812,loss: 0.008485487\n",
            "\n",
            "Global step: 3813,loss: 0.008362436\n",
            "\n",
            "Global step: 3814,loss: 0.009013245\n",
            "\n",
            "Global step: 3815,loss: 0.0076585608\n",
            "\n",
            "Global step: 3816,loss: 0.007656293\n",
            "\n",
            "Global step: 3817,loss: 0.008134424\n",
            "\n",
            "Global step: 3818,loss: 0.008580318\n",
            "\n",
            "Global step: 3819,loss: 0.007725492\n",
            "\n",
            "Global step: 3820,loss: 0.011834715\n",
            "\n",
            "Global step: 3821,loss: 0.008585253\n",
            "\n",
            "Global step: 3822,loss: 0.0075861258\n",
            "\n",
            "Global step: 3823,loss: 0.0075738165\n",
            "\n",
            "Global step: 3824,loss: 0.008128071\n",
            "\n",
            "Global step: 3825,loss: 0.011469297\n",
            "\n",
            "Global step: 3826,loss: 0.008082348\n",
            "\n",
            "Global step: 3827,loss: 0.0080946395\n",
            "\n",
            "Global step: 3828,loss: 0.0077021196\n",
            "\n",
            "Global step: 3829,loss: 0.00766581\n",
            "\n",
            "Global step: 3830,loss: 0.007417334\n",
            "\n",
            "Global step: 3831,loss: 0.007707705\n",
            "\n",
            "Global step: 3832,loss: 0.011133106\n",
            "\n",
            "Global step: 3833,loss: 0.011093972\n",
            "\n",
            "Global step: 3834,loss: 0.0072823167\n",
            "\n",
            "Global step: 3835,loss: 0.008354519\n",
            "\n",
            "Global step: 3836,loss: 0.0077911126\n",
            "\n",
            "Global step: 3837,loss: 0.007864573\n",
            "\n",
            "Global step: 3838,loss: 0.00770396\n",
            "\n",
            "Global step: 3839,loss: 0.007841962\n",
            "\n",
            "Global step: 3840,loss: 0.008619087\n",
            "\n",
            "Global step: 3841,loss: 0.008590068\n",
            "\n",
            "Global step: 3842,loss: 0.00833572\n",
            "\n",
            "Global step: 3843,loss: 0.00792169\n",
            "\n",
            "Global step: 3844,loss: 0.008045023\n",
            "\n",
            "Global step: 3845,loss: 0.007723046\n",
            "\n",
            "Global step: 3846,loss: 0.008620719\n",
            "\n",
            "Global step: 3847,loss: 0.008732734\n",
            "\n",
            "Global step: 3848,loss: 0.008203357\n",
            "\n",
            "Global step: 3849,loss: 0.0082358625\n",
            "\n",
            "Global step: 3850,loss: 0.007824784\n",
            "\n",
            "Global step: 3851,loss: 0.007943777\n",
            "\n",
            "Global step: 3852,loss: 0.008594164\n",
            "\n",
            "Global step: 3853,loss: 0.006944278\n",
            "\n",
            "Global step: 3854,loss: 0.008000225\n",
            "\n",
            "Global step: 3855,loss: 0.0082506705\n",
            "\n",
            "Global step: 3856,loss: 0.01035895\n",
            "\n",
            "Global step: 3857,loss: 0.00747433\n",
            "\n",
            "Global step: 3858,loss: 0.008489974\n",
            "\n",
            "Global step: 3859,loss: 0.0071645197\n",
            "\n",
            "Global step: 3860,loss: 0.00891549\n",
            "\n",
            "Global step: 3861,loss: 0.008243821\n",
            "\n",
            "Global step: 3862,loss: 0.008183553\n",
            "\n",
            "Global step: 3863,loss: 0.009523686\n",
            "\n",
            "Global step: 3864,loss: 0.010633558\n",
            "\n",
            "Global step: 3865,loss: 0.008327209\n",
            "\n",
            "Global step: 3866,loss: 0.007666477\n",
            "\n",
            "Global step: 3867,loss: 0.007920414\n",
            "\n",
            "Global step: 3868,loss: 0.007377243\n",
            "\n",
            "Global step: 3869,loss: 0.008721674\n",
            "\n",
            "Global step: 3870,loss: 0.008641161\n",
            "\n",
            "Global step: 3871,loss: 0.009788236\n",
            "\n",
            "Global step: 3872,loss: 0.007203852\n",
            "\n",
            "Global step: 3873,loss: 0.008875242\n",
            "\n",
            "Global step: 3874,loss: 0.008089895\n",
            "\n",
            "Global step: 3875,loss: 0.00814058\n",
            "\n",
            "Global step: 3876,loss: 0.009581365\n",
            "\n",
            "Global step: 3877,loss: 0.008593731\n",
            "\n",
            "Global step: 3878,loss: 0.0076686144\n",
            "\n",
            "Global step: 3879,loss: 0.007941652\n",
            "\n",
            "Global step: 3880,loss: 0.010524151\n",
            "\n",
            "Global step: 3881,loss: 0.007999679\n",
            "\n",
            "Global step: 3882,loss: 0.007505294\n",
            "\n",
            "Global step: 3883,loss: 0.0078020655\n",
            "\n",
            "Global step: 3884,loss: 0.0077092415\n",
            "\n",
            "Global step: 3885,loss: 0.007905533\n",
            "\n",
            "Global step: 3886,loss: 0.008783955\n",
            "\n",
            "Global step: 3887,loss: 0.0076441746\n",
            "\n",
            "Global step: 3888,loss: 0.007990284\n",
            "\n",
            "Global step: 3889,loss: 0.007918434\n",
            "\n",
            "Global step: 3890,loss: 0.008108878\n",
            "\n",
            "Global step: 3891,loss: 0.015446451\n",
            "\n",
            "Global step: 3892,loss: 0.007975617\n",
            "\n",
            "Global step: 3893,loss: 0.007989421\n",
            "\n",
            "Global step: 3894,loss: 0.011050236\n",
            "\n",
            "Global step: 3895,loss: 0.00833412\n",
            "\n",
            "Global step: 3896,loss: 0.0074860323\n",
            "\n",
            "Global step: 3897,loss: 0.008607222\n",
            "\n",
            "Global step: 3898,loss: 0.0073236423\n",
            "\n",
            "Global step: 3899,loss: 0.008027716\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 3899,val_loss: 0.019750447573665626\n",
            "\n",
            "Training for epoch 11/16:\n",
            "Global step: 3900,loss: 0.008222526\n",
            "\n",
            "Global step: 3901,loss: 0.007162427\n",
            "\n",
            "Global step: 3902,loss: 0.009197892\n",
            "\n",
            "Global step: 3903,loss: 0.0075738947\n",
            "\n",
            "Global step: 3904,loss: 0.0075581972\n",
            "\n",
            "Global step: 3905,loss: 0.007584656\n",
            "\n",
            "Global step: 3906,loss: 0.007685587\n",
            "\n",
            "Global step: 3907,loss: 0.007750232\n",
            "\n",
            "Global step: 3908,loss: 0.0075002164\n",
            "\n",
            "Global step: 3909,loss: 0.0076565878\n",
            "\n",
            "Global step: 3910,loss: 0.0077640573\n",
            "\n",
            "Global step: 3911,loss: 0.008739594\n",
            "\n",
            "Global step: 3912,loss: 0.0115304105\n",
            "\n",
            "Global step: 3913,loss: 0.011241764\n",
            "\n",
            "Global step: 3914,loss: 0.007810695\n",
            "\n",
            "Global step: 3915,loss: 0.008740321\n",
            "\n",
            "Global step: 3916,loss: 0.0077795843\n",
            "\n",
            "Global step: 3917,loss: 0.009895869\n",
            "\n",
            "Global step: 3918,loss: 0.008312978\n",
            "\n",
            "Global step: 3919,loss: 0.00799001\n",
            "\n",
            "Global step: 3920,loss: 0.007458511\n",
            "\n",
            "Global step: 3921,loss: 0.007496576\n",
            "\n",
            "Global step: 3922,loss: 0.0076555456\n",
            "\n",
            "Global step: 3923,loss: 0.007828961\n",
            "\n",
            "Global step: 3924,loss: 0.0075578713\n",
            "\n",
            "Global step: 3925,loss: 0.007490213\n",
            "\n",
            "Global step: 3926,loss: 0.008035221\n",
            "\n",
            "Global step: 3927,loss: 0.0076505644\n",
            "\n",
            "Global step: 3928,loss: 0.00761903\n",
            "\n",
            "Global step: 3929,loss: 0.0072196703\n",
            "\n",
            "Global step: 3930,loss: 0.007689367\n",
            "\n",
            "Global step: 3931,loss: 0.0075876405\n",
            "\n",
            "Global step: 3932,loss: 0.007394868\n",
            "\n",
            "Global step: 3933,loss: 0.007351656\n",
            "\n",
            "Global step: 3934,loss: 0.0073666526\n",
            "\n",
            "Global step: 3935,loss: 0.007548024\n",
            "\n",
            "Global step: 3936,loss: 0.007675317\n",
            "\n",
            "Global step: 3937,loss: 0.007119316\n",
            "\n",
            "Global step: 3938,loss: 0.0074986685\n",
            "\n",
            "Global step: 3939,loss: 0.008217733\n",
            "\n",
            "Global step: 3940,loss: 0.00796374\n",
            "\n",
            "Global step: 3941,loss: 0.007598857\n",
            "\n",
            "Global step: 3942,loss: 0.0077417823\n",
            "\n",
            "Global step: 3943,loss: 0.0075196335\n",
            "\n",
            "Global step: 3944,loss: 0.0072146733\n",
            "\n",
            "Global step: 3945,loss: 0.008620141\n",
            "\n",
            "Global step: 3946,loss: 0.007583008\n",
            "\n",
            "Global step: 3947,loss: 0.0073759337\n",
            "\n",
            "Global step: 3948,loss: 0.007770309\n",
            "\n",
            "Global step: 3949,loss: 0.007700206\n",
            "\n",
            "Global step: 3950,loss: 0.008172067\n",
            "\n",
            "Global step: 3951,loss: 0.007995737\n",
            "\n",
            "Global step: 3952,loss: 0.0077232704\n",
            "\n",
            "Global step: 3953,loss: 0.007618962\n",
            "\n",
            "Global step: 3954,loss: 0.0074695516\n",
            "\n",
            "Global step: 3955,loss: 0.008010689\n",
            "\n",
            "Global step: 3956,loss: 0.008130902\n",
            "\n",
            "Global step: 3957,loss: 0.007534976\n",
            "\n",
            "Global step: 3958,loss: 0.0071942746\n",
            "\n",
            "Global step: 3959,loss: 0.0071678674\n",
            "\n",
            "Global step: 3960,loss: 0.008800492\n",
            "\n",
            "Global step: 3961,loss: 0.0070561613\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3962.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:14:52.454993 139735711573760 supervisor.py:1050] Recording summary at step 3962.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.83344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:14:52.493093 139735703181056 supervisor.py:1099] global_step/sec: 6.83344\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3962,loss: 0.0072412277\n",
            "\n",
            "Global step: 3963,loss: 0.00786913\n",
            "\n",
            "Global step: 3964,loss: 0.008273258\n",
            "\n",
            "Global step: 3965,loss: 0.007354804\n",
            "\n",
            "Global step: 3966,loss: 0.0074330335\n",
            "\n",
            "Global step: 3967,loss: 0.007284551\n",
            "\n",
            "Global step: 3968,loss: 0.007340731\n",
            "\n",
            "Global step: 3969,loss: 0.012701389\n",
            "\n",
            "Global step: 3970,loss: 0.007769138\n",
            "\n",
            "Global step: 3971,loss: 0.007864434\n",
            "\n",
            "Global step: 3972,loss: 0.007639863\n",
            "\n",
            "Global step: 3973,loss: 0.0071434146\n",
            "\n",
            "Global step: 3974,loss: 0.007254414\n",
            "\n",
            "Global step: 3975,loss: 0.007414104\n",
            "\n",
            "Global step: 3976,loss: 0.0077109686\n",
            "\n",
            "Global step: 3977,loss: 0.0073947976\n",
            "\n",
            "Global step: 3978,loss: 0.0071411864\n",
            "\n",
            "Global step: 3979,loss: 0.007460516\n",
            "\n",
            "Global step: 3980,loss: 0.007157027\n",
            "\n",
            "Global step: 3981,loss: 0.0073399735\n",
            "\n",
            "Global step: 3982,loss: 0.007156741\n",
            "\n",
            "Global step: 3983,loss: 0.0076755825\n",
            "\n",
            "Global step: 3984,loss: 0.007231135\n",
            "\n",
            "Global step: 3985,loss: 0.007203158\n",
            "\n",
            "Global step: 3986,loss: 0.0068123545\n",
            "\n",
            "Global step: 3987,loss: 0.007876456\n",
            "\n",
            "Global step: 3988,loss: 0.00750573\n",
            "\n",
            "Global step: 3989,loss: 0.007885864\n",
            "\n",
            "Global step: 3990,loss: 0.010642106\n",
            "\n",
            "Global step: 3991,loss: 0.0074372636\n",
            "\n",
            "Global step: 3992,loss: 0.007808956\n",
            "\n",
            "Global step: 3993,loss: 0.0075739156\n",
            "\n",
            "Global step: 3994,loss: 0.0077172257\n",
            "\n",
            "Global step: 3995,loss: 0.0074756304\n",
            "\n",
            "Global step: 3996,loss: 0.00782183\n",
            "\n",
            "Global step: 3997,loss: 0.007652184\n",
            "\n",
            "Global step: 3998,loss: 0.007614233\n",
            "\n",
            "Global step: 3999,loss: 0.0078072636\n",
            "\n",
            "Global step: 4000,loss: 0.0077081607\n",
            "\n",
            "Global step: 4001,loss: 0.0075675854\n",
            "\n",
            "Global step: 4002,loss: 0.0077516483\n",
            "\n",
            "Global step: 4003,loss: 0.007760326\n",
            "\n",
            "Global step: 4004,loss: 0.007980503\n",
            "\n",
            "Global step: 4005,loss: 0.0072653964\n",
            "\n",
            "Global step: 4006,loss: 0.0077927737\n",
            "\n",
            "Global step: 4007,loss: 0.0079137515\n",
            "\n",
            "Global step: 4008,loss: 0.0076151816\n",
            "\n",
            "Global step: 4009,loss: 0.0073906905\n",
            "\n",
            "Global step: 4010,loss: 0.0073937904\n",
            "\n",
            "Global step: 4011,loss: 0.00815216\n",
            "\n",
            "Global step: 4012,loss: 0.0072337007\n",
            "\n",
            "Global step: 4013,loss: 0.007506317\n",
            "\n",
            "Global step: 4014,loss: 0.007199214\n",
            "\n",
            "Global step: 4015,loss: 0.0073961094\n",
            "\n",
            "Global step: 4016,loss: 0.007368219\n",
            "\n",
            "Global step: 4017,loss: 0.0075802132\n",
            "\n",
            "Global step: 4018,loss: 0.006947579\n",
            "\n",
            "Global step: 4019,loss: 0.0077485293\n",
            "\n",
            "Global step: 4020,loss: 0.007262652\n",
            "\n",
            "Global step: 4021,loss: 0.009384004\n",
            "\n",
            "Global step: 4022,loss: 0.0073871603\n",
            "\n",
            "Global step: 4023,loss: 0.00749701\n",
            "\n",
            "Global step: 4024,loss: 0.007666759\n",
            "\n",
            "Global step: 4025,loss: 0.0074531953\n",
            "\n",
            "Global step: 4026,loss: 0.007464849\n",
            "\n",
            "Global step: 4027,loss: 0.0073454324\n",
            "\n",
            "Global step: 4028,loss: 0.007526438\n",
            "\n",
            "Global step: 4029,loss: 0.007489784\n",
            "\n",
            "Global step: 4030,loss: 0.007644077\n",
            "\n",
            "Global step: 4031,loss: 0.008160631\n",
            "\n",
            "Global step: 4032,loss: 0.008827234\n",
            "\n",
            "Global step: 4033,loss: 0.007360886\n",
            "\n",
            "Global step: 4034,loss: 0.0073560686\n",
            "\n",
            "Global step: 4035,loss: 0.0077483654\n",
            "\n",
            "Global step: 4036,loss: 0.010462383\n",
            "\n",
            "Global step: 4037,loss: 0.007425002\n",
            "\n",
            "Global step: 4038,loss: 0.0073694726\n",
            "\n",
            "Global step: 4039,loss: 0.0073585054\n",
            "\n",
            "Global step: 4040,loss: 0.0077037998\n",
            "\n",
            "Global step: 4041,loss: 0.007963357\n",
            "\n",
            "Global step: 4042,loss: 0.007155952\n",
            "\n",
            "Global step: 4043,loss: 0.007253475\n",
            "\n",
            "Global step: 4044,loss: 0.00786119\n",
            "\n",
            "Global step: 4045,loss: 0.0071948757\n",
            "\n",
            "Global step: 4046,loss: 0.0074058734\n",
            "\n",
            "Global step: 4047,loss: 0.007697905\n",
            "\n",
            "Global step: 4048,loss: 0.0076120324\n",
            "\n",
            "Global step: 4049,loss: 0.007741095\n",
            "\n",
            "Global step: 4050,loss: 0.0073848157\n",
            "\n",
            "Global step: 4051,loss: 0.0074925185\n",
            "\n",
            "Global step: 4052,loss: 0.0114345625\n",
            "\n",
            "Global step: 4053,loss: 0.007881684\n",
            "\n",
            "Global step: 4054,loss: 0.0076529486\n",
            "\n",
            "Global step: 4055,loss: 0.007534034\n",
            "\n",
            "Global step: 4056,loss: 0.00710133\n",
            "\n",
            "Global step: 4057,loss: 0.007590229\n",
            "\n",
            "Global step: 4058,loss: 0.007514964\n",
            "\n",
            "Global step: 4059,loss: 0.007605974\n",
            "\n",
            "Global step: 4060,loss: 0.008055628\n",
            "\n",
            "Global step: 4061,loss: 0.0074873134\n",
            "\n",
            "Global step: 4062,loss: 0.010562688\n",
            "\n",
            "Global step: 4063,loss: 0.008057223\n",
            "\n",
            "Global step: 4064,loss: 0.007340721\n",
            "\n",
            "Global step: 4065,loss: 0.007889855\n",
            "\n",
            "Global step: 4066,loss: 0.0076719965\n",
            "\n",
            "Global step: 4067,loss: 0.0077627767\n",
            "\n",
            "Global step: 4068,loss: 0.0077517424\n",
            "\n",
            "Global step: 4069,loss: 0.007818242\n",
            "\n",
            "Global step: 4070,loss: 0.006902685\n",
            "\n",
            "Global step: 4071,loss: 0.0072376467\n",
            "\n",
            "Global step: 4072,loss: 0.008565022\n",
            "\n",
            "Global step: 4073,loss: 0.0077695013\n",
            "\n",
            "Global step: 4074,loss: 0.007280809\n",
            "\n",
            "Global step: 4075,loss: 0.0075393156\n",
            "\n",
            "Global step: 4076,loss: 0.0068340953\n",
            "\n",
            "Global step: 4077,loss: 0.007800476\n",
            "\n",
            "Global step: 4078,loss: 0.0070910463\n",
            "\n",
            "Global step: 4079,loss: 0.0071826354\n",
            "\n",
            "Global step: 4080,loss: 0.0074268878\n",
            "\n",
            "Global step: 4081,loss: 0.007821946\n",
            "\n",
            "Global step: 4082,loss: 0.013932109\n",
            "\n",
            "Global step: 4083,loss: 0.0075047137\n",
            "\n",
            "Global step: 4084,loss: 0.0074405596\n",
            "\n",
            "Global step: 4085,loss: 0.0075887195\n",
            "\n",
            "Global step: 4086,loss: 0.0067313835\n",
            "\n",
            "Global step: 4087,loss: 0.007816524\n",
            "\n",
            "Global step: 4088,loss: 0.007396176\n",
            "\n",
            "Global step: 4089,loss: 0.008097373\n",
            "\n",
            "Global step: 4090,loss: 0.0075611277\n",
            "\n",
            "Global step: 4091,loss: 0.007509259\n",
            "\n",
            "Global step: 4092,loss: 0.00699556\n",
            "\n",
            "Global step: 4093,loss: 0.0070914696\n",
            "\n",
            "Global step: 4094,loss: 0.0075463364\n",
            "\n",
            "Global step: 4095,loss: 0.00699992\n",
            "\n",
            "Global step: 4096,loss: 0.012154585\n",
            "\n",
            "Global step: 4097,loss: 0.0071090227\n",
            "\n",
            "Global step: 4098,loss: 0.008189558\n",
            "\n",
            "Global step: 4099,loss: 0.0076267375\n",
            "\n",
            "Global step: 4100,loss: 0.0079818275\n",
            "\n",
            "Global step: 4101,loss: 0.00768145\n",
            "\n",
            "Global step: 4102,loss: 0.0075529697\n",
            "\n",
            "Global step: 4103,loss: 0.007455456\n",
            "\n",
            "Global step: 4104,loss: 0.007546562\n",
            "\n",
            "Global step: 4105,loss: 0.007121565\n",
            "\n",
            "Global step: 4106,loss: 0.0074664517\n",
            "\n",
            "Global step: 4107,loss: 0.0078121996\n",
            "\n",
            "Global step: 4108,loss: 0.007016004\n",
            "\n",
            "Global step: 4109,loss: 0.007979442\n",
            "\n",
            "Global step: 4110,loss: 0.007513439\n",
            "\n",
            "Global step: 4111,loss: 0.0073276428\n",
            "\n",
            "Global step: 4112,loss: 0.007759265\n",
            "\n",
            "Global step: 4113,loss: 0.0076929936\n",
            "\n",
            "Global step: 4114,loss: 0.0077292626\n",
            "\n",
            "Global step: 4115,loss: 0.007857249\n",
            "\n",
            "Global step: 4116,loss: 0.0072100367\n",
            "\n",
            "Global step: 4117,loss: 0.0067228917\n",
            "\n",
            "Global step: 4118,loss: 0.0072712656\n",
            "\n",
            "Global step: 4119,loss: 0.0069584846\n",
            "\n",
            "Global step: 4120,loss: 0.007297302\n",
            "\n",
            "Global step: 4121,loss: 0.007155382\n",
            "\n",
            "Global step: 4122,loss: 0.0066212043\n",
            "\n",
            "Global step: 4123,loss: 0.0073049674\n",
            "\n",
            "Global step: 4124,loss: 0.007321918\n",
            "\n",
            "Global step: 4125,loss: 0.0076929834\n",
            "\n",
            "Global step: 4126,loss: 0.008060852\n",
            "\n",
            "Global step: 4127,loss: 0.0070963334\n",
            "\n",
            "Global step: 4128,loss: 0.007941682\n",
            "\n",
            "Global step: 4129,loss: 0.0070082843\n",
            "\n",
            "Global step: 4130,loss: 0.007995227\n",
            "\n",
            "Global step: 4131,loss: 0.008422903\n",
            "\n",
            "Global step: 4132,loss: 0.0075946054\n",
            "\n",
            "Global step: 4133,loss: 0.007438835\n",
            "\n",
            "Global step: 4134,loss: 0.00844695\n",
            "\n",
            "Global step: 4135,loss: 0.0070764236\n",
            "\n",
            "Global step: 4136,loss: 0.0075615565\n",
            "\n",
            "Global step: 4137,loss: 0.008208568\n",
            "\n",
            "Global step: 4138,loss: 0.008158108\n",
            "\n",
            "Global step: 4139,loss: 0.007704852\n",
            "\n",
            "Global step: 4140,loss: 0.007121253\n",
            "\n",
            "Global step: 4141,loss: 0.007153212\n",
            "\n",
            "Global step: 4142,loss: 0.0073209233\n",
            "\n",
            "Global step: 4143,loss: 0.0069732545\n",
            "\n",
            "Global step: 4144,loss: 0.009281148\n",
            "\n",
            "Global step: 4145,loss: 0.0071982006\n",
            "\n",
            "Global step: 4146,loss: 0.007620786\n",
            "\n",
            "Global step: 4147,loss: 0.0077096215\n",
            "\n",
            "Global step: 4148,loss: 0.008082246\n",
            "\n",
            "Global step: 4149,loss: 0.007847296\n",
            "\n",
            "Global step: 4150,loss: 0.0076906565\n",
            "\n",
            "Global step: 4151,loss: 0.007359472\n",
            "\n",
            "Global step: 4152,loss: 0.007173772\n",
            "\n",
            "Global step: 4153,loss: 0.009392843\n",
            "\n",
            "Global step: 4154,loss: 0.0072921566\n",
            "\n",
            "Global step: 4155,loss: 0.0071500144\n",
            "\n",
            "Global step: 4156,loss: 0.007281571\n",
            "\n",
            "Global step: 4157,loss: 0.008253387\n",
            "\n",
            "Global step: 4158,loss: 0.0075813965\n",
            "\n",
            "Global step: 4159,loss: 0.008018487\n",
            "\n",
            "Global step: 4160,loss: 0.0071441378\n",
            "\n",
            "Global step: 4161,loss: 0.0076447967\n",
            "\n",
            "Global step: 4162,loss: 0.007997495\n",
            "\n",
            "Global step: 4163,loss: 0.007699861\n",
            "\n",
            "Global step: 4164,loss: 0.0071607325\n",
            "\n",
            "Global step: 4165,loss: 0.0074429554\n",
            "\n",
            "Global step: 4166,loss: 0.008193539\n",
            "\n",
            "Global step: 4167,loss: 0.0071361004\n",
            "\n",
            "Global step: 4168,loss: 0.0075368467\n",
            "\n",
            "Global step: 4169,loss: 0.0075276894\n",
            "\n",
            "Global step: 4170,loss: 0.0071138595\n",
            "\n",
            "Global step: 4171,loss: 0.0074217576\n",
            "\n",
            "Global step: 4172,loss: 0.007039457\n",
            "\n",
            "Global step: 4173,loss: 0.007840108\n",
            "\n",
            "Global step: 4174,loss: 0.0074840407\n",
            "\n",
            "Global step: 4175,loss: 0.008197215\n",
            "\n",
            "Global step: 4176,loss: 0.00834158\n",
            "\n",
            "Global step: 4177,loss: 0.006940687\n",
            "\n",
            "Global step: 4178,loss: 0.0075739715\n",
            "\n",
            "Global step: 4179,loss: 0.007393962\n",
            "\n",
            "Global step: 4180,loss: 0.007461762\n",
            "\n",
            "Global step: 4181,loss: 0.0075231697\n",
            "\n",
            "Global step: 4182,loss: 0.007078169\n",
            "\n",
            "Global step: 4183,loss: 0.0072843414\n",
            "\n",
            "Global step: 4184,loss: 0.006926138\n",
            "\n",
            "Global step: 4185,loss: 0.0076365145\n",
            "\n",
            "Global step: 4186,loss: 0.0074523375\n",
            "\n",
            "Global step: 4187,loss: 0.007283333\n",
            "\n",
            "Global step: 4188,loss: 0.007943224\n",
            "\n",
            "Global step: 4189,loss: 0.006841998\n",
            "\n",
            "Global step: 4190,loss: 0.0075812642\n",
            "\n",
            "Global step: 4191,loss: 0.007568598\n",
            "\n",
            "Global step: 4192,loss: 0.0067796605\n",
            "\n",
            "Global step: 4193,loss: 0.008564519\n",
            "\n",
            "Global step: 4194,loss: 0.0069957776\n",
            "\n",
            "Global step: 4195,loss: 0.007077976\n",
            "\n",
            "Global step: 4196,loss: 0.007713427\n",
            "\n",
            "Global step: 4197,loss: 0.0073650694\n",
            "\n",
            "Global step: 4198,loss: 0.0075529055\n",
            "\n",
            "Global step: 4199,loss: 0.0074370364\n",
            "\n",
            "Global step: 4200,loss: 0.007553065\n",
            "\n",
            "Global step: 4201,loss: 0.007917857\n",
            "\n",
            "Global step: 4202,loss: 0.0072787013\n",
            "\n",
            "Global step: 4203,loss: 0.007366141\n",
            "\n",
            "Global step: 4204,loss: 0.007497699\n",
            "\n",
            "Global step: 4205,loss: 0.0076063653\n",
            "\n",
            "Global step: 4206,loss: 0.0070416215\n",
            "\n",
            "Global step: 4207,loss: 0.0074495566\n",
            "\n",
            "Global step: 4208,loss: 0.007682521\n",
            "\n",
            "Global step: 4209,loss: 0.0074515087\n",
            "\n",
            "Global step: 4210,loss: 0.007130743\n",
            "\n",
            "Global step: 4211,loss: 0.0075403075\n",
            "\n",
            "Global step: 4212,loss: 0.0072811632\n",
            "\n",
            "Global step: 4213,loss: 0.007303196\n",
            "\n",
            "Global step: 4214,loss: 0.0072531076\n",
            "\n",
            "Global step: 4215,loss: 0.007395882\n",
            "\n",
            "Global step: 4216,loss: 0.007386689\n",
            "\n",
            "Global step: 4217,loss: 0.008238026\n",
            "\n",
            "Global step: 4218,loss: 0.0083855335\n",
            "\n",
            "Global step: 4219,loss: 0.007525065\n",
            "\n",
            "Global step: 4220,loss: 0.0070417635\n",
            "\n",
            "Global step: 4221,loss: 0.0075706993\n",
            "\n",
            "Global step: 4222,loss: 0.007223829\n",
            "\n",
            "Global step: 4223,loss: 0.007673636\n",
            "\n",
            "Global step: 4224,loss: 0.0074037598\n",
            "\n",
            "Global step: 4225,loss: 0.0075717596\n",
            "\n",
            "Global step: 4226,loss: 0.00886412\n",
            "\n",
            "Global step: 4227,loss: 0.0074380767\n",
            "\n",
            "Global step: 4228,loss: 0.007289279\n",
            "\n",
            "Global step: 4229,loss: 0.007151294\n",
            "\n",
            "Global step: 4230,loss: 0.007486552\n",
            "\n",
            "Global step: 4231,loss: 0.008000329\n",
            "\n",
            "Global step: 4232,loss: 0.011797032\n",
            "\n",
            "Global step: 4233,loss: 0.008281167\n",
            "\n",
            "Global step: 4234,loss: 0.0077539813\n",
            "\n",
            "Global step: 4235,loss: 0.008528115\n",
            "\n",
            "Global step: 4236,loss: 0.007440753\n",
            "\n",
            "Global step: 4237,loss: 0.007033848\n",
            "\n",
            "Global step: 4238,loss: 0.0073415907\n",
            "\n",
            "Global step: 4239,loss: 0.006982482\n",
            "\n",
            "Global step: 4240,loss: 0.0076192557\n",
            "\n",
            "Global step: 4241,loss: 0.0072356993\n",
            "\n",
            "Global step: 4242,loss: 0.007852942\n",
            "\n",
            "Global step: 4243,loss: 0.0074995635\n",
            "\n",
            "Global step: 4244,loss: 0.0071237925\n",
            "\n",
            "Global step: 4245,loss: 0.007286546\n",
            "\n",
            "Global step: 4246,loss: 0.0071834885\n",
            "\n",
            "Global step: 4247,loss: 0.0069575794\n",
            "\n",
            "Global step: 4248,loss: 0.0077688647\n",
            "\n",
            "Global step: 4249,loss: 0.0069855703\n",
            "\n",
            "Global step: 4250,loss: 0.0071959198\n",
            "\n",
            "Global step: 4251,loss: 0.0070554316\n",
            "\n",
            "Global step: 4252,loss: 0.009348252\n",
            "\n",
            "Global step: 4253,loss: 0.0069475602\n",
            "\n",
            "Global step: 4254,loss: 0.007585799\n",
            "\n",
            "Global step: 4255,loss: 0.007821276\n",
            "\n",
            "Global step: 4256,loss: 0.008277879\n",
            "\n",
            "Global step: 4257,loss: 0.0073465947\n",
            "\n",
            "Global step: 4258,loss: 0.007241204\n",
            "\n",
            "Global step: 4259,loss: 0.0083095385\n",
            "\n",
            "Global step: 4260,loss: 0.010083252\n",
            "\n",
            "Global step: 4261,loss: 0.007759779\n",
            "\n",
            "Global step: 4262,loss: 0.007355257\n",
            "\n",
            "Global step: 4263,loss: 0.0084071215\n",
            "\n",
            "Global step: 4264,loss: 0.0076900125\n",
            "\n",
            "Global step: 4265,loss: 0.007102462\n",
            "\n",
            "Global step: 4266,loss: 0.007349227\n",
            "\n",
            "Global step: 4267,loss: 0.007307697\n",
            "\n",
            "Global step: 4268,loss: 0.007020845\n",
            "\n",
            "Global step: 4269,loss: 0.007900441\n",
            "\n",
            "Global step: 4270,loss: 0.0081605045\n",
            "\n",
            "Global step: 4271,loss: 0.007868751\n",
            "\n",
            "Global step: 4272,loss: 0.0069593736\n",
            "\n",
            "Global step: 4273,loss: 0.006992449\n",
            "\n",
            "Global step: 4274,loss: 0.0079597\n",
            "\n",
            "Global step: 4275,loss: 0.008623916\n",
            "\n",
            "Global step: 4276,loss: 0.007381379\n",
            "\n",
            "Global step: 4277,loss: 0.008019467\n",
            "\n",
            "Global step: 4278,loss: 0.0073719798\n",
            "\n",
            "Global step: 4279,loss: 0.0073265205\n",
            "\n",
            "Global step: 4280,loss: 0.0074072247\n",
            "\n",
            "Global step: 4281,loss: 0.007123263\n",
            "\n",
            "Global step: 4282,loss: 0.0072043594\n",
            "\n",
            "Global step: 4283,loss: 0.0069349115\n",
            "\n",
            "Global step: 4284,loss: 0.007603781\n",
            "\n",
            "Global step: 4285,loss: 0.0069770413\n",
            "\n",
            "Global step: 4286,loss: 0.0070599746\n",
            "\n",
            "Global step: 4287,loss: 0.0071467655\n",
            "\n",
            "Global step: 4288,loss: 0.0073930603\n",
            "\n",
            "Global step: 4289,loss: 0.0073248316\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 4289,val_loss: 0.018504019599789992\n",
            "\n",
            "Training for epoch 12/16:\n",
            "Global step: 4290,loss: 0.007153536\n",
            "\n",
            "Global step: 4291,loss: 0.0077637746\n",
            "\n",
            "Global step: 4292,loss: 0.007128484\n",
            "\n",
            "Global step: 4293,loss: 0.0067364015\n",
            "\n",
            "Global step: 4294,loss: 0.0067204875\n",
            "\n",
            "Global step: 4295,loss: 0.0072283368\n",
            "\n",
            "Global step: 4296,loss: 0.006981014\n",
            "\n",
            "Global step: 4297,loss: 0.007041944\n",
            "\n",
            "Global step: 4298,loss: 0.0075493725\n",
            "\n",
            "Global step: 4299,loss: 0.007212695\n",
            "\n",
            "Global step: 4300,loss: 0.010381596\n",
            "\n",
            "Global step: 4301,loss: 0.006880403\n",
            "\n",
            "Global step: 4302,loss: 0.007741711\n",
            "\n",
            "Global step: 4303,loss: 0.0075013177\n",
            "\n",
            "Global step: 4304,loss: 0.0070707654\n",
            "\n",
            "Global step: 4305,loss: 0.0075321826\n",
            "\n",
            "Global step: 4306,loss: 0.008587554\n",
            "\n",
            "Global step: 4307,loss: 0.007276228\n",
            "\n",
            "Global step: 4308,loss: 0.007319697\n",
            "\n",
            "Global step: 4309,loss: 0.006834539\n",
            "\n",
            "Global step: 4310,loss: 0.0072186333\n",
            "\n",
            "Global step: 4311,loss: 0.0067862356\n",
            "\n",
            "Global step: 4312,loss: 0.0068763127\n",
            "\n",
            "Global step: 4313,loss: 0.007389131\n",
            "\n",
            "Global step: 4314,loss: 0.0070015015\n",
            "\n",
            "Global step: 4315,loss: 0.00741117\n",
            "\n",
            "Global step: 4316,loss: 0.00751001\n",
            "\n",
            "Global step: 4317,loss: 0.007037459\n",
            "\n",
            "Global step: 4318,loss: 0.0075055645\n",
            "\n",
            "Global step: 4319,loss: 0.00733801\n",
            "\n",
            "Global step: 4320,loss: 0.007315008\n",
            "\n",
            "Global step: 4321,loss: 0.007304224\n",
            "\n",
            "Global step: 4322,loss: 0.006886144\n",
            "\n",
            "Global step: 4323,loss: 0.00690864\n",
            "\n",
            "Global step: 4324,loss: 0.0071706558\n",
            "\n",
            "Global step: 4325,loss: 0.0072257486\n",
            "\n",
            "Global step: 4326,loss: 0.010489929\n",
            "\n",
            "Global step: 4327,loss: 0.0075607467\n",
            "\n",
            "Global step: 4328,loss: 0.007211148\n",
            "\n",
            "Global step: 4329,loss: 0.007912596\n",
            "\n",
            "Global step: 4330,loss: 0.006689101\n",
            "\n",
            "Global step: 4331,loss: 0.010129001\n",
            "\n",
            "Global step: 4332,loss: 0.0073211035\n",
            "\n",
            "Global step: 4333,loss: 0.0069754682\n",
            "\n",
            "Global step: 4334,loss: 0.006766898\n",
            "\n",
            "Global step: 4335,loss: 0.0068105324\n",
            "\n",
            "Global step: 4336,loss: 0.007286487\n",
            "\n",
            "Global step: 4337,loss: 0.0075641274\n",
            "\n",
            "Global step: 4338,loss: 0.007259868\n",
            "\n",
            "Global step: 4339,loss: 0.006501792\n",
            "\n",
            "Global step: 4340,loss: 0.0072782505\n",
            "\n",
            "Global step: 4341,loss: 0.007696574\n",
            "\n",
            "Global step: 4342,loss: 0.007136224\n",
            "\n",
            "Global step: 4343,loss: 0.007232564\n",
            "\n",
            "Global step: 4344,loss: 0.00689362\n",
            "\n",
            "Global step: 4345,loss: 0.0071902024\n",
            "\n",
            "Global step: 4346,loss: 0.00707276\n",
            "\n",
            "Global step: 4347,loss: 0.0073158783\n",
            "\n",
            "Global step: 4348,loss: 0.006940496\n",
            "\n",
            "Global step: 4349,loss: 0.0070841163\n",
            "\n",
            "Global step: 4350,loss: 0.006835318\n",
            "\n",
            "Global step: 4351,loss: 0.007282634\n",
            "\n",
            "Global step: 4352,loss: 0.006943306\n",
            "\n",
            "Global step: 4353,loss: 0.007467945\n",
            "\n",
            "Global step: 4354,loss: 0.006914597\n",
            "\n",
            "Global step: 4355,loss: 0.007011031\n",
            "\n",
            "Global step: 4356,loss: 0.0073032705\n",
            "\n",
            "Global step: 4357,loss: 0.010421701\n",
            "\n",
            "Global step: 4358,loss: 0.0070239543\n",
            "\n",
            "Global step: 4359,loss: 0.006488149\n",
            "\n",
            "Global step: 4360,loss: 0.0072656283\n",
            "\n",
            "Global step: 4361,loss: 0.007094416\n",
            "\n",
            "Global step: 4362,loss: 0.007035198\n",
            "\n",
            "Global step: 4363,loss: 0.0070052883\n",
            "\n",
            "Global step: 4364,loss: 0.0069870967\n",
            "\n",
            "Global step: 4365,loss: 0.006942068\n",
            "\n",
            "Global step: 4366,loss: 0.007393094\n",
            "\n",
            "Global step: 4367,loss: 0.0072833532\n",
            "\n",
            "Global step: 4368,loss: 0.0068550366\n",
            "\n",
            "Global step: 4369,loss: 0.0072739623\n",
            "\n",
            "Global step: 4370,loss: 0.006838774\n",
            "\n",
            "Global step: 4371,loss: 0.006749939\n",
            "\n",
            "Global step: 4372,loss: 0.0076610446\n",
            "\n",
            "Global step: 4373,loss: 0.007088008\n",
            "\n",
            "Global step: 4374,loss: 0.00715084\n",
            "\n",
            "Global step: 4375,loss: 0.007490119\n",
            "\n",
            "Global step: 4376,loss: 0.00717301\n",
            "\n",
            "Global step: 4377,loss: 0.0065774117\n",
            "\n",
            "Global step: 4378,loss: 0.0066600638\n",
            "\n",
            "Global step: 4379,loss: 0.007003109\n",
            "\n",
            "Global step: 4380,loss: 0.0072797113\n",
            "\n",
            "Global step: 4381,loss: 0.0073768483\n",
            "\n",
            "Global step: 4382,loss: 0.0071145967\n",
            "\n",
            "Global step: 4383,loss: 0.0069961115\n",
            "\n",
            "Global step: 4384,loss: 0.007494715\n",
            "\n",
            "Global step: 4385,loss: 0.0069906153\n",
            "\n",
            "Global step: 4386,loss: 0.00686198\n",
            "\n",
            "Global step: 4387,loss: 0.0070907683\n",
            "\n",
            "Global step: 4388,loss: 0.0066997223\n",
            "\n",
            "Global step: 4389,loss: 0.007649014\n",
            "\n",
            "Global step: 4390,loss: 0.0070376457\n",
            "\n",
            "Global step: 4391,loss: 0.0073848586\n",
            "\n",
            "Global step: 4392,loss: 0.007408548\n",
            "\n",
            "Global step: 4393,loss: 0.0073817004\n",
            "\n",
            "Global step: 4394,loss: 0.0077497894\n",
            "\n",
            "Global step: 4395,loss: 0.0068525784\n",
            "\n",
            "Global step: 4396,loss: 0.0065716114\n",
            "\n",
            "Global step: 4397,loss: 0.00685024\n",
            "\n",
            "Global step: 4398,loss: 0.007259849\n",
            "\n",
            "Global step: 4399,loss: 0.0066630165\n",
            "\n",
            "Global step: 4400,loss: 0.00732298\n",
            "\n",
            "Global step: 4401,loss: 0.0076811723\n",
            "\n",
            "Global step: 4402,loss: 0.0073714755\n",
            "\n",
            "Global step: 4403,loss: 0.0068689976\n",
            "\n",
            "Global step: 4404,loss: 0.006829325\n",
            "\n",
            "Global step: 4405,loss: 0.0069598383\n",
            "\n",
            "Global step: 4406,loss: 0.006727984\n",
            "\n",
            "Global step: 4407,loss: 0.0068749795\n",
            "\n",
            "Global step: 4408,loss: 0.00707376\n",
            "\n",
            "Global step: 4409,loss: 0.007006985\n",
            "\n",
            "Global step: 4410,loss: 0.006405373\n",
            "\n",
            "Global step: 4411,loss: 0.007178671\n",
            "\n",
            "Global step: 4412,loss: 0.0073071322\n",
            "\n",
            "Global step: 4413,loss: 0.0073151784\n",
            "\n",
            "Global step: 4414,loss: 0.0073402254\n",
            "\n",
            "Global step: 4415,loss: 0.0072669755\n",
            "\n",
            "Global step: 4416,loss: 0.0071998383\n",
            "\n",
            "Global step: 4417,loss: 0.0071527185\n",
            "\n",
            "Global step: 4418,loss: 0.014058774\n",
            "\n",
            "Global step: 4419,loss: 0.0072118947\n",
            "\n",
            "Global step: 4420,loss: 0.0071632327\n",
            "\n",
            "Global step: 4421,loss: 0.0067304745\n",
            "\n",
            "Global step: 4422,loss: 0.0068986653\n",
            "\n",
            "Global step: 4423,loss: 0.007243187\n",
            "\n",
            "Global step: 4424,loss: 0.0071807387\n",
            "\n",
            "Global step: 4425,loss: 0.007961603\n",
            "\n",
            "Global step: 4426,loss: 0.007108567\n",
            "\n",
            "Global step: 4427,loss: 0.0065417644\n",
            "\n",
            "Global step: 4428,loss: 0.007295418\n",
            "\n",
            "Global step: 4429,loss: 0.006998798\n",
            "\n",
            "Global step: 4430,loss: 0.0072191777\n",
            "\n",
            "Global step: 4431,loss: 0.007129624\n",
            "\n",
            "Global step: 4432,loss: 0.006767848\n",
            "\n",
            "Global step: 4433,loss: 0.006984103\n",
            "\n",
            "Global step: 4434,loss: 0.0075836945\n",
            "\n",
            "Global step: 4435,loss: 0.0074471794\n",
            "\n",
            "Global step: 4436,loss: 0.006936406\n",
            "\n",
            "Global step: 4437,loss: 0.00730101\n",
            "\n",
            "Global step: 4438,loss: 0.0069074933\n",
            "\n",
            "Global step: 4439,loss: 0.007432587\n",
            "\n",
            "Global step: 4440,loss: 0.007878006\n",
            "\n",
            "Global step: 4441,loss: 0.007345754\n",
            "\n",
            "Global step: 4442,loss: 0.0071321186\n",
            "\n",
            "Global step: 4443,loss: 0.007509478\n",
            "\n",
            "Global step: 4444,loss: 0.007190896\n",
            "\n",
            "Global step: 4445,loss: 0.0072859204\n",
            "\n",
            "Global step: 4446,loss: 0.0085907\n",
            "\n",
            "Global step: 4447,loss: 0.0063628103\n",
            "\n",
            "Global step: 4448,loss: 0.007629629\n",
            "\n",
            "Global step: 4449,loss: 0.0072563025\n",
            "\n",
            "Global step: 4450,loss: 0.007113836\n",
            "\n",
            "Global step: 4451,loss: 0.0075589647\n",
            "\n",
            "Global step: 4452,loss: 0.00755147\n",
            "\n",
            "Global step: 4453,loss: 0.0069708396\n",
            "\n",
            "Global step: 4454,loss: 0.006885481\n",
            "\n",
            "Global step: 4455,loss: 0.0070987716\n",
            "\n",
            "Global step: 4456,loss: 0.0070725936\n",
            "\n",
            "Global step: 4457,loss: 0.0071964352\n",
            "\n",
            "Global step: 4458,loss: 0.007742488\n",
            "\n",
            "Global step: 4459,loss: 0.006761643\n",
            "\n",
            "Global step: 4460,loss: 0.007670757\n",
            "\n",
            "Global step: 4461,loss: 0.007480452\n",
            "\n",
            "Global step: 4462,loss: 0.007506663\n",
            "\n",
            "Global step: 4463,loss: 0.0077483403\n",
            "\n",
            "Global step: 4464,loss: 0.0070427656\n",
            "\n",
            "Global step: 4465,loss: 0.0065635885\n",
            "\n",
            "Global step: 4466,loss: 0.0070723994\n",
            "\n",
            "Global step: 4467,loss: 0.006932597\n",
            "\n",
            "Global step: 4468,loss: 0.00701382\n",
            "\n",
            "Global step: 4469,loss: 0.0073796553\n",
            "\n",
            "Global step: 4470,loss: 0.006809271\n",
            "\n",
            "Global step: 4471,loss: 0.006791346\n",
            "\n",
            "Global step: 4472,loss: 0.006888801\n",
            "\n",
            "Global step: 4473,loss: 0.0065484224\n",
            "\n",
            "Global step: 4474,loss: 0.0070123165\n",
            "\n",
            "Global step: 4475,loss: 0.0066409027\n",
            "\n",
            "Global step: 4476,loss: 0.007185321\n",
            "\n",
            "Global step: 4477,loss: 0.0069133746\n",
            "\n",
            "Global step: 4478,loss: 0.011988586\n",
            "\n",
            "Global step: 4479,loss: 0.0070910277\n",
            "\n",
            "Global step: 4480,loss: 0.007877159\n",
            "\n",
            "Global step: 4481,loss: 0.007619623\n",
            "\n",
            "Global step: 4482,loss: 0.00742077\n",
            "\n",
            "Global step: 4483,loss: 0.0073315087\n",
            "\n",
            "Global step: 4484,loss: 0.0073532052\n",
            "\n",
            "Global step: 4485,loss: 0.0076123443\n",
            "\n",
            "Global step: 4486,loss: 0.0066071134\n",
            "\n",
            "Global step: 4487,loss: 0.007141152\n",
            "\n",
            "Global step: 4488,loss: 0.008831797\n",
            "\n",
            "Global step: 4489,loss: 0.0071389363\n",
            "\n",
            "Global step: 4490,loss: 0.0076037603\n",
            "\n",
            "Global step: 4491,loss: 0.0072562434\n",
            "\n",
            "Global step: 4492,loss: 0.0074733095\n",
            "\n",
            "Global step: 4493,loss: 0.007800944\n",
            "\n",
            "Global step: 4494,loss: 0.0071997484\n",
            "\n",
            "Global step: 4495,loss: 0.007055657\n",
            "\n",
            "Global step: 4496,loss: 0.0068051764\n",
            "\n",
            "Global step: 4497,loss: 0.0071906587\n",
            "\n",
            "Global step: 4498,loss: 0.006955445\n",
            "\n",
            "Global step: 4499,loss: 0.0072748256\n",
            "\n",
            "Global step: 4500,loss: 0.0068156784\n",
            "\n",
            "Global step: 4501,loss: 0.0066159265\n",
            "\n",
            "Global step: 4502,loss: 0.007125121\n",
            "\n",
            "Global step: 4503,loss: 0.00713352\n",
            "\n",
            "Global step: 4504,loss: 0.0072419043\n",
            "\n",
            "Global step: 4505,loss: 0.0073703127\n",
            "\n",
            "Global step: 4506,loss: 0.00738371\n",
            "\n",
            "Global step: 4507,loss: 0.011073336\n",
            "\n",
            "Global step: 4508,loss: 0.0070424415\n",
            "\n",
            "Global step: 4509,loss: 0.006451773\n",
            "\n",
            "Global step: 4510,loss: 0.011816727\n",
            "\n",
            "Global step: 4511,loss: 0.012075365\n",
            "\n",
            "Global step: 4512,loss: 0.006781145\n",
            "\n",
            "Global step: 4513,loss: 0.007295929\n",
            "\n",
            "Global step: 4514,loss: 0.007041768\n",
            "\n",
            "Global step: 4515,loss: 0.0074797855\n",
            "\n",
            "Global step: 4516,loss: 0.007282312\n",
            "\n",
            "Global step: 4517,loss: 0.0072241277\n",
            "\n",
            "Global step: 4518,loss: 0.007376531\n",
            "\n",
            "Global step: 4519,loss: 0.0074899895\n",
            "\n",
            "Global step: 4520,loss: 0.0069498383\n",
            "\n",
            "Global step: 4521,loss: 0.008388753\n",
            "\n",
            "Global step: 4522,loss: 0.006936685\n",
            "\n",
            "Global step: 4523,loss: 0.007529867\n",
            "\n",
            "Global step: 4524,loss: 0.0071773482\n",
            "\n",
            "Global step: 4525,loss: 0.0065347073\n",
            "\n",
            "Global step: 4526,loss: 0.0069694384\n",
            "\n",
            "Global step: 4527,loss: 0.008239028\n",
            "\n",
            "Global step: 4528,loss: 0.0076742405\n",
            "\n",
            "Global step: 4529,loss: 0.0068833884\n",
            "\n",
            "Global step: 4530,loss: 0.0068688816\n",
            "\n",
            "Global step: 4531,loss: 0.0071571027\n",
            "\n",
            "Global step: 4532,loss: 0.0074414127\n",
            "\n",
            "Global step: 4533,loss: 0.0076732356\n",
            "\n",
            "Global step: 4534,loss: 0.0072535644\n",
            "\n",
            "Global step: 4535,loss: 0.0088235475\n",
            "\n",
            "Global step: 4536,loss: 0.007551612\n",
            "\n",
            "Global step: 4537,loss: 0.007365482\n",
            "\n",
            "Global step: 4538,loss: 0.007122069\n",
            "\n",
            "Global step: 4539,loss: 0.007407171\n",
            "\n",
            "Global step: 4540,loss: 0.0074218595\n",
            "\n",
            "Global step: 4541,loss: 0.0073920107\n",
            "\n",
            "Global step: 4542,loss: 0.007380307\n",
            "\n",
            "Global step: 4543,loss: 0.0069589675\n",
            "\n",
            "Global step: 4544,loss: 0.0072333026\n",
            "\n",
            "Global step: 4545,loss: 0.006727078\n",
            "\n",
            "Global step: 4546,loss: 0.0073761917\n",
            "\n",
            "Global step: 4547,loss: 0.0069549964\n",
            "\n",
            "Global step: 4548,loss: 0.007051105\n",
            "\n",
            "Global step: 4549,loss: 0.0070357276\n",
            "\n",
            "Global step: 4550,loss: 0.0071653463\n",
            "\n",
            "Global step: 4551,loss: 0.007387835\n",
            "\n",
            "Global step: 4552,loss: 0.007168403\n",
            "\n",
            "Global step: 4553,loss: 0.0071520275\n",
            "\n",
            "Global step: 4554,loss: 0.0068863346\n",
            "\n",
            "Global step: 4555,loss: 0.007258525\n",
            "\n",
            "Global step: 4556,loss: 0.006965014\n",
            "\n",
            "Global step: 4557,loss: 0.0067711663\n",
            "\n",
            "Global step: 4558,loss: 0.0066838744\n",
            "\n",
            "Global step: 4559,loss: 0.0069462047\n",
            "\n",
            "Global step: 4560,loss: 0.0068033175\n",
            "\n",
            "Global step: 4561,loss: 0.0069401287\n",
            "\n",
            "Global step: 4562,loss: 0.00690923\n",
            "\n",
            "Global step: 4563,loss: 0.0071128984\n",
            "\n",
            "Global step: 4564,loss: 0.007724121\n",
            "\n",
            "Global step: 4565,loss: 0.0065245316\n",
            "\n",
            "Global step: 4566,loss: 0.00721786\n",
            "\n",
            "Global step: 4567,loss: 0.0069551365\n",
            "\n",
            "Global step: 4568,loss: 0.0070687276\n",
            "\n",
            "Global step: 4569,loss: 0.0065022474\n",
            "\n",
            "Global step: 4570,loss: 0.007012862\n",
            "\n",
            "Global step: 4571,loss: 0.0070531894\n",
            "\n",
            "Global step: 4572,loss: 0.0073674666\n",
            "\n",
            "Global step: 4573,loss: 0.009772161\n",
            "\n",
            "Global step: 4574,loss: 0.0070439205\n",
            "\n",
            "Global step: 4575,loss: 0.00730571\n",
            "\n",
            "Global step: 4576,loss: 0.008045867\n",
            "\n",
            "Global step: 4577,loss: 0.006708441\n",
            "\n",
            "Global step: 4578,loss: 0.0075142803\n",
            "\n",
            "Global step: 4579,loss: 0.0070277015\n",
            "\n",
            "Global step: 4580,loss: 0.0069830837\n",
            "\n",
            "Global step: 4581,loss: 0.0070331725\n",
            "\n",
            "Global step: 4582,loss: 0.007599017\n",
            "\n",
            "Global step: 4583,loss: 0.0076246955\n",
            "\n",
            "Global step: 4584,loss: 0.006890051\n",
            "\n",
            "Global step: 4585,loss: 0.007094479\n",
            "\n",
            "Global step: 4586,loss: 0.0072093895\n",
            "\n",
            "Global step: 4587,loss: 0.006799794\n",
            "\n",
            "Global step: 4588,loss: 0.006897563\n",
            "\n",
            "Global step: 4589,loss: 0.007101543\n",
            "\n",
            "Global step: 4590,loss: 0.007018382\n",
            "\n",
            "Global step: 4591,loss: 0.007028236\n",
            "\n",
            "Global step: 4592,loss: 0.006882756\n",
            "\n",
            "Global step: 4593,loss: 0.0066925236\n",
            "\n",
            "Global step: 4594,loss: 0.006436134\n",
            "\n",
            "Global step: 4595,loss: 0.0071389624\n",
            "\n",
            "Global step: 4596,loss: 0.007209365\n",
            "\n",
            "Global step: 4597,loss: 0.006480442\n",
            "\n",
            "Global step: 4598,loss: 0.0075973193\n",
            "\n",
            "Global step: 4599,loss: 0.0067421445\n",
            "\n",
            "Global step: 4600,loss: 0.0069654454\n",
            "\n",
            "Global step: 4601,loss: 0.0065608197\n",
            "\n",
            "Global step: 4602,loss: 0.0067012017\n",
            "\n",
            "Global step: 4603,loss: 0.009068188\n",
            "\n",
            "Global step: 4604,loss: 0.0068578534\n",
            "\n",
            "Global step: 4605,loss: 0.0074940952\n",
            "\n",
            "Global step: 4606,loss: 0.0067819697\n",
            "\n",
            "Global step: 4607,loss: 0.0067163184\n",
            "\n",
            "Global step: 4608,loss: 0.007122904\n",
            "\n",
            "Global step: 4609,loss: 0.007694397\n",
            "\n",
            "Global step: 4610,loss: 0.0069498834\n",
            "\n",
            "Global step: 4611,loss: 0.007237228\n",
            "\n",
            "Global step: 4612,loss: 0.00693741\n",
            "\n",
            "Global step: 4613,loss: 0.007157531\n",
            "\n",
            "Global step: 4614,loss: 0.007423714\n",
            "\n",
            "Global step: 4615,loss: 0.0069231354\n",
            "\n",
            "Global step: 4616,loss: 0.006775262\n",
            "\n",
            "Global step: 4617,loss: 0.0072805407\n",
            "\n",
            "Global step: 4618,loss: 0.007052611\n",
            "\n",
            "Global step: 4619,loss: 0.006797637\n",
            "\n",
            "Global step: 4620,loss: 0.0066984436\n",
            "\n",
            "Global step: 4621,loss: 0.006736882\n",
            "\n",
            "Global step: 4622,loss: 0.0074458253\n",
            "\n",
            "Global step: 4623,loss: 0.0068813595\n",
            "\n",
            "Global step: 4624,loss: 0.007098202\n",
            "\n",
            "Global step: 4625,loss: 0.0069692926\n",
            "\n",
            "Global step: 4626,loss: 0.006805611\n",
            "\n",
            "Global step: 4627,loss: 0.0067814793\n",
            "\n",
            "Global step: 4628,loss: 0.0074424786\n",
            "\n",
            "Global step: 4629,loss: 0.006751976\n",
            "\n",
            "Global step: 4630,loss: 0.0070402366\n",
            "\n",
            "Global step: 4631,loss: 0.0074283173\n",
            "\n",
            "Global step: 4632,loss: 0.0068207206\n",
            "\n",
            "Global step: 4633,loss: 0.0074084103\n",
            "\n",
            "Global step: 4634,loss: 0.007874849\n",
            "\n",
            "Global step: 4635,loss: 0.00697565\n",
            "\n",
            "Global step: 4636,loss: 0.0072363215\n",
            "\n",
            "Global step: 4637,loss: 0.007300311\n",
            "\n",
            "Global step: 4638,loss: 0.006813254\n",
            "\n",
            "Global step: 4639,loss: 0.0069151036\n",
            "\n",
            "Global step: 4640,loss: 0.0071761166\n",
            "\n",
            "Global step: 4641,loss: 0.0069909114\n",
            "\n",
            "Global step: 4642,loss: 0.007115491\n",
            "\n",
            "Global step: 4643,loss: 0.0068627014\n",
            "\n",
            "Global step: 4644,loss: 0.007132453\n",
            "\n",
            "Global step: 4645,loss: 0.0067440188\n",
            "\n",
            "Global step: 4646,loss: 0.009838563\n",
            "\n",
            "Global step: 4647,loss: 0.0073980056\n",
            "\n",
            "Global step: 4648,loss: 0.006983941\n",
            "\n",
            "Global step: 4649,loss: 0.0073370403\n",
            "\n",
            "Global step: 4650,loss: 0.0075276396\n",
            "\n",
            "Global step: 4651,loss: 0.0066857995\n",
            "\n",
            "Global step: 4652,loss: 0.0065654726\n",
            "\n",
            "Global step: 4653,loss: 0.0070750746\n",
            "\n",
            "Global step: 4654,loss: 0.007316424\n",
            "\n",
            "Global step: 4655,loss: 0.006854695\n",
            "\n",
            "Global step: 4656,loss: 0.007045015\n",
            "\n",
            "Global step: 4657,loss: 0.0065230913\n",
            "\n",
            "Global step: 4658,loss: 0.00696698\n",
            "\n",
            "Global step: 4659,loss: 0.0070898575\n",
            "\n",
            "Global step: 4660,loss: 0.006981826\n",
            "\n",
            "Global step: 4661,loss: 0.0077094575\n",
            "\n",
            "Global step: 4662,loss: 0.0073140226\n",
            "\n",
            "Global step: 4663,loss: 0.007251447\n",
            "\n",
            "Global step: 4664,loss: 0.0069108335\n",
            "\n",
            "Global step: 4665,loss: 0.007483086\n",
            "\n",
            "Global step: 4666,loss: 0.007325125\n",
            "\n",
            "Global step: 4667,loss: 0.0074871555\n",
            "\n",
            "Global step: 4668,loss: 0.008993689\n",
            "\n",
            "Global step: 4669,loss: 0.007099042\n",
            "\n",
            "Global step: 4670,loss: 0.007206627\n",
            "\n",
            "Global step: 4671,loss: 0.0066746064\n",
            "\n",
            "Global step: 4672,loss: 0.009566179\n",
            "\n",
            "Global step: 4673,loss: 0.0075218175\n",
            "\n",
            "Global step: 4674,loss: 0.00692559\n",
            "\n",
            "Global step: 4675,loss: 0.006684803\n",
            "\n",
            "Global step: 4676,loss: 0.0070644626\n",
            "\n",
            "Global step: 4677,loss: 0.007623809\n",
            "\n",
            "Global step: 4678,loss: 0.0068955333\n",
            "\n",
            "Global step: 4679,loss: 0.0071682883\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 4679,val_loss: 0.018139871899993755\n",
            "\n",
            "Training for epoch 13/16:\n",
            "Global step: 4680,loss: 0.006935899\n",
            "\n",
            "Global step: 4681,loss: 0.007472952\n",
            "\n",
            "Global step: 4682,loss: 0.006658471\n",
            "\n",
            "Global step: 4683,loss: 0.0065825777\n",
            "\n",
            "Global step: 4684,loss: 0.0070058615\n",
            "\n",
            "Global step: 4685,loss: 0.0069161616\n",
            "\n",
            "Global step: 4686,loss: 0.0068319445\n",
            "\n",
            "Global step: 4687,loss: 0.0066884807\n",
            "\n",
            "Global step: 4688,loss: 0.0075120474\n",
            "\n",
            "Global step: 4689,loss: 0.0070078224\n",
            "\n",
            "Global step: 4690,loss: 0.0069424314\n",
            "\n",
            "Global step: 4691,loss: 0.0070037865\n",
            "\n",
            "Global step: 4692,loss: 0.0068130186\n",
            "\n",
            "Global step: 4693,loss: 0.007768908\n",
            "\n",
            "Global step: 4694,loss: 0.0064193145\n",
            "\n",
            "Global step: 4695,loss: 0.006648774\n",
            "\n",
            "Global step: 4696,loss: 0.007000405\n",
            "\n",
            "Global step: 4697,loss: 0.007279264\n",
            "\n",
            "Global step: 4698,loss: 0.006909835\n",
            "\n",
            "Global step: 4699,loss: 0.0071698865\n",
            "\n",
            "Global step: 4700,loss: 0.0064401943\n",
            "\n",
            "Global step: 4701,loss: 0.0070589255\n",
            "\n",
            "Global step: 4702,loss: 0.006670008\n",
            "\n",
            "Global step: 4703,loss: 0.0072437166\n",
            "\n",
            "Global step: 4704,loss: 0.006813654\n",
            "\n",
            "Global step: 4705,loss: 0.0077291327\n",
            "\n",
            "Global step: 4706,loss: 0.006666378\n",
            "\n",
            "Global step: 4707,loss: 0.0069521614\n",
            "\n",
            "Global step: 4708,loss: 0.006894917\n",
            "\n",
            "Global step: 4709,loss: 0.006929716\n",
            "\n",
            "Global step: 4710,loss: 0.007613918\n",
            "\n",
            "Global step: 4711,loss: 0.0065349387\n",
            "\n",
            "Global step: 4712,loss: 0.0067804246\n",
            "\n",
            "Global step: 4713,loss: 0.006686479\n",
            "\n",
            "Global step: 4714,loss: 0.0065627084\n",
            "\n",
            "Global step: 4715,loss: 0.0070943246\n",
            "\n",
            "Global step: 4716,loss: 0.00679162\n",
            "\n",
            "Global step: 4717,loss: 0.0069487887\n",
            "\n",
            "Global step: 4718,loss: 0.0070218625\n",
            "\n",
            "Global step: 4719,loss: 0.006605531\n",
            "\n",
            "Global step: 4720,loss: 0.0065658363\n",
            "\n",
            "Global step: 4721,loss: 0.006949211\n",
            "\n",
            "Global step: 4722,loss: 0.0071071447\n",
            "\n",
            "Global step: 4723,loss: 0.006651008\n",
            "\n",
            "Global step: 4724,loss: 0.0070972284\n",
            "\n",
            "Global step: 4725,loss: 0.00664515\n",
            "\n",
            "Global step: 4726,loss: 0.006585522\n",
            "\n",
            "Global step: 4727,loss: 0.0068391627\n",
            "\n",
            "Global step: 4728,loss: 0.0071728714\n",
            "\n",
            "Global step: 4729,loss: 0.010981656\n",
            "\n",
            "Global step: 4730,loss: 0.0068220953\n",
            "\n",
            "Global step: 4731,loss: 0.006777622\n",
            "\n",
            "Global step: 4732,loss: 0.007008983\n",
            "\n",
            "Global step: 4733,loss: 0.006913791\n",
            "\n",
            "Global step: 4734,loss: 0.006696314\n",
            "\n",
            "Global step: 4735,loss: 0.0076136612\n",
            "\n",
            "Global step: 4736,loss: 0.0067039\n",
            "\n",
            "Global step: 4737,loss: 0.0065311017\n",
            "\n",
            "Global step: 4738,loss: 0.006970815\n",
            "\n",
            "Global step: 4739,loss: 0.009723774\n",
            "\n",
            "Global step: 4740,loss: 0.0066987127\n",
            "\n",
            "Global step: 4741,loss: 0.007386591\n",
            "\n",
            "Global step: 4742,loss: 0.006735405\n",
            "\n",
            "Global step: 4743,loss: 0.006962359\n",
            "\n",
            "Global step: 4744,loss: 0.00681909\n",
            "\n",
            "Global step: 4745,loss: 0.0066598505\n",
            "\n",
            "Global step: 4746,loss: 0.0067246314\n",
            "\n",
            "Global step: 4747,loss: 0.0068232263\n",
            "\n",
            "Global step: 4748,loss: 0.0067487597\n",
            "\n",
            "Global step: 4749,loss: 0.0068344106\n",
            "\n",
            "Global step: 4750,loss: 0.007016117\n",
            "\n",
            "Global step: 4751,loss: 0.006501703\n",
            "\n",
            "Global step: 4752,loss: 0.006476458\n",
            "\n",
            "Global step: 4753,loss: 0.0069381827\n",
            "\n",
            "Global step: 4754,loss: 0.006620531\n",
            "\n",
            "Global step: 4755,loss: 0.010968465\n",
            "\n",
            "Global step: 4756,loss: 0.0069123204\n",
            "\n",
            "Global step: 4757,loss: 0.006897985\n",
            "\n",
            "Global step: 4758,loss: 0.006525311\n",
            "\n",
            "Global step: 4759,loss: 0.0068313265\n",
            "\n",
            "Global step: 4760,loss: 0.0073435823\n",
            "\n",
            "Global step: 4761,loss: 0.0065915054\n",
            "\n",
            "Global step: 4762,loss: 0.006727239\n",
            "\n",
            "Global step: 4763,loss: 0.006813004\n",
            "\n",
            "Global step: 4764,loss: 0.006897293\n",
            "\n",
            "Global step: 4765,loss: 0.006914328\n",
            "\n",
            "Global step: 4766,loss: 0.00733154\n",
            "\n",
            "Global step: 4767,loss: 0.0065733492\n",
            "\n",
            "Global step: 4768,loss: 0.006886038\n",
            "\n",
            "Global step: 4769,loss: 0.006807023\n",
            "\n",
            "Global step: 4770,loss: 0.0066556805\n",
            "\n",
            "Global step: 4771,loss: 0.006826121\n",
            "\n",
            "Global step: 4772,loss: 0.0066719977\n",
            "\n",
            "Global step: 4773,loss: 0.0074315574\n",
            "\n",
            "Global step: 4774,loss: 0.0070041367\n",
            "\n",
            "Global step: 4775,loss: 0.0062459074\n",
            "\n",
            "Global step: 4776,loss: 0.0070106625\n",
            "\n",
            "Global step: 4777,loss: 0.006951089\n",
            "\n",
            "Global step: 4778,loss: 0.0068227733\n",
            "\n",
            "Global step: 4779,loss: 0.006725949\n",
            "\n",
            "Global step: 4780,loss: 0.006500507\n",
            "\n",
            "Global step: 4781,loss: 0.007022274\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 4782.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:16:52.434868 139735711573760 supervisor.py:1050] Recording summary at step 4782.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.82855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:16:52.577191 139735703181056 supervisor.py:1099] global_step/sec: 6.82855\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4782,loss: 0.006777059\n",
            "\n",
            "Global step: 4783,loss: 0.0075422227\n",
            "\n",
            "Global step: 4784,loss: 0.006932754\n",
            "\n",
            "Global step: 4785,loss: 0.006667882\n",
            "\n",
            "Global step: 4786,loss: 0.0065260567\n",
            "\n",
            "Global step: 4787,loss: 0.007014403\n",
            "\n",
            "Global step: 4788,loss: 0.006713775\n",
            "\n",
            "Global step: 4789,loss: 0.0066321134\n",
            "\n",
            "Global step: 4790,loss: 0.0068817325\n",
            "\n",
            "Global step: 4791,loss: 0.0067687975\n",
            "\n",
            "Global step: 4792,loss: 0.006593717\n",
            "\n",
            "Global step: 4793,loss: 0.006853223\n",
            "\n",
            "Global step: 4794,loss: 0.0069905673\n",
            "\n",
            "Global step: 4795,loss: 0.00706808\n",
            "\n",
            "Global step: 4796,loss: 0.0063490127\n",
            "\n",
            "Global step: 4797,loss: 0.006370666\n",
            "\n",
            "Global step: 4798,loss: 0.0068859314\n",
            "\n",
            "Global step: 4799,loss: 0.0061856806\n",
            "\n",
            "Global step: 4800,loss: 0.0069978964\n",
            "\n",
            "Global step: 4801,loss: 0.0066948473\n",
            "\n",
            "Global step: 4802,loss: 0.0064299083\n",
            "\n",
            "Global step: 4803,loss: 0.006134994\n",
            "\n",
            "Global step: 4804,loss: 0.0071388753\n",
            "\n",
            "Global step: 4805,loss: 0.0063277585\n",
            "\n",
            "Global step: 4806,loss: 0.0068539875\n",
            "\n",
            "Global step: 4807,loss: 0.006390185\n",
            "\n",
            "Global step: 4808,loss: 0.0067708865\n",
            "\n",
            "Global step: 4809,loss: 0.0068793823\n",
            "\n",
            "Global step: 4810,loss: 0.0064684977\n",
            "\n",
            "Global step: 4811,loss: 0.006409408\n",
            "\n",
            "Global step: 4812,loss: 0.0066231936\n",
            "\n",
            "Global step: 4813,loss: 0.0064319707\n",
            "\n",
            "Global step: 4814,loss: 0.0064269085\n",
            "\n",
            "Global step: 4815,loss: 0.0067641838\n",
            "\n",
            "Global step: 4816,loss: 0.006685741\n",
            "\n",
            "Global step: 4817,loss: 0.0064431443\n",
            "\n",
            "Global step: 4818,loss: 0.006364531\n",
            "\n",
            "Global step: 4819,loss: 0.0069009694\n",
            "\n",
            "Global step: 4820,loss: 0.0061962665\n",
            "\n",
            "Global step: 4821,loss: 0.0064886743\n",
            "\n",
            "Global step: 4822,loss: 0.0065300907\n",
            "\n",
            "Global step: 4823,loss: 0.006555991\n",
            "\n",
            "Global step: 4824,loss: 0.006572332\n",
            "\n",
            "Global step: 4825,loss: 0.00629963\n",
            "\n",
            "Global step: 4826,loss: 0.006301996\n",
            "\n",
            "Global step: 4827,loss: 0.0068295724\n",
            "\n",
            "Global step: 4828,loss: 0.006666463\n",
            "\n",
            "Global step: 4829,loss: 0.006366098\n",
            "\n",
            "Global step: 4830,loss: 0.0066351383\n",
            "\n",
            "Global step: 4831,loss: 0.0067591346\n",
            "\n",
            "Global step: 4832,loss: 0.0069468003\n",
            "\n",
            "Global step: 4833,loss: 0.0063600237\n",
            "\n",
            "Global step: 4834,loss: 0.0062897163\n",
            "\n",
            "Global step: 4835,loss: 0.0066502644\n",
            "\n",
            "Global step: 4836,loss: 0.006997466\n",
            "\n",
            "Global step: 4837,loss: 0.0067186602\n",
            "\n",
            "Global step: 4838,loss: 0.006399454\n",
            "\n",
            "Global step: 4839,loss: 0.0066880845\n",
            "\n",
            "Global step: 4840,loss: 0.0065241517\n",
            "\n",
            "Global step: 4841,loss: 0.006496834\n",
            "\n",
            "Global step: 4842,loss: 0.00628861\n",
            "\n",
            "Global step: 4843,loss: 0.0067367926\n",
            "\n",
            "Global step: 4844,loss: 0.006715047\n",
            "\n",
            "Global step: 4845,loss: 0.0065576965\n",
            "\n",
            "Global step: 4846,loss: 0.006475335\n",
            "\n",
            "Global step: 4847,loss: 0.006412574\n",
            "\n",
            "Global step: 4848,loss: 0.0062428713\n",
            "\n",
            "Global step: 4849,loss: 0.006809853\n",
            "\n",
            "Global step: 4850,loss: 0.006941559\n",
            "\n",
            "Global step: 4851,loss: 0.0066155083\n",
            "\n",
            "Global step: 4852,loss: 0.006683285\n",
            "\n",
            "Global step: 4853,loss: 0.0067950697\n",
            "\n",
            "Global step: 4854,loss: 0.006678002\n",
            "\n",
            "Global step: 4855,loss: 0.0064543253\n",
            "\n",
            "Global step: 4856,loss: 0.0065930807\n",
            "\n",
            "Global step: 4857,loss: 0.006354031\n",
            "\n",
            "Global step: 4858,loss: 0.006893797\n",
            "\n",
            "Global step: 4859,loss: 0.0064663193\n",
            "\n",
            "Global step: 4860,loss: 0.006307397\n",
            "\n",
            "Global step: 4861,loss: 0.006835326\n",
            "\n",
            "Global step: 4862,loss: 0.006744006\n",
            "\n",
            "Global step: 4863,loss: 0.0070732622\n",
            "\n",
            "Global step: 4864,loss: 0.0068983804\n",
            "\n",
            "Global step: 4865,loss: 0.0066163456\n",
            "\n",
            "Global step: 4866,loss: 0.0066153016\n",
            "\n",
            "Global step: 4867,loss: 0.0063345833\n",
            "\n",
            "Global step: 4868,loss: 0.0060370318\n",
            "\n",
            "Global step: 4869,loss: 0.006834675\n",
            "\n",
            "Global step: 4870,loss: 0.0067129587\n",
            "\n",
            "Global step: 4871,loss: 0.0062956256\n",
            "\n",
            "Global step: 4872,loss: 0.0065781195\n",
            "\n",
            "Global step: 4873,loss: 0.006464481\n",
            "\n",
            "Global step: 4874,loss: 0.009432132\n",
            "\n",
            "Global step: 4875,loss: 0.006223249\n",
            "\n",
            "Global step: 4876,loss: 0.0065883133\n",
            "\n",
            "Global step: 4877,loss: 0.006239084\n",
            "\n",
            "Global step: 4878,loss: 0.0069271936\n",
            "\n",
            "Global step: 4879,loss: 0.006931124\n",
            "\n",
            "Global step: 4880,loss: 0.0063144476\n",
            "\n",
            "Global step: 4881,loss: 0.0064847744\n",
            "\n",
            "Global step: 4882,loss: 0.0067175026\n",
            "\n",
            "Global step: 4883,loss: 0.0068683606\n",
            "\n",
            "Global step: 4884,loss: 0.0067055067\n",
            "\n",
            "Global step: 4885,loss: 0.006844167\n",
            "\n",
            "Global step: 4886,loss: 0.00694043\n",
            "\n",
            "Global step: 4887,loss: 0.0068205786\n",
            "\n",
            "Global step: 4888,loss: 0.008639164\n",
            "\n",
            "Global step: 4889,loss: 0.006488453\n",
            "\n",
            "Global step: 4890,loss: 0.006804061\n",
            "\n",
            "Global step: 4891,loss: 0.006606051\n",
            "\n",
            "Global step: 4892,loss: 0.006616798\n",
            "\n",
            "Global step: 4893,loss: 0.0064574312\n",
            "\n",
            "Global step: 4894,loss: 0.006457435\n",
            "\n",
            "Global step: 4895,loss: 0.00744396\n",
            "\n",
            "Global step: 4896,loss: 0.006614101\n",
            "\n",
            "Global step: 4897,loss: 0.0064561\n",
            "\n",
            "Global step: 4898,loss: 0.006887001\n",
            "\n",
            "Global step: 4899,loss: 0.0061893915\n",
            "\n",
            "Global step: 4900,loss: 0.006553571\n",
            "\n",
            "Global step: 4901,loss: 0.008090856\n",
            "\n",
            "Global step: 4902,loss: 0.0074327164\n",
            "\n",
            "Global step: 4903,loss: 0.0064230487\n",
            "\n",
            "Global step: 4904,loss: 0.006534006\n",
            "\n",
            "Global step: 4905,loss: 0.006777285\n",
            "\n",
            "Global step: 4906,loss: 0.0067058452\n",
            "\n",
            "Global step: 4907,loss: 0.006452223\n",
            "\n",
            "Global step: 4908,loss: 0.007215757\n",
            "\n",
            "Global step: 4909,loss: 0.006625985\n",
            "\n",
            "Global step: 4910,loss: 0.0063641616\n",
            "\n",
            "Global step: 4911,loss: 0.00665614\n",
            "\n",
            "Global step: 4912,loss: 0.006999026\n",
            "\n",
            "Global step: 4913,loss: 0.0065510906\n",
            "\n",
            "Global step: 4914,loss: 0.006357983\n",
            "\n",
            "Global step: 4915,loss: 0.0066723595\n",
            "\n",
            "Global step: 4916,loss: 0.007370676\n",
            "\n",
            "Global step: 4917,loss: 0.0066168024\n",
            "\n",
            "Global step: 4918,loss: 0.0064206407\n",
            "\n",
            "Global step: 4919,loss: 0.006936768\n",
            "\n",
            "Global step: 4920,loss: 0.0065636015\n",
            "\n",
            "Global step: 4921,loss: 0.0071726395\n",
            "\n",
            "Global step: 4922,loss: 0.0068587353\n",
            "\n",
            "Global step: 4923,loss: 0.0065303156\n",
            "\n",
            "Global step: 4924,loss: 0.0063694445\n",
            "\n",
            "Global step: 4925,loss: 0.006835858\n",
            "\n",
            "Global step: 4926,loss: 0.0067881285\n",
            "\n",
            "Global step: 4927,loss: 0.0065979525\n",
            "\n",
            "Global step: 4928,loss: 0.006916256\n",
            "\n",
            "Global step: 4929,loss: 0.0066276705\n",
            "\n",
            "Global step: 4930,loss: 0.006735385\n",
            "\n",
            "Global step: 4931,loss: 0.0065184794\n",
            "\n",
            "Global step: 4932,loss: 0.0068216235\n",
            "\n",
            "Global step: 4933,loss: 0.007081611\n",
            "\n",
            "Global step: 4934,loss: 0.006408781\n",
            "\n",
            "Global step: 4935,loss: 0.007250521\n",
            "\n",
            "Global step: 4936,loss: 0.006880586\n",
            "\n",
            "Global step: 4937,loss: 0.007028832\n",
            "\n",
            "Global step: 4938,loss: 0.0071202787\n",
            "\n",
            "Global step: 4939,loss: 0.0063491836\n",
            "\n",
            "Global step: 4940,loss: 0.0065585067\n",
            "\n",
            "Global step: 4941,loss: 0.0064600166\n",
            "\n",
            "Global step: 4942,loss: 0.006745486\n",
            "\n",
            "Global step: 4943,loss: 0.00620815\n",
            "\n",
            "Global step: 4944,loss: 0.006467949\n",
            "\n",
            "Global step: 4945,loss: 0.006945615\n",
            "\n",
            "Global step: 4946,loss: 0.006563911\n",
            "\n",
            "Global step: 4947,loss: 0.0062467987\n",
            "\n",
            "Global step: 4948,loss: 0.0065030092\n",
            "\n",
            "Global step: 4949,loss: 0.0064415666\n",
            "\n",
            "Global step: 4950,loss: 0.0064493087\n",
            "\n",
            "Global step: 4951,loss: 0.0068240277\n",
            "\n",
            "Global step: 4952,loss: 0.0063130213\n",
            "\n",
            "Global step: 4953,loss: 0.006763836\n",
            "\n",
            "Global step: 4954,loss: 0.0068374667\n",
            "\n",
            "Global step: 4955,loss: 0.00717374\n",
            "\n",
            "Global step: 4956,loss: 0.0066564647\n",
            "\n",
            "Global step: 4957,loss: 0.0070061237\n",
            "\n",
            "Global step: 4958,loss: 0.006790026\n",
            "\n",
            "Global step: 4959,loss: 0.0065159258\n",
            "\n",
            "Global step: 4960,loss: 0.006679251\n",
            "\n",
            "Global step: 4961,loss: 0.0071770395\n",
            "\n",
            "Global step: 4962,loss: 0.0064557213\n",
            "\n",
            "Global step: 4963,loss: 0.006577947\n",
            "\n",
            "Global step: 4964,loss: 0.0066372645\n",
            "\n",
            "Global step: 4965,loss: 0.0066668135\n",
            "\n",
            "Global step: 4966,loss: 0.006699309\n",
            "\n",
            "Global step: 4967,loss: 0.0067323353\n",
            "\n",
            "Global step: 4968,loss: 0.0065857866\n",
            "\n",
            "Global step: 4969,loss: 0.0066835615\n",
            "\n",
            "Global step: 4970,loss: 0.0068584783\n",
            "\n",
            "Global step: 4971,loss: 0.0066268565\n",
            "\n",
            "Global step: 4972,loss: 0.006083404\n",
            "\n",
            "Global step: 4973,loss: 0.006666864\n",
            "\n",
            "Global step: 4974,loss: 0.007021112\n",
            "\n",
            "Global step: 4975,loss: 0.007557566\n",
            "\n",
            "Global step: 4976,loss: 0.006515899\n",
            "\n",
            "Global step: 4977,loss: 0.006821667\n",
            "\n",
            "Global step: 4978,loss: 0.0062706713\n",
            "\n",
            "Global step: 4979,loss: 0.0070176157\n",
            "\n",
            "Global step: 4980,loss: 0.006368181\n",
            "\n",
            "Global step: 4981,loss: 0.0065143146\n",
            "\n",
            "Global step: 4982,loss: 0.0068142405\n",
            "\n",
            "Global step: 4983,loss: 0.006423457\n",
            "\n",
            "Global step: 4984,loss: 0.006744864\n",
            "\n",
            "Global step: 4985,loss: 0.006819823\n",
            "\n",
            "Global step: 4986,loss: 0.007440544\n",
            "\n",
            "Global step: 4987,loss: 0.0067850444\n",
            "\n",
            "Global step: 4988,loss: 0.006760381\n",
            "\n",
            "Global step: 4989,loss: 0.006783773\n",
            "\n",
            "Global step: 4990,loss: 0.0067927297\n",
            "\n",
            "Global step: 4991,loss: 0.006772069\n",
            "\n",
            "Global step: 4992,loss: 0.007058019\n",
            "\n",
            "Global step: 4993,loss: 0.0065148324\n",
            "\n",
            "Global step: 4994,loss: 0.0066643604\n",
            "\n",
            "Global step: 4995,loss: 0.006421231\n",
            "\n",
            "Global step: 4996,loss: 0.006900627\n",
            "\n",
            "Global step: 4997,loss: 0.0063453894\n",
            "\n",
            "Global step: 4998,loss: 0.0065131728\n",
            "\n",
            "Global step: 4999,loss: 0.006890273\n",
            "\n",
            "Global step: 5000,loss: 0.007013809\n",
            "\n",
            "Global step: 5001,loss: 0.0068344725\n",
            "\n",
            "Global step: 5002,loss: 0.0072060903\n",
            "\n",
            "Global step: 5003,loss: 0.006811128\n",
            "\n",
            "Global step: 5004,loss: 0.006678669\n",
            "\n",
            "Global step: 5005,loss: 0.010202881\n",
            "\n",
            "Global step: 5006,loss: 0.006835062\n",
            "\n",
            "Global step: 5007,loss: 0.00658671\n",
            "\n",
            "Global step: 5008,loss: 0.0066989535\n",
            "\n",
            "Global step: 5009,loss: 0.007910169\n",
            "\n",
            "Global step: 5010,loss: 0.0074158832\n",
            "\n",
            "Global step: 5011,loss: 0.006470927\n",
            "\n",
            "Global step: 5012,loss: 0.0062908516\n",
            "\n",
            "Global step: 5013,loss: 0.0066301837\n",
            "\n",
            "Global step: 5014,loss: 0.0070042936\n",
            "\n",
            "Global step: 5015,loss: 0.0065337843\n",
            "\n",
            "Global step: 5016,loss: 0.0068867593\n",
            "\n",
            "Global step: 5017,loss: 0.0071202964\n",
            "\n",
            "Global step: 5018,loss: 0.006718354\n",
            "\n",
            "Global step: 5019,loss: 0.006573102\n",
            "\n",
            "Global step: 5020,loss: 0.0068053682\n",
            "\n",
            "Global step: 5021,loss: 0.0064644488\n",
            "\n",
            "Global step: 5022,loss: 0.006669065\n",
            "\n",
            "Global step: 5023,loss: 0.0066399216\n",
            "\n",
            "Global step: 5024,loss: 0.010592591\n",
            "\n",
            "Global step: 5025,loss: 0.0067238715\n",
            "\n",
            "Global step: 5026,loss: 0.0066641527\n",
            "\n",
            "Global step: 5027,loss: 0.007275559\n",
            "\n",
            "Global step: 5028,loss: 0.0068582413\n",
            "\n",
            "Global step: 5029,loss: 0.008593988\n",
            "\n",
            "Global step: 5030,loss: 0.0067219827\n",
            "\n",
            "Global step: 5031,loss: 0.006543803\n",
            "\n",
            "Global step: 5032,loss: 0.0070284638\n",
            "\n",
            "Global step: 5033,loss: 0.0065982873\n",
            "\n",
            "Global step: 5034,loss: 0.0065109297\n",
            "\n",
            "Global step: 5035,loss: 0.0070846234\n",
            "\n",
            "Global step: 5036,loss: 0.007285201\n",
            "\n",
            "Global step: 5037,loss: 0.0067132697\n",
            "\n",
            "Global step: 5038,loss: 0.00653157\n",
            "\n",
            "Global step: 5039,loss: 0.0062901205\n",
            "\n",
            "Global step: 5040,loss: 0.006376185\n",
            "\n",
            "Global step: 5041,loss: 0.0066683344\n",
            "\n",
            "Global step: 5042,loss: 0.0067897416\n",
            "\n",
            "Global step: 5043,loss: 0.00631926\n",
            "\n",
            "Global step: 5044,loss: 0.0069016484\n",
            "\n",
            "Global step: 5045,loss: 0.006666802\n",
            "\n",
            "Global step: 5046,loss: 0.009900057\n",
            "\n",
            "Global step: 5047,loss: 0.0069333697\n",
            "\n",
            "Global step: 5048,loss: 0.006370911\n",
            "\n",
            "Global step: 5049,loss: 0.006553063\n",
            "\n",
            "Global step: 5050,loss: 0.006949521\n",
            "\n",
            "Global step: 5051,loss: 0.00652369\n",
            "\n",
            "Global step: 5052,loss: 0.0065393373\n",
            "\n",
            "Global step: 5053,loss: 0.0063388427\n",
            "\n",
            "Global step: 5054,loss: 0.006793725\n",
            "\n",
            "Global step: 5055,loss: 0.0066790017\n",
            "\n",
            "Global step: 5056,loss: 0.008311626\n",
            "\n",
            "Global step: 5057,loss: 0.006353185\n",
            "\n",
            "Global step: 5058,loss: 0.0070297616\n",
            "\n",
            "Global step: 5059,loss: 0.006819841\n",
            "\n",
            "Global step: 5060,loss: 0.006323835\n",
            "\n",
            "Global step: 5061,loss: 0.006068631\n",
            "\n",
            "Global step: 5062,loss: 0.0065096105\n",
            "\n",
            "Global step: 5063,loss: 0.0067450735\n",
            "\n",
            "Global step: 5064,loss: 0.0069060903\n",
            "\n",
            "Global step: 5065,loss: 0.005716047\n",
            "\n",
            "Global step: 5066,loss: 0.006415375\n",
            "\n",
            "Global step: 5067,loss: 0.006791479\n",
            "\n",
            "Global step: 5068,loss: 0.0068253195\n",
            "\n",
            "Global step: 5069,loss: 0.0067297537\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 5069,val_loss: 0.017715342187633116\n",
            "\n",
            "Training for epoch 14/16:\n",
            "Global step: 5070,loss: 0.0064813667\n",
            "\n",
            "Global step: 5071,loss: 0.006349254\n",
            "\n",
            "Global step: 5072,loss: 0.006384951\n",
            "\n",
            "Global step: 5073,loss: 0.0065929783\n",
            "\n",
            "Global step: 5074,loss: 0.0065765292\n",
            "\n",
            "Global step: 5075,loss: 0.0063182204\n",
            "\n",
            "Global step: 5076,loss: 0.006156646\n",
            "\n",
            "Global step: 5077,loss: 0.00601555\n",
            "\n",
            "Global step: 5078,loss: 0.006612517\n",
            "\n",
            "Global step: 5079,loss: 0.006941753\n",
            "\n",
            "Global step: 5080,loss: 0.0061508357\n",
            "\n",
            "Global step: 5081,loss: 0.0066203866\n",
            "\n",
            "Global step: 5082,loss: 0.0063852407\n",
            "\n",
            "Global step: 5083,loss: 0.0062389164\n",
            "\n",
            "Global step: 5084,loss: 0.0065652365\n",
            "\n",
            "Global step: 5085,loss: 0.006651961\n",
            "\n",
            "Global step: 5086,loss: 0.006238727\n",
            "\n",
            "Global step: 5087,loss: 0.0060551334\n",
            "\n",
            "Global step: 5088,loss: 0.0067909905\n",
            "\n",
            "Global step: 5089,loss: 0.0064366153\n",
            "\n",
            "Global step: 5090,loss: 0.006190307\n",
            "\n",
            "Global step: 5091,loss: 0.006382279\n",
            "\n",
            "Global step: 5092,loss: 0.0064576115\n",
            "\n",
            "Global step: 5093,loss: 0.0062875017\n",
            "\n",
            "Global step: 5094,loss: 0.0061309985\n",
            "\n",
            "Global step: 5095,loss: 0.006313883\n",
            "\n",
            "Global step: 5096,loss: 0.0067094085\n",
            "\n",
            "Global step: 5097,loss: 0.006030885\n",
            "\n",
            "Global step: 5098,loss: 0.006300228\n",
            "\n",
            "Global step: 5099,loss: 0.0063078613\n",
            "\n",
            "Global step: 5100,loss: 0.0062157833\n",
            "\n",
            "Global step: 5101,loss: 0.0063111614\n",
            "\n",
            "Global step: 5102,loss: 0.006741598\n",
            "\n",
            "Global step: 5103,loss: 0.006620531\n",
            "\n",
            "Global step: 5104,loss: 0.005669293\n",
            "\n",
            "Global step: 5105,loss: 0.0068908767\n",
            "\n",
            "Global step: 5106,loss: 0.0057347617\n",
            "\n",
            "Global step: 5107,loss: 0.0066216234\n",
            "\n",
            "Global step: 5108,loss: 0.006292489\n",
            "\n",
            "Global step: 5109,loss: 0.006006867\n",
            "\n",
            "Global step: 5110,loss: 0.006004902\n",
            "\n",
            "Global step: 5111,loss: 0.006835644\n",
            "\n",
            "Global step: 5112,loss: 0.0062325797\n",
            "\n",
            "Global step: 5113,loss: 0.0058899573\n",
            "\n",
            "Global step: 5114,loss: 0.006391908\n",
            "\n",
            "Global step: 5115,loss: 0.006442338\n",
            "\n",
            "Global step: 5116,loss: 0.006478725\n",
            "\n",
            "Global step: 5117,loss: 0.0062151463\n",
            "\n",
            "Global step: 5118,loss: 0.006214526\n",
            "\n",
            "Global step: 5119,loss: 0.010261352\n",
            "\n",
            "Global step: 5120,loss: 0.006194345\n",
            "\n",
            "Global step: 5121,loss: 0.006938608\n",
            "\n",
            "Global step: 5122,loss: 0.006338924\n",
            "\n",
            "Global step: 5123,loss: 0.006745826\n",
            "\n",
            "Global step: 5124,loss: 0.006131747\n",
            "\n",
            "Global step: 5125,loss: 0.006011213\n",
            "\n",
            "Global step: 5126,loss: 0.006415349\n",
            "\n",
            "Global step: 5127,loss: 0.0063612554\n",
            "\n",
            "Global step: 5128,loss: 0.0063688937\n",
            "\n",
            "Global step: 5129,loss: 0.006737876\n",
            "\n",
            "Global step: 5130,loss: 0.0061338097\n",
            "\n",
            "Global step: 5131,loss: 0.0064699166\n",
            "\n",
            "Global step: 5132,loss: 0.006583418\n",
            "\n",
            "Global step: 5133,loss: 0.0059429384\n",
            "\n",
            "Global step: 5134,loss: 0.006266455\n",
            "\n",
            "Global step: 5135,loss: 0.006461404\n",
            "\n",
            "Global step: 5136,loss: 0.0067243744\n",
            "\n",
            "Global step: 5137,loss: 0.007106954\n",
            "\n",
            "Global step: 5138,loss: 0.0063265692\n",
            "\n",
            "Global step: 5139,loss: 0.006471355\n",
            "\n",
            "Global step: 5140,loss: 0.0062758303\n",
            "\n",
            "Global step: 5141,loss: 0.006120394\n",
            "\n",
            "Global step: 5142,loss: 0.0066166134\n",
            "\n",
            "Global step: 5143,loss: 0.00645855\n",
            "\n",
            "Global step: 5144,loss: 0.0063520777\n",
            "\n",
            "Global step: 5145,loss: 0.006380417\n",
            "\n",
            "Global step: 5146,loss: 0.006980091\n",
            "\n",
            "Global step: 5147,loss: 0.006753431\n",
            "\n",
            "Global step: 5148,loss: 0.010544985\n",
            "\n",
            "Global step: 5149,loss: 0.0059975353\n",
            "\n",
            "Global step: 5150,loss: 0.006272558\n",
            "\n",
            "Global step: 5151,loss: 0.006790174\n",
            "\n",
            "Global step: 5152,loss: 0.007387643\n",
            "\n",
            "Global step: 5153,loss: 0.006108079\n",
            "\n",
            "Global step: 5154,loss: 0.006142645\n",
            "\n",
            "Global step: 5155,loss: 0.009195918\n",
            "\n",
            "Global step: 5156,loss: 0.006468617\n",
            "\n",
            "Global step: 5157,loss: 0.006213182\n",
            "\n",
            "Global step: 5158,loss: 0.0064233188\n",
            "\n",
            "Global step: 5159,loss: 0.0065020355\n",
            "\n",
            "Global step: 5160,loss: 0.0065466743\n",
            "\n",
            "Global step: 5161,loss: 0.006627277\n",
            "\n",
            "Global step: 5162,loss: 0.006543413\n",
            "\n",
            "Global step: 5163,loss: 0.0066383756\n",
            "\n",
            "Global step: 5164,loss: 0.0068888683\n",
            "\n",
            "Global step: 5165,loss: 0.008164758\n",
            "\n",
            "Global step: 5166,loss: 0.006413417\n",
            "\n",
            "Global step: 5167,loss: 0.006295028\n",
            "\n",
            "Global step: 5168,loss: 0.0064945915\n",
            "\n",
            "Global step: 5169,loss: 0.0064512445\n",
            "\n",
            "Global step: 5170,loss: 0.006049647\n",
            "\n",
            "Global step: 5171,loss: 0.006331227\n",
            "\n",
            "Global step: 5172,loss: 0.0068609403\n",
            "\n",
            "Global step: 5173,loss: 0.006285033\n",
            "\n",
            "Global step: 5174,loss: 0.006347344\n",
            "\n",
            "Global step: 5175,loss: 0.0064069345\n",
            "\n",
            "Global step: 5176,loss: 0.006090123\n",
            "\n",
            "Global step: 5177,loss: 0.006602734\n",
            "\n",
            "Global step: 5178,loss: 0.0063221175\n",
            "\n",
            "Global step: 5179,loss: 0.006360553\n",
            "\n",
            "Global step: 5180,loss: 0.0064153792\n",
            "\n",
            "Global step: 5181,loss: 0.0061672456\n",
            "\n",
            "Global step: 5182,loss: 0.0063999663\n",
            "\n",
            "Global step: 5183,loss: 0.006436822\n",
            "\n",
            "Global step: 5184,loss: 0.006042366\n",
            "\n",
            "Global step: 5185,loss: 0.0058246716\n",
            "\n",
            "Global step: 5186,loss: 0.006716375\n",
            "\n",
            "Global step: 5187,loss: 0.0069010435\n",
            "\n",
            "Global step: 5188,loss: 0.007036872\n",
            "\n",
            "Global step: 5189,loss: 0.006621158\n",
            "\n",
            "Global step: 5190,loss: 0.0060702064\n",
            "\n",
            "Global step: 5191,loss: 0.006492244\n",
            "\n",
            "Global step: 5192,loss: 0.00638309\n",
            "\n",
            "Global step: 5193,loss: 0.0064236927\n",
            "\n",
            "Global step: 5194,loss: 0.0062227794\n",
            "\n",
            "Global step: 5195,loss: 0.0060677836\n",
            "\n",
            "Global step: 5196,loss: 0.0059688212\n",
            "\n",
            "Global step: 5197,loss: 0.006783182\n",
            "\n",
            "Global step: 5198,loss: 0.0061415317\n",
            "\n",
            "Global step: 5199,loss: 0.006635336\n",
            "\n",
            "Global step: 5200,loss: 0.0066855177\n",
            "\n",
            "Global step: 5201,loss: 0.005960186\n",
            "\n",
            "Global step: 5202,loss: 0.0066833207\n",
            "\n",
            "Global step: 5203,loss: 0.006318256\n",
            "\n",
            "Global step: 5204,loss: 0.0065795765\n",
            "\n",
            "Global step: 5205,loss: 0.006405823\n",
            "\n",
            "Global step: 5206,loss: 0.006773945\n",
            "\n",
            "Global step: 5207,loss: 0.006328959\n",
            "\n",
            "Global step: 5208,loss: 0.0060498524\n",
            "\n",
            "Global step: 5209,loss: 0.005908144\n",
            "\n",
            "Global step: 5210,loss: 0.006489646\n",
            "\n",
            "Global step: 5211,loss: 0.0065491595\n",
            "\n",
            "Global step: 5212,loss: 0.0064384267\n",
            "\n",
            "Global step: 5213,loss: 0.006401228\n",
            "\n",
            "Global step: 5214,loss: 0.0060367985\n",
            "\n",
            "Global step: 5215,loss: 0.0065954495\n",
            "\n",
            "Global step: 5216,loss: 0.0057013757\n",
            "\n",
            "Global step: 5217,loss: 0.00620789\n",
            "\n",
            "Global step: 5218,loss: 0.0065077604\n",
            "\n",
            "Global step: 5219,loss: 0.006568394\n",
            "\n",
            "Global step: 5220,loss: 0.0066756187\n",
            "\n",
            "Global step: 5221,loss: 0.006256691\n",
            "\n",
            "Global step: 5222,loss: 0.006213177\n",
            "\n",
            "Global step: 5223,loss: 0.0064922357\n",
            "\n",
            "Global step: 5224,loss: 0.0065244283\n",
            "\n",
            "Global step: 5225,loss: 0.0070366245\n",
            "\n",
            "Global step: 5226,loss: 0.006365856\n",
            "\n",
            "Global step: 5227,loss: 0.0066164676\n",
            "\n",
            "Global step: 5228,loss: 0.0062398557\n",
            "\n",
            "Global step: 5229,loss: 0.0063152704\n",
            "\n",
            "Global step: 5230,loss: 0.006176136\n",
            "\n",
            "Global step: 5231,loss: 0.007212663\n",
            "\n",
            "Global step: 5232,loss: 0.006525229\n",
            "\n",
            "Global step: 5233,loss: 0.0063085346\n",
            "\n",
            "Global step: 5234,loss: 0.0067830244\n",
            "\n",
            "Global step: 5235,loss: 0.008370955\n",
            "\n",
            "Global step: 5236,loss: 0.0067865406\n",
            "\n",
            "Global step: 5237,loss: 0.0064611216\n",
            "\n",
            "Global step: 5238,loss: 0.0062099583\n",
            "\n",
            "Global step: 5239,loss: 0.006858996\n",
            "\n",
            "Global step: 5240,loss: 0.0061994623\n",
            "\n",
            "Global step: 5241,loss: 0.0074101286\n",
            "\n",
            "Global step: 5242,loss: 0.0065835924\n",
            "\n",
            "Global step: 5243,loss: 0.006597694\n",
            "\n",
            "Global step: 5244,loss: 0.0065241326\n",
            "\n",
            "Global step: 5245,loss: 0.007035883\n",
            "\n",
            "Global step: 5246,loss: 0.0064579817\n",
            "\n",
            "Global step: 5247,loss: 0.0061986395\n",
            "\n",
            "Global step: 5248,loss: 0.0062901494\n",
            "\n",
            "Global step: 5249,loss: 0.006530137\n",
            "\n",
            "Global step: 5250,loss: 0.006806597\n",
            "\n",
            "Global step: 5251,loss: 0.00639961\n",
            "\n",
            "Global step: 5252,loss: 0.0068993433\n",
            "\n",
            "Global step: 5253,loss: 0.006415452\n",
            "\n",
            "Global step: 5254,loss: 0.0063528926\n",
            "\n",
            "Global step: 5255,loss: 0.009395144\n",
            "\n",
            "Global step: 5256,loss: 0.0065961024\n",
            "\n",
            "Global step: 5257,loss: 0.0066304845\n",
            "\n",
            "Global step: 5258,loss: 0.0068890667\n",
            "\n",
            "Global step: 5259,loss: 0.007151728\n",
            "\n",
            "Global step: 5260,loss: 0.006068406\n",
            "\n",
            "Global step: 5261,loss: 0.0066656144\n",
            "\n",
            "Global step: 5262,loss: 0.0071337377\n",
            "\n",
            "Global step: 5263,loss: 0.0067439964\n",
            "\n",
            "Global step: 5264,loss: 0.0060796263\n",
            "\n",
            "Global step: 5265,loss: 0.0060310783\n",
            "\n",
            "Global step: 5266,loss: 0.006325425\n",
            "\n",
            "Global step: 5267,loss: 0.009071374\n",
            "\n",
            "Global step: 5268,loss: 0.0060147843\n",
            "\n",
            "Global step: 5269,loss: 0.0061304225\n",
            "\n",
            "Global step: 5270,loss: 0.0080798175\n",
            "\n",
            "Global step: 5271,loss: 0.006252083\n",
            "\n",
            "Global step: 5272,loss: 0.0063247164\n",
            "\n",
            "Global step: 5273,loss: 0.0069500944\n",
            "\n",
            "Global step: 5274,loss: 0.006343201\n",
            "\n",
            "Global step: 5275,loss: 0.0064715147\n",
            "\n",
            "Global step: 5276,loss: 0.0066351807\n",
            "\n",
            "Global step: 5277,loss: 0.0067467093\n",
            "\n",
            "Global step: 5278,loss: 0.0065588253\n",
            "\n",
            "Global step: 5279,loss: 0.0067067253\n",
            "\n",
            "Global step: 5280,loss: 0.007937194\n",
            "\n",
            "Global step: 5281,loss: 0.006271038\n",
            "\n",
            "Global step: 5282,loss: 0.0065483376\n",
            "\n",
            "Global step: 5283,loss: 0.0065234574\n",
            "\n",
            "Global step: 5284,loss: 0.006553307\n",
            "\n",
            "Global step: 5285,loss: 0.0063875862\n",
            "\n",
            "Global step: 5286,loss: 0.006673013\n",
            "\n",
            "Global step: 5287,loss: 0.006534003\n",
            "\n",
            "Global step: 5288,loss: 0.0066665327\n",
            "\n",
            "Global step: 5289,loss: 0.0066189943\n",
            "\n",
            "Global step: 5290,loss: 0.0064182403\n",
            "\n",
            "Global step: 5291,loss: 0.006818592\n",
            "\n",
            "Global step: 5292,loss: 0.00634809\n",
            "\n",
            "Global step: 5293,loss: 0.0065540476\n",
            "\n",
            "Global step: 5294,loss: 0.006345735\n",
            "\n",
            "Global step: 5295,loss: 0.0070236735\n",
            "\n",
            "Global step: 5296,loss: 0.0065071057\n",
            "\n",
            "Global step: 5297,loss: 0.006512974\n",
            "\n",
            "Global step: 5298,loss: 0.0065531796\n",
            "\n",
            "Global step: 5299,loss: 0.006201984\n",
            "\n",
            "Global step: 5300,loss: 0.0062811626\n",
            "\n",
            "Global step: 5301,loss: 0.006377646\n",
            "\n",
            "Global step: 5302,loss: 0.00649269\n",
            "\n",
            "Global step: 5303,loss: 0.007312339\n",
            "\n",
            "Global step: 5304,loss: 0.006622195\n",
            "\n",
            "Global step: 5305,loss: 0.0065682638\n",
            "\n",
            "Global step: 5306,loss: 0.006062896\n",
            "\n",
            "Global step: 5307,loss: 0.0062701316\n",
            "\n",
            "Global step: 5308,loss: 0.0061619454\n",
            "\n",
            "Global step: 5309,loss: 0.006581378\n",
            "\n",
            "Global step: 5310,loss: 0.0067198193\n",
            "\n",
            "Global step: 5311,loss: 0.007485815\n",
            "\n",
            "Global step: 5312,loss: 0.0059979083\n",
            "\n",
            "Global step: 5313,loss: 0.006086369\n",
            "\n",
            "Global step: 5314,loss: 0.0062006144\n",
            "\n",
            "Global step: 5315,loss: 0.007441053\n",
            "\n",
            "Global step: 5316,loss: 0.006955226\n",
            "\n",
            "Global step: 5317,loss: 0.0063808523\n",
            "\n",
            "Global step: 5318,loss: 0.0067603462\n",
            "\n",
            "Global step: 5319,loss: 0.006880231\n",
            "\n",
            "Global step: 5320,loss: 0.0061801383\n",
            "\n",
            "Global step: 5321,loss: 0.006246839\n",
            "\n",
            "Global step: 5322,loss: 0.006395884\n",
            "\n",
            "Global step: 5323,loss: 0.0075888042\n",
            "\n",
            "Global step: 5324,loss: 0.00688214\n",
            "\n",
            "Global step: 5325,loss: 0.006173708\n",
            "\n",
            "Global step: 5326,loss: 0.006434388\n",
            "\n",
            "Global step: 5327,loss: 0.0071718544\n",
            "\n",
            "Global step: 5328,loss: 0.0071208673\n",
            "\n",
            "Global step: 5329,loss: 0.006929436\n",
            "\n",
            "Global step: 5330,loss: 0.00635344\n",
            "\n",
            "Global step: 5331,loss: 0.0069492506\n",
            "\n",
            "Global step: 5332,loss: 0.0067961747\n",
            "\n",
            "Global step: 5333,loss: 0.0066765295\n",
            "\n",
            "Global step: 5334,loss: 0.0063630342\n",
            "\n",
            "Global step: 5335,loss: 0.0066452995\n",
            "\n",
            "Global step: 5336,loss: 0.0064902734\n",
            "\n",
            "Global step: 5337,loss: 0.0064922613\n",
            "\n",
            "Global step: 5338,loss: 0.0061598583\n",
            "\n",
            "Global step: 5339,loss: 0.00630869\n",
            "\n",
            "Global step: 5340,loss: 0.006785538\n",
            "\n",
            "Global step: 5341,loss: 0.0063716387\n",
            "\n",
            "Global step: 5342,loss: 0.0063506234\n",
            "\n",
            "Global step: 5343,loss: 0.0067823348\n",
            "\n",
            "Global step: 5344,loss: 0.0063806027\n",
            "\n",
            "Global step: 5345,loss: 0.006724486\n",
            "\n",
            "Global step: 5346,loss: 0.0066920165\n",
            "\n",
            "Global step: 5347,loss: 0.006777231\n",
            "\n",
            "Global step: 5348,loss: 0.006450775\n",
            "\n",
            "Global step: 5349,loss: 0.006564057\n",
            "\n",
            "Global step: 5350,loss: 0.006396259\n",
            "\n",
            "Global step: 5351,loss: 0.0068124807\n",
            "\n",
            "Global step: 5352,loss: 0.007171142\n",
            "\n",
            "Global step: 5353,loss: 0.0063597364\n",
            "\n",
            "Global step: 5354,loss: 0.006903391\n",
            "\n",
            "Global step: 5355,loss: 0.006552543\n",
            "\n",
            "Global step: 5356,loss: 0.0059287217\n",
            "\n",
            "Global step: 5357,loss: 0.006374812\n",
            "\n",
            "Global step: 5358,loss: 0.0060056215\n",
            "\n",
            "Global step: 5359,loss: 0.0064214068\n",
            "\n",
            "Global step: 5360,loss: 0.006475308\n",
            "\n",
            "Global step: 5361,loss: 0.0070976964\n",
            "\n",
            "Global step: 5362,loss: 0.006711274\n",
            "\n",
            "Global step: 5363,loss: 0.0061175297\n",
            "\n",
            "Global step: 5364,loss: 0.0065540895\n",
            "\n",
            "Global step: 5365,loss: 0.006315248\n",
            "\n",
            "Global step: 5366,loss: 0.007726953\n",
            "\n",
            "Global step: 5367,loss: 0.006135706\n",
            "\n",
            "Global step: 5368,loss: 0.0065172287\n",
            "\n",
            "Global step: 5369,loss: 0.0068026725\n",
            "\n",
            "Global step: 5370,loss: 0.006857091\n",
            "\n",
            "Global step: 5371,loss: 0.0063191545\n",
            "\n",
            "Global step: 5372,loss: 0.008793692\n",
            "\n",
            "Global step: 5373,loss: 0.0075856047\n",
            "\n",
            "Global step: 5374,loss: 0.0062575205\n",
            "\n",
            "Global step: 5375,loss: 0.008721083\n",
            "\n",
            "Global step: 5376,loss: 0.0069496944\n",
            "\n",
            "Global step: 5377,loss: 0.0067153936\n",
            "\n",
            "Global step: 5378,loss: 0.0074781035\n",
            "\n",
            "Global step: 5379,loss: 0.0076248394\n",
            "\n",
            "Global step: 5380,loss: 0.0063427975\n",
            "\n",
            "Global step: 5381,loss: 0.0068213623\n",
            "\n",
            "Global step: 5382,loss: 0.0064400127\n",
            "\n",
            "Global step: 5383,loss: 0.0071708327\n",
            "\n",
            "Global step: 5384,loss: 0.007020861\n",
            "\n",
            "Global step: 5385,loss: 0.006820434\n",
            "\n",
            "Global step: 5386,loss: 0.0066122552\n",
            "\n",
            "Global step: 5387,loss: 0.009116089\n",
            "\n",
            "Global step: 5388,loss: 0.006667811\n",
            "\n",
            "Global step: 5389,loss: 0.006592445\n",
            "\n",
            "Global step: 5390,loss: 0.0071892384\n",
            "\n",
            "Global step: 5391,loss: 0.006844667\n",
            "\n",
            "Global step: 5392,loss: 0.0071820696\n",
            "\n",
            "Global step: 5393,loss: 0.0068778703\n",
            "\n",
            "Global step: 5394,loss: 0.007668363\n",
            "\n",
            "Global step: 5395,loss: 0.0068616048\n",
            "\n",
            "Global step: 5396,loss: 0.008142694\n",
            "\n",
            "Global step: 5397,loss: 0.0077438504\n",
            "\n",
            "Global step: 5398,loss: 0.0069591478\n",
            "\n",
            "Global step: 5399,loss: 0.0074164085\n",
            "\n",
            "Global step: 5400,loss: 0.0064067794\n",
            "\n",
            "Global step: 5401,loss: 0.006996708\n",
            "\n",
            "Global step: 5402,loss: 0.0074617215\n",
            "\n",
            "Global step: 5403,loss: 0.0065343254\n",
            "\n",
            "Global step: 5404,loss: 0.007574197\n",
            "\n",
            "Global step: 5405,loss: 0.0075459112\n",
            "\n",
            "Global step: 5406,loss: 0.0066337497\n",
            "\n",
            "Global step: 5407,loss: 0.006371814\n",
            "\n",
            "Global step: 5408,loss: 0.0071448795\n",
            "\n",
            "Global step: 5409,loss: 0.0073944503\n",
            "\n",
            "Global step: 5410,loss: 0.0073624672\n",
            "\n",
            "Global step: 5411,loss: 0.0073118033\n",
            "\n",
            "Global step: 5412,loss: 0.0067931926\n",
            "\n",
            "Global step: 5413,loss: 0.010804644\n",
            "\n",
            "Global step: 5414,loss: 0.0073822076\n",
            "\n",
            "Global step: 5415,loss: 0.007945298\n",
            "\n",
            "Global step: 5416,loss: 0.0077300873\n",
            "\n",
            "Global step: 5417,loss: 0.007523316\n",
            "\n",
            "Global step: 5418,loss: 0.006952337\n",
            "\n",
            "Global step: 5419,loss: 0.011123221\n",
            "\n",
            "Global step: 5420,loss: 0.007335233\n",
            "\n",
            "Global step: 5421,loss: 0.0075502256\n",
            "\n",
            "Global step: 5422,loss: 0.0072701927\n",
            "\n",
            "Global step: 5423,loss: 0.0069428454\n",
            "\n",
            "Global step: 5424,loss: 0.00882461\n",
            "\n",
            "Global step: 5425,loss: 0.0073705697\n",
            "\n",
            "Global step: 5426,loss: 0.010166617\n",
            "\n",
            "Global step: 5427,loss: 0.008747799\n",
            "\n",
            "Global step: 5428,loss: 0.008421728\n",
            "\n",
            "Global step: 5429,loss: 0.007203779\n",
            "\n",
            "Global step: 5430,loss: 0.006608213\n",
            "\n",
            "Global step: 5431,loss: 0.0065109907\n",
            "\n",
            "Global step: 5432,loss: 0.0076613654\n",
            "\n",
            "Global step: 5433,loss: 0.007735363\n",
            "\n",
            "Global step: 5434,loss: 0.0076916306\n",
            "\n",
            "Global step: 5435,loss: 0.008014835\n",
            "\n",
            "Global step: 5436,loss: 0.0074717943\n",
            "\n",
            "Global step: 5437,loss: 0.009691672\n",
            "\n",
            "Global step: 5438,loss: 0.0072836624\n",
            "\n",
            "Global step: 5439,loss: 0.0090051005\n",
            "\n",
            "Global step: 5440,loss: 0.0072840154\n",
            "\n",
            "Global step: 5441,loss: 0.009266824\n",
            "\n",
            "Global step: 5442,loss: 0.00804211\n",
            "\n",
            "Global step: 5443,loss: 0.008243831\n",
            "\n",
            "Global step: 5444,loss: 0.008119145\n",
            "\n",
            "Global step: 5445,loss: 0.011641253\n",
            "\n",
            "Global step: 5446,loss: 0.0076934854\n",
            "\n",
            "Global step: 5447,loss: 0.014093775\n",
            "\n",
            "Global step: 5448,loss: 0.011676105\n",
            "\n",
            "Global step: 5449,loss: 0.013367234\n",
            "\n",
            "Global step: 5450,loss: 0.011122193\n",
            "\n",
            "Global step: 5451,loss: 0.015962923\n",
            "\n",
            "Global step: 5452,loss: 0.0081688985\n",
            "\n",
            "Global step: 5453,loss: 0.0194298\n",
            "\n",
            "Global step: 5454,loss: 0.007927899\n",
            "\n",
            "Global step: 5455,loss: 0.014050582\n",
            "\n",
            "Global step: 5456,loss: 0.013314601\n",
            "\n",
            "Global step: 5457,loss: 0.013526833\n",
            "\n",
            "Global step: 5458,loss: 0.009181386\n",
            "\n",
            "Global step: 5459,loss: 0.011153419\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 5459,val_loss: 0.02238926607876634\n",
            "\n",
            "Training for epoch 15/16:\n",
            "Global step: 5460,loss: 0.011352952\n",
            "\n",
            "Global step: 5461,loss: 0.0093836505\n",
            "\n",
            "Global step: 5462,loss: 0.010998426\n",
            "\n",
            "Global step: 5463,loss: 0.008657834\n",
            "\n",
            "Global step: 5464,loss: 0.009346911\n",
            "\n",
            "Global step: 5465,loss: 0.012082329\n",
            "\n",
            "Global step: 5466,loss: 0.010161513\n",
            "\n",
            "Global step: 5467,loss: 0.01574377\n",
            "\n",
            "Global step: 5468,loss: 0.014320778\n",
            "\n",
            "Global step: 5469,loss: 0.011877608\n",
            "\n",
            "Global step: 5470,loss: 0.012251385\n",
            "\n",
            "Global step: 5471,loss: 0.010108686\n",
            "\n",
            "Global step: 5472,loss: 0.012274543\n",
            "\n",
            "Global step: 5473,loss: 0.009910965\n",
            "\n",
            "Global step: 5474,loss: 0.012189024\n",
            "\n",
            "Global step: 5475,loss: 0.010062024\n",
            "\n",
            "Global step: 5476,loss: 0.009255159\n",
            "\n",
            "Global step: 5477,loss: 0.013312409\n",
            "\n",
            "Global step: 5478,loss: 0.010004785\n",
            "\n",
            "Global step: 5479,loss: 0.014664915\n",
            "\n",
            "Global step: 5480,loss: 0.01069399\n",
            "\n",
            "Global step: 5481,loss: 0.009905616\n",
            "\n",
            "Global step: 5482,loss: 0.010703625\n",
            "\n",
            "Global step: 5483,loss: 0.008888316\n",
            "\n",
            "Global step: 5484,loss: 0.009743279\n",
            "\n",
            "Global step: 5485,loss: 0.008619598\n",
            "\n",
            "Global step: 5486,loss: 0.00911543\n",
            "\n",
            "Global step: 5487,loss: 0.010850204\n",
            "\n",
            "Global step: 5488,loss: 0.012534402\n",
            "\n",
            "Global step: 5489,loss: 0.009740778\n",
            "\n",
            "Global step: 5490,loss: 0.0102162445\n",
            "\n",
            "Global step: 5491,loss: 0.010491431\n",
            "\n",
            "Global step: 5492,loss: 0.01031096\n",
            "\n",
            "Global step: 5493,loss: 0.012326732\n",
            "\n",
            "Global step: 5494,loss: 0.010286266\n",
            "\n",
            "Global step: 5495,loss: 0.009268571\n",
            "\n",
            "Global step: 5496,loss: 0.011054868\n",
            "\n",
            "Global step: 5497,loss: 0.010092045\n",
            "\n",
            "Global step: 5498,loss: 0.009959219\n",
            "\n",
            "Global step: 5499,loss: 0.012344219\n",
            "\n",
            "Global step: 5500,loss: 0.011261124\n",
            "\n",
            "Global step: 5501,loss: 0.010666538\n",
            "\n",
            "Global step: 5502,loss: 0.011227159\n",
            "\n",
            "Global step: 5503,loss: 0.011189582\n",
            "\n",
            "Global step: 5504,loss: 0.011299146\n",
            "\n",
            "Global step: 5505,loss: 0.01205424\n",
            "\n",
            "Global step: 5506,loss: 0.011067886\n",
            "\n",
            "Global step: 5507,loss: 0.009416843\n",
            "\n",
            "Global step: 5508,loss: 0.008914885\n",
            "\n",
            "Global step: 5509,loss: 0.008499222\n",
            "\n",
            "Global step: 5510,loss: 0.010766216\n",
            "\n",
            "Global step: 5511,loss: 0.010070302\n",
            "\n",
            "Global step: 5512,loss: 0.008311766\n",
            "\n",
            "Global step: 5513,loss: 0.008049222\n",
            "\n",
            "Global step: 5514,loss: 0.009530005\n",
            "\n",
            "Global step: 5515,loss: 0.0075588236\n",
            "\n",
            "Global step: 5516,loss: 0.011012351\n",
            "\n",
            "Global step: 5517,loss: 0.009471866\n",
            "\n",
            "Global step: 5518,loss: 0.0074822237\n",
            "\n",
            "Global step: 5519,loss: 0.015485577\n",
            "\n",
            "Global step: 5520,loss: 0.010971663\n",
            "\n",
            "Global step: 5521,loss: 0.008990978\n",
            "\n",
            "Global step: 5522,loss: 0.0074171396\n",
            "\n",
            "Global step: 5523,loss: 0.014085798\n",
            "\n",
            "Global step: 5524,loss: 0.009232812\n",
            "\n",
            "Global step: 5525,loss: 0.0081619285\n",
            "\n",
            "Global step: 5526,loss: 0.014224186\n",
            "\n",
            "Global step: 5527,loss: 0.007930265\n",
            "\n",
            "Global step: 5528,loss: 0.0110596325\n",
            "\n",
            "Global step: 5529,loss: 0.008825981\n",
            "\n",
            "Global step: 5530,loss: 0.00851123\n",
            "\n",
            "Global step: 5531,loss: 0.011923768\n",
            "\n",
            "Global step: 5532,loss: 0.009943436\n",
            "\n",
            "Global step: 5533,loss: 0.00820373\n",
            "\n",
            "Global step: 5534,loss: 0.009756321\n",
            "\n",
            "Global step: 5535,loss: 0.009042749\n",
            "\n",
            "Global step: 5536,loss: 0.008281403\n",
            "\n",
            "Global step: 5537,loss: 0.008108636\n",
            "\n",
            "Global step: 5538,loss: 0.009628525\n",
            "\n",
            "Global step: 5539,loss: 0.010353129\n",
            "\n",
            "Global step: 5540,loss: 0.008194947\n",
            "\n",
            "Global step: 5541,loss: 0.00874726\n",
            "\n",
            "Global step: 5542,loss: 0.009419342\n",
            "\n",
            "Global step: 5543,loss: 0.008049302\n",
            "\n",
            "Global step: 5544,loss: 0.008667904\n",
            "\n",
            "Global step: 5545,loss: 0.008561363\n",
            "\n",
            "Global step: 5546,loss: 0.007976763\n",
            "\n",
            "Global step: 5547,loss: 0.011679896\n",
            "\n",
            "Global step: 5548,loss: 0.007572144\n",
            "\n",
            "Global step: 5549,loss: 0.009486212\n",
            "\n",
            "Global step: 5550,loss: 0.007342997\n",
            "\n",
            "Global step: 5551,loss: 0.009052416\n",
            "\n",
            "Global step: 5552,loss: 0.007486133\n",
            "\n",
            "Global step: 5553,loss: 0.0082726395\n",
            "\n",
            "Global step: 5554,loss: 0.010046144\n",
            "\n",
            "Global step: 5555,loss: 0.0076544913\n",
            "\n",
            "Global step: 5556,loss: 0.008448957\n",
            "\n",
            "Global step: 5557,loss: 0.00959548\n",
            "\n",
            "Global step: 5558,loss: 0.00799813\n",
            "\n",
            "Global step: 5559,loss: 0.009908584\n",
            "\n",
            "Global step: 5560,loss: 0.007329086\n",
            "\n",
            "Global step: 5561,loss: 0.009163965\n",
            "\n",
            "Global step: 5562,loss: 0.009996825\n",
            "\n",
            "Global step: 5563,loss: 0.009536247\n",
            "\n",
            "Global step: 5564,loss: 0.008624272\n",
            "\n",
            "Global step: 5565,loss: 0.008520413\n",
            "\n",
            "Global step: 5566,loss: 0.0085917115\n",
            "\n",
            "Global step: 5567,loss: 0.00772274\n",
            "\n",
            "Global step: 5568,loss: 0.009663587\n",
            "\n",
            "Global step: 5569,loss: 0.009576581\n",
            "\n",
            "Global step: 5570,loss: 0.008429528\n",
            "\n",
            "Global step: 5571,loss: 0.008279489\n",
            "\n",
            "Global step: 5572,loss: 0.008511053\n",
            "\n",
            "Global step: 5573,loss: 0.007590785\n",
            "\n",
            "Global step: 5574,loss: 0.008361031\n",
            "\n",
            "Global step: 5575,loss: 0.010341824\n",
            "\n",
            "Global step: 5576,loss: 0.0077234274\n",
            "\n",
            "Global step: 5577,loss: 0.008535268\n",
            "\n",
            "Global step: 5578,loss: 0.007977348\n",
            "\n",
            "Global step: 5579,loss: 0.011690183\n",
            "\n",
            "Global step: 5580,loss: 0.009234358\n",
            "\n",
            "Global step: 5581,loss: 0.0073005934\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 5582.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:18:52.443457 139735711573760 supervisor.py:1050] Recording summary at step 5582.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.67135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:18:52.492827 139735703181056 supervisor.py:1099] global_step/sec: 6.67135\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5582,loss: 0.009118574\n",
            "\n",
            "Global step: 5583,loss: 0.008708395\n",
            "\n",
            "Global step: 5584,loss: 0.011854198\n",
            "\n",
            "Global step: 5585,loss: 0.008749703\n",
            "\n",
            "Global step: 5586,loss: 0.009568879\n",
            "\n",
            "Global step: 5587,loss: 0.008775931\n",
            "\n",
            "Global step: 5588,loss: 0.011785828\n",
            "\n",
            "Global step: 5589,loss: 0.008281715\n",
            "\n",
            "Global step: 5590,loss: 0.0071160235\n",
            "\n",
            "Global step: 5591,loss: 0.008524633\n",
            "\n",
            "Global step: 5592,loss: 0.008145527\n",
            "\n",
            "Global step: 5593,loss: 0.0075587137\n",
            "\n",
            "Global step: 5594,loss: 0.009676316\n",
            "\n",
            "Global step: 5595,loss: 0.010142857\n",
            "\n",
            "Global step: 5596,loss: 0.011124147\n",
            "\n",
            "Global step: 5597,loss: 0.008739995\n",
            "\n",
            "Global step: 5598,loss: 0.008347003\n",
            "\n",
            "Global step: 5599,loss: 0.007384679\n",
            "\n",
            "Global step: 5600,loss: 0.013360244\n",
            "\n",
            "Global step: 5601,loss: 0.011811178\n",
            "\n",
            "Global step: 5602,loss: 0.009398264\n",
            "\n",
            "Global step: 5603,loss: 0.007649317\n",
            "\n",
            "Global step: 5604,loss: 0.009141941\n",
            "\n",
            "Global step: 5605,loss: 0.011778055\n",
            "\n",
            "Global step: 5606,loss: 0.010227515\n",
            "\n",
            "Global step: 5607,loss: 0.0096826255\n",
            "\n",
            "Global step: 5608,loss: 0.016344994\n",
            "\n",
            "Global step: 5609,loss: 0.00843404\n",
            "\n",
            "Global step: 5610,loss: 0.0086775655\n",
            "\n",
            "Global step: 5611,loss: 0.012390461\n",
            "\n",
            "Global step: 5612,loss: 0.010994118\n",
            "\n",
            "Global step: 5613,loss: 0.009015151\n",
            "\n",
            "Global step: 5614,loss: 0.008793371\n",
            "\n",
            "Global step: 5615,loss: 0.008794785\n",
            "\n",
            "Global step: 5616,loss: 0.009531941\n",
            "\n",
            "Global step: 5617,loss: 0.010837685\n",
            "\n",
            "Global step: 5618,loss: 0.008929816\n",
            "\n",
            "Global step: 5619,loss: 0.0081943795\n",
            "\n",
            "Global step: 5620,loss: 0.009320906\n",
            "\n",
            "Global step: 5621,loss: 0.008859695\n",
            "\n",
            "Global step: 5622,loss: 0.011008311\n",
            "\n",
            "Global step: 5623,loss: 0.009174032\n",
            "\n",
            "Global step: 5624,loss: 0.008981782\n",
            "\n",
            "Global step: 5625,loss: 0.008444436\n",
            "\n",
            "Global step: 5626,loss: 0.009868202\n",
            "\n",
            "Global step: 5627,loss: 0.013602636\n",
            "\n",
            "Global step: 5628,loss: 0.009921936\n",
            "\n",
            "Global step: 5629,loss: 0.008236537\n",
            "\n",
            "Global step: 5630,loss: 0.009282722\n",
            "\n",
            "Global step: 5631,loss: 0.011849158\n",
            "\n",
            "Global step: 5632,loss: 0.007497147\n",
            "\n",
            "Global step: 5633,loss: 0.00751714\n",
            "\n",
            "Global step: 5634,loss: 0.008523733\n",
            "\n",
            "Global step: 5635,loss: 0.00964923\n",
            "\n",
            "Global step: 5636,loss: 0.008184759\n",
            "\n",
            "Global step: 5637,loss: 0.007987127\n",
            "\n",
            "Global step: 5638,loss: 0.013646677\n",
            "\n",
            "Global step: 5639,loss: 0.00973487\n",
            "\n",
            "Global step: 5640,loss: 0.008755243\n",
            "\n",
            "Global step: 5641,loss: 0.00781691\n",
            "\n",
            "Global step: 5642,loss: 0.009554617\n",
            "\n",
            "Global step: 5643,loss: 0.009136517\n",
            "\n",
            "Global step: 5644,loss: 0.007285378\n",
            "\n",
            "Global step: 5645,loss: 0.008341229\n",
            "\n",
            "Global step: 5646,loss: 0.012561621\n",
            "\n",
            "Global step: 5647,loss: 0.007663691\n",
            "\n",
            "Global step: 5648,loss: 0.009030398\n",
            "\n",
            "Global step: 5649,loss: 0.0085146725\n",
            "\n",
            "Global step: 5650,loss: 0.0076221293\n",
            "\n",
            "Global step: 5651,loss: 0.012540216\n",
            "\n",
            "Global step: 5652,loss: 0.008704484\n",
            "\n",
            "Global step: 5653,loss: 0.008689289\n",
            "\n",
            "Global step: 5654,loss: 0.016376723\n",
            "\n",
            "Global step: 5655,loss: 0.010154119\n",
            "\n",
            "Global step: 5656,loss: 0.008124867\n",
            "\n",
            "Global step: 5657,loss: 0.009914397\n",
            "\n",
            "Global step: 5658,loss: 0.0090869535\n",
            "\n",
            "Global step: 5659,loss: 0.009625676\n",
            "\n",
            "Global step: 5660,loss: 0.010563147\n",
            "\n",
            "Global step: 5661,loss: 0.0077985716\n",
            "\n",
            "Global step: 5662,loss: 0.007841052\n",
            "\n",
            "Global step: 5663,loss: 0.0087875575\n",
            "\n",
            "Global step: 5664,loss: 0.008253656\n",
            "\n",
            "Global step: 5665,loss: 0.008266082\n",
            "\n",
            "Global step: 5666,loss: 0.011579944\n",
            "\n",
            "Global step: 5667,loss: 0.0084483065\n",
            "\n",
            "Global step: 5668,loss: 0.0071421876\n",
            "\n",
            "Global step: 5669,loss: 0.0105334\n",
            "\n",
            "Global step: 5670,loss: 0.0069774296\n",
            "\n",
            "Global step: 5671,loss: 0.009235935\n",
            "\n",
            "Global step: 5672,loss: 0.009437809\n",
            "\n",
            "Global step: 5673,loss: 0.008569974\n",
            "\n",
            "Global step: 5674,loss: 0.014510272\n",
            "\n",
            "Global step: 5675,loss: 0.011741227\n",
            "\n",
            "Global step: 5676,loss: 0.0095116375\n",
            "\n",
            "Global step: 5677,loss: 0.009668703\n",
            "\n",
            "Global step: 5678,loss: 0.0071870545\n",
            "\n",
            "Global step: 5679,loss: 0.008282773\n",
            "\n",
            "Global step: 5680,loss: 0.009535619\n",
            "\n",
            "Global step: 5681,loss: 0.009102785\n",
            "\n",
            "Global step: 5682,loss: 0.0075434316\n",
            "\n",
            "Global step: 5683,loss: 0.008252777\n",
            "\n",
            "Global step: 5684,loss: 0.013736142\n",
            "\n",
            "Global step: 5685,loss: 0.0096983155\n",
            "\n",
            "Global step: 5686,loss: 0.008468265\n",
            "\n",
            "Global step: 5687,loss: 0.007786608\n",
            "\n",
            "Global step: 5688,loss: 0.0078098695\n",
            "\n",
            "Global step: 5689,loss: 0.011819471\n",
            "\n",
            "Global step: 5690,loss: 0.011446166\n",
            "\n",
            "Global step: 5691,loss: 0.009743689\n",
            "\n",
            "Global step: 5692,loss: 0.008289393\n",
            "\n",
            "Global step: 5693,loss: 0.010642966\n",
            "\n",
            "Global step: 5694,loss: 0.012170849\n",
            "\n",
            "Global step: 5695,loss: 0.011108935\n",
            "\n",
            "Global step: 5696,loss: 0.010907326\n",
            "\n",
            "Global step: 5697,loss: 0.008540025\n",
            "\n",
            "Global step: 5698,loss: 0.008430568\n",
            "\n",
            "Global step: 5699,loss: 0.010942691\n",
            "\n",
            "Global step: 5700,loss: 0.009368779\n",
            "\n",
            "Global step: 5701,loss: 0.008965799\n",
            "\n",
            "Global step: 5702,loss: 0.008180488\n",
            "\n",
            "Global step: 5703,loss: 0.011418806\n",
            "\n",
            "Global step: 5704,loss: 0.008840053\n",
            "\n",
            "Global step: 5705,loss: 0.008700653\n",
            "\n",
            "Global step: 5706,loss: 0.009106727\n",
            "\n",
            "Global step: 5707,loss: 0.008877416\n",
            "\n",
            "Global step: 5708,loss: 0.011312226\n",
            "\n",
            "Global step: 5709,loss: 0.008911141\n",
            "\n",
            "Global step: 5710,loss: 0.011038706\n",
            "\n",
            "Global step: 5711,loss: 0.009798064\n",
            "\n",
            "Global step: 5712,loss: 0.009423742\n",
            "\n",
            "Global step: 5713,loss: 0.0143070435\n",
            "\n",
            "Global step: 5714,loss: 0.006936026\n",
            "\n",
            "Global step: 5715,loss: 0.009083156\n",
            "\n",
            "Global step: 5716,loss: 0.007987456\n",
            "\n",
            "Global step: 5717,loss: 0.009943539\n",
            "\n",
            "Global step: 5718,loss: 0.010530587\n",
            "\n",
            "Global step: 5719,loss: 0.008716542\n",
            "\n",
            "Global step: 5720,loss: 0.010088615\n",
            "\n",
            "Global step: 5721,loss: 0.009110622\n",
            "\n",
            "Global step: 5722,loss: 0.008344805\n",
            "\n",
            "Global step: 5723,loss: 0.00837159\n",
            "\n",
            "Global step: 5724,loss: 0.0078865625\n",
            "\n",
            "Global step: 5725,loss: 0.009198955\n",
            "\n",
            "Global step: 5726,loss: 0.008124489\n",
            "\n",
            "Global step: 5727,loss: 0.0072748195\n",
            "\n",
            "Global step: 5728,loss: 0.010225454\n",
            "\n",
            "Global step: 5729,loss: 0.010799569\n",
            "\n",
            "Global step: 5730,loss: 0.0077286847\n",
            "\n",
            "Global step: 5731,loss: 0.008525919\n",
            "\n",
            "Global step: 5732,loss: 0.008428528\n",
            "\n",
            "Global step: 5733,loss: 0.0076976367\n",
            "\n",
            "Global step: 5734,loss: 0.008381655\n",
            "\n",
            "Global step: 5735,loss: 0.008254418\n",
            "\n",
            "Global step: 5736,loss: 0.00888103\n",
            "\n",
            "Global step: 5737,loss: 0.007302759\n",
            "\n",
            "Global step: 5738,loss: 0.009808602\n",
            "\n",
            "Global step: 5739,loss: 0.01073287\n",
            "\n",
            "Global step: 5740,loss: 0.0084129535\n",
            "\n",
            "Global step: 5741,loss: 0.007145116\n",
            "\n",
            "Global step: 5742,loss: 0.008413102\n",
            "\n",
            "Global step: 5743,loss: 0.008002935\n",
            "\n",
            "Global step: 5744,loss: 0.0072931033\n",
            "\n",
            "Global step: 5745,loss: 0.007901304\n",
            "\n",
            "Global step: 5746,loss: 0.00886688\n",
            "\n",
            "Global step: 5747,loss: 0.007360903\n",
            "\n",
            "Global step: 5748,loss: 0.008901919\n",
            "\n",
            "Global step: 5749,loss: 0.0075633954\n",
            "\n",
            "Global step: 5750,loss: 0.008297846\n",
            "\n",
            "Global step: 5751,loss: 0.007597696\n",
            "\n",
            "Global step: 5752,loss: 0.009659653\n",
            "\n",
            "Global step: 5753,loss: 0.00886053\n",
            "\n",
            "Global step: 5754,loss: 0.008243899\n",
            "\n",
            "Global step: 5755,loss: 0.008162446\n",
            "\n",
            "Global step: 5756,loss: 0.008156823\n",
            "\n",
            "Global step: 5757,loss: 0.009750935\n",
            "\n",
            "Global step: 5758,loss: 0.00877762\n",
            "\n",
            "Global step: 5759,loss: 0.01008988\n",
            "\n",
            "Global step: 5760,loss: 0.007894775\n",
            "\n",
            "Global step: 5761,loss: 0.009023527\n",
            "\n",
            "Global step: 5762,loss: 0.008003939\n",
            "\n",
            "Global step: 5763,loss: 0.009029694\n",
            "\n",
            "Global step: 5764,loss: 0.00805998\n",
            "\n",
            "Global step: 5765,loss: 0.0075295023\n",
            "\n",
            "Global step: 5766,loss: 0.008894639\n",
            "\n",
            "Global step: 5767,loss: 0.009285796\n",
            "\n",
            "Global step: 5768,loss: 0.008856373\n",
            "\n",
            "Global step: 5769,loss: 0.0070079667\n",
            "\n",
            "Global step: 5770,loss: 0.008129143\n",
            "\n",
            "Global step: 5771,loss: 0.0077815535\n",
            "\n",
            "Global step: 5772,loss: 0.013162781\n",
            "\n",
            "Global step: 5773,loss: 0.008978849\n",
            "\n",
            "Global step: 5774,loss: 0.0071546966\n",
            "\n",
            "Global step: 5775,loss: 0.009189285\n",
            "\n",
            "Global step: 5776,loss: 0.0086902315\n",
            "\n",
            "Global step: 5777,loss: 0.00794131\n",
            "\n",
            "Global step: 5778,loss: 0.0075145145\n",
            "\n",
            "Global step: 5779,loss: 0.0074318103\n",
            "\n",
            "Global step: 5780,loss: 0.0068487856\n",
            "\n",
            "Global step: 5781,loss: 0.007156324\n",
            "\n",
            "Global step: 5782,loss: 0.01014444\n",
            "\n",
            "Global step: 5783,loss: 0.008949341\n",
            "\n",
            "Global step: 5784,loss: 0.0077271764\n",
            "\n",
            "Global step: 5785,loss: 0.008041982\n",
            "\n",
            "Global step: 5786,loss: 0.008903151\n",
            "\n",
            "Global step: 5787,loss: 0.008018492\n",
            "\n",
            "Global step: 5788,loss: 0.0074193757\n",
            "\n",
            "Global step: 5789,loss: 0.009566056\n",
            "\n",
            "Global step: 5790,loss: 0.007091933\n",
            "\n",
            "Global step: 5791,loss: 0.0070851305\n",
            "\n",
            "Global step: 5792,loss: 0.007740712\n",
            "\n",
            "Global step: 5793,loss: 0.0068517216\n",
            "\n",
            "Global step: 5794,loss: 0.009634449\n",
            "\n",
            "Global step: 5795,loss: 0.00842649\n",
            "\n",
            "Global step: 5796,loss: 0.0069229365\n",
            "\n",
            "Global step: 5797,loss: 0.011198498\n",
            "\n",
            "Global step: 5798,loss: 0.0077842595\n",
            "\n",
            "Global step: 5799,loss: 0.009285539\n",
            "\n",
            "Global step: 5800,loss: 0.0075265258\n",
            "\n",
            "Global step: 5801,loss: 0.011228148\n",
            "\n",
            "Global step: 5802,loss: 0.008350259\n",
            "\n",
            "Global step: 5803,loss: 0.008265008\n",
            "\n",
            "Global step: 5804,loss: 0.008007774\n",
            "\n",
            "Global step: 5805,loss: 0.007648192\n",
            "\n",
            "Global step: 5806,loss: 0.007060457\n",
            "\n",
            "Global step: 5807,loss: 0.007913171\n",
            "\n",
            "Global step: 5808,loss: 0.008038812\n",
            "\n",
            "Global step: 5809,loss: 0.0073139025\n",
            "\n",
            "Global step: 5810,loss: 0.007244985\n",
            "\n",
            "Global step: 5811,loss: 0.012460856\n",
            "\n",
            "Global step: 5812,loss: 0.0071470505\n",
            "\n",
            "Global step: 5813,loss: 0.008622321\n",
            "\n",
            "Global step: 5814,loss: 0.010305967\n",
            "\n",
            "Global step: 5815,loss: 0.008092788\n",
            "\n",
            "Global step: 5816,loss: 0.007262282\n",
            "\n",
            "Global step: 5817,loss: 0.0073247585\n",
            "\n",
            "Global step: 5818,loss: 0.008114788\n",
            "\n",
            "Global step: 5819,loss: 0.007702022\n",
            "\n",
            "Global step: 5820,loss: 0.011674865\n",
            "\n",
            "Global step: 5821,loss: 0.0072612567\n",
            "\n",
            "Global step: 5822,loss: 0.00851194\n",
            "\n",
            "Global step: 5823,loss: 0.0090177255\n",
            "\n",
            "Global step: 5824,loss: 0.008164583\n",
            "\n",
            "Global step: 5825,loss: 0.006912552\n",
            "\n",
            "Global step: 5826,loss: 0.00648507\n",
            "\n",
            "Global step: 5827,loss: 0.0076034637\n",
            "\n",
            "Global step: 5828,loss: 0.007519541\n",
            "\n",
            "Global step: 5829,loss: 0.0068168445\n",
            "\n",
            "Global step: 5830,loss: 0.0077954847\n",
            "\n",
            "Global step: 5831,loss: 0.008345531\n",
            "\n",
            "Global step: 5832,loss: 0.007913229\n",
            "\n",
            "Global step: 5833,loss: 0.008336828\n",
            "\n",
            "Global step: 5834,loss: 0.007599894\n",
            "\n",
            "Global step: 5835,loss: 0.007967285\n",
            "\n",
            "Global step: 5836,loss: 0.0070178322\n",
            "\n",
            "Global step: 5837,loss: 0.008126146\n",
            "\n",
            "Global step: 5838,loss: 0.007084675\n",
            "\n",
            "Global step: 5839,loss: 0.006725588\n",
            "\n",
            "Global step: 5840,loss: 0.0066304104\n",
            "\n",
            "Global step: 5841,loss: 0.009682873\n",
            "\n",
            "Global step: 5842,loss: 0.0071124574\n",
            "\n",
            "Global step: 5843,loss: 0.0078458935\n",
            "\n",
            "Global step: 5844,loss: 0.0063623358\n",
            "\n",
            "Global step: 5845,loss: 0.007568435\n",
            "\n",
            "Global step: 5846,loss: 0.0073932437\n",
            "\n",
            "Global step: 5847,loss: 0.007211716\n",
            "\n",
            "Global step: 5848,loss: 0.007002783\n",
            "\n",
            "Global step: 5849,loss: 0.006464794\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 5849,val_loss: 0.018980534842763193\n",
            "\n",
            "Training for epoch 16/16:\n",
            "Global step: 5850,loss: 0.007297015\n",
            "\n",
            "Global step: 5851,loss: 0.0078120427\n",
            "\n",
            "Global step: 5852,loss: 0.006759093\n",
            "\n",
            "Global step: 5853,loss: 0.006226578\n",
            "\n",
            "Global step: 5854,loss: 0.007442544\n",
            "\n",
            "Global step: 5855,loss: 0.008137922\n",
            "\n",
            "Global step: 5856,loss: 0.00621299\n",
            "\n",
            "Global step: 5857,loss: 0.0071917474\n",
            "\n",
            "Global step: 5858,loss: 0.006484225\n",
            "\n",
            "Global step: 5859,loss: 0.0064287144\n",
            "\n",
            "Global step: 5860,loss: 0.0095497975\n",
            "\n",
            "Global step: 5861,loss: 0.006169547\n",
            "\n",
            "Global step: 5862,loss: 0.007025826\n",
            "\n",
            "Global step: 5863,loss: 0.0073243277\n",
            "\n",
            "Global step: 5864,loss: 0.0077687125\n",
            "\n",
            "Global step: 5865,loss: 0.0060731927\n",
            "\n",
            "Global step: 5866,loss: 0.006811799\n",
            "\n",
            "Global step: 5867,loss: 0.0066180713\n",
            "\n",
            "Global step: 5868,loss: 0.006539925\n",
            "\n",
            "Global step: 5869,loss: 0.006890753\n",
            "\n",
            "Global step: 5870,loss: 0.006760652\n",
            "\n",
            "Global step: 5871,loss: 0.006466097\n",
            "\n",
            "Global step: 5872,loss: 0.007290763\n",
            "\n",
            "Global step: 5873,loss: 0.006884794\n",
            "\n",
            "Global step: 5874,loss: 0.0062985513\n",
            "\n",
            "Global step: 5875,loss: 0.006327704\n",
            "\n",
            "Global step: 5876,loss: 0.007649882\n",
            "\n",
            "Global step: 5877,loss: 0.0061504296\n",
            "\n",
            "Global step: 5878,loss: 0.006658284\n",
            "\n",
            "Global step: 5879,loss: 0.006890904\n",
            "\n",
            "Global step: 5880,loss: 0.00610283\n",
            "\n",
            "Global step: 5881,loss: 0.007605752\n",
            "\n",
            "Global step: 5882,loss: 0.0064146384\n",
            "\n",
            "Global step: 5883,loss: 0.010234626\n",
            "\n",
            "Global step: 5884,loss: 0.0074317185\n",
            "\n",
            "Global step: 5885,loss: 0.008864092\n",
            "\n",
            "Global step: 5886,loss: 0.0076274155\n",
            "\n",
            "Global step: 5887,loss: 0.0063386695\n",
            "\n",
            "Global step: 5888,loss: 0.006983574\n",
            "\n",
            "Global step: 5889,loss: 0.006725493\n",
            "\n",
            "Global step: 5890,loss: 0.0066130944\n",
            "\n",
            "Global step: 5891,loss: 0.006530223\n",
            "\n",
            "Global step: 5892,loss: 0.0072165183\n",
            "\n",
            "Global step: 5893,loss: 0.0070564887\n",
            "\n",
            "Global step: 5894,loss: 0.0073986547\n",
            "\n",
            "Global step: 5895,loss: 0.0065364004\n",
            "\n",
            "Global step: 5896,loss: 0.0069709476\n",
            "\n",
            "Global step: 5897,loss: 0.007631692\n",
            "\n",
            "Global step: 5898,loss: 0.006979177\n",
            "\n",
            "Global step: 5899,loss: 0.0060916394\n",
            "\n",
            "Global step: 5900,loss: 0.007741699\n",
            "\n",
            "Global step: 5901,loss: 0.006402826\n",
            "\n",
            "Global step: 5902,loss: 0.0071421973\n",
            "\n",
            "Global step: 5903,loss: 0.0066556856\n",
            "\n",
            "Global step: 5904,loss: 0.0077747134\n",
            "\n",
            "Global step: 5905,loss: 0.006126662\n",
            "\n",
            "Global step: 5906,loss: 0.007374336\n",
            "\n",
            "Global step: 5907,loss: 0.0068361135\n",
            "\n",
            "Global step: 5908,loss: 0.0067164265\n",
            "\n",
            "Global step: 5909,loss: 0.0060826135\n",
            "\n",
            "Global step: 5910,loss: 0.006525005\n",
            "\n",
            "Global step: 5911,loss: 0.006485837\n",
            "\n",
            "Global step: 5912,loss: 0.006285724\n",
            "\n",
            "Global step: 5913,loss: 0.007459821\n",
            "\n",
            "Global step: 5914,loss: 0.0071860217\n",
            "\n",
            "Global step: 5915,loss: 0.006713348\n",
            "\n",
            "Global step: 5916,loss: 0.0067329113\n",
            "\n",
            "Global step: 5917,loss: 0.0062507126\n",
            "\n",
            "Global step: 5918,loss: 0.0060128877\n",
            "\n",
            "Global step: 5919,loss: 0.0063854447\n",
            "\n",
            "Global step: 5920,loss: 0.0064928\n",
            "\n",
            "Global step: 5921,loss: 0.012995105\n",
            "\n",
            "Global step: 5922,loss: 0.006727337\n",
            "\n",
            "Global step: 5923,loss: 0.006786514\n",
            "\n",
            "Global step: 5924,loss: 0.006668995\n",
            "\n",
            "Global step: 5925,loss: 0.006741648\n",
            "\n",
            "Global step: 5926,loss: 0.0066116597\n",
            "\n",
            "Global step: 5927,loss: 0.0063709\n",
            "\n",
            "Global step: 5928,loss: 0.006047617\n",
            "\n",
            "Global step: 5929,loss: 0.0061027505\n",
            "\n",
            "Global step: 5930,loss: 0.00683012\n",
            "\n",
            "Global step: 5931,loss: 0.006814264\n",
            "\n",
            "Global step: 5932,loss: 0.006192866\n",
            "\n",
            "Global step: 5933,loss: 0.0074356776\n",
            "\n",
            "Global step: 5934,loss: 0.00634259\n",
            "\n",
            "Global step: 5935,loss: 0.0063705873\n",
            "\n",
            "Global step: 5936,loss: 0.006296137\n",
            "\n",
            "Global step: 5937,loss: 0.0067836386\n",
            "\n",
            "Global step: 5938,loss: 0.009960329\n",
            "\n",
            "Global step: 5939,loss: 0.0063419347\n",
            "\n",
            "Global step: 5940,loss: 0.006310802\n",
            "\n",
            "Global step: 5941,loss: 0.0062913513\n",
            "\n",
            "Global step: 5942,loss: 0.0067935707\n",
            "\n",
            "Global step: 5943,loss: 0.00638668\n",
            "\n",
            "Global step: 5944,loss: 0.006248196\n",
            "\n",
            "Global step: 5945,loss: 0.006846056\n",
            "\n",
            "Global step: 5946,loss: 0.0062628016\n",
            "\n",
            "Global step: 5947,loss: 0.0062272362\n",
            "\n",
            "Global step: 5948,loss: 0.0059795734\n",
            "\n",
            "Global step: 5949,loss: 0.0060515366\n",
            "\n",
            "Global step: 5950,loss: 0.0071323263\n",
            "\n",
            "Global step: 5951,loss: 0.0068602916\n",
            "\n",
            "Global step: 5952,loss: 0.006677449\n",
            "\n",
            "Global step: 5953,loss: 0.006707547\n",
            "\n",
            "Global step: 5954,loss: 0.0072907037\n",
            "\n",
            "Global step: 5955,loss: 0.0059519485\n",
            "\n",
            "Global step: 5956,loss: 0.0067584324\n",
            "\n",
            "Global step: 5957,loss: 0.006235169\n",
            "\n",
            "Global step: 5958,loss: 0.005915357\n",
            "\n",
            "Global step: 5959,loss: 0.0059085554\n",
            "\n",
            "Global step: 5960,loss: 0.0064021978\n",
            "\n",
            "Global step: 5961,loss: 0.006708616\n",
            "\n",
            "Global step: 5962,loss: 0.0060636443\n",
            "\n",
            "Global step: 5963,loss: 0.007124593\n",
            "\n",
            "Global step: 5964,loss: 0.0060027665\n",
            "\n",
            "Global step: 5965,loss: 0.0069026086\n",
            "\n",
            "Global step: 5966,loss: 0.008192774\n",
            "\n",
            "Global step: 5967,loss: 0.005932311\n",
            "\n",
            "Global step: 5968,loss: 0.006317302\n",
            "\n",
            "Global step: 5969,loss: 0.0078097163\n",
            "\n",
            "Global step: 5970,loss: 0.0066515408\n",
            "\n",
            "Global step: 5971,loss: 0.005919178\n",
            "\n",
            "Global step: 5972,loss: 0.0066361213\n",
            "\n",
            "Global step: 5973,loss: 0.008906966\n",
            "\n",
            "Global step: 5974,loss: 0.0066092587\n",
            "\n",
            "Global step: 5975,loss: 0.006501411\n",
            "\n",
            "Global step: 5976,loss: 0.0065221093\n",
            "\n",
            "Global step: 5977,loss: 0.0064230217\n",
            "\n",
            "Global step: 5978,loss: 0.006132798\n",
            "\n",
            "Global step: 5979,loss: 0.0066641644\n",
            "\n",
            "Global step: 5980,loss: 0.008600683\n",
            "\n",
            "Global step: 5981,loss: 0.0062851897\n",
            "\n",
            "Global step: 5982,loss: 0.0062886905\n",
            "\n",
            "Global step: 5983,loss: 0.0065842955\n",
            "\n",
            "Global step: 5984,loss: 0.006608484\n",
            "\n",
            "Global step: 5985,loss: 0.006161462\n",
            "\n",
            "Global step: 5986,loss: 0.007077664\n",
            "\n",
            "Global step: 5987,loss: 0.009321464\n",
            "\n",
            "Global step: 5988,loss: 0.00623635\n",
            "\n",
            "Global step: 5989,loss: 0.009347785\n",
            "\n",
            "Global step: 5990,loss: 0.006273569\n",
            "\n",
            "Global step: 5991,loss: 0.006149187\n",
            "\n",
            "Global step: 5992,loss: 0.006407072\n",
            "\n",
            "Global step: 5993,loss: 0.006239658\n",
            "\n",
            "Global step: 5994,loss: 0.008262042\n",
            "\n",
            "Global step: 5995,loss: 0.006422718\n",
            "\n",
            "Global step: 5996,loss: 0.0074856053\n",
            "\n",
            "Global step: 5997,loss: 0.006763221\n",
            "\n",
            "Global step: 5998,loss: 0.00635795\n",
            "\n",
            "Global step: 5999,loss: 0.0063404227\n",
            "\n",
            "Global step: 6000,loss: 0.0069214455\n",
            "\n",
            "Global step: 6001,loss: 0.0064183315\n",
            "\n",
            "Global step: 6002,loss: 0.005931901\n",
            "\n",
            "Global step: 6003,loss: 0.0060734595\n",
            "\n",
            "Global step: 6004,loss: 0.00705674\n",
            "\n",
            "Global step: 6005,loss: 0.006667606\n",
            "\n",
            "Global step: 6006,loss: 0.0063828304\n",
            "\n",
            "Global step: 6007,loss: 0.006523527\n",
            "\n",
            "Global step: 6008,loss: 0.0061973706\n",
            "\n",
            "Global step: 6009,loss: 0.007376849\n",
            "\n",
            "Global step: 6010,loss: 0.007770016\n",
            "\n",
            "Global step: 6011,loss: 0.006267622\n",
            "\n",
            "Global step: 6012,loss: 0.0066807377\n",
            "\n",
            "Global step: 6013,loss: 0.007072443\n",
            "\n",
            "Global step: 6014,loss: 0.006830679\n",
            "\n",
            "Global step: 6015,loss: 0.0065399627\n",
            "\n",
            "Global step: 6016,loss: 0.0065060174\n",
            "\n",
            "Global step: 6017,loss: 0.0059894924\n",
            "\n",
            "Global step: 6018,loss: 0.0060600443\n",
            "\n",
            "Global step: 6019,loss: 0.0063484763\n",
            "\n",
            "Global step: 6020,loss: 0.005988244\n",
            "\n",
            "Global step: 6021,loss: 0.00609432\n",
            "\n",
            "Global step: 6022,loss: 0.005974581\n",
            "\n",
            "Global step: 6023,loss: 0.0070869224\n",
            "\n",
            "Global step: 6024,loss: 0.011635003\n",
            "\n",
            "Global step: 6025,loss: 0.0062328875\n",
            "\n",
            "Global step: 6026,loss: 0.0067115747\n",
            "\n",
            "Global step: 6027,loss: 0.0067843013\n",
            "\n",
            "Global step: 6028,loss: 0.005724933\n",
            "\n",
            "Global step: 6029,loss: 0.0065300697\n",
            "\n",
            "Global step: 6030,loss: 0.006011366\n",
            "\n",
            "Global step: 6031,loss: 0.006662531\n",
            "\n",
            "Global step: 6032,loss: 0.0066385167\n",
            "\n",
            "Global step: 6033,loss: 0.006195789\n",
            "\n",
            "Global step: 6034,loss: 0.006130188\n",
            "\n",
            "Global step: 6035,loss: 0.006157498\n",
            "\n",
            "Global step: 6036,loss: 0.0061673378\n",
            "\n",
            "Global step: 6037,loss: 0.0064980052\n",
            "\n",
            "Global step: 6038,loss: 0.0067648934\n",
            "\n",
            "Global step: 6039,loss: 0.0063861944\n",
            "\n",
            "Global step: 6040,loss: 0.006415484\n",
            "\n",
            "Global step: 6041,loss: 0.006527813\n",
            "\n",
            "Global step: 6042,loss: 0.0076453015\n",
            "\n",
            "Global step: 6043,loss: 0.006316772\n",
            "\n",
            "Global step: 6044,loss: 0.006283803\n",
            "\n",
            "Global step: 6045,loss: 0.0063673127\n",
            "\n",
            "Global step: 6046,loss: 0.0071380716\n",
            "\n",
            "Global step: 6047,loss: 0.0073833913\n",
            "\n",
            "Global step: 6048,loss: 0.005778791\n",
            "\n",
            "Global step: 6049,loss: 0.0069285785\n",
            "\n",
            "Global step: 6050,loss: 0.010551937\n",
            "\n",
            "Global step: 6051,loss: 0.006340295\n",
            "\n",
            "Global step: 6052,loss: 0.0066948626\n",
            "\n",
            "Global step: 6053,loss: 0.006257881\n",
            "\n",
            "Global step: 6054,loss: 0.005828154\n",
            "\n",
            "Global step: 6055,loss: 0.009801006\n",
            "\n",
            "Global step: 6056,loss: 0.00582862\n",
            "\n",
            "Global step: 6057,loss: 0.0066361013\n",
            "\n",
            "Global step: 6058,loss: 0.006560103\n",
            "\n",
            "Global step: 6059,loss: 0.006414203\n",
            "\n",
            "Global step: 6060,loss: 0.0064701117\n",
            "\n",
            "Global step: 6061,loss: 0.0060375645\n",
            "\n",
            "Global step: 6062,loss: 0.0060289814\n",
            "\n",
            "Global step: 6063,loss: 0.0060856137\n",
            "\n",
            "Global step: 6064,loss: 0.00805594\n",
            "\n",
            "Global step: 6065,loss: 0.009917026\n",
            "\n",
            "Global step: 6066,loss: 0.0059470623\n",
            "\n",
            "Global step: 6067,loss: 0.006236249\n",
            "\n",
            "Global step: 6068,loss: 0.0063708522\n",
            "\n",
            "Global step: 6069,loss: 0.006366759\n",
            "\n",
            "Global step: 6070,loss: 0.0066802665\n",
            "\n",
            "Global step: 6071,loss: 0.008616881\n",
            "\n",
            "Global step: 6072,loss: 0.0059139854\n",
            "\n",
            "Global step: 6073,loss: 0.008670859\n",
            "\n",
            "Global step: 6074,loss: 0.0060376115\n",
            "\n",
            "Global step: 6075,loss: 0.006165909\n",
            "\n",
            "Global step: 6076,loss: 0.0063089137\n",
            "\n",
            "Global step: 6077,loss: 0.0063362094\n",
            "\n",
            "Global step: 6078,loss: 0.0060065035\n",
            "\n",
            "Global step: 6079,loss: 0.006407225\n",
            "\n",
            "Global step: 6080,loss: 0.0074160635\n",
            "\n",
            "Global step: 6081,loss: 0.0064846887\n",
            "\n",
            "Global step: 6082,loss: 0.007421228\n",
            "\n",
            "Global step: 6083,loss: 0.006267927\n",
            "\n",
            "Global step: 6084,loss: 0.006200879\n",
            "\n",
            "Global step: 6085,loss: 0.006468123\n",
            "\n",
            "Global step: 6086,loss: 0.0059105037\n",
            "\n",
            "Global step: 6087,loss: 0.006577877\n",
            "\n",
            "Global step: 6088,loss: 0.0062005385\n",
            "\n",
            "Global step: 6089,loss: 0.0062431255\n",
            "\n",
            "Global step: 6090,loss: 0.0056084637\n",
            "\n",
            "Global step: 6091,loss: 0.0065390966\n",
            "\n",
            "Global step: 6092,loss: 0.0060149804\n",
            "\n",
            "Global step: 6093,loss: 0.0065979776\n",
            "\n",
            "Global step: 6094,loss: 0.0060603702\n",
            "\n",
            "Global step: 6095,loss: 0.006519951\n",
            "\n",
            "Global step: 6096,loss: 0.0064645247\n",
            "\n",
            "Global step: 6097,loss: 0.006257935\n",
            "\n",
            "Global step: 6098,loss: 0.0054609617\n",
            "\n",
            "Global step: 6099,loss: 0.0064285225\n",
            "\n",
            "Global step: 6100,loss: 0.005760726\n",
            "\n",
            "Global step: 6101,loss: 0.007793033\n",
            "\n",
            "Global step: 6102,loss: 0.0057351333\n",
            "\n",
            "Global step: 6103,loss: 0.005724694\n",
            "\n",
            "Global step: 6104,loss: 0.006023297\n",
            "\n",
            "Global step: 6105,loss: 0.006359726\n",
            "\n",
            "Global step: 6106,loss: 0.006581174\n",
            "\n",
            "Global step: 6107,loss: 0.005843196\n",
            "\n",
            "Global step: 6108,loss: 0.0072056716\n",
            "\n",
            "Global step: 6109,loss: 0.0060593253\n",
            "\n",
            "Global step: 6110,loss: 0.0062383357\n",
            "\n",
            "Global step: 6111,loss: 0.0063483687\n",
            "\n",
            "Global step: 6112,loss: 0.0064582117\n",
            "\n",
            "Global step: 6113,loss: 0.00593994\n",
            "\n",
            "Global step: 6114,loss: 0.006769482\n",
            "\n",
            "Global step: 6115,loss: 0.006495299\n",
            "\n",
            "Global step: 6116,loss: 0.0056128516\n",
            "\n",
            "Global step: 6117,loss: 0.006352488\n",
            "\n",
            "Global step: 6118,loss: 0.0061357133\n",
            "\n",
            "Global step: 6119,loss: 0.0060489685\n",
            "\n",
            "Global step: 6120,loss: 0.0060255295\n",
            "\n",
            "Global step: 6121,loss: 0.006168266\n",
            "\n",
            "Global step: 6122,loss: 0.005863467\n",
            "\n",
            "Global step: 6123,loss: 0.0058528124\n",
            "\n",
            "Global step: 6124,loss: 0.006711698\n",
            "\n",
            "Global step: 6125,loss: 0.0067724413\n",
            "\n",
            "Global step: 6126,loss: 0.005526665\n",
            "\n",
            "Global step: 6127,loss: 0.0067830863\n",
            "\n",
            "Global step: 6128,loss: 0.0065107914\n",
            "\n",
            "Global step: 6129,loss: 0.0061450265\n",
            "\n",
            "Global step: 6130,loss: 0.005736404\n",
            "\n",
            "Global step: 6131,loss: 0.006800648\n",
            "\n",
            "Global step: 6132,loss: 0.005972401\n",
            "\n",
            "Global step: 6133,loss: 0.005848904\n",
            "\n",
            "Global step: 6134,loss: 0.0061721867\n",
            "\n",
            "Global step: 6135,loss: 0.0060606105\n",
            "\n",
            "Global step: 6136,loss: 0.005976815\n",
            "\n",
            "Global step: 6137,loss: 0.005975306\n",
            "\n",
            "Global step: 6138,loss: 0.0055708475\n",
            "\n",
            "Global step: 6139,loss: 0.0062626298\n",
            "\n",
            "Global step: 6140,loss: 0.0059541864\n",
            "\n",
            "Global step: 6141,loss: 0.0062191547\n",
            "\n",
            "Global step: 6142,loss: 0.0068025794\n",
            "\n",
            "Global step: 6143,loss: 0.0058320914\n",
            "\n",
            "Global step: 6144,loss: 0.0058739465\n",
            "\n",
            "Global step: 6145,loss: 0.0069468003\n",
            "\n",
            "Global step: 6146,loss: 0.0065734657\n",
            "\n",
            "Global step: 6147,loss: 0.0057176948\n",
            "\n",
            "Global step: 6148,loss: 0.0063442662\n",
            "\n",
            "Global step: 6149,loss: 0.006406152\n",
            "\n",
            "Global step: 6150,loss: 0.0065098535\n",
            "\n",
            "Global step: 6151,loss: 0.0061291195\n",
            "\n",
            "Global step: 6152,loss: 0.0065676505\n",
            "\n",
            "Global step: 6153,loss: 0.006680816\n",
            "\n",
            "Global step: 6154,loss: 0.005820499\n",
            "\n",
            "Global step: 6155,loss: 0.0063154097\n",
            "\n",
            "Global step: 6156,loss: 0.0069675646\n",
            "\n",
            "Global step: 6157,loss: 0.0064499155\n",
            "\n",
            "Global step: 6158,loss: 0.005768074\n",
            "\n",
            "Global step: 6159,loss: 0.006160144\n",
            "\n",
            "Global step: 6160,loss: 0.006817699\n",
            "\n",
            "Global step: 6161,loss: 0.005990432\n",
            "\n",
            "Global step: 6162,loss: 0.005699385\n",
            "\n",
            "Global step: 6163,loss: 0.005696003\n",
            "\n",
            "Global step: 6164,loss: 0.006283443\n",
            "\n",
            "Global step: 6165,loss: 0.005801516\n",
            "\n",
            "Global step: 6166,loss: 0.0057132198\n",
            "\n",
            "Global step: 6167,loss: 0.0060651936\n",
            "\n",
            "Global step: 6168,loss: 0.0060612336\n",
            "\n",
            "Global step: 6169,loss: 0.005721265\n",
            "\n",
            "Global step: 6170,loss: 0.0070119225\n",
            "\n",
            "Global step: 6171,loss: 0.007113185\n",
            "\n",
            "Global step: 6172,loss: 0.0060971766\n",
            "\n",
            "Global step: 6173,loss: 0.0069372216\n",
            "\n",
            "Global step: 6174,loss: 0.005614167\n",
            "\n",
            "Global step: 6175,loss: 0.006267558\n",
            "\n",
            "Global step: 6176,loss: 0.0068942364\n",
            "\n",
            "Global step: 6177,loss: 0.005930014\n",
            "\n",
            "Global step: 6178,loss: 0.0065176776\n",
            "\n",
            "Global step: 6179,loss: 0.006359608\n",
            "\n",
            "Global step: 6180,loss: 0.005576238\n",
            "\n",
            "Global step: 6181,loss: 0.0056614885\n",
            "\n",
            "Global step: 6182,loss: 0.006195193\n",
            "\n",
            "Global step: 6183,loss: 0.006202702\n",
            "\n",
            "Global step: 6184,loss: 0.0057682665\n",
            "\n",
            "Global step: 6185,loss: 0.006519354\n",
            "\n",
            "Global step: 6186,loss: 0.005871208\n",
            "\n",
            "Global step: 6187,loss: 0.006355386\n",
            "\n",
            "Global step: 6188,loss: 0.006389536\n",
            "\n",
            "Global step: 6189,loss: 0.006568852\n",
            "\n",
            "Global step: 6190,loss: 0.0063508484\n",
            "\n",
            "Global step: 6191,loss: 0.005889166\n",
            "\n",
            "Global step: 6192,loss: 0.006554717\n",
            "\n",
            "Global step: 6193,loss: 0.005958911\n",
            "\n",
            "Global step: 6194,loss: 0.0059017725\n",
            "\n",
            "Global step: 6195,loss: 0.0063221846\n",
            "\n",
            "Global step: 6196,loss: 0.006246169\n",
            "\n",
            "Global step: 6197,loss: 0.0059223664\n",
            "\n",
            "Global step: 6198,loss: 0.006413942\n",
            "\n",
            "Global step: 6199,loss: 0.006215216\n",
            "\n",
            "Global step: 6200,loss: 0.00607058\n",
            "\n",
            "Global step: 6201,loss: 0.0058956053\n",
            "\n",
            "Global step: 6202,loss: 0.005907254\n",
            "\n",
            "Global step: 6203,loss: 0.00612677\n",
            "\n",
            "Global step: 6204,loss: 0.0059716203\n",
            "\n",
            "Global step: 6205,loss: 0.0065839933\n",
            "\n",
            "Global step: 6206,loss: 0.005933181\n",
            "\n",
            "Global step: 6207,loss: 0.0066333152\n",
            "\n",
            "Global step: 6208,loss: 0.0061261053\n",
            "\n",
            "Global step: 6209,loss: 0.0059128916\n",
            "\n",
            "Global step: 6210,loss: 0.00615831\n",
            "\n",
            "Global step: 6211,loss: 0.0071282717\n",
            "\n",
            "Global step: 6212,loss: 0.0062992033\n",
            "\n",
            "Global step: 6213,loss: 0.007060061\n",
            "\n",
            "Global step: 6214,loss: 0.006109675\n",
            "\n",
            "Global step: 6215,loss: 0.0060348357\n",
            "\n",
            "Global step: 6216,loss: 0.006530142\n",
            "\n",
            "Global step: 6217,loss: 0.0062004393\n",
            "\n",
            "Global step: 6218,loss: 0.005916164\n",
            "\n",
            "Global step: 6219,loss: 0.006156096\n",
            "\n",
            "Global step: 6220,loss: 0.006880518\n",
            "\n",
            "Global step: 6221,loss: 0.0061355694\n",
            "\n",
            "Global step: 6222,loss: 0.0061241114\n",
            "\n",
            "Global step: 6223,loss: 0.0057528126\n",
            "\n",
            "Global step: 6224,loss: 0.005644417\n",
            "\n",
            "Global step: 6225,loss: 0.0059534064\n",
            "\n",
            "Global step: 6226,loss: 0.00594\n",
            "\n",
            "Global step: 6227,loss: 0.006059775\n",
            "\n",
            "Global step: 6228,loss: 0.005331741\n",
            "\n",
            "Global step: 6229,loss: 0.00585502\n",
            "\n",
            "Global step: 6230,loss: 0.006457092\n",
            "\n",
            "Global step: 6231,loss: 0.006476394\n",
            "\n",
            "Global step: 6232,loss: 0.006179749\n",
            "\n",
            "Global step: 6233,loss: 0.0058512376\n",
            "\n",
            "Global step: 6234,loss: 0.0060292967\n",
            "\n",
            "Global step: 6235,loss: 0.006294465\n",
            "\n",
            "Global step: 6236,loss: 0.006020193\n",
            "\n",
            "Global step: 6237,loss: 0.006016409\n",
            "\n",
            "Global step: 6238,loss: 0.005809814\n",
            "\n",
            "Global step: 6239,loss: 0.005715694\n",
            "\n",
            "\n",
            "######NOT SAVING MODEL #########\n",
            "\n",
            "Global Step: 6239,val_loss: 0.016086727703133456\n",
            "\n",
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:20:33.682474 139738004481920 <ipython-input-10-fd696e1d0d24>:16] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "outputId": "d20ee93e-63ef-4fb5-ab7d-a9bb6e2030f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "cfg.is_training=False\n",
        "\n",
        "try:\n",
        "  def main(_):\n",
        "      tf.logging.info(' Loading Graph...')\n",
        "      num_label = 10\n",
        "      model = CapsNet()\n",
        "      tf.logging.info(' Graph loaded')\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "      sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "      if cfg.is_training:\n",
        "          tf.logging.info(' Start training...')\n",
        "          train(model, sv, num_label)\n",
        "          tf.logging.info('Training done')\n",
        "      else:\n",
        "          evaluation(model, sv, num_label)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      tf.app.run()\n",
        "\n",
        "except:\n",
        "  print(\"\\Completed\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:24:52.461792 139738004481920 <ipython-input-12-d583bcff4fbf>:5]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Data Augmentation\n",
            "\n",
            "Finished Augmentation\n",
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:25:01.494881 139738004481920 <ipython-input-7-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:25:01.497150 139738004481920 <ipython-input-12-d583bcff4fbf>:8]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:25:02.361317 139738004481920 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1112 10:25:02.392027 139738004481920 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIK29G4RIeCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}