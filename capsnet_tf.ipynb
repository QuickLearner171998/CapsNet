{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "outputId": "6a1627be-79f4-49b8-f67d-bf3004d9c0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "outputId": "ca08a0d5-20c7-40a4-defe-ec59f5568429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   data     results\t\t  Try-1   Try-4\n",
            " capsnet_tf.py\t     logdir   tensorboard.ipynb   Try-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "outputId": "5821dc76-9a6f-4fa8-9477-973d154571ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "        trX = trainX[:55000] / 255.\n",
        "        trY = trainY[:55000]\n",
        "\n",
        "        valX = trainX[55000:, ] / 255.\n",
        "        valY = trainY[55000:]\n",
        "\n",
        "        num_tr_batch = 55000 // batch_size\n",
        "        num_val_batch = 5000 // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.99, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.01, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.6, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 15, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "## org\n",
        "#flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "flags.DEFINE_float('regularization_scale', 0.7,'modified original 0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "# ############################\n",
        "# #   distributed setting    #\n",
        "# ############################\n",
        "# flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "# flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "# flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch+1, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (num_val_batch)\n",
        "\n",
        "                if (val_loss < best_val_loss):\n",
        "                  best_val_loss = val_loss\n",
        "                  print(\"\\n##################### Saving Model ############################\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\n######NOT SAVING MODEL #########\\n\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "outputId": "f7067274-9fcf-4e4e-afe4-596541c3a2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cfg.is_training=True\n",
        "\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:41.038627 139651229792128 <ipython-input-9-72518a14b1af>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:43.925774 139651229792128 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.268098 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.300000 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.303859 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.308092 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.311283 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.327007 139651229792128 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:44.347249 139651229792128 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I1110 21:18:44.809952 139651229792128 utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:45.071690 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:45.276728 139651229792128 deprecation.py:323] From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:45.428299 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:46.016201 139651229792128 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:46.017825 139651229792128 <ipython-input-9-72518a14b1af>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-72518a14b1af>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:18:46.019461 139651229792128 deprecation.py:323] From <ipython-input-9-72518a14b1af>:11: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:46.411169 139651229792128 <ipython-input-9-72518a14b1af>:14]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:50.733302 139651229792128 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:18:50.757484 139651229792128 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:19:21.568948 139651229792128 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:19:22.132267 139651229792128 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:19:22.133307 139648999347968 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 1/15:\n",
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:19:27.855586 139649007740672 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.9809171\n",
            "\n",
            "Global step: 1,loss: 0.96941394\n",
            "\n",
            "Global step: 2,loss: 1.0557588\n",
            "\n",
            "Global step: 3,loss: 0.9301882\n",
            "\n",
            "Global step: 4,loss: 0.9305286\n",
            "\n",
            "Global step: 5,loss: 0.9235004\n",
            "\n",
            "Global step: 6,loss: 0.8864584\n",
            "\n",
            "Global step: 7,loss: 0.8128822\n",
            "\n",
            "Global step: 8,loss: 0.7480244\n",
            "\n",
            "Global step: 9,loss: 0.68148065\n",
            "\n",
            "Global step: 10,loss: 0.64669013\n",
            "\n",
            "Global step: 11,loss: 0.5679749\n",
            "\n",
            "Global step: 12,loss: 0.51280075\n",
            "\n",
            "Global step: 13,loss: 0.5241927\n",
            "\n",
            "Global step: 14,loss: 0.46341592\n",
            "\n",
            "Global step: 15,loss: 0.4229249\n",
            "\n",
            "Global step: 16,loss: 0.3960571\n",
            "\n",
            "Global step: 17,loss: 0.37967846\n",
            "\n",
            "Global step: 18,loss: 0.3870006\n",
            "\n",
            "Global step: 19,loss: 0.32867795\n",
            "\n",
            "Global step: 20,loss: 0.3503825\n",
            "\n",
            "Global step: 21,loss: 0.31347632\n",
            "\n",
            "Global step: 22,loss: 0.32073092\n",
            "\n",
            "Global step: 23,loss: 0.33370656\n",
            "\n",
            "Global step: 24,loss: 0.29634058\n",
            "\n",
            "Global step: 25,loss: 0.29594415\n",
            "\n",
            "Global step: 26,loss: 0.30527171\n",
            "\n",
            "Global step: 27,loss: 0.28240612\n",
            "\n",
            "Global step: 28,loss: 0.26715717\n",
            "\n",
            "Global step: 29,loss: 0.29689342\n",
            "\n",
            "Global step: 30,loss: 0.25188494\n",
            "\n",
            "Global step: 31,loss: 0.248444\n",
            "\n",
            "Global step: 32,loss: 0.24198589\n",
            "\n",
            "Global step: 33,loss: 0.24748828\n",
            "\n",
            "Global step: 34,loss: 0.230323\n",
            "\n",
            "Global step: 35,loss: 0.2750962\n",
            "\n",
            "Global step: 36,loss: 0.2310043\n",
            "\n",
            "Global step: 37,loss: 0.24312383\n",
            "\n",
            "Global step: 38,loss: 0.19000581\n",
            "\n",
            "Global step: 39,loss: 0.18910733\n",
            "\n",
            "Global step: 40,loss: 0.21568218\n",
            "\n",
            "Global step: 41,loss: 0.17083\n",
            "\n",
            "Global step: 42,loss: 0.19183952\n",
            "\n",
            "Global step: 43,loss: 0.18028706\n",
            "\n",
            "Global step: 44,loss: 0.16690025\n",
            "\n",
            "Global step: 45,loss: 0.2109309\n",
            "\n",
            "Global step: 46,loss: 0.19916691\n",
            "\n",
            "Global step: 47,loss: 0.20363623\n",
            "\n",
            "Global step: 48,loss: 0.16850002\n",
            "\n",
            "Global step: 49,loss: 0.19470143\n",
            "\n",
            "Global step: 50,loss: 0.16642278\n",
            "\n",
            "Global step: 51,loss: 0.15105091\n",
            "\n",
            "Global step: 52,loss: 0.16383423\n",
            "\n",
            "Global step: 53,loss: 0.19031355\n",
            "\n",
            "Global step: 54,loss: 0.15464552\n",
            "\n",
            "Global step: 55,loss: 0.16739361\n",
            "\n",
            "Global step: 56,loss: 0.15722853\n",
            "\n",
            "Global step: 57,loss: 0.15375103\n",
            "\n",
            "Global step: 58,loss: 0.13008064\n",
            "\n",
            "Global step: 59,loss: 0.1596429\n",
            "\n",
            "Global step: 60,loss: 0.13565063\n",
            "\n",
            "Global step: 61,loss: 0.13815756\n",
            "\n",
            "Global step: 62,loss: 0.1487999\n",
            "\n",
            "Global step: 63,loss: 0.16098304\n",
            "\n",
            "Global step: 64,loss: 0.12819996\n",
            "\n",
            "Global step: 65,loss: 0.10957219\n",
            "\n",
            "Global step: 66,loss: 0.1394678\n",
            "\n",
            "Global step: 67,loss: 0.16097827\n",
            "\n",
            "Global step: 68,loss: 0.124997616\n",
            "\n",
            "Global step: 69,loss: 0.1191561\n",
            "\n",
            "Global step: 70,loss: 0.13037485\n",
            "\n",
            "Global step: 71,loss: 0.1483163\n",
            "\n",
            "Global step: 72,loss: 0.14268212\n",
            "\n",
            "Global step: 73,loss: 0.1474782\n",
            "\n",
            "Global step: 74,loss: 0.13014519\n",
            "\n",
            "Global step: 75,loss: 0.14808542\n",
            "\n",
            "Global step: 76,loss: 0.104266256\n",
            "\n",
            "Global step: 77,loss: 0.13303697\n",
            "\n",
            "Global step: 78,loss: 0.12313127\n",
            "\n",
            "Global step: 79,loss: 0.1295171\n",
            "\n",
            "Global step: 80,loss: 0.11394943\n",
            "\n",
            "Global step: 81,loss: 0.10366177\n",
            "\n",
            "Global step: 82,loss: 0.14408876\n",
            "\n",
            "Global step: 83,loss: 0.12601218\n",
            "\n",
            "Global step: 84,loss: 0.103805035\n",
            "\n",
            "Global step: 85,loss: 0.108096376\n",
            "\n",
            "Global step: 86,loss: 0.13370323\n",
            "\n",
            "Global step: 87,loss: 0.13294107\n",
            "\n",
            "Global step: 88,loss: 0.111375555\n",
            "\n",
            "Global step: 89,loss: 0.12420529\n",
            "\n",
            "Global step: 90,loss: 0.11963352\n",
            "\n",
            "Global step: 91,loss: 0.11102593\n",
            "\n",
            "Global step: 92,loss: 0.10292399\n",
            "\n",
            "Global step: 93,loss: 0.109928645\n",
            "\n",
            "Global step: 94,loss: 0.09649226\n",
            "\n",
            "Global step: 95,loss: 0.10404475\n",
            "\n",
            "Global step: 96,loss: 0.08990103\n",
            "\n",
            "Global step: 97,loss: 0.101929024\n",
            "\n",
            "Global step: 98,loss: 0.09296286\n",
            "\n",
            "Global step: 99,loss: 0.09495108\n",
            "\n",
            "Global step: 100,loss: 0.07715413\n",
            "\n",
            "Global step: 101,loss: 0.08707588\n",
            "\n",
            "Global step: 102,loss: 0.12037564\n",
            "\n",
            "Global step: 103,loss: 0.10446865\n",
            "\n",
            "Global step: 104,loss: 0.12209788\n",
            "\n",
            "Global step: 105,loss: 0.10033903\n",
            "\n",
            "Global step: 106,loss: 0.091047406\n",
            "\n",
            "Global step: 107,loss: 0.10320045\n",
            "\n",
            "Global step: 108,loss: 0.11166359\n",
            "\n",
            "Global step: 109,loss: 0.08873561\n",
            "\n",
            "Global step: 110,loss: 0.09208462\n",
            "\n",
            "Global step: 111,loss: 0.094509736\n",
            "\n",
            "Global step: 112,loss: 0.07731085\n",
            "\n",
            "Global step: 113,loss: 0.068183\n",
            "\n",
            "Global step: 114,loss: 0.08170582\n",
            "\n",
            "Global step: 115,loss: 0.10120325\n",
            "\n",
            "Global step: 116,loss: 0.08743107\n",
            "\n",
            "Global step: 117,loss: 0.0950115\n",
            "\n",
            "Global step: 118,loss: 0.0916437\n",
            "\n",
            "Global step: 119,loss: 0.10139642\n",
            "\n",
            "Global step: 120,loss: 0.083728716\n",
            "\n",
            "Global step: 121,loss: 0.09044674\n",
            "\n",
            "Global step: 122,loss: 0.084297\n",
            "\n",
            "Global step: 123,loss: 0.09237069\n",
            "\n",
            "Global step: 124,loss: 0.082750715\n",
            "\n",
            "Global step: 125,loss: 0.0844975\n",
            "\n",
            "Global step: 126,loss: 0.106038414\n",
            "\n",
            "Global step: 127,loss: 0.10358128\n",
            "\n",
            "Global step: 128,loss: 0.09091363\n",
            "\n",
            "Global step: 129,loss: 0.087935805\n",
            "\n",
            "Global step: 130,loss: 0.10857831\n",
            "\n",
            "Global step: 131,loss: 0.076376125\n",
            "\n",
            "Global step: 132,loss: 0.074398406\n",
            "\n",
            "Global step: 133,loss: 0.07879123\n",
            "\n",
            "Global step: 134,loss: 0.089698695\n",
            "\n",
            "Global step: 135,loss: 0.08672746\n",
            "\n",
            "Global step: 136,loss: 0.08006062\n",
            "\n",
            "Global step: 137,loss: 0.08222903\n",
            "\n",
            "Global step: 138,loss: 0.1046696\n",
            "\n",
            "Global step: 139,loss: 0.082417\n",
            "\n",
            "Global step: 140,loss: 0.088171825\n",
            "\n",
            "Global step: 141,loss: 0.111864515\n",
            "\n",
            "Global step: 142,loss: 0.0708336\n",
            "\n",
            "Global step: 143,loss: 0.10765322\n",
            "\n",
            "Global step: 144,loss: 0.07137295\n",
            "\n",
            "Global step: 145,loss: 0.111674845\n",
            "\n",
            "Global step: 146,loss: 0.090038024\n",
            "\n",
            "Global step: 147,loss: 0.0895511\n",
            "\n",
            "Global step: 148,loss: 0.089055225\n",
            "\n",
            "Global step: 149,loss: 0.10441001\n",
            "\n",
            "Global step: 150,loss: 0.079400584\n",
            "\n",
            "Global step: 151,loss: 0.08078435\n",
            "\n",
            "Global step: 152,loss: 0.08615008\n",
            "\n",
            "Global step: 153,loss: 0.07484989\n",
            "\n",
            "Global step: 154,loss: 0.078617066\n",
            "\n",
            "Global step: 155,loss: 0.07739639\n",
            "\n",
            "Global step: 156,loss: 0.10279243\n",
            "\n",
            "Global step: 157,loss: 0.08692387\n",
            "\n",
            "Global step: 158,loss: 0.07651452\n",
            "\n",
            "Global step: 159,loss: 0.09141285\n",
            "\n",
            "Global step: 160,loss: 0.091702625\n",
            "\n",
            "Global step: 161,loss: 0.08127871\n",
            "\n",
            "Global step: 162,loss: 0.08654268\n",
            "\n",
            "Global step: 163,loss: 0.0929368\n",
            "\n",
            "Global step: 164,loss: 0.08951473\n",
            "\n",
            "Global step: 165,loss: 0.07738356\n",
            "\n",
            "Global step: 166,loss: 0.07721251\n",
            "\n",
            "Global step: 167,loss: 0.07961248\n",
            "\n",
            "Global step: 168,loss: 0.086790726\n",
            "\n",
            "Global step: 169,loss: 0.05466998\n",
            "\n",
            "Global step: 170,loss: 0.06559226\n",
            "\n",
            "Global step: 171,loss: 0.09012459\n",
            "\n",
            "Global step: 172,loss: 0.089981325\n",
            "\n",
            "Global step: 173,loss: 0.07101018\n",
            "\n",
            "Global step: 174,loss: 0.06273848\n",
            "\n",
            "Global step: 175,loss: 0.0665251\n",
            "\n",
            "Global step: 176,loss: 0.07212448\n",
            "\n",
            "Global step: 177,loss: 0.07641182\n",
            "\n",
            "Global step: 178,loss: 0.085725866\n",
            "\n",
            "Global step: 179,loss: 0.074335754\n",
            "\n",
            "Global step: 180,loss: 0.06500286\n",
            "\n",
            "Global step: 181,loss: 0.07311025\n",
            "\n",
            "Global step: 182,loss: 0.092453934\n",
            "\n",
            "Global step: 183,loss: 0.07633863\n",
            "\n",
            "Global step: 184,loss: 0.06399733\n",
            "\n",
            "Global step: 185,loss: 0.069192365\n",
            "\n",
            "Global step: 186,loss: 0.0814644\n",
            "\n",
            "Global step: 187,loss: 0.093868285\n",
            "\n",
            "Global step: 188,loss: 0.069224015\n",
            "\n",
            "Global step: 189,loss: 0.07204731\n",
            "\n",
            "Global step: 190,loss: 0.060371496\n",
            "\n",
            "Global step: 191,loss: 0.06846957\n",
            "\n",
            "Global step: 192,loss: 0.0638323\n",
            "\n",
            "Global step: 193,loss: 0.08351804\n",
            "\n",
            "Global step: 194,loss: 0.0741138\n",
            "\n",
            "Global step: 195,loss: 0.08250569\n",
            "\n",
            "Global step: 196,loss: 0.064072564\n",
            "\n",
            "Global step: 197,loss: 0.07882917\n",
            "\n",
            "Global step: 198,loss: 0.07871607\n",
            "\n",
            "Global step: 199,loss: 0.064366356\n",
            "\n",
            "Global step: 200,loss: 0.07738189\n",
            "\n",
            "Global step: 201,loss: 0.051049374\n",
            "\n",
            "Global step: 202,loss: 0.0746552\n",
            "\n",
            "Global step: 203,loss: 0.059927635\n",
            "\n",
            "Global step: 204,loss: 0.07344599\n",
            "\n",
            "Global step: 205,loss: 0.07060731\n",
            "\n",
            "Global step: 206,loss: 0.06265975\n",
            "\n",
            "Global step: 207,loss: 0.066078335\n",
            "\n",
            "Global step: 208,loss: 0.07713655\n",
            "\n",
            "Global step: 209,loss: 0.075481236\n",
            "\n",
            "Global step: 210,loss: 0.06056472\n",
            "\n",
            "Global step: 211,loss: 0.088113\n",
            "\n",
            "Global step: 212,loss: 0.05722802\n",
            "\n",
            "Global step: 213,loss: 0.060285255\n",
            "\n",
            "Global step: 214,loss: 0.06283383\n",
            "\n",
            "Global step: 215,loss: 0.07008198\n",
            "\n",
            "Global step: 216,loss: 0.061044216\n",
            "\n",
            "Global step: 217,loss: 0.06836144\n",
            "\n",
            "Global step: 218,loss: 0.08373144\n",
            "\n",
            "Global step: 219,loss: 0.06043296\n",
            "\n",
            "Global step: 220,loss: 0.054547828\n",
            "\n",
            "Global step: 221,loss: 0.061311916\n",
            "\n",
            "Global step: 222,loss: 0.056236826\n",
            "\n",
            "Global step: 223,loss: 0.07436251\n",
            "\n",
            "Global step: 224,loss: 0.061890934\n",
            "\n",
            "Global step: 225,loss: 0.071564086\n",
            "\n",
            "Global step: 226,loss: 0.059059348\n",
            "\n",
            "Global step: 227,loss: 0.06554645\n",
            "\n",
            "Global step: 228,loss: 0.05892802\n",
            "\n",
            "Global step: 229,loss: 0.10308621\n",
            "\n",
            "Global step: 230,loss: 0.05632199\n",
            "\n",
            "Global step: 231,loss: 0.059426926\n",
            "\n",
            "Global step: 232,loss: 0.06344128\n",
            "\n",
            "Global step: 233,loss: 0.08990583\n",
            "\n",
            "Global step: 234,loss: 0.055809643\n",
            "\n",
            "Global step: 235,loss: 0.067076415\n",
            "\n",
            "Global step: 236,loss: 0.085295096\n",
            "\n",
            "Global step: 237,loss: 0.059645697\n",
            "\n",
            "Global step: 238,loss: 0.071480036\n",
            "\n",
            "Global step: 239,loss: 0.07674154\n",
            "\n",
            "Global step: 240,loss: 0.061434697\n",
            "\n",
            "Global step: 241,loss: 0.053612445\n",
            "\n",
            "Global step: 242,loss: 0.07397534\n",
            "\n",
            "Global step: 243,loss: 0.07187161\n",
            "\n",
            "Global step: 244,loss: 0.068651415\n",
            "\n",
            "Global step: 245,loss: 0.06067189\n",
            "\n",
            "Global step: 246,loss: 0.068214156\n",
            "\n",
            "Global step: 247,loss: 0.07654028\n",
            "\n",
            "Global step: 248,loss: 0.059411347\n",
            "\n",
            "Global step: 249,loss: 0.06536597\n",
            "\n",
            "Global step: 250,loss: 0.05407858\n",
            "\n",
            "Global step: 251,loss: 0.062095687\n",
            "\n",
            "Global step: 252,loss: 0.0589253\n",
            "\n",
            "Global step: 253,loss: 0.06354289\n",
            "\n",
            "Global step: 254,loss: 0.077106856\n",
            "\n",
            "Global step: 255,loss: 0.067485444\n",
            "\n",
            "Global step: 256,loss: 0.08276101\n",
            "\n",
            "Global step: 257,loss: 0.071405016\n",
            "\n",
            "Global step: 258,loss: 0.06954736\n",
            "\n",
            "Global step: 259,loss: 0.06300697\n",
            "\n",
            "Global step: 260,loss: 0.061933015\n",
            "\n",
            "Global step: 261,loss: 0.061775595\n",
            "\n",
            "Global step: 262,loss: 0.05920943\n",
            "\n",
            "Global step: 263,loss: 0.057590246\n",
            "\n",
            "Global step: 264,loss: 0.08129955\n",
            "\n",
            "Global step: 265,loss: 0.0638231\n",
            "\n",
            "Global step: 266,loss: 0.06564365\n",
            "\n",
            "Global step: 267,loss: 0.06861077\n",
            "\n",
            "Global step: 268,loss: 0.057592466\n",
            "\n",
            "Global step: 269,loss: 0.06855458\n",
            "\n",
            "Global step: 270,loss: 0.094579116\n",
            "\n",
            "Global step: 271,loss: 0.059643395\n",
            "\n",
            "Global step: 272,loss: 0.060119085\n",
            "\n",
            "Global step: 273,loss: 0.06991504\n",
            "\n",
            "Global step: 274,loss: 0.06724639\n",
            "\n",
            "Global step: 275,loss: 0.066984154\n",
            "\n",
            "Global step: 276,loss: 0.06659874\n",
            "\n",
            "Global step: 277,loss: 0.05461911\n",
            "\n",
            "Global step: 278,loss: 0.07523764\n",
            "\n",
            "Global step: 279,loss: 0.055832945\n",
            "\n",
            "Global step: 280,loss: 0.057958875\n",
            "\n",
            "Global step: 281,loss: 0.06190904\n",
            "\n",
            "Global step: 282,loss: 0.068253145\n",
            "\n",
            "Global step: 283,loss: 0.071606606\n",
            "\n",
            "Global step: 284,loss: 0.062475875\n",
            "\n",
            "Global step: 285,loss: 0.06696151\n",
            "\n",
            "Global step: 286,loss: 0.04933756\n",
            "\n",
            "Global step: 287,loss: 0.054860435\n",
            "\n",
            "Global step: 288,loss: 0.061586082\n",
            "\n",
            "Global step: 289,loss: 0.04947762\n",
            "\n",
            "Global step: 290,loss: 0.06655075\n",
            "\n",
            "Global step: 291,loss: 0.051144313\n",
            "\n",
            "Global step: 292,loss: 0.04908241\n",
            "\n",
            "Global step: 293,loss: 0.060837813\n",
            "\n",
            "Global step: 294,loss: 0.07149553\n",
            "\n",
            "Global step: 295,loss: 0.08400414\n",
            "\n",
            "Global step: 296,loss: 0.070108786\n",
            "\n",
            "Global step: 297,loss: 0.049387597\n",
            "\n",
            "Global step: 298,loss: 0.055610433\n",
            "\n",
            "Global step: 299,loss: 0.05432276\n",
            "\n",
            "Global step: 300,loss: 0.068827845\n",
            "\n",
            "Global step: 301,loss: 0.06443248\n",
            "\n",
            "Global step: 302,loss: 0.057003647\n",
            "\n",
            "Global step: 303,loss: 0.052435458\n",
            "\n",
            "Global step: 304,loss: 0.050500367\n",
            "\n",
            "Global step: 305,loss: 0.064786896\n",
            "\n",
            "Global step: 306,loss: 0.05696739\n",
            "\n",
            "Global step: 307,loss: 0.051615454\n",
            "\n",
            "Global step: 308,loss: 0.08408442\n",
            "\n",
            "Global step: 309,loss: 0.053212427\n",
            "\n",
            "Global step: 310,loss: 0.05625192\n",
            "\n",
            "Global step: 311,loss: 0.07057247\n",
            "\n",
            "Global step: 312,loss: 0.052824054\n",
            "\n",
            "Global step: 313,loss: 0.07225057\n",
            "\n",
            "Global step: 314,loss: 0.05436633\n",
            "\n",
            "Global step: 315,loss: 0.04822383\n",
            "\n",
            "Global step: 316,loss: 0.072080284\n",
            "\n",
            "Global step: 317,loss: 0.05016221\n",
            "\n",
            "Global step: 318,loss: 0.048813608\n",
            "\n",
            "Global step: 319,loss: 0.052363608\n",
            "\n",
            "Global step: 320,loss: 0.053260922\n",
            "\n",
            "Global step: 321,loss: 0.055792987\n",
            "\n",
            "Global step: 322,loss: 0.05358957\n",
            "\n",
            "Global step: 323,loss: 0.05458975\n",
            "\n",
            "Global step: 324,loss: 0.052648135\n",
            "\n",
            "Global step: 325,loss: 0.07273745\n",
            "\n",
            "Global step: 326,loss: 0.056364357\n",
            "\n",
            "Global step: 327,loss: 0.05474557\n",
            "\n",
            "Global step: 328,loss: 0.0441595\n",
            "\n",
            "Global step: 329,loss: 0.07907848\n",
            "\n",
            "Global step: 330,loss: 0.05147774\n",
            "\n",
            "Global step: 331,loss: 0.049572397\n",
            "\n",
            "Global step: 332,loss: 0.06289034\n",
            "\n",
            "Global step: 333,loss: 0.053332057\n",
            "\n",
            "Global step: 334,loss: 0.072521225\n",
            "\n",
            "Global step: 335,loss: 0.045606785\n",
            "\n",
            "Global step: 336,loss: 0.05430782\n",
            "\n",
            "Global step: 337,loss: 0.06628102\n",
            "\n",
            "Global step: 338,loss: 0.05215162\n",
            "\n",
            "Global step: 339,loss: 0.050826523\n",
            "\n",
            "Global step: 340,loss: 0.053370513\n",
            "\n",
            "Global step: 341,loss: 0.054048344\n",
            "\n",
            "Global step: 342,loss: 0.055532817\n",
            "\n",
            "Global step: 343,loss: 0.053512253\n",
            "\n",
            "Global step: 344,loss: 0.05640358\n",
            "\n",
            "Global step: 345,loss: 0.05468565\n",
            "\n",
            "Global step: 346,loss: 0.055108435\n",
            "\n",
            "Global step: 347,loss: 0.056646936\n",
            "\n",
            "Global step: 348,loss: 0.04954478\n",
            "\n",
            "Global step: 349,loss: 0.051326662\n",
            "\n",
            "Global step: 350,loss: 0.06424557\n",
            "\n",
            "Global step: 351,loss: 0.05974465\n",
            "\n",
            "Global step: 352,loss: 0.055364743\n",
            "\n",
            "Global step: 353,loss: 0.04942818\n",
            "\n",
            "Global step: 354,loss: 0.04212509\n",
            "\n",
            "Global step: 355,loss: 0.057855688\n",
            "\n",
            "Global step: 356,loss: 0.058348797\n",
            "\n",
            "Global step: 357,loss: 0.043072432\n",
            "\n",
            "Global step: 358,loss: 0.057928298\n",
            "\n",
            "Global step: 359,loss: 0.046879157\n",
            "\n",
            "Global step: 360,loss: 0.039750334\n",
            "\n",
            "Global step: 361,loss: 0.06416995\n",
            "\n",
            "Global step: 362,loss: 0.058009207\n",
            "\n",
            "Global step: 363,loss: 0.05923453\n",
            "\n",
            "Global step: 364,loss: 0.061866317\n",
            "\n",
            "Global step: 365,loss: 0.06054158\n",
            "\n",
            "Global step: 366,loss: 0.059970915\n",
            "\n",
            "Global step: 367,loss: 0.062221263\n",
            "\n",
            "Global step: 368,loss: 0.068072945\n",
            "\n",
            "Global step: 369,loss: 0.040310062\n",
            "\n",
            "Global step: 370,loss: 0.05444493\n",
            "\n",
            "Global step: 371,loss: 0.07051504\n",
            "\n",
            "Global step: 372,loss: 0.05356913\n",
            "\n",
            "Global step: 373,loss: 0.051400036\n",
            "\n",
            "Global step: 374,loss: 0.0569525\n",
            "\n",
            "Global step: 375,loss: 0.04258683\n",
            "\n",
            "Global step: 376,loss: 0.06742473\n",
            "\n",
            "Global step: 377,loss: 0.07008859\n",
            "\n",
            "Global step: 378,loss: 0.05692836\n",
            "\n",
            "Global step: 379,loss: 0.06748249\n",
            "\n",
            "Global step: 380,loss: 0.060879305\n",
            "\n",
            "Global step: 381,loss: 0.04458941\n",
            "\n",
            "Global step: 382,loss: 0.06221448\n",
            "\n",
            "Global step: 383,loss: 0.042425998\n",
            "\n",
            "Global step: 384,loss: 0.047224067\n",
            "\n",
            "Global step: 385,loss: 0.043242067\n",
            "\n",
            "Global step: 386,loss: 0.0447575\n",
            "\n",
            "Global step: 387,loss: 0.052486956\n",
            "\n",
            "Global step: 388,loss: 0.055620447\n",
            "\n",
            "Global step: 389,loss: 0.048727654\n",
            "\n",
            "Global step: 390,loss: 0.051300876\n",
            "\n",
            "Global step: 391,loss: 0.0675083\n",
            "\n",
            "Global step: 392,loss: 0.045878064\n",
            "\n",
            "Global step: 393,loss: 0.049753726\n",
            "\n",
            "Global step: 394,loss: 0.05159965\n",
            "\n",
            "Global step: 395,loss: 0.061602727\n",
            "\n",
            "Global step: 396,loss: 0.052752048\n",
            "\n",
            "Global step: 397,loss: 0.05138795\n",
            "\n",
            "Global step: 398,loss: 0.06946278\n",
            "\n",
            "Global step: 399,loss: 0.06662123\n",
            "\n",
            "Global step: 400,loss: 0.04317441\n",
            "\n",
            "Global step: 401,loss: 0.05258686\n",
            "\n",
            "Global step: 402,loss: 0.04595511\n",
            "\n",
            "Global step: 403,loss: 0.053686842\n",
            "\n",
            "Global step: 404,loss: 0.0444848\n",
            "\n",
            "Global step: 405,loss: 0.039910104\n",
            "\n",
            "Global step: 406,loss: 0.049505662\n",
            "\n",
            "Global step: 407,loss: 0.047663953\n",
            "\n",
            "Global step: 408,loss: 0.049232543\n",
            "\n",
            "Global step: 409,loss: 0.06470522\n",
            "\n",
            "Global step: 410,loss: 0.0460226\n",
            "\n",
            "Global step: 411,loss: 0.048325367\n",
            "\n",
            "Global step: 412,loss: 0.042952195\n",
            "\n",
            "Global step: 413,loss: 0.042008124\n",
            "\n",
            "Global step: 414,loss: 0.041814864\n",
            "\n",
            "Global step: 415,loss: 0.046312638\n",
            "\n",
            "Global step: 416,loss: 0.04589174\n",
            "\n",
            "Global step: 417,loss: 0.049373582\n",
            "\n",
            "Global step: 418,loss: 0.041929737\n",
            "\n",
            "Global step: 419,loss: 0.049119987\n",
            "\n",
            "Global step: 420,loss: 0.052253053\n",
            "\n",
            "Global step: 421,loss: 0.049492575\n",
            "\n",
            "Global step: 422,loss: 0.04700684\n",
            "\n",
            "Global step: 423,loss: 0.043971654\n",
            "\n",
            "Global step: 424,loss: 0.0498738\n",
            "\n",
            "Global step: 425,loss: 0.04923258\n",
            "\n",
            "Global step: 426,loss: 0.05742699\n",
            "\n",
            "Global step: 427,loss: 0.04792983\n",
            "\n",
            "Global step: 428,loss: 0.0466943\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 428,Val_Loss: 0.04381050761693563,  Val_acc: 0.9961939102564102 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:20:26.778396 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/15:\n",
            "Global step: 429,loss: 0.06442272\n",
            "\n",
            "Global step: 430,loss: 0.053754583\n",
            "\n",
            "Global step: 431,loss: 0.045379043\n",
            "\n",
            "Global step: 432,loss: 0.047494337\n",
            "\n",
            "Global step: 433,loss: 0.054542955\n",
            "\n",
            "Global step: 434,loss: 0.052992225\n",
            "\n",
            "Global step: 435,loss: 0.05973582\n",
            "\n",
            "Global step: 436,loss: 0.04826972\n",
            "\n",
            "Global step: 437,loss: 0.05200215\n",
            "\n",
            "Global step: 438,loss: 0.04057987\n",
            "\n",
            "Global step: 439,loss: 0.05127898\n",
            "\n",
            "Global step: 440,loss: 0.044194654\n",
            "\n",
            "Global step: 441,loss: 0.047001585\n",
            "\n",
            "Global step: 442,loss: 0.056375965\n",
            "\n",
            "Global step: 443,loss: 0.05828644\n",
            "\n",
            "Global step: 444,loss: 0.05121477\n",
            "\n",
            "Global step: 445,loss: 0.04304357\n",
            "\n",
            "Global step: 446,loss: 0.04304192\n",
            "\n",
            "Global step: 447,loss: 0.046891265\n",
            "\n",
            "Global step: 448,loss: 0.04767503\n",
            "\n",
            "Global step: 449,loss: 0.046893485\n",
            "\n",
            "Global step: 450,loss: 0.05658175\n",
            "\n",
            "Global step: 451,loss: 0.052171133\n",
            "\n",
            "Global step: 452,loss: 0.05009917\n",
            "\n",
            "Global step: 453,loss: 0.048036918\n",
            "\n",
            "Global step: 454,loss: 0.044925567\n",
            "\n",
            "Global step: 455,loss: 0.038248435\n",
            "\n",
            "Global step: 456,loss: 0.042602196\n",
            "\n",
            "Global step: 457,loss: 0.05383616\n",
            "\n",
            "Global step: 458,loss: 0.056671217\n",
            "\n",
            "Global step: 459,loss: 0.052615747\n",
            "\n",
            "Global step: 460,loss: 0.07123197\n",
            "\n",
            "Global step: 461,loss: 0.041970953\n",
            "\n",
            "Global step: 462,loss: 0.056393504\n",
            "\n",
            "Global step: 463,loss: 0.047802042\n",
            "\n",
            "Global step: 464,loss: 0.036221154\n",
            "\n",
            "Global step: 465,loss: 0.04530395\n",
            "\n",
            "Global step: 466,loss: 0.048180208\n",
            "\n",
            "Global step: 467,loss: 0.038955864\n",
            "\n",
            "Global step: 468,loss: 0.033334125\n",
            "\n",
            "Global step: 469,loss: 0.046418443\n",
            "\n",
            "Global step: 470,loss: 0.05099243\n",
            "\n",
            "Global step: 471,loss: 0.0418849\n",
            "\n",
            "Global step: 472,loss: 0.049266227\n",
            "\n",
            "Global step: 473,loss: 0.046664737\n",
            "\n",
            "Global step: 474,loss: 0.043782003\n",
            "\n",
            "Global step: 475,loss: 0.04823337\n",
            "\n",
            "Global step: 476,loss: 0.06316044\n",
            "\n",
            "Global step: 477,loss: 0.037269816\n",
            "\n",
            "Global step: 478,loss: 0.04463683\n",
            "\n",
            "Global step: 479,loss: 0.04808057\n",
            "\n",
            "Global step: 480,loss: 0.046955157\n",
            "\n",
            "Global step: 481,loss: 0.06288915\n",
            "\n",
            "Global step: 482,loss: 0.04043659\n",
            "\n",
            "Global step: 483,loss: 0.049246717\n",
            "\n",
            "Global step: 484,loss: 0.049502414\n",
            "\n",
            "Global step: 485,loss: 0.07095467\n",
            "\n",
            "Global step: 486,loss: 0.045471817\n",
            "\n",
            "Global step: 487,loss: 0.038439214\n",
            "\n",
            "Global step: 488,loss: 0.043444447\n",
            "\n",
            "Global step: 489,loss: 0.040658392\n",
            "\n",
            "Global step: 490,loss: 0.057448678\n",
            "\n",
            "Global step: 491,loss: 0.051017232\n",
            "\n",
            "Global step: 492,loss: 0.0393534\n",
            "\n",
            "Global step: 493,loss: 0.04614736\n",
            "\n",
            "Global step: 494,loss: 0.04165283\n",
            "\n",
            "Global step: 495,loss: 0.05140549\n",
            "\n",
            "Global step: 496,loss: 0.043177046\n",
            "\n",
            "Global step: 497,loss: 0.038948216\n",
            "\n",
            "Global step: 498,loss: 0.044990536\n",
            "\n",
            "Global step: 499,loss: 0.052190717\n",
            "\n",
            "Global step: 500,loss: 0.054126874\n",
            "\n",
            "Global step: 501,loss: 0.046350475\n",
            "\n",
            "Global step: 502,loss: 0.038448524\n",
            "\n",
            "Global step: 503,loss: 0.04284104\n",
            "\n",
            "Global step: 504,loss: 0.05228676\n",
            "\n",
            "Global step: 505,loss: 0.058910206\n",
            "\n",
            "Global step: 506,loss: 0.062085055\n",
            "\n",
            "Global step: 507,loss: 0.041043706\n",
            "\n",
            "Global step: 508,loss: 0.054188795\n",
            "\n",
            "Global step: 509,loss: 0.04474155\n",
            "\n",
            "Global step: 510,loss: 0.035720184\n",
            "\n",
            "Global step: 511,loss: 0.042570446\n",
            "\n",
            "Global step: 512,loss: 0.04596314\n",
            "\n",
            "Global step: 513,loss: 0.048552975\n",
            "\n",
            "Global step: 514,loss: 0.043652795\n",
            "\n",
            "Global step: 515,loss: 0.04130086\n",
            "\n",
            "Global step: 516,loss: 0.05678249\n",
            "\n",
            "Global step: 517,loss: 0.055759534\n",
            "\n",
            "Global step: 518,loss: 0.03631777\n",
            "\n",
            "Global step: 519,loss: 0.042829845\n",
            "\n",
            "Global step: 520,loss: 0.043772675\n",
            "\n",
            "Global step: 521,loss: 0.040076368\n",
            "\n",
            "Global step: 522,loss: 0.05058893\n",
            "\n",
            "Global step: 523,loss: 0.044889197\n",
            "\n",
            "Global step: 524,loss: 0.042711653\n",
            "\n",
            "Global step: 525,loss: 0.050026126\n",
            "\n",
            "Global step: 526,loss: 0.04027997\n",
            "\n",
            "Global step: 527,loss: 0.047257826\n",
            "\n",
            "Global step: 528,loss: 0.04577993\n",
            "\n",
            "Global step: 529,loss: 0.043378316\n",
            "\n",
            "Global step: 530,loss: 0.036788277\n",
            "\n",
            "Global step: 531,loss: 0.044576127\n",
            "\n",
            "Global step: 532,loss: 0.044707157\n",
            "\n",
            "Global step: 533,loss: 0.036976192\n",
            "\n",
            "Global step: 534,loss: 0.041275345\n",
            "\n",
            "Global step: 535,loss: 0.038059436\n",
            "\n",
            "Global step: 536,loss: 0.054214247\n",
            "\n",
            "Global step: 537,loss: 0.036912926\n",
            "\n",
            "Global step: 538,loss: 0.037660044\n",
            "\n",
            "Global step: 539,loss: 0.040573463\n",
            "\n",
            "Global step: 540,loss: 0.0412404\n",
            "\n",
            "Global step: 541,loss: 0.058237527\n",
            "\n",
            "Global step: 542,loss: 0.03661163\n",
            "\n",
            "Global step: 543,loss: 0.040373117\n",
            "\n",
            "Global step: 544,loss: 0.032657716\n",
            "\n",
            "Global step: 545,loss: 0.032016635\n",
            "\n",
            "Global step: 546,loss: 0.041266344\n",
            "\n",
            "Global step: 547,loss: 0.03782209\n",
            "\n",
            "Global step: 548,loss: 0.037912972\n",
            "\n",
            "Global step: 549,loss: 0.040956106\n",
            "\n",
            "Global step: 550,loss: 0.03869176\n",
            "\n",
            "Global step: 551,loss: 0.041325368\n",
            "\n",
            "Global step: 552,loss: 0.03763589\n",
            "\n",
            "Global step: 553,loss: 0.043084458\n",
            "\n",
            "Global step: 554,loss: 0.03821738\n",
            "\n",
            "Global step: 555,loss: 0.047793753\n",
            "\n",
            "Global step: 556,loss: 0.036938746\n",
            "\n",
            "Global step: 557,loss: 0.03451893\n",
            "\n",
            "Global step: 558,loss: 0.037596427\n",
            "\n",
            "Global step: 559,loss: 0.04732513\n",
            "\n",
            "Global step: 560,loss: 0.043876074\n",
            "\n",
            "Global step: 561,loss: 0.054076605\n",
            "\n",
            "Global step: 562,loss: 0.03595105\n",
            "\n",
            "Global step: 563,loss: 0.045268677\n",
            "\n",
            "Global step: 564,loss: 0.040052593\n",
            "\n",
            "Global step: 565,loss: 0.036706723\n",
            "\n",
            "Global step: 566,loss: 0.048351213\n",
            "\n",
            "Global step: 567,loss: 0.035915732\n",
            "\n",
            "Global step: 568,loss: 0.04027291\n",
            "\n",
            "Global step: 569,loss: 0.039986953\n",
            "\n",
            "Global step: 570,loss: 0.037525468\n",
            "\n",
            "Global step: 571,loss: 0.042230677\n",
            "\n",
            "Global step: 572,loss: 0.04843475\n",
            "\n",
            "Global step: 573,loss: 0.053703412\n",
            "\n",
            "Global step: 574,loss: 0.038054004\n",
            "\n",
            "Global step: 575,loss: 0.041290134\n",
            "\n",
            "Global step: 576,loss: 0.037402384\n",
            "\n",
            "Global step: 577,loss: 0.035458576\n",
            "\n",
            "Global step: 578,loss: 0.04060354\n",
            "\n",
            "Global step: 579,loss: 0.047785133\n",
            "\n",
            "Global step: 580,loss: 0.04140595\n",
            "\n",
            "Global step: 581,loss: 0.054397\n",
            "\n",
            "Global step: 582,loss: 0.043615256\n",
            "\n",
            "Global step: 583,loss: 0.047459114\n",
            "\n",
            "Global step: 584,loss: 0.044364758\n",
            "\n",
            "Global step: 585,loss: 0.05798761\n",
            "\n",
            "Global step: 586,loss: 0.03595163\n",
            "\n",
            "Global step: 587,loss: 0.052139603\n",
            "\n",
            "Global step: 588,loss: 0.053571425\n",
            "\n",
            "Global step: 589,loss: 0.049938202\n",
            "\n",
            "Global step: 590,loss: 0.041687366\n",
            "\n",
            "Global step: 591,loss: 0.059155196\n",
            "\n",
            "Global step: 592,loss: 0.052318856\n",
            "\n",
            "Global step: 593,loss: 0.04594895\n",
            "\n",
            "Global step: 594,loss: 0.042450402\n",
            "\n",
            "Global step: 595,loss: 0.038952302\n",
            "\n",
            "Global step: 596,loss: 0.034990266\n",
            "\n",
            "Global step: 597,loss: 0.048978224\n",
            "\n",
            "Global step: 598,loss: 0.060438655\n",
            "\n",
            "Global step: 599,loss: 0.039351065\n",
            "\n",
            "Global step: 600,loss: 0.033267993\n",
            "\n",
            "Global step: 601,loss: 0.03562856\n",
            "\n",
            "Global step: 602,loss: 0.044392347\n",
            "\n",
            "Global step: 603,loss: 0.044301227\n",
            "\n",
            "Global step: 604,loss: 0.04226543\n",
            "\n",
            "Global step: 605,loss: 0.047142517\n",
            "\n",
            "Global step: 606,loss: 0.03441067\n",
            "\n",
            "Global step: 607,loss: 0.047198497\n",
            "\n",
            "Global step: 608,loss: 0.03991055\n",
            "\n",
            "Global step: 609,loss: 0.04862711\n",
            "\n",
            "Global step: 610,loss: 0.03734685\n",
            "\n",
            "Global step: 611,loss: 0.051792443\n",
            "\n",
            "Global step: 612,loss: 0.040592246\n",
            "\n",
            "Global step: 613,loss: 0.037943505\n",
            "\n",
            "Global step: 614,loss: 0.05376645\n",
            "\n",
            "Global step: 615,loss: 0.049255356\n",
            "\n",
            "Global step: 616,loss: 0.052157916\n",
            "\n",
            "Global step: 617,loss: 0.052314505\n",
            "\n",
            "Global step: 618,loss: 0.041601233\n",
            "\n",
            "Global step: 619,loss: 0.040392727\n",
            "\n",
            "Global step: 620,loss: 0.04700884\n",
            "\n",
            "Global step: 621,loss: 0.038210087\n",
            "\n",
            "Global step: 622,loss: 0.040526703\n",
            "\n",
            "Global step: 623,loss: 0.04685521\n",
            "\n",
            "Global step: 624,loss: 0.04348708\n",
            "\n",
            "Global step: 625,loss: 0.04184398\n",
            "\n",
            "Global step: 626,loss: 0.03837335\n",
            "\n",
            "Global step: 627,loss: 0.034889672\n",
            "\n",
            "Global step: 628,loss: 0.039928682\n",
            "\n",
            "Global step: 629,loss: 0.038098685\n",
            "\n",
            "Global step: 630,loss: 0.04543984\n",
            "\n",
            "Global step: 631,loss: 0.046557035\n",
            "\n",
            "Global step: 632,loss: 0.045134265\n",
            "\n",
            "Global step: 633,loss: 0.048044294\n",
            "\n",
            "Global step: 634,loss: 0.04884494\n",
            "\n",
            "Global step: 635,loss: 0.06304777\n",
            "\n",
            "Global step: 636,loss: 0.031999912\n",
            "\n",
            "Global step: 637,loss: 0.0531692\n",
            "\n",
            "Global step: 638,loss: 0.04238759\n",
            "\n",
            "Global step: 639,loss: 0.048308738\n",
            "\n",
            "Global step: 640,loss: 0.04366049\n",
            "\n",
            "Global step: 641,loss: 0.0376082\n",
            "\n",
            "Global step: 642,loss: 0.0496215\n",
            "\n",
            "Global step: 643,loss: 0.03817819\n",
            "\n",
            "Global step: 644,loss: 0.03890402\n",
            "\n",
            "Global step: 645,loss: 0.04767689\n",
            "\n",
            "Global step: 646,loss: 0.051333513\n",
            "\n",
            "Global step: 647,loss: 0.04394078\n",
            "\n",
            "Global step: 648,loss: 0.04386747\n",
            "\n",
            "Global step: 649,loss: 0.03425627\n",
            "\n",
            "Global step: 650,loss: 0.041551437\n",
            "\n",
            "Global step: 651,loss: 0.05626498\n",
            "\n",
            "Global step: 652,loss: 0.03490846\n",
            "\n",
            "Global step: 653,loss: 0.046451025\n",
            "\n",
            "Global step: 654,loss: 0.034961443\n",
            "\n",
            "Global step: 655,loss: 0.043930832\n",
            "\n",
            "Global step: 656,loss: 0.041061457\n",
            "\n",
            "Global step: 657,loss: 0.04446432\n",
            "\n",
            "Global step: 658,loss: 0.04227799\n",
            "\n",
            "Global step: 659,loss: 0.038610984\n",
            "\n",
            "Global step: 660,loss: 0.03515343\n",
            "\n",
            "Global step: 661,loss: 0.03328107\n",
            "\n",
            "Global step: 662,loss: 0.049644854\n",
            "\n",
            "Global step: 663,loss: 0.03148844\n",
            "\n",
            "Global step: 664,loss: 0.046479046\n",
            "\n",
            "Global step: 665,loss: 0.040432014\n",
            "\n",
            "Global step: 666,loss: 0.039750066\n",
            "\n",
            "Global step: 667,loss: 0.04562808\n",
            "\n",
            "Global step: 668,loss: 0.0496167\n",
            "\n",
            "Global step: 669,loss: 0.03631727\n",
            "\n",
            "Global step: 670,loss: 0.042807788\n",
            "\n",
            "Global step: 671,loss: 0.04012266\n",
            "\n",
            "Global step: 672,loss: 0.0402085\n",
            "\n",
            "Global step: 673,loss: 0.038707417\n",
            "\n",
            "Global step: 674,loss: 0.031140111\n",
            "\n",
            "Global step: 675,loss: 0.06688821\n",
            "\n",
            "Global step: 676,loss: 0.04839827\n",
            "\n",
            "Global step: 677,loss: 0.040091485\n",
            "\n",
            "Global step: 678,loss: 0.047420315\n",
            "\n",
            "Global step: 679,loss: 0.042755075\n",
            "\n",
            "Global step: 680,loss: 0.032326106\n",
            "\n",
            "Global step: 681,loss: 0.0407615\n",
            "\n",
            "Global step: 682,loss: 0.03827492\n",
            "\n",
            "Global step: 683,loss: 0.043948215\n",
            "\n",
            "Global step: 684,loss: 0.043631807\n",
            "\n",
            "Global step: 685,loss: 0.039461903\n",
            "\n",
            "Global step: 686,loss: 0.04275518\n",
            "\n",
            "Global step: 687,loss: 0.04014333\n",
            "\n",
            "Global step: 688,loss: 0.043395907\n",
            "\n",
            "Global step: 689,loss: 0.052510902\n",
            "\n",
            "Global step: 690,loss: 0.04397498\n",
            "\n",
            "Global step: 691,loss: 0.04167934\n",
            "\n",
            "Global step: 692,loss: 0.04168783\n",
            "\n",
            "Global step: 693,loss: 0.036285672\n",
            "\n",
            "Global step: 694,loss: 0.042183593\n",
            "\n",
            "Global step: 695,loss: 0.04383538\n",
            "\n",
            "Global step: 696,loss: 0.05944234\n",
            "\n",
            "Global step: 697,loss: 0.04497818\n",
            "\n",
            "Global step: 698,loss: 0.035999052\n",
            "\n",
            "Global step: 699,loss: 0.043716587\n",
            "\n",
            "Global step: 700,loss: 0.03836277\n",
            "\n",
            "Global step: 701,loss: 0.037974957\n",
            "\n",
            "Global step: 702,loss: 0.040041465\n",
            "\n",
            "Global step: 703,loss: 0.03666095\n",
            "\n",
            "Global step: 704,loss: 0.03556509\n",
            "\n",
            "Global step: 705,loss: 0.042477272\n",
            "\n",
            "Global step: 706,loss: 0.034618076\n",
            "\n",
            "Global step: 707,loss: 0.034905843\n",
            "\n",
            "Global step: 708,loss: 0.034857444\n",
            "\n",
            "Global step: 709,loss: 0.045437943\n",
            "\n",
            "Global step: 710,loss: 0.050124407\n",
            "\n",
            "Global step: 711,loss: 0.03386752\n",
            "\n",
            "Global step: 712,loss: 0.037923202\n",
            "\n",
            "Global step: 713,loss: 0.047375247\n",
            "\n",
            "Global step: 714,loss: 0.049135447\n",
            "\n",
            "Global step: 715,loss: 0.039085574\n",
            "\n",
            "Global step: 716,loss: 0.05348778\n",
            "\n",
            "Global step: 717,loss: 0.036103673\n",
            "\n",
            "Global step: 718,loss: 0.03368521\n",
            "\n",
            "Global step: 719,loss: 0.053466998\n",
            "\n",
            "Global step: 720,loss: 0.04072915\n",
            "\n",
            "Global step: 721,loss: 0.038030095\n",
            "\n",
            "Global step: 722,loss: 0.042656872\n",
            "\n",
            "Global step: 723,loss: 0.036737025\n",
            "\n",
            "Global step: 724,loss: 0.0387314\n",
            "\n",
            "Global step: 725,loss: 0.05113284\n",
            "\n",
            "Global step: 726,loss: 0.038762398\n",
            "\n",
            "Global step: 727,loss: 0.05263596\n",
            "\n",
            "Global step: 728,loss: 0.036305193\n",
            "\n",
            "Global step: 729,loss: 0.044175833\n",
            "\n",
            "Global step: 730,loss: 0.034355596\n",
            "\n",
            "Global step: 731,loss: 0.041436817\n",
            "\n",
            "Global step: 732,loss: 0.04195031\n",
            "\n",
            "Global step: 733,loss: 0.050109968\n",
            "\n",
            "Global step: 734,loss: 0.055396616\n",
            "\n",
            "Global step: 735,loss: 0.04812742\n",
            "\n",
            "Global step: 736,loss: 0.044818662\n",
            "\n",
            "Global step: 737,loss: 0.03122608\n",
            "\n",
            "Global step: 738,loss: 0.042814508\n",
            "\n",
            "Global step: 739,loss: 0.040777214\n",
            "\n",
            "Global step: 740,loss: 0.046504617\n",
            "\n",
            "Global step: 741,loss: 0.035660807\n",
            "\n",
            "Global step: 742,loss: 0.041201446\n",
            "\n",
            "Global step: 743,loss: 0.0461529\n",
            "\n",
            "Global step: 744,loss: 0.04127448\n",
            "\n",
            "Global step: 745,loss: 0.03873613\n",
            "\n",
            "Global step: 746,loss: 0.03918777\n",
            "\n",
            "Global step: 747,loss: 0.030801658\n",
            "\n",
            "Global step: 748,loss: 0.043643273\n",
            "\n",
            "Global step: 749,loss: 0.042631105\n",
            "\n",
            "Global step: 750,loss: 0.03721136\n",
            "\n",
            "Global step: 751,loss: 0.031731796\n",
            "\n",
            "Global step: 752,loss: 0.053681903\n",
            "\n",
            "Global step: 753,loss: 0.03521883\n",
            "\n",
            "Global step: 754,loss: 0.040981516\n",
            "\n",
            "Global step: 755,loss: 0.048883907\n",
            "\n",
            "Global step: 756,loss: 0.03815524\n",
            "\n",
            "Global step: 757,loss: 0.039322242\n",
            "\n",
            "Global step: 758,loss: 0.04272353\n",
            "\n",
            "Global step: 759,loss: 0.031247042\n",
            "\n",
            "Global step: 760,loss: 0.048931383\n",
            "\n",
            "Global step: 761,loss: 0.03581147\n",
            "\n",
            "Global step: 762,loss: 0.055934697\n",
            "\n",
            "Global step: 763,loss: 0.039974954\n",
            "\n",
            "Global step: 764,loss: 0.04152445\n",
            "\n",
            "Global step: 765,loss: 0.04858748\n",
            "\n",
            "Global step: 766,loss: 0.039615616\n",
            "\n",
            "Global step: 767,loss: 0.045338783\n",
            "\n",
            "Global step: 768,loss: 0.04250433\n",
            "\n",
            "Global step: 769,loss: 0.035244312\n",
            "\n",
            "Global step: 770,loss: 0.052006327\n",
            "\n",
            "Global step: 771,loss: 0.031631727\n",
            "\n",
            "Global step: 772,loss: 0.038276386\n",
            "\n",
            "Global step: 773,loss: 0.046941556\n",
            "\n",
            "Global step: 774,loss: 0.04530511\n",
            "\n",
            "Global step: 775,loss: 0.04427836\n",
            "\n",
            "Global step: 776,loss: 0.047070816\n",
            "\n",
            "Global step: 777,loss: 0.04013588\n",
            "\n",
            "Global step: 778,loss: 0.03561976\n",
            "\n",
            "Global step: 779,loss: 0.05248587\n",
            "\n",
            "Global step: 780,loss: 0.038928248\n",
            "\n",
            "Global step: 781,loss: 0.03565318\n",
            "\n",
            "Global step: 782,loss: 0.05130531\n",
            "\n",
            "Global step: 783,loss: 0.037923794\n",
            "\n",
            "Global step: 784,loss: 0.0324911\n",
            "\n",
            "Global step: 785,loss: 0.047367737\n",
            "\n",
            "Global step: 786,loss: 0.037174195\n",
            "\n",
            "Global step: 787,loss: 0.05169433\n",
            "\n",
            "Global step: 788,loss: 0.043082397\n",
            "\n",
            "Global step: 789,loss: 0.037411794\n",
            "\n",
            "Global step: 790,loss: 0.037425\n",
            "\n",
            "Global step: 791,loss: 0.044402942\n",
            "\n",
            "Global step: 792,loss: 0.037253745\n",
            "\n",
            "Global step: 793,loss: 0.038457245\n",
            "\n",
            "Global step: 794,loss: 0.038838953\n",
            "\n",
            "Global step: 795,loss: 0.03970862\n",
            "\n",
            "Global step: 796,loss: 0.031363398\n",
            "\n",
            "Global step: 797,loss: 0.036798365\n",
            "\n",
            "Global step: 798,loss: 0.034892246\n",
            "\n",
            "Global step: 799,loss: 0.035864837\n",
            "\n",
            "Global step: 800,loss: 0.029665492\n",
            "\n",
            "Global step: 801,loss: 0.03549946\n",
            "\n",
            "Global step: 802,loss: 0.041846372\n",
            "\n",
            "Global step: 803,loss: 0.030400442\n",
            "\n",
            "Global step: 804,loss: 0.03034544\n",
            "\n",
            "Global step: 805,loss: 0.033287574\n",
            "\n",
            "Global step: 806,loss: 0.036553755\n",
            "\n",
            "Global step: 807,loss: 0.039541885\n",
            "\n",
            "Global step: 808,loss: 0.04970277\n",
            "\n",
            "Global step: 809,loss: 0.038485087\n",
            "\n",
            "Global step: 810,loss: 0.053268213\n",
            "\n",
            "Global step: 811,loss: 0.03707373\n",
            "\n",
            "Global step: 812,loss: 0.0331431\n",
            "\n",
            "Global step: 813,loss: 0.041207075\n",
            "\n",
            "Global step: 814,loss: 0.038464934\n",
            "\n",
            "Global step: 815,loss: 0.043573767\n",
            "\n",
            "Global step: 816,loss: 0.032790445\n",
            "\n",
            "Global step: 817,loss: 0.033931177\n",
            "\n",
            "Global step: 818,loss: 0.042434063\n",
            "\n",
            "Global step: 819,loss: 0.03505267\n",
            "\n",
            "Global step: 820,loss: 0.03765095\n",
            "\n",
            "Global step: 821,loss: 0.04439205\n",
            "\n",
            "Global step: 822,loss: 0.037821047\n",
            "\n",
            "Global step: 823,loss: 0.03007178\n",
            "\n",
            "Global step: 824,loss: 0.037972573\n",
            "\n",
            "Global step: 825,loss: 0.031000167\n",
            "\n",
            "Global step: 826,loss: 0.04544973\n",
            "\n",
            "Global step: 827,loss: 0.032068156\n",
            "\n",
            "Global step: 828,loss: 0.034119006\n",
            "\n",
            "Global step: 829,loss: 0.036585867\n",
            "\n",
            "Global step: 830,loss: 0.04261652\n",
            "\n",
            "Global step: 831,loss: 0.037553035\n",
            "\n",
            "Global step: 832,loss: 0.041361794\n",
            "\n",
            "Global step: 833,loss: 0.035068754\n",
            "\n",
            "Global step: 834,loss: 0.02954473\n",
            "\n",
            "Global step: 835,loss: 0.030380186\n",
            "\n",
            "Global step: 836,loss: 0.043988302\n",
            "\n",
            "Global step: 837,loss: 0.038724475\n",
            "\n",
            "Global step: 838,loss: 0.04931403\n",
            "\n",
            "Global step: 839,loss: 0.061247617\n",
            "\n",
            "Global step: 840,loss: 0.029653765\n",
            "\n",
            "Global step: 841,loss: 0.045064587\n",
            "\n",
            "Global step: 842,loss: 0.043424256\n",
            "\n",
            "Global step: 843,loss: 0.035110585\n",
            "\n",
            "Global step: 844,loss: 0.039564785\n",
            "\n",
            "Global step: 845,loss: 0.03386618\n",
            "\n",
            "Global step: 846,loss: 0.034625884\n",
            "\n",
            "Global step: 847,loss: 0.040787157\n",
            "\n",
            "Global step: 848,loss: 0.03369393\n",
            "\n",
            "Global step: 849,loss: 0.031586308\n",
            "\n",
            "Global step: 850,loss: 0.039023668\n",
            "\n",
            "Global step: 851,loss: 0.033295393\n",
            "\n",
            "Global step: 852,loss: 0.033444125\n",
            "\n",
            "Global step: 853,loss: 0.029917812\n",
            "\n",
            "Global step: 854,loss: 0.037679926\n",
            "\n",
            "Global step: 855,loss: 0.03871854\n",
            "\n",
            "Global step: 856,loss: 0.035985414\n",
            "\n",
            "Global step: 857,loss: 0.03326579\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.14958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:21:22.140401 139648999347968 supervisor.py:1099] global_step/sec: 7.14958\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 858.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:21:22.194414 139649007740672 supervisor.py:1050] Recording summary at step 858.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 857,Val_Loss: 0.03485616798011156,  Val_acc: 0.9961939102564102 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:21:23.388648 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/15:\n",
            "Global step: 858,loss: 0.047126703\n",
            "\n",
            "Global step: 859,loss: 0.037237715\n",
            "\n",
            "Global step: 860,loss: 0.036433388\n",
            "\n",
            "Global step: 861,loss: 0.029865392\n",
            "\n",
            "Global step: 862,loss: 0.029676914\n",
            "\n",
            "Global step: 863,loss: 0.04143175\n",
            "\n",
            "Global step: 864,loss: 0.034837976\n",
            "\n",
            "Global step: 865,loss: 0.037722904\n",
            "\n",
            "Global step: 866,loss: 0.032614876\n",
            "\n",
            "Global step: 867,loss: 0.034924403\n",
            "\n",
            "Global step: 868,loss: 0.03178548\n",
            "\n",
            "Global step: 869,loss: 0.038985476\n",
            "\n",
            "Global step: 870,loss: 0.03509192\n",
            "\n",
            "Global step: 871,loss: 0.033898644\n",
            "\n",
            "Global step: 872,loss: 0.032319397\n",
            "\n",
            "Global step: 873,loss: 0.036246765\n",
            "\n",
            "Global step: 874,loss: 0.0335517\n",
            "\n",
            "Global step: 875,loss: 0.04218956\n",
            "\n",
            "Global step: 876,loss: 0.03476584\n",
            "\n",
            "Global step: 877,loss: 0.043328688\n",
            "\n",
            "Global step: 878,loss: 0.034841713\n",
            "\n",
            "Global step: 879,loss: 0.033455342\n",
            "\n",
            "Global step: 880,loss: 0.03760562\n",
            "\n",
            "Global step: 881,loss: 0.029914817\n",
            "\n",
            "Global step: 882,loss: 0.031724162\n",
            "\n",
            "Global step: 883,loss: 0.037085086\n",
            "\n",
            "Global step: 884,loss: 0.043459635\n",
            "\n",
            "Global step: 885,loss: 0.035048764\n",
            "\n",
            "Global step: 886,loss: 0.046763755\n",
            "\n",
            "Global step: 887,loss: 0.03375566\n",
            "\n",
            "Global step: 888,loss: 0.03941191\n",
            "\n",
            "Global step: 889,loss: 0.0349151\n",
            "\n",
            "Global step: 890,loss: 0.03260498\n",
            "\n",
            "Global step: 891,loss: 0.036002044\n",
            "\n",
            "Global step: 892,loss: 0.047852695\n",
            "\n",
            "Global step: 893,loss: 0.030411527\n",
            "\n",
            "Global step: 894,loss: 0.03892769\n",
            "\n",
            "Global step: 895,loss: 0.040580958\n",
            "\n",
            "Global step: 896,loss: 0.032488413\n",
            "\n",
            "Global step: 897,loss: 0.041826647\n",
            "\n",
            "Global step: 898,loss: 0.03879975\n",
            "\n",
            "Global step: 899,loss: 0.031482495\n",
            "\n",
            "Global step: 900,loss: 0.03904915\n",
            "\n",
            "Global step: 901,loss: 0.03245621\n",
            "\n",
            "Global step: 902,loss: 0.03333839\n",
            "\n",
            "Global step: 903,loss: 0.030045062\n",
            "\n",
            "Global step: 904,loss: 0.03299721\n",
            "\n",
            "Global step: 905,loss: 0.034421586\n",
            "\n",
            "Global step: 906,loss: 0.029098153\n",
            "\n",
            "Global step: 907,loss: 0.038739514\n",
            "\n",
            "Global step: 908,loss: 0.03683836\n",
            "\n",
            "Global step: 909,loss: 0.034435418\n",
            "\n",
            "Global step: 910,loss: 0.043633286\n",
            "\n",
            "Global step: 911,loss: 0.028788704\n",
            "\n",
            "Global step: 912,loss: 0.031694807\n",
            "\n",
            "Global step: 913,loss: 0.036764234\n",
            "\n",
            "Global step: 914,loss: 0.04388759\n",
            "\n",
            "Global step: 915,loss: 0.030983966\n",
            "\n",
            "Global step: 916,loss: 0.036983054\n",
            "\n",
            "Global step: 917,loss: 0.029081948\n",
            "\n",
            "Global step: 918,loss: 0.03432096\n",
            "\n",
            "Global step: 919,loss: 0.034437705\n",
            "\n",
            "Global step: 920,loss: 0.043425485\n",
            "\n",
            "Global step: 921,loss: 0.038675442\n",
            "\n",
            "Global step: 922,loss: 0.044760253\n",
            "\n",
            "Global step: 923,loss: 0.041426085\n",
            "\n",
            "Global step: 924,loss: 0.04714845\n",
            "\n",
            "Global step: 925,loss: 0.02868314\n",
            "\n",
            "Global step: 926,loss: 0.030658586\n",
            "\n",
            "Global step: 927,loss: 0.037195455\n",
            "\n",
            "Global step: 928,loss: 0.03078955\n",
            "\n",
            "Global step: 929,loss: 0.03699969\n",
            "\n",
            "Global step: 930,loss: 0.031808138\n",
            "\n",
            "Global step: 931,loss: 0.04640463\n",
            "\n",
            "Global step: 932,loss: 0.04246041\n",
            "\n",
            "Global step: 933,loss: 0.03266968\n",
            "\n",
            "Global step: 934,loss: 0.03972531\n",
            "\n",
            "Global step: 935,loss: 0.034373526\n",
            "\n",
            "Global step: 936,loss: 0.03240837\n",
            "\n",
            "Global step: 937,loss: 0.03151367\n",
            "\n",
            "Global step: 938,loss: 0.038766462\n",
            "\n",
            "Global step: 939,loss: 0.03666212\n",
            "\n",
            "Global step: 940,loss: 0.031141762\n",
            "\n",
            "Global step: 941,loss: 0.028199334\n",
            "\n",
            "Global step: 942,loss: 0.033266872\n",
            "\n",
            "Global step: 943,loss: 0.032190975\n",
            "\n",
            "Global step: 944,loss: 0.028240934\n",
            "\n",
            "Global step: 945,loss: 0.028510392\n",
            "\n",
            "Global step: 946,loss: 0.03424198\n",
            "\n",
            "Global step: 947,loss: 0.053226307\n",
            "\n",
            "Global step: 948,loss: 0.03501288\n",
            "\n",
            "Global step: 949,loss: 0.033827774\n",
            "\n",
            "Global step: 950,loss: 0.03531179\n",
            "\n",
            "Global step: 951,loss: 0.037322007\n",
            "\n",
            "Global step: 952,loss: 0.028427115\n",
            "\n",
            "Global step: 953,loss: 0.0269125\n",
            "\n",
            "Global step: 954,loss: 0.040317897\n",
            "\n",
            "Global step: 955,loss: 0.05176822\n",
            "\n",
            "Global step: 956,loss: 0.039035156\n",
            "\n",
            "Global step: 957,loss: 0.028553976\n",
            "\n",
            "Global step: 958,loss: 0.037072785\n",
            "\n",
            "Global step: 959,loss: 0.029322721\n",
            "\n",
            "Global step: 960,loss: 0.031900436\n",
            "\n",
            "Global step: 961,loss: 0.03232339\n",
            "\n",
            "Global step: 962,loss: 0.041273583\n",
            "\n",
            "Global step: 963,loss: 0.046142217\n",
            "\n",
            "Global step: 964,loss: 0.04155405\n",
            "\n",
            "Global step: 965,loss: 0.031764604\n",
            "\n",
            "Global step: 966,loss: 0.030532517\n",
            "\n",
            "Global step: 967,loss: 0.035555817\n",
            "\n",
            "Global step: 968,loss: 0.038258024\n",
            "\n",
            "Global step: 969,loss: 0.031975895\n",
            "\n",
            "Global step: 970,loss: 0.030863367\n",
            "\n",
            "Global step: 971,loss: 0.03251364\n",
            "\n",
            "Global step: 972,loss: 0.043570492\n",
            "\n",
            "Global step: 973,loss: 0.029262312\n",
            "\n",
            "Global step: 974,loss: 0.028826691\n",
            "\n",
            "Global step: 975,loss: 0.032776542\n",
            "\n",
            "Global step: 976,loss: 0.027795577\n",
            "\n",
            "Global step: 977,loss: 0.02854152\n",
            "\n",
            "Global step: 978,loss: 0.029715113\n",
            "\n",
            "Global step: 979,loss: 0.0370551\n",
            "\n",
            "Global step: 980,loss: 0.032648772\n",
            "\n",
            "Global step: 981,loss: 0.03402166\n",
            "\n",
            "Global step: 982,loss: 0.028733265\n",
            "\n",
            "Global step: 983,loss: 0.03232149\n",
            "\n",
            "Global step: 984,loss: 0.037831046\n",
            "\n",
            "Global step: 985,loss: 0.03883775\n",
            "\n",
            "Global step: 986,loss: 0.030783098\n",
            "\n",
            "Global step: 987,loss: 0.02827518\n",
            "\n",
            "Global step: 988,loss: 0.032663148\n",
            "\n",
            "Global step: 989,loss: 0.038345113\n",
            "\n",
            "Global step: 990,loss: 0.02978658\n",
            "\n",
            "Global step: 991,loss: 0.033862222\n",
            "\n",
            "Global step: 992,loss: 0.03432759\n",
            "\n",
            "Global step: 993,loss: 0.029139638\n",
            "\n",
            "Global step: 994,loss: 0.028206777\n",
            "\n",
            "Global step: 995,loss: 0.03989452\n",
            "\n",
            "Global step: 996,loss: 0.027574465\n",
            "\n",
            "Global step: 997,loss: 0.03046991\n",
            "\n",
            "Global step: 998,loss: 0.030517679\n",
            "\n",
            "Global step: 999,loss: 0.030429322\n",
            "\n",
            "Global step: 1000,loss: 0.028485263\n",
            "\n",
            "Global step: 1001,loss: 0.033765145\n",
            "\n",
            "Global step: 1002,loss: 0.03487052\n",
            "\n",
            "Global step: 1003,loss: 0.027829383\n",
            "\n",
            "Global step: 1004,loss: 0.033673618\n",
            "\n",
            "Global step: 1005,loss: 0.035612173\n",
            "\n",
            "Global step: 1006,loss: 0.031127341\n",
            "\n",
            "Global step: 1007,loss: 0.036669858\n",
            "\n",
            "Global step: 1008,loss: 0.032994345\n",
            "\n",
            "Global step: 1009,loss: 0.0458211\n",
            "\n",
            "Global step: 1010,loss: 0.03937736\n",
            "\n",
            "Global step: 1011,loss: 0.04184597\n",
            "\n",
            "Global step: 1012,loss: 0.027911872\n",
            "\n",
            "Global step: 1013,loss: 0.04593134\n",
            "\n",
            "Global step: 1014,loss: 0.03390629\n",
            "\n",
            "Global step: 1015,loss: 0.043939635\n",
            "\n",
            "Global step: 1016,loss: 0.034062948\n",
            "\n",
            "Global step: 1017,loss: 0.03378532\n",
            "\n",
            "Global step: 1018,loss: 0.027461404\n",
            "\n",
            "Global step: 1019,loss: 0.027939405\n",
            "\n",
            "Global step: 1020,loss: 0.029539373\n",
            "\n",
            "Global step: 1021,loss: 0.036501713\n",
            "\n",
            "Global step: 1022,loss: 0.04001715\n",
            "\n",
            "Global step: 1023,loss: 0.052722752\n",
            "\n",
            "Global step: 1024,loss: 0.038753532\n",
            "\n",
            "Global step: 1025,loss: 0.031052314\n",
            "\n",
            "Global step: 1026,loss: 0.040666934\n",
            "\n",
            "Global step: 1027,loss: 0.04156004\n",
            "\n",
            "Global step: 1028,loss: 0.0410351\n",
            "\n",
            "Global step: 1029,loss: 0.02926512\n",
            "\n",
            "Global step: 1030,loss: 0.02992126\n",
            "\n",
            "Global step: 1031,loss: 0.040706478\n",
            "\n",
            "Global step: 1032,loss: 0.034822065\n",
            "\n",
            "Global step: 1033,loss: 0.049202852\n",
            "\n",
            "Global step: 1034,loss: 0.033135913\n",
            "\n",
            "Global step: 1035,loss: 0.02857989\n",
            "\n",
            "Global step: 1036,loss: 0.0384212\n",
            "\n",
            "Global step: 1037,loss: 0.030035371\n",
            "\n",
            "Global step: 1038,loss: 0.028958177\n",
            "\n",
            "Global step: 1039,loss: 0.038329415\n",
            "\n",
            "Global step: 1040,loss: 0.036177203\n",
            "\n",
            "Global step: 1041,loss: 0.027391229\n",
            "\n",
            "Global step: 1042,loss: 0.0335628\n",
            "\n",
            "Global step: 1043,loss: 0.036518928\n",
            "\n",
            "Global step: 1044,loss: 0.036051694\n",
            "\n",
            "Global step: 1045,loss: 0.03205487\n",
            "\n",
            "Global step: 1046,loss: 0.03591205\n",
            "\n",
            "Global step: 1047,loss: 0.042384766\n",
            "\n",
            "Global step: 1048,loss: 0.033759948\n",
            "\n",
            "Global step: 1049,loss: 0.029034588\n",
            "\n",
            "Global step: 1050,loss: 0.038607128\n",
            "\n",
            "Global step: 1051,loss: 0.040018823\n",
            "\n",
            "Global step: 1052,loss: 0.038265835\n",
            "\n",
            "Global step: 1053,loss: 0.032451753\n",
            "\n",
            "Global step: 1054,loss: 0.037334807\n",
            "\n",
            "Global step: 1055,loss: 0.030007467\n",
            "\n",
            "Global step: 1056,loss: 0.02724793\n",
            "\n",
            "Global step: 1057,loss: 0.03821627\n",
            "\n",
            "Global step: 1058,loss: 0.03128854\n",
            "\n",
            "Global step: 1059,loss: 0.03634496\n",
            "\n",
            "Global step: 1060,loss: 0.029975649\n",
            "\n",
            "Global step: 1061,loss: 0.037305187\n",
            "\n",
            "Global step: 1062,loss: 0.04345566\n",
            "\n",
            "Global step: 1063,loss: 0.038159348\n",
            "\n",
            "Global step: 1064,loss: 0.026870435\n",
            "\n",
            "Global step: 1065,loss: 0.030233715\n",
            "\n",
            "Global step: 1066,loss: 0.032202933\n",
            "\n",
            "Global step: 1067,loss: 0.03916875\n",
            "\n",
            "Global step: 1068,loss: 0.03209327\n",
            "\n",
            "Global step: 1069,loss: 0.03706721\n",
            "\n",
            "Global step: 1070,loss: 0.036761943\n",
            "\n",
            "Global step: 1071,loss: 0.030696832\n",
            "\n",
            "Global step: 1072,loss: 0.03465327\n",
            "\n",
            "Global step: 1073,loss: 0.028745553\n",
            "\n",
            "Global step: 1074,loss: 0.028752718\n",
            "\n",
            "Global step: 1075,loss: 0.031822838\n",
            "\n",
            "Global step: 1076,loss: 0.03113484\n",
            "\n",
            "Global step: 1077,loss: 0.051687457\n",
            "\n",
            "Global step: 1078,loss: 0.03691917\n",
            "\n",
            "Global step: 1079,loss: 0.033023685\n",
            "\n",
            "Global step: 1080,loss: 0.033141233\n",
            "\n",
            "Global step: 1081,loss: 0.048071265\n",
            "\n",
            "Global step: 1082,loss: 0.03015313\n",
            "\n",
            "Global step: 1083,loss: 0.029942216\n",
            "\n",
            "Global step: 1084,loss: 0.043314934\n",
            "\n",
            "Global step: 1085,loss: 0.036466647\n",
            "\n",
            "Global step: 1086,loss: 0.038369194\n",
            "\n",
            "Global step: 1087,loss: 0.038833037\n",
            "\n",
            "Global step: 1088,loss: 0.04550066\n",
            "\n",
            "Global step: 1089,loss: 0.030874694\n",
            "\n",
            "Global step: 1090,loss: 0.030209316\n",
            "\n",
            "Global step: 1091,loss: 0.03283225\n",
            "\n",
            "Global step: 1092,loss: 0.035487995\n",
            "\n",
            "Global step: 1093,loss: 0.02950202\n",
            "\n",
            "Global step: 1094,loss: 0.026110115\n",
            "\n",
            "Global step: 1095,loss: 0.052782357\n",
            "\n",
            "Global step: 1096,loss: 0.026699614\n",
            "\n",
            "Global step: 1097,loss: 0.039347727\n",
            "\n",
            "Global step: 1098,loss: 0.030634914\n",
            "\n",
            "Global step: 1099,loss: 0.028854383\n",
            "\n",
            "Global step: 1100,loss: 0.030040141\n",
            "\n",
            "Global step: 1101,loss: 0.030482687\n",
            "\n",
            "Global step: 1102,loss: 0.03119203\n",
            "\n",
            "Global step: 1103,loss: 0.030493565\n",
            "\n",
            "Global step: 1104,loss: 0.027682638\n",
            "\n",
            "Global step: 1105,loss: 0.050232306\n",
            "\n",
            "Global step: 1106,loss: 0.028104924\n",
            "\n",
            "Global step: 1107,loss: 0.032713175\n",
            "\n",
            "Global step: 1108,loss: 0.034106776\n",
            "\n",
            "Global step: 1109,loss: 0.032632656\n",
            "\n",
            "Global step: 1110,loss: 0.03387668\n",
            "\n",
            "Global step: 1111,loss: 0.029038658\n",
            "\n",
            "Global step: 1112,loss: 0.029616756\n",
            "\n",
            "Global step: 1113,loss: 0.03286413\n",
            "\n",
            "Global step: 1114,loss: 0.025116872\n",
            "\n",
            "Global step: 1115,loss: 0.026794951\n",
            "\n",
            "Global step: 1116,loss: 0.03763674\n",
            "\n",
            "Global step: 1117,loss: 0.030657854\n",
            "\n",
            "Global step: 1118,loss: 0.028486019\n",
            "\n",
            "Global step: 1119,loss: 0.03202087\n",
            "\n",
            "Global step: 1120,loss: 0.031210642\n",
            "\n",
            "Global step: 1121,loss: 0.030241808\n",
            "\n",
            "Global step: 1122,loss: 0.028719474\n",
            "\n",
            "Global step: 1123,loss: 0.027129697\n",
            "\n",
            "Global step: 1124,loss: 0.035883367\n",
            "\n",
            "Global step: 1125,loss: 0.036635794\n",
            "\n",
            "Global step: 1126,loss: 0.034741692\n",
            "\n",
            "Global step: 1127,loss: 0.0279307\n",
            "\n",
            "Global step: 1128,loss: 0.027561449\n",
            "\n",
            "Global step: 1129,loss: 0.029515244\n",
            "\n",
            "Global step: 1130,loss: 0.034763794\n",
            "\n",
            "Global step: 1131,loss: 0.03558497\n",
            "\n",
            "Global step: 1132,loss: 0.04023221\n",
            "\n",
            "Global step: 1133,loss: 0.030308055\n",
            "\n",
            "Global step: 1134,loss: 0.035523072\n",
            "\n",
            "Global step: 1135,loss: 0.034226667\n",
            "\n",
            "Global step: 1136,loss: 0.027495176\n",
            "\n",
            "Global step: 1137,loss: 0.031062353\n",
            "\n",
            "Global step: 1138,loss: 0.0286523\n",
            "\n",
            "Global step: 1139,loss: 0.046574086\n",
            "\n",
            "Global step: 1140,loss: 0.030360555\n",
            "\n",
            "Global step: 1141,loss: 0.029103372\n",
            "\n",
            "Global step: 1142,loss: 0.04098594\n",
            "\n",
            "Global step: 1143,loss: 0.035990156\n",
            "\n",
            "Global step: 1144,loss: 0.02526489\n",
            "\n",
            "Global step: 1145,loss: 0.030476728\n",
            "\n",
            "Global step: 1146,loss: 0.043319963\n",
            "\n",
            "Global step: 1147,loss: 0.03178045\n",
            "\n",
            "Global step: 1148,loss: 0.037784606\n",
            "\n",
            "Global step: 1149,loss: 0.033491757\n",
            "\n",
            "Global step: 1150,loss: 0.032030143\n",
            "\n",
            "Global step: 1151,loss: 0.03153905\n",
            "\n",
            "Global step: 1152,loss: 0.031739615\n",
            "\n",
            "Global step: 1153,loss: 0.03250961\n",
            "\n",
            "Global step: 1154,loss: 0.034934618\n",
            "\n",
            "Global step: 1155,loss: 0.028834697\n",
            "\n",
            "Global step: 1156,loss: 0.032405954\n",
            "\n",
            "Global step: 1157,loss: 0.028915366\n",
            "\n",
            "Global step: 1158,loss: 0.042134237\n",
            "\n",
            "Global step: 1159,loss: 0.0349236\n",
            "\n",
            "Global step: 1160,loss: 0.04057967\n",
            "\n",
            "Global step: 1161,loss: 0.02974066\n",
            "\n",
            "Global step: 1162,loss: 0.029730845\n",
            "\n",
            "Global step: 1163,loss: 0.038206086\n",
            "\n",
            "Global step: 1164,loss: 0.040209796\n",
            "\n",
            "Global step: 1165,loss: 0.026335783\n",
            "\n",
            "Global step: 1166,loss: 0.03556733\n",
            "\n",
            "Global step: 1167,loss: 0.03299468\n",
            "\n",
            "Global step: 1168,loss: 0.027649265\n",
            "\n",
            "Global step: 1169,loss: 0.03682644\n",
            "\n",
            "Global step: 1170,loss: 0.027731918\n",
            "\n",
            "Global step: 1171,loss: 0.034348983\n",
            "\n",
            "Global step: 1172,loss: 0.03235156\n",
            "\n",
            "Global step: 1173,loss: 0.045143068\n",
            "\n",
            "Global step: 1174,loss: 0.044333246\n",
            "\n",
            "Global step: 1175,loss: 0.031421512\n",
            "\n",
            "Global step: 1176,loss: 0.03187135\n",
            "\n",
            "Global step: 1177,loss: 0.03211143\n",
            "\n",
            "Global step: 1178,loss: 0.02984209\n",
            "\n",
            "Global step: 1179,loss: 0.02985255\n",
            "\n",
            "Global step: 1180,loss: 0.036381766\n",
            "\n",
            "Global step: 1181,loss: 0.03486172\n",
            "\n",
            "Global step: 1182,loss: 0.03710154\n",
            "\n",
            "Global step: 1183,loss: 0.027206173\n",
            "\n",
            "Global step: 1184,loss: 0.030518807\n",
            "\n",
            "Global step: 1185,loss: 0.040541284\n",
            "\n",
            "Global step: 1186,loss: 0.02901658\n",
            "\n",
            "Global step: 1187,loss: 0.027254675\n",
            "\n",
            "Global step: 1188,loss: 0.028967042\n",
            "\n",
            "Global step: 1189,loss: 0.028441703\n",
            "\n",
            "Global step: 1190,loss: 0.035451397\n",
            "\n",
            "Global step: 1191,loss: 0.033555865\n",
            "\n",
            "Global step: 1192,loss: 0.029382136\n",
            "\n",
            "Global step: 1193,loss: 0.029968023\n",
            "\n",
            "Global step: 1194,loss: 0.03685024\n",
            "\n",
            "Global step: 1195,loss: 0.031400535\n",
            "\n",
            "Global step: 1196,loss: 0.025773171\n",
            "\n",
            "Global step: 1197,loss: 0.033336665\n",
            "\n",
            "Global step: 1198,loss: 0.028196296\n",
            "\n",
            "Global step: 1199,loss: 0.027361719\n",
            "\n",
            "Global step: 1200,loss: 0.028130656\n",
            "\n",
            "Global step: 1201,loss: 0.029036641\n",
            "\n",
            "Global step: 1202,loss: 0.03356789\n",
            "\n",
            "Global step: 1203,loss: 0.028145181\n",
            "\n",
            "Global step: 1204,loss: 0.02747581\n",
            "\n",
            "Global step: 1205,loss: 0.03340833\n",
            "\n",
            "Global step: 1206,loss: 0.034801763\n",
            "\n",
            "Global step: 1207,loss: 0.036057226\n",
            "\n",
            "Global step: 1208,loss: 0.037101805\n",
            "\n",
            "Global step: 1209,loss: 0.048677456\n",
            "\n",
            "Global step: 1210,loss: 0.0254765\n",
            "\n",
            "Global step: 1211,loss: 0.032146867\n",
            "\n",
            "Global step: 1212,loss: 0.045472566\n",
            "\n",
            "Global step: 1213,loss: 0.032248925\n",
            "\n",
            "Global step: 1214,loss: 0.034309886\n",
            "\n",
            "Global step: 1215,loss: 0.030222202\n",
            "\n",
            "Global step: 1216,loss: 0.028080586\n",
            "\n",
            "Global step: 1217,loss: 0.033762828\n",
            "\n",
            "Global step: 1218,loss: 0.029641327\n",
            "\n",
            "Global step: 1219,loss: 0.02954631\n",
            "\n",
            "Global step: 1220,loss: 0.032383204\n",
            "\n",
            "Global step: 1221,loss: 0.02900535\n",
            "\n",
            "Global step: 1222,loss: 0.039756883\n",
            "\n",
            "Global step: 1223,loss: 0.053025298\n",
            "\n",
            "Global step: 1224,loss: 0.050772622\n",
            "\n",
            "Global step: 1225,loss: 0.03509379\n",
            "\n",
            "Global step: 1226,loss: 0.032197542\n",
            "\n",
            "Global step: 1227,loss: 0.037095048\n",
            "\n",
            "Global step: 1228,loss: 0.029109126\n",
            "\n",
            "Global step: 1229,loss: 0.029034406\n",
            "\n",
            "Global step: 1230,loss: 0.033706464\n",
            "\n",
            "Global step: 1231,loss: 0.029866409\n",
            "\n",
            "Global step: 1232,loss: 0.035701234\n",
            "\n",
            "Global step: 1233,loss: 0.034057345\n",
            "\n",
            "Global step: 1234,loss: 0.030120138\n",
            "\n",
            "Global step: 1235,loss: 0.028312448\n",
            "\n",
            "Global step: 1236,loss: 0.032954786\n",
            "\n",
            "Global step: 1237,loss: 0.061189808\n",
            "\n",
            "Global step: 1238,loss: 0.039925024\n",
            "\n",
            "Global step: 1239,loss: 0.03790555\n",
            "\n",
            "Global step: 1240,loss: 0.039909385\n",
            "\n",
            "Global step: 1241,loss: 0.039725013\n",
            "\n",
            "Global step: 1242,loss: 0.028469324\n",
            "\n",
            "Global step: 1243,loss: 0.028558226\n",
            "\n",
            "Global step: 1244,loss: 0.030149495\n",
            "\n",
            "Global step: 1245,loss: 0.02771804\n",
            "\n",
            "Global step: 1246,loss: 0.03463973\n",
            "\n",
            "Global step: 1247,loss: 0.034098797\n",
            "\n",
            "Global step: 1248,loss: 0.04102994\n",
            "\n",
            "Global step: 1249,loss: 0.033754203\n",
            "\n",
            "Global step: 1250,loss: 0.046691477\n",
            "\n",
            "Global step: 1251,loss: 0.027792752\n",
            "\n",
            "Global step: 1252,loss: 0.03560754\n",
            "\n",
            "Global step: 1253,loss: 0.031009499\n",
            "\n",
            "Global step: 1254,loss: 0.029165132\n",
            "\n",
            "Global step: 1255,loss: 0.03615349\n",
            "\n",
            "Global step: 1256,loss: 0.0340269\n",
            "\n",
            "Global step: 1257,loss: 0.030663086\n",
            "\n",
            "Global step: 1258,loss: 0.03240396\n",
            "\n",
            "Global step: 1259,loss: 0.028473431\n",
            "\n",
            "Global step: 1260,loss: 0.02883487\n",
            "\n",
            "Global step: 1261,loss: 0.030844484\n",
            "\n",
            "Global step: 1262,loss: 0.032282047\n",
            "\n",
            "Global step: 1263,loss: 0.03845673\n",
            "\n",
            "Global step: 1264,loss: 0.030809218\n",
            "\n",
            "Global step: 1265,loss: 0.0362107\n",
            "\n",
            "Global step: 1266,loss: 0.03163072\n",
            "\n",
            "Global step: 1267,loss: 0.033854984\n",
            "\n",
            "Global step: 1268,loss: 0.025287597\n",
            "\n",
            "Global step: 1269,loss: 0.033081513\n",
            "\n",
            "Global step: 1270,loss: 0.030141354\n",
            "\n",
            "Global step: 1271,loss: 0.024659043\n",
            "\n",
            "Global step: 1272,loss: 0.03785625\n",
            "\n",
            "Global step: 1273,loss: 0.03802755\n",
            "\n",
            "Global step: 1274,loss: 0.028925931\n",
            "\n",
            "Global step: 1275,loss: 0.034224644\n",
            "\n",
            "Global step: 1276,loss: 0.040357474\n",
            "\n",
            "Global step: 1277,loss: 0.035503495\n",
            "\n",
            "Global step: 1278,loss: 0.02824024\n",
            "\n",
            "Global step: 1279,loss: 0.032878906\n",
            "\n",
            "Global step: 1280,loss: 0.028881632\n",
            "\n",
            "Global step: 1281,loss: 0.04442686\n",
            "\n",
            "Global step: 1282,loss: 0.028455142\n",
            "\n",
            "Global step: 1283,loss: 0.0286\n",
            "\n",
            "Global step: 1284,loss: 0.04017275\n",
            "\n",
            "Global step: 1285,loss: 0.0336854\n",
            "\n",
            "Global step: 1286,loss: 0.033221792\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1286,Val_Loss: 0.03113669978502469,  Val_acc: 0.9967948717948718 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:22:19.585001 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/15:\n",
            "Global step: 1287,loss: 0.02909257\n",
            "\n",
            "Global step: 1288,loss: 0.029098324\n",
            "\n",
            "Global step: 1289,loss: 0.034113914\n",
            "\n",
            "Global step: 1290,loss: 0.02724864\n",
            "\n",
            "Global step: 1291,loss: 0.03348192\n",
            "\n",
            "Global step: 1292,loss: 0.031263076\n",
            "\n",
            "Global step: 1293,loss: 0.027552666\n",
            "\n",
            "Global step: 1294,loss: 0.028993137\n",
            "\n",
            "Global step: 1295,loss: 0.039047543\n",
            "\n",
            "Global step: 1296,loss: 0.0326323\n",
            "\n",
            "Global step: 1297,loss: 0.025946984\n",
            "\n",
            "Global step: 1298,loss: 0.028027575\n",
            "\n",
            "Global step: 1299,loss: 0.027611692\n",
            "\n",
            "Global step: 1300,loss: 0.025588538\n",
            "\n",
            "Global step: 1301,loss: 0.032231368\n",
            "\n",
            "Global step: 1302,loss: 0.028100537\n",
            "\n",
            "Global step: 1303,loss: 0.026600838\n",
            "\n",
            "Global step: 1304,loss: 0.025181107\n",
            "\n",
            "Global step: 1305,loss: 0.029056953\n",
            "\n",
            "Global step: 1306,loss: 0.030382007\n",
            "\n",
            "Global step: 1307,loss: 0.025574345\n",
            "\n",
            "Global step: 1308,loss: 0.039339587\n",
            "\n",
            "Global step: 1309,loss: 0.033387236\n",
            "\n",
            "Global step: 1310,loss: 0.025536194\n",
            "\n",
            "Global step: 1311,loss: 0.032300867\n",
            "\n",
            "Global step: 1312,loss: 0.02847841\n",
            "\n",
            "Global step: 1313,loss: 0.024353903\n",
            "\n",
            "Global step: 1314,loss: 0.037915275\n",
            "\n",
            "Global step: 1315,loss: 0.024382435\n",
            "\n",
            "Global step: 1316,loss: 0.029506203\n",
            "\n",
            "Global step: 1317,loss: 0.02867996\n",
            "\n",
            "Global step: 1318,loss: 0.043374475\n",
            "\n",
            "Global step: 1319,loss: 0.03230094\n",
            "\n",
            "Global step: 1320,loss: 0.027573455\n",
            "\n",
            "Global step: 1321,loss: 0.034324795\n",
            "\n",
            "Global step: 1322,loss: 0.024862086\n",
            "\n",
            "Global step: 1323,loss: 0.024740035\n",
            "\n",
            "Global step: 1324,loss: 0.027191572\n",
            "\n",
            "Global step: 1325,loss: 0.027142845\n",
            "\n",
            "Global step: 1326,loss: 0.025783492\n",
            "\n",
            "Global step: 1327,loss: 0.026286244\n",
            "\n",
            "Global step: 1328,loss: 0.026392376\n",
            "\n",
            "Global step: 1329,loss: 0.024806809\n",
            "\n",
            "Global step: 1330,loss: 0.027787423\n",
            "\n",
            "Global step: 1331,loss: 0.02749179\n",
            "\n",
            "Global step: 1332,loss: 0.02481357\n",
            "\n",
            "Global step: 1333,loss: 0.026149582\n",
            "\n",
            "Global step: 1334,loss: 0.028429743\n",
            "\n",
            "Global step: 1335,loss: 0.029798018\n",
            "\n",
            "Global step: 1336,loss: 0.025241707\n",
            "\n",
            "Global step: 1337,loss: 0.026435968\n",
            "\n",
            "Global step: 1338,loss: 0.03125601\n",
            "\n",
            "Global step: 1339,loss: 0.034519978\n",
            "\n",
            "Global step: 1340,loss: 0.040181898\n",
            "\n",
            "Global step: 1341,loss: 0.027291704\n",
            "\n",
            "Global step: 1342,loss: 0.031870387\n",
            "\n",
            "Global step: 1343,loss: 0.026002048\n",
            "\n",
            "Global step: 1344,loss: 0.030160356\n",
            "\n",
            "Global step: 1345,loss: 0.026757415\n",
            "\n",
            "Global step: 1346,loss: 0.02840876\n",
            "\n",
            "Global step: 1347,loss: 0.026109776\n",
            "\n",
            "Global step: 1348,loss: 0.039355434\n",
            "\n",
            "Global step: 1349,loss: 0.02968271\n",
            "\n",
            "Global step: 1350,loss: 0.024760034\n",
            "\n",
            "Global step: 1351,loss: 0.029703718\n",
            "\n",
            "Global step: 1352,loss: 0.022373242\n",
            "\n",
            "Global step: 1353,loss: 0.026217096\n",
            "\n",
            "Global step: 1354,loss: 0.027756661\n",
            "\n",
            "Global step: 1355,loss: 0.045537792\n",
            "\n",
            "Global step: 1356,loss: 0.028763395\n",
            "\n",
            "Global step: 1357,loss: 0.032843187\n",
            "\n",
            "Global step: 1358,loss: 0.039232034\n",
            "\n",
            "Global step: 1359,loss: 0.029785823\n",
            "\n",
            "Global step: 1360,loss: 0.048539817\n",
            "\n",
            "Global step: 1361,loss: 0.025783438\n",
            "\n",
            "Global step: 1362,loss: 0.02913978\n",
            "\n",
            "Global step: 1363,loss: 0.028268546\n",
            "\n",
            "Global step: 1364,loss: 0.025138637\n",
            "\n",
            "Global step: 1365,loss: 0.027180597\n",
            "\n",
            "Global step: 1366,loss: 0.02933396\n",
            "\n",
            "Global step: 1367,loss: 0.03297059\n",
            "\n",
            "Global step: 1368,loss: 0.026762974\n",
            "\n",
            "Global step: 1369,loss: 0.02940527\n",
            "\n",
            "Global step: 1370,loss: 0.03371286\n",
            "\n",
            "Global step: 1371,loss: 0.029860968\n",
            "\n",
            "Global step: 1372,loss: 0.036459066\n",
            "\n",
            "Global step: 1373,loss: 0.03575471\n",
            "\n",
            "Global step: 1374,loss: 0.030024806\n",
            "\n",
            "Global step: 1375,loss: 0.03641109\n",
            "\n",
            "Global step: 1376,loss: 0.027044635\n",
            "\n",
            "Global step: 1377,loss: 0.029184502\n",
            "\n",
            "Global step: 1378,loss: 0.03268171\n",
            "\n",
            "Global step: 1379,loss: 0.03241533\n",
            "\n",
            "Global step: 1380,loss: 0.04372462\n",
            "\n",
            "Global step: 1381,loss: 0.033492055\n",
            "\n",
            "Global step: 1382,loss: 0.024321303\n",
            "\n",
            "Global step: 1383,loss: 0.02537251\n",
            "\n",
            "Global step: 1384,loss: 0.029175546\n",
            "\n",
            "Global step: 1385,loss: 0.03353432\n",
            "\n",
            "Global step: 1386,loss: 0.029318148\n",
            "\n",
            "Global step: 1387,loss: 0.03196834\n",
            "\n",
            "Global step: 1388,loss: 0.028122155\n",
            "\n",
            "Global step: 1389,loss: 0.025042834\n",
            "\n",
            "Global step: 1390,loss: 0.027223254\n",
            "\n",
            "Global step: 1391,loss: 0.031311475\n",
            "\n",
            "Global step: 1392,loss: 0.027987525\n",
            "\n",
            "Global step: 1393,loss: 0.029207036\n",
            "\n",
            "Global step: 1394,loss: 0.034016438\n",
            "\n",
            "Global step: 1395,loss: 0.045530282\n",
            "\n",
            "Global step: 1396,loss: 0.03191489\n",
            "\n",
            "Global step: 1397,loss: 0.028721306\n",
            "\n",
            "Global step: 1398,loss: 0.037682038\n",
            "\n",
            "Global step: 1399,loss: 0.02505607\n",
            "\n",
            "Global step: 1400,loss: 0.029427156\n",
            "\n",
            "Global step: 1401,loss: 0.026069596\n",
            "\n",
            "Global step: 1402,loss: 0.031817786\n",
            "\n",
            "Global step: 1403,loss: 0.026741747\n",
            "\n",
            "Global step: 1404,loss: 0.026434587\n",
            "\n",
            "Global step: 1405,loss: 0.033375468\n",
            "\n",
            "Global step: 1406,loss: 0.035642937\n",
            "\n",
            "Global step: 1407,loss: 0.039615177\n",
            "\n",
            "Global step: 1408,loss: 0.02422106\n",
            "\n",
            "Global step: 1409,loss: 0.030161817\n",
            "\n",
            "Global step: 1410,loss: 0.027407259\n",
            "\n",
            "Global step: 1411,loss: 0.02576868\n",
            "\n",
            "Global step: 1412,loss: 0.027547631\n",
            "\n",
            "Global step: 1413,loss: 0.030740455\n",
            "\n",
            "Global step: 1414,loss: 0.026314009\n",
            "\n",
            "Global step: 1415,loss: 0.024825664\n",
            "\n",
            "Global step: 1416,loss: 0.02870716\n",
            "\n",
            "Global step: 1417,loss: 0.042803697\n",
            "\n",
            "Global step: 1418,loss: 0.03560579\n",
            "\n",
            "Global step: 1419,loss: 0.0280148\n",
            "\n",
            "Global step: 1420,loss: 0.026417283\n",
            "\n",
            "Global step: 1421,loss: 0.044719383\n",
            "\n",
            "Global step: 1422,loss: 0.029380014\n",
            "\n",
            "Global step: 1423,loss: 0.02790893\n",
            "\n",
            "Global step: 1424,loss: 0.032063354\n",
            "\n",
            "Global step: 1425,loss: 0.026350096\n",
            "\n",
            "Global step: 1426,loss: 0.037050404\n",
            "\n",
            "Global step: 1427,loss: 0.027393706\n",
            "\n",
            "Global step: 1428,loss: 0.027788553\n",
            "\n",
            "Global step: 1429,loss: 0.040511556\n",
            "\n",
            "Global step: 1430,loss: 0.026227143\n",
            "\n",
            "Global step: 1431,loss: 0.032139484\n",
            "\n",
            "Global step: 1432,loss: 0.030381093\n",
            "\n",
            "Global step: 1433,loss: 0.026967645\n",
            "\n",
            "Global step: 1434,loss: 0.024776302\n",
            "\n",
            "Global step: 1435,loss: 0.026026485\n",
            "\n",
            "Global step: 1436,loss: 0.027592428\n",
            "\n",
            "Global step: 1437,loss: 0.026422402\n",
            "\n",
            "Global step: 1438,loss: 0.0268679\n",
            "\n",
            "Global step: 1439,loss: 0.024467016\n",
            "\n",
            "Global step: 1440,loss: 0.028878478\n",
            "\n",
            "Global step: 1441,loss: 0.02815532\n",
            "\n",
            "Global step: 1442,loss: 0.026820106\n",
            "\n",
            "Global step: 1443,loss: 0.02590198\n",
            "\n",
            "Global step: 1444,loss: 0.03376593\n",
            "\n",
            "Global step: 1445,loss: 0.026985224\n",
            "\n",
            "Global step: 1446,loss: 0.030957896\n",
            "\n",
            "Global step: 1447,loss: 0.026637718\n",
            "\n",
            "Global step: 1448,loss: 0.027373279\n",
            "\n",
            "Global step: 1449,loss: 0.028267216\n",
            "\n",
            "Global step: 1450,loss: 0.035829686\n",
            "\n",
            "Global step: 1451,loss: 0.026655594\n",
            "\n",
            "Global step: 1452,loss: 0.031174555\n",
            "\n",
            "Global step: 1453,loss: 0.024935868\n",
            "\n",
            "Global step: 1454,loss: 0.025639815\n",
            "\n",
            "Global step: 1455,loss: 0.029777544\n",
            "\n",
            "Global step: 1456,loss: 0.030543583\n",
            "\n",
            "Global step: 1457,loss: 0.026446365\n",
            "\n",
            "Global step: 1458,loss: 0.034560002\n",
            "\n",
            "Global step: 1459,loss: 0.026421541\n",
            "\n",
            "Global step: 1460,loss: 0.031535424\n",
            "\n",
            "Global step: 1461,loss: 0.03125661\n",
            "\n",
            "Global step: 1462,loss: 0.03383849\n",
            "\n",
            "Global step: 1463,loss: 0.03359484\n",
            "\n",
            "Global step: 1464,loss: 0.0272743\n",
            "\n",
            "Global step: 1465,loss: 0.031092579\n",
            "\n",
            "Global step: 1466,loss: 0.035154108\n",
            "\n",
            "Global step: 1467,loss: 0.02765818\n",
            "\n",
            "Global step: 1468,loss: 0.027895024\n",
            "\n",
            "Global step: 1469,loss: 0.02822042\n",
            "\n",
            "Global step: 1470,loss: 0.031911675\n",
            "\n",
            "Global step: 1471,loss: 0.029226677\n",
            "\n",
            "Global step: 1472,loss: 0.03382266\n",
            "\n",
            "Global step: 1473,loss: 0.025600363\n",
            "\n",
            "Global step: 1474,loss: 0.027565595\n",
            "\n",
            "Global step: 1475,loss: 0.024868514\n",
            "\n",
            "Global step: 1476,loss: 0.026813652\n",
            "\n",
            "Global step: 1477,loss: 0.026111342\n",
            "\n",
            "Global step: 1478,loss: 0.029198995\n",
            "\n",
            "Global step: 1479,loss: 0.025961932\n",
            "\n",
            "Global step: 1480,loss: 0.031934768\n",
            "\n",
            "Global step: 1481,loss: 0.028965924\n",
            "\n",
            "Global step: 1482,loss: 0.026948528\n",
            "\n",
            "Global step: 1483,loss: 0.029228374\n",
            "\n",
            "Global step: 1484,loss: 0.031653896\n",
            "\n",
            "Global step: 1485,loss: 0.023750644\n",
            "\n",
            "Global step: 1486,loss: 0.024633605\n",
            "\n",
            "Global step: 1487,loss: 0.034980312\n",
            "\n",
            "Global step: 1488,loss: 0.030586466\n",
            "\n",
            "Global step: 1489,loss: 0.025038254\n",
            "\n",
            "Global step: 1490,loss: 0.02657944\n",
            "\n",
            "Global step: 1491,loss: 0.031104768\n",
            "\n",
            "Global step: 1492,loss: 0.03354335\n",
            "\n",
            "Global step: 1493,loss: 0.031749308\n",
            "\n",
            "Global step: 1494,loss: 0.03860324\n",
            "\n",
            "Global step: 1495,loss: 0.031749513\n",
            "\n",
            "Global step: 1496,loss: 0.029076869\n",
            "\n",
            "Global step: 1497,loss: 0.030505076\n",
            "\n",
            "Global step: 1498,loss: 0.033776972\n",
            "\n",
            "Global step: 1499,loss: 0.025703454\n",
            "\n",
            "Global step: 1500,loss: 0.033311434\n",
            "\n",
            "Global step: 1501,loss: 0.025604744\n",
            "\n",
            "Global step: 1502,loss: 0.030940209\n",
            "\n",
            "Global step: 1503,loss: 0.025368879\n",
            "\n",
            "Global step: 1504,loss: 0.031284288\n",
            "\n",
            "Global step: 1505,loss: 0.03677666\n",
            "\n",
            "Global step: 1506,loss: 0.033503726\n",
            "\n",
            "Global step: 1507,loss: 0.031566218\n",
            "\n",
            "Global step: 1508,loss: 0.031979226\n",
            "\n",
            "Global step: 1509,loss: 0.026080757\n",
            "\n",
            "Global step: 1510,loss: 0.02817628\n",
            "\n",
            "Global step: 1511,loss: 0.03082084\n",
            "\n",
            "Global step: 1512,loss: 0.024386715\n",
            "\n",
            "Global step: 1513,loss: 0.035063684\n",
            "\n",
            "Global step: 1514,loss: 0.031820424\n",
            "\n",
            "Global step: 1515,loss: 0.028151145\n",
            "\n",
            "Global step: 1516,loss: 0.028405497\n",
            "\n",
            "Global step: 1517,loss: 0.02605832\n",
            "\n",
            "Global step: 1518,loss: 0.02842357\n",
            "\n",
            "Global step: 1519,loss: 0.035252154\n",
            "\n",
            "Global step: 1520,loss: 0.028868333\n",
            "\n",
            "Global step: 1521,loss: 0.026600596\n",
            "\n",
            "Global step: 1522,loss: 0.052164655\n",
            "\n",
            "Global step: 1523,loss: 0.023864107\n",
            "\n",
            "Global step: 1524,loss: 0.03016578\n",
            "\n",
            "Global step: 1525,loss: 0.032575265\n",
            "\n",
            "Global step: 1526,loss: 0.029591799\n",
            "\n",
            "Global step: 1527,loss: 0.024476355\n",
            "\n",
            "Global step: 1528,loss: 0.02998592\n",
            "\n",
            "Global step: 1529,loss: 0.023820216\n",
            "\n",
            "Global step: 1530,loss: 0.030070417\n",
            "\n",
            "Global step: 1531,loss: 0.0380605\n",
            "\n",
            "Global step: 1532,loss: 0.03015451\n",
            "\n",
            "Global step: 1533,loss: 0.026066447\n",
            "\n",
            "Global step: 1534,loss: 0.027443752\n",
            "\n",
            "Global step: 1535,loss: 0.02479538\n",
            "\n",
            "Global step: 1536,loss: 0.032239582\n",
            "\n",
            "Global step: 1537,loss: 0.024344724\n",
            "\n",
            "Global step: 1538,loss: 0.029591436\n",
            "\n",
            "Global step: 1539,loss: 0.036246635\n",
            "\n",
            "Global step: 1540,loss: 0.02708577\n",
            "\n",
            "Global step: 1541,loss: 0.039114334\n",
            "\n",
            "Global step: 1542,loss: 0.03145147\n",
            "\n",
            "Global step: 1543,loss: 0.023038588\n",
            "\n",
            "Global step: 1544,loss: 0.027367018\n",
            "\n",
            "Global step: 1545,loss: 0.031069087\n",
            "\n",
            "Global step: 1546,loss: 0.03958022\n",
            "\n",
            "Global step: 1547,loss: 0.025733683\n",
            "\n",
            "Global step: 1548,loss: 0.029322408\n",
            "\n",
            "Global step: 1549,loss: 0.02966065\n",
            "\n",
            "Global step: 1550,loss: 0.029309371\n",
            "\n",
            "Global step: 1551,loss: 0.025811989\n",
            "\n",
            "Global step: 1552,loss: 0.028323611\n",
            "\n",
            "Global step: 1553,loss: 0.039516196\n",
            "\n",
            "Global step: 1554,loss: 0.03228491\n",
            "\n",
            "Global step: 1555,loss: 0.02491913\n",
            "\n",
            "Global step: 1556,loss: 0.031662248\n",
            "\n",
            "Global step: 1557,loss: 0.03545408\n",
            "\n",
            "Global step: 1558,loss: 0.027359238\n",
            "\n",
            "Global step: 1559,loss: 0.032897994\n",
            "\n",
            "Global step: 1560,loss: 0.023583286\n",
            "\n",
            "Global step: 1561,loss: 0.040254563\n",
            "\n",
            "Global step: 1562,loss: 0.04400502\n",
            "\n",
            "Global step: 1563,loss: 0.02366281\n",
            "\n",
            "Global step: 1564,loss: 0.023416277\n",
            "\n",
            "Global step: 1565,loss: 0.029898327\n",
            "\n",
            "Global step: 1566,loss: 0.025817867\n",
            "\n",
            "Global step: 1567,loss: 0.029590793\n",
            "\n",
            "Global step: 1568,loss: 0.024382222\n",
            "\n",
            "Global step: 1569,loss: 0.023634542\n",
            "\n",
            "Global step: 1570,loss: 0.028849015\n",
            "\n",
            "Global step: 1571,loss: 0.0242892\n",
            "\n",
            "Global step: 1572,loss: 0.028595256\n",
            "\n",
            "Global step: 1573,loss: 0.039135553\n",
            "\n",
            "Global step: 1574,loss: 0.027009668\n",
            "\n",
            "Global step: 1575,loss: 0.027025875\n",
            "\n",
            "Global step: 1576,loss: 0.02417458\n",
            "\n",
            "Global step: 1577,loss: 0.026230652\n",
            "\n",
            "Global step: 1578,loss: 0.027364725\n",
            "\n",
            "Global step: 1579,loss: 0.03593248\n",
            "\n",
            "Global step: 1580,loss: 0.03557047\n",
            "\n",
            "Global step: 1581,loss: 0.023782365\n",
            "\n",
            "Global step: 1582,loss: 0.02304854\n",
            "\n",
            "Global step: 1583,loss: 0.031322125\n",
            "\n",
            "Global step: 1584,loss: 0.02742098\n",
            "\n",
            "Global step: 1585,loss: 0.026386807\n",
            "\n",
            "Global step: 1586,loss: 0.026462272\n",
            "\n",
            "Global step: 1587,loss: 0.034973536\n",
            "\n",
            "Global step: 1588,loss: 0.029750867\n",
            "\n",
            "Global step: 1589,loss: 0.027918775\n",
            "\n",
            "Global step: 1590,loss: 0.027537035\n",
            "\n",
            "Global step: 1591,loss: 0.03700834\n",
            "\n",
            "Global step: 1592,loss: 0.024988923\n",
            "\n",
            "Global step: 1593,loss: 0.036606435\n",
            "\n",
            "Global step: 1594,loss: 0.031140741\n",
            "\n",
            "Global step: 1595,loss: 0.026336942\n",
            "\n",
            "Global step: 1596,loss: 0.03798916\n",
            "\n",
            "Global step: 1597,loss: 0.029423216\n",
            "\n",
            "Global step: 1598,loss: 0.025189124\n",
            "\n",
            "Global step: 1599,loss: 0.025641674\n",
            "\n",
            "Global step: 1600,loss: 0.033368602\n",
            "\n",
            "Global step: 1601,loss: 0.02665179\n",
            "\n",
            "Global step: 1602,loss: 0.026736997\n",
            "\n",
            "Global step: 1603,loss: 0.028344702\n",
            "\n",
            "Global step: 1604,loss: 0.026358694\n",
            "\n",
            "Global step: 1605,loss: 0.030058473\n",
            "\n",
            "Global step: 1606,loss: 0.024812672\n",
            "\n",
            "Global step: 1607,loss: 0.024152607\n",
            "\n",
            "Global step: 1608,loss: 0.025637463\n",
            "\n",
            "Global step: 1609,loss: 0.03400998\n",
            "\n",
            "Global step: 1610,loss: 0.030998776\n",
            "\n",
            "Global step: 1611,loss: 0.02470281\n",
            "\n",
            "Global step: 1612,loss: 0.024698589\n",
            "\n",
            "Global step: 1613,loss: 0.03618229\n",
            "\n",
            "Global step: 1614,loss: 0.02681151\n",
            "\n",
            "Global step: 1615,loss: 0.029434018\n",
            "\n",
            "Global step: 1616,loss: 0.02667279\n",
            "\n",
            "Global step: 1617,loss: 0.025155436\n",
            "\n",
            "Global step: 1618,loss: 0.032063678\n",
            "\n",
            "Global step: 1619,loss: 0.028376138\n",
            "\n",
            "Global step: 1620,loss: 0.0257643\n",
            "\n",
            "Global step: 1621,loss: 0.028317053\n",
            "\n",
            "Global step: 1622,loss: 0.028500773\n",
            "\n",
            "Global step: 1623,loss: 0.032210972\n",
            "\n",
            "Global step: 1624,loss: 0.026692044\n",
            "\n",
            "Global step: 1625,loss: 0.02614084\n",
            "\n",
            "Global step: 1626,loss: 0.02993883\n",
            "\n",
            "Global step: 1627,loss: 0.03043854\n",
            "\n",
            "Global step: 1628,loss: 0.02243193\n",
            "\n",
            "Global step: 1629,loss: 0.03218688\n",
            "\n",
            "Global step: 1630,loss: 0.035216115\n",
            "\n",
            "Global step: 1631,loss: 0.024083143\n",
            "\n",
            "Global step: 1632,loss: 0.028942982\n",
            "\n",
            "Global step: 1633,loss: 0.0360113\n",
            "\n",
            "Global step: 1634,loss: 0.025524179\n",
            "\n",
            "Global step: 1635,loss: 0.02405657\n",
            "\n",
            "Global step: 1636,loss: 0.02793619\n",
            "\n",
            "Global step: 1637,loss: 0.02673209\n",
            "\n",
            "Global step: 1638,loss: 0.03259475\n",
            "\n",
            "Global step: 1639,loss: 0.022812132\n",
            "\n",
            "Global step: 1640,loss: 0.024314063\n",
            "\n",
            "Global step: 1641,loss: 0.0248044\n",
            "\n",
            "Global step: 1642,loss: 0.023776613\n",
            "\n",
            "Global step: 1643,loss: 0.034640476\n",
            "\n",
            "Global step: 1644,loss: 0.030589545\n",
            "\n",
            "Global step: 1645,loss: 0.026197268\n",
            "\n",
            "Global step: 1646,loss: 0.02587842\n",
            "\n",
            "Global step: 1647,loss: 0.025820376\n",
            "\n",
            "Global step: 1648,loss: 0.024822287\n",
            "\n",
            "Global step: 1649,loss: 0.026118487\n",
            "\n",
            "Global step: 1650,loss: 0.025871731\n",
            "\n",
            "Global step: 1651,loss: 0.025748666\n",
            "\n",
            "Global step: 1652,loss: 0.036248863\n",
            "\n",
            "Global step: 1653,loss: 0.025052117\n",
            "\n",
            "Global step: 1654,loss: 0.026596168\n",
            "\n",
            "Global step: 1655,loss: 0.03186886\n",
            "\n",
            "Global step: 1656,loss: 0.028752051\n",
            "\n",
            "Global step: 1657,loss: 0.023425478\n",
            "\n",
            "Global step: 1658,loss: 0.029704943\n",
            "\n",
            "Global step: 1659,loss: 0.02376704\n",
            "\n",
            "Global step: 1660,loss: 0.023378665\n",
            "\n",
            "Global step: 1661,loss: 0.027798425\n",
            "\n",
            "Global step: 1662,loss: 0.030309394\n",
            "\n",
            "Global step: 1663,loss: 0.029920004\n",
            "\n",
            "Global step: 1664,loss: 0.027181532\n",
            "\n",
            "Global step: 1665,loss: 0.034252927\n",
            "\n",
            "Global step: 1666,loss: 0.030622149\n",
            "\n",
            "Global step: 1667,loss: 0.030744713\n",
            "\n",
            "Global step: 1668,loss: 0.026018105\n",
            "\n",
            "Global step: 1669,loss: 0.027174346\n",
            "\n",
            "Global step: 1670,loss: 0.025139337\n",
            "\n",
            "Global step: 1671,loss: 0.023842376\n",
            "\n",
            "Global step: 1672,loss: 0.025542937\n",
            "\n",
            "Global step: 1673,loss: 0.023484081\n",
            "\n",
            "Global step: 1674,loss: 0.030007828\n",
            "\n",
            "Global step: 1675,loss: 0.034841374\n",
            "\n",
            "Global step: 1676,loss: 0.02740437\n",
            "\n",
            "Global step: 1677,loss: 0.025683898\n",
            "\n",
            "Global step: 1678,loss: 0.028720725\n",
            "\n",
            "Global step: 1679,loss: 0.02597884\n",
            "\n",
            "Global step: 1680,loss: 0.030383192\n",
            "\n",
            "Global step: 1681,loss: 0.026477542\n",
            "\n",
            "Global step: 1682,loss: 0.028262356\n",
            "\n",
            "Global step: 1683,loss: 0.027220983\n",
            "\n",
            "Global step: 1684,loss: 0.024185736\n",
            "\n",
            "Global step: 1685,loss: 0.02507991\n",
            "\n",
            "Global step: 1686,loss: 0.026139673\n",
            "\n",
            "Global step: 1687,loss: 0.024944942\n",
            "\n",
            "Global step: 1688,loss: 0.029340222\n",
            "\n",
            "Global step: 1689,loss: 0.023241147\n",
            "\n",
            "Global step: 1690,loss: 0.0269013\n",
            "\n",
            "Global step: 1691,loss: 0.023647578\n",
            "\n",
            "Global step: 1692,loss: 0.026618132\n",
            "\n",
            "Global step: 1693,loss: 0.024454115\n",
            "\n",
            "Global step: 1694,loss: 0.024388619\n",
            "\n",
            "Global step: 1695,loss: 0.033941403\n",
            "\n",
            "Global step: 1696,loss: 0.03717932\n",
            "\n",
            "Global step: 1697,loss: 0.027175104\n",
            "\n",
            "Global step: 1698,loss: 0.025834259\n",
            "\n",
            "Global step: 1699,loss: 0.025252754\n",
            "\n",
            "Global step: 1700,loss: 0.024986535\n",
            "\n",
            "Global step: 1701,loss: 0.023513298\n",
            "\n",
            "Global step: 1702,loss: 0.02454326\n",
            "\n",
            "Global step: 1703,loss: 0.035644803\n",
            "\n",
            "Global step: 1704,loss: 0.039393194\n",
            "\n",
            "Global step: 1705,loss: 0.022949586\n",
            "\n",
            "Global step: 1706,loss: 0.025271261\n",
            "\n",
            "Global step: 1707,loss: 0.022862144\n",
            "\n",
            "Global step: 1708,loss: 0.02375091\n",
            "\n",
            "Global step: 1709,loss: 0.026076296\n",
            "\n",
            "Global step: 1710,loss: 0.023416849\n",
            "\n",
            "Global step: 1711,loss: 0.027242735\n",
            "\n",
            "Global step: 1712,loss: 0.026708663\n",
            "\n",
            "Global step: 1713,loss: 0.02225679\n",
            "\n",
            "Global step: 1714,loss: 0.03214164\n",
            "\n",
            "Global step: 1715,loss: 0.024263661\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 1715,Val_Loss: 0.027817820174953878,  Val_acc: 0.9971955128205128 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:23:15.478070 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/15:\n",
            "Global step: 1716,loss: 0.02266743\n",
            "\n",
            "Global step: 1717,loss: 0.023874385\n",
            "\n",
            "Global step: 1718,loss: 0.024102744\n",
            "\n",
            "Global step: 1719,loss: 0.024585038\n",
            "\n",
            "Global step: 1720,loss: 0.026426671\n",
            "\n",
            "Global step: 1721,loss: 0.029871844\n",
            "\n",
            "Global step: 1722,loss: 0.024828758\n",
            "\n",
            "Global step: 1723,loss: 0.026649732\n",
            "\n",
            "Global step: 1724,loss: 0.027412612\n",
            "\n",
            "Global step: 1725,loss: 0.023945892\n",
            "\n",
            "Global step: 1726,loss: 0.030354446\n",
            "\n",
            "Global step: 1727,loss: 0.024456486\n",
            "\n",
            "Global step: 1728,loss: 0.032758743\n",
            "\n",
            "Global step: 1729,loss: 0.026359499\n",
            "\n",
            "Global step: 1730,loss: 0.028312502\n",
            "\n",
            "Global step: 1731,loss: 0.027823241\n",
            "\n",
            "Global step: 1732,loss: 0.028925795\n",
            "\n",
            "Global step: 1733,loss: 0.022972003\n",
            "\n",
            "Global step: 1734,loss: 0.028232582\n",
            "\n",
            "Global step: 1735,loss: 0.02487611\n",
            "\n",
            "Global step: 1736,loss: 0.02351971\n",
            "\n",
            "Global step: 1737,loss: 0.024476726\n",
            "\n",
            "Global step: 1738,loss: 0.03694629\n",
            "\n",
            "Global step: 1739,loss: 0.025087392\n",
            "\n",
            "Global step: 1740,loss: 0.027461503\n",
            "\n",
            "Global step: 1741,loss: 0.030669093\n",
            "\n",
            "Global step: 1742,loss: 0.024337342\n",
            "\n",
            "Global step: 1743,loss: 0.026614297\n",
            "\n",
            "Global step: 1744,loss: 0.026824232\n",
            "\n",
            "Global step: 1745,loss: 0.02318156\n",
            "\n",
            "Global step: 1746,loss: 0.028124603\n",
            "\n",
            "Global step: 1747,loss: 0.024637459\n",
            "\n",
            "Global step: 1748,loss: 0.02529081\n",
            "\n",
            "Global step: 1749,loss: 0.024269741\n",
            "\n",
            "Global step: 1750,loss: 0.029412141\n",
            "\n",
            "Global step: 1751,loss: 0.026430959\n",
            "\n",
            "Global step: 1752,loss: 0.026674809\n",
            "\n",
            "Global step: 1753,loss: 0.024666026\n",
            "\n",
            "Global step: 1754,loss: 0.028414316\n",
            "\n",
            "Global step: 1755,loss: 0.027197316\n",
            "\n",
            "Global step: 1756,loss: 0.03039125\n",
            "\n",
            "Global step: 1757,loss: 0.023697356\n",
            "\n",
            "Global step: 1758,loss: 0.028101753\n",
            "\n",
            "Global step: 1759,loss: 0.023129284\n",
            "\n",
            "Global step: 1760,loss: 0.026260935\n",
            "\n",
            "Global step: 1761,loss: 0.02426242\n",
            "\n",
            "Global step: 1762,loss: 0.02226497\n",
            "\n",
            "Global step: 1763,loss: 0.022823509\n",
            "\n",
            "Global step: 1764,loss: 0.03393414\n",
            "\n",
            "Global step: 1765,loss: 0.025305234\n",
            "\n",
            "Global step: 1766,loss: 0.028615704\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.57536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:23:22.134600 139648999347968 supervisor.py:1099] global_step/sec: 7.57536\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1767,loss: 0.023928482\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1768.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:23:22.304656 139649007740672 supervisor.py:1050] Recording summary at step 1768.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1768,loss: 0.028096016\n",
            "\n",
            "Global step: 1769,loss: 0.03533276\n",
            "\n",
            "Global step: 1770,loss: 0.027028572\n",
            "\n",
            "Global step: 1771,loss: 0.02816004\n",
            "\n",
            "Global step: 1772,loss: 0.023994027\n",
            "\n",
            "Global step: 1773,loss: 0.023516957\n",
            "\n",
            "Global step: 1774,loss: 0.022237753\n",
            "\n",
            "Global step: 1775,loss: 0.024988297\n",
            "\n",
            "Global step: 1776,loss: 0.023955133\n",
            "\n",
            "Global step: 1777,loss: 0.024967777\n",
            "\n",
            "Global step: 1778,loss: 0.025759552\n",
            "\n",
            "Global step: 1779,loss: 0.027310178\n",
            "\n",
            "Global step: 1780,loss: 0.022584427\n",
            "\n",
            "Global step: 1781,loss: 0.03845367\n",
            "\n",
            "Global step: 1782,loss: 0.02586191\n",
            "\n",
            "Global step: 1783,loss: 0.033605605\n",
            "\n",
            "Global step: 1784,loss: 0.022385513\n",
            "\n",
            "Global step: 1785,loss: 0.032639325\n",
            "\n",
            "Global step: 1786,loss: 0.036703475\n",
            "\n",
            "Global step: 1787,loss: 0.025126016\n",
            "\n",
            "Global step: 1788,loss: 0.023099488\n",
            "\n",
            "Global step: 1789,loss: 0.037752897\n",
            "\n",
            "Global step: 1790,loss: 0.028502543\n",
            "\n",
            "Global step: 1791,loss: 0.029286787\n",
            "\n",
            "Global step: 1792,loss: 0.02766313\n",
            "\n",
            "Global step: 1793,loss: 0.038767688\n",
            "\n",
            "Global step: 1794,loss: 0.030332519\n",
            "\n",
            "Global step: 1795,loss: 0.025440883\n",
            "\n",
            "Global step: 1796,loss: 0.039017558\n",
            "\n",
            "Global step: 1797,loss: 0.03028205\n",
            "\n",
            "Global step: 1798,loss: 0.02725022\n",
            "\n",
            "Global step: 1799,loss: 0.029872006\n",
            "\n",
            "Global step: 1800,loss: 0.023687972\n",
            "\n",
            "Global step: 1801,loss: 0.023725603\n",
            "\n",
            "Global step: 1802,loss: 0.025117315\n",
            "\n",
            "Global step: 1803,loss: 0.0343282\n",
            "\n",
            "Global step: 1804,loss: 0.027964363\n",
            "\n",
            "Global step: 1805,loss: 0.032817565\n",
            "\n",
            "Global step: 1806,loss: 0.026353363\n",
            "\n",
            "Global step: 1807,loss: 0.025516946\n",
            "\n",
            "Global step: 1808,loss: 0.027828958\n",
            "\n",
            "Global step: 1809,loss: 0.02496529\n",
            "\n",
            "Global step: 1810,loss: 0.025755826\n",
            "\n",
            "Global step: 1811,loss: 0.0389345\n",
            "\n",
            "Global step: 1812,loss: 0.025107231\n",
            "\n",
            "Global step: 1813,loss: 0.02377118\n",
            "\n",
            "Global step: 1814,loss: 0.02524883\n",
            "\n",
            "Global step: 1815,loss: 0.024217071\n",
            "\n",
            "Global step: 1816,loss: 0.023426048\n",
            "\n",
            "Global step: 1817,loss: 0.025014464\n",
            "\n",
            "Global step: 1818,loss: 0.024995765\n",
            "\n",
            "Global step: 1819,loss: 0.029034084\n",
            "\n",
            "Global step: 1820,loss: 0.029386727\n",
            "\n",
            "Global step: 1821,loss: 0.027355095\n",
            "\n",
            "Global step: 1822,loss: 0.026787017\n",
            "\n",
            "Global step: 1823,loss: 0.022136997\n",
            "\n",
            "Global step: 1824,loss: 0.030648613\n",
            "\n",
            "Global step: 1825,loss: 0.023432605\n",
            "\n",
            "Global step: 1826,loss: 0.022988513\n",
            "\n",
            "Global step: 1827,loss: 0.022763731\n",
            "\n",
            "Global step: 1828,loss: 0.025822444\n",
            "\n",
            "Global step: 1829,loss: 0.030768763\n",
            "\n",
            "Global step: 1830,loss: 0.029253714\n",
            "\n",
            "Global step: 1831,loss: 0.024661701\n",
            "\n",
            "Global step: 1832,loss: 0.03424112\n",
            "\n",
            "Global step: 1833,loss: 0.024649786\n",
            "\n",
            "Global step: 1834,loss: 0.02532154\n",
            "\n",
            "Global step: 1835,loss: 0.02357363\n",
            "\n",
            "Global step: 1836,loss: 0.029461302\n",
            "\n",
            "Global step: 1837,loss: 0.023345087\n",
            "\n",
            "Global step: 1838,loss: 0.027039584\n",
            "\n",
            "Global step: 1839,loss: 0.0406596\n",
            "\n",
            "Global step: 1840,loss: 0.023652274\n",
            "\n",
            "Global step: 1841,loss: 0.027374668\n",
            "\n",
            "Global step: 1842,loss: 0.027313188\n",
            "\n",
            "Global step: 1843,loss: 0.02723072\n",
            "\n",
            "Global step: 1844,loss: 0.027439483\n",
            "\n",
            "Global step: 1845,loss: 0.03839861\n",
            "\n",
            "Global step: 1846,loss: 0.029915579\n",
            "\n",
            "Global step: 1847,loss: 0.024472311\n",
            "\n",
            "Global step: 1848,loss: 0.031568874\n",
            "\n",
            "Global step: 1849,loss: 0.026423674\n",
            "\n",
            "Global step: 1850,loss: 0.025047634\n",
            "\n",
            "Global step: 1851,loss: 0.022999361\n",
            "\n",
            "Global step: 1852,loss: 0.024304807\n",
            "\n",
            "Global step: 1853,loss: 0.03619734\n",
            "\n",
            "Global step: 1854,loss: 0.023773413\n",
            "\n",
            "Global step: 1855,loss: 0.022220483\n",
            "\n",
            "Global step: 1856,loss: 0.031643122\n",
            "\n",
            "Global step: 1857,loss: 0.02276203\n",
            "\n",
            "Global step: 1858,loss: 0.024666792\n",
            "\n",
            "Global step: 1859,loss: 0.023980604\n",
            "\n",
            "Global step: 1860,loss: 0.02394331\n",
            "\n",
            "Global step: 1861,loss: 0.023123605\n",
            "\n",
            "Global step: 1862,loss: 0.038771536\n",
            "\n",
            "Global step: 1863,loss: 0.029230427\n",
            "\n",
            "Global step: 1864,loss: 0.026332997\n",
            "\n",
            "Global step: 1865,loss: 0.02563659\n",
            "\n",
            "Global step: 1866,loss: 0.028011547\n",
            "\n",
            "Global step: 1867,loss: 0.022660479\n",
            "\n",
            "Global step: 1868,loss: 0.025777852\n",
            "\n",
            "Global step: 1869,loss: 0.025421923\n",
            "\n",
            "Global step: 1870,loss: 0.025230672\n",
            "\n",
            "Global step: 1871,loss: 0.032055594\n",
            "\n",
            "Global step: 1872,loss: 0.023055024\n",
            "\n",
            "Global step: 1873,loss: 0.030450376\n",
            "\n",
            "Global step: 1874,loss: 0.024956333\n",
            "\n",
            "Global step: 1875,loss: 0.022800995\n",
            "\n",
            "Global step: 1876,loss: 0.029798161\n",
            "\n",
            "Global step: 1877,loss: 0.025066495\n",
            "\n",
            "Global step: 1878,loss: 0.022983767\n",
            "\n",
            "Global step: 1879,loss: 0.02319573\n",
            "\n",
            "Global step: 1880,loss: 0.023503529\n",
            "\n",
            "Global step: 1881,loss: 0.03182483\n",
            "\n",
            "Global step: 1882,loss: 0.02270451\n",
            "\n",
            "Global step: 1883,loss: 0.023282472\n",
            "\n",
            "Global step: 1884,loss: 0.02379993\n",
            "\n",
            "Global step: 1885,loss: 0.02294002\n",
            "\n",
            "Global step: 1886,loss: 0.022641273\n",
            "\n",
            "Global step: 1887,loss: 0.025040593\n",
            "\n",
            "Global step: 1888,loss: 0.028069612\n",
            "\n",
            "Global step: 1889,loss: 0.022954745\n",
            "\n",
            "Global step: 1890,loss: 0.024670085\n",
            "\n",
            "Global step: 1891,loss: 0.024573456\n",
            "\n",
            "Global step: 1892,loss: 0.031305365\n",
            "\n",
            "Global step: 1893,loss: 0.024339998\n",
            "\n",
            "Global step: 1894,loss: 0.025263643\n",
            "\n",
            "Global step: 1895,loss: 0.02273979\n",
            "\n",
            "Global step: 1896,loss: 0.027862757\n",
            "\n",
            "Global step: 1897,loss: 0.023380864\n",
            "\n",
            "Global step: 1898,loss: 0.02450226\n",
            "\n",
            "Global step: 1899,loss: 0.021482103\n",
            "\n",
            "Global step: 1900,loss: 0.025027633\n",
            "\n",
            "Global step: 1901,loss: 0.02548473\n",
            "\n",
            "Global step: 1902,loss: 0.021982614\n",
            "\n",
            "Global step: 1903,loss: 0.038555656\n",
            "\n",
            "Global step: 1904,loss: 0.022776185\n",
            "\n",
            "Global step: 1905,loss: 0.024292145\n",
            "\n",
            "Global step: 1906,loss: 0.02316313\n",
            "\n",
            "Global step: 1907,loss: 0.024440102\n",
            "\n",
            "Global step: 1908,loss: 0.023757184\n",
            "\n",
            "Global step: 1909,loss: 0.037304875\n",
            "\n",
            "Global step: 1910,loss: 0.024924144\n",
            "\n",
            "Global step: 1911,loss: 0.024610354\n",
            "\n",
            "Global step: 1912,loss: 0.023494873\n",
            "\n",
            "Global step: 1913,loss: 0.022949569\n",
            "\n",
            "Global step: 1914,loss: 0.027354231\n",
            "\n",
            "Global step: 1915,loss: 0.04120067\n",
            "\n",
            "Global step: 1916,loss: 0.029744752\n",
            "\n",
            "Global step: 1917,loss: 0.025535844\n",
            "\n",
            "Global step: 1918,loss: 0.027573347\n",
            "\n",
            "Global step: 1919,loss: 0.02911543\n",
            "\n",
            "Global step: 1920,loss: 0.025103286\n",
            "\n",
            "Global step: 1921,loss: 0.023123339\n",
            "\n",
            "Global step: 1922,loss: 0.026383974\n",
            "\n",
            "Global step: 1923,loss: 0.023675244\n",
            "\n",
            "Global step: 1924,loss: 0.023443112\n",
            "\n",
            "Global step: 1925,loss: 0.02415904\n",
            "\n",
            "Global step: 1926,loss: 0.030453913\n",
            "\n",
            "Global step: 1927,loss: 0.03707595\n",
            "\n",
            "Global step: 1928,loss: 0.02554819\n",
            "\n",
            "Global step: 1929,loss: 0.025308197\n",
            "\n",
            "Global step: 1930,loss: 0.02315382\n",
            "\n",
            "Global step: 1931,loss: 0.029755406\n",
            "\n",
            "Global step: 1932,loss: 0.024112139\n",
            "\n",
            "Global step: 1933,loss: 0.025412722\n",
            "\n",
            "Global step: 1934,loss: 0.024765909\n",
            "\n",
            "Global step: 1935,loss: 0.023626596\n",
            "\n",
            "Global step: 1936,loss: 0.025480486\n",
            "\n",
            "Global step: 1937,loss: 0.030226357\n",
            "\n",
            "Global step: 1938,loss: 0.025539989\n",
            "\n",
            "Global step: 1939,loss: 0.022354918\n",
            "\n",
            "Global step: 1940,loss: 0.02854481\n",
            "\n",
            "Global step: 1941,loss: 0.028548338\n",
            "\n",
            "Global step: 1942,loss: 0.03152866\n",
            "\n",
            "Global step: 1943,loss: 0.026802123\n",
            "\n",
            "Global step: 1944,loss: 0.022867043\n",
            "\n",
            "Global step: 1945,loss: 0.03351333\n",
            "\n",
            "Global step: 1946,loss: 0.028463908\n",
            "\n",
            "Global step: 1947,loss: 0.026458126\n",
            "\n",
            "Global step: 1948,loss: 0.027849525\n",
            "\n",
            "Global step: 1949,loss: 0.03386446\n",
            "\n",
            "Global step: 1950,loss: 0.026732806\n",
            "\n",
            "Global step: 1951,loss: 0.024756042\n",
            "\n",
            "Global step: 1952,loss: 0.02792226\n",
            "\n",
            "Global step: 1953,loss: 0.027574921\n",
            "\n",
            "Global step: 1954,loss: 0.023246573\n",
            "\n",
            "Global step: 1955,loss: 0.023258204\n",
            "\n",
            "Global step: 1956,loss: 0.029612279\n",
            "\n",
            "Global step: 1957,loss: 0.024132537\n",
            "\n",
            "Global step: 1958,loss: 0.03457604\n",
            "\n",
            "Global step: 1959,loss: 0.023243694\n",
            "\n",
            "Global step: 1960,loss: 0.022967743\n",
            "\n",
            "Global step: 1961,loss: 0.027066262\n",
            "\n",
            "Global step: 1962,loss: 0.025026469\n",
            "\n",
            "Global step: 1963,loss: 0.029598992\n",
            "\n",
            "Global step: 1964,loss: 0.023702666\n",
            "\n",
            "Global step: 1965,loss: 0.0245504\n",
            "\n",
            "Global step: 1966,loss: 0.024392478\n",
            "\n",
            "Global step: 1967,loss: 0.02394525\n",
            "\n",
            "Global step: 1968,loss: 0.024076877\n",
            "\n",
            "Global step: 1969,loss: 0.024392957\n",
            "\n",
            "Global step: 1970,loss: 0.024950763\n",
            "\n",
            "Global step: 1971,loss: 0.02167414\n",
            "\n",
            "Global step: 1972,loss: 0.02406805\n",
            "\n",
            "Global step: 1973,loss: 0.032265663\n",
            "\n",
            "Global step: 1974,loss: 0.023775367\n",
            "\n",
            "Global step: 1975,loss: 0.047444075\n",
            "\n",
            "Global step: 1976,loss: 0.024506968\n",
            "\n",
            "Global step: 1977,loss: 0.021722237\n",
            "\n",
            "Global step: 1978,loss: 0.023103427\n",
            "\n",
            "Global step: 1979,loss: 0.02284156\n",
            "\n",
            "Global step: 1980,loss: 0.025356583\n",
            "\n",
            "Global step: 1981,loss: 0.02531067\n",
            "\n",
            "Global step: 1982,loss: 0.024596887\n",
            "\n",
            "Global step: 1983,loss: 0.02409211\n",
            "\n",
            "Global step: 1984,loss: 0.027039394\n",
            "\n",
            "Global step: 1985,loss: 0.026833102\n",
            "\n",
            "Global step: 1986,loss: 0.023947245\n",
            "\n",
            "Global step: 1987,loss: 0.025715489\n",
            "\n",
            "Global step: 1988,loss: 0.031597123\n",
            "\n",
            "Global step: 1989,loss: 0.024252709\n",
            "\n",
            "Global step: 1990,loss: 0.032100156\n",
            "\n",
            "Global step: 1991,loss: 0.025644524\n",
            "\n",
            "Global step: 1992,loss: 0.025811877\n",
            "\n",
            "Global step: 1993,loss: 0.027582647\n",
            "\n",
            "Global step: 1994,loss: 0.023945242\n",
            "\n",
            "Global step: 1995,loss: 0.023997659\n",
            "\n",
            "Global step: 1996,loss: 0.024406128\n",
            "\n",
            "Global step: 1997,loss: 0.030157298\n",
            "\n",
            "Global step: 1998,loss: 0.027216624\n",
            "\n",
            "Global step: 1999,loss: 0.026985163\n",
            "\n",
            "Global step: 2000,loss: 0.024351932\n",
            "\n",
            "Global step: 2001,loss: 0.028290033\n",
            "\n",
            "Global step: 2002,loss: 0.027002338\n",
            "\n",
            "Global step: 2003,loss: 0.029160842\n",
            "\n",
            "Global step: 2004,loss: 0.025807835\n",
            "\n",
            "Global step: 2005,loss: 0.02385948\n",
            "\n",
            "Global step: 2006,loss: 0.024438743\n",
            "\n",
            "Global step: 2007,loss: 0.023825383\n",
            "\n",
            "Global step: 2008,loss: 0.025408117\n",
            "\n",
            "Global step: 2009,loss: 0.03267785\n",
            "\n",
            "Global step: 2010,loss: 0.023985762\n",
            "\n",
            "Global step: 2011,loss: 0.023661276\n",
            "\n",
            "Global step: 2012,loss: 0.022057056\n",
            "\n",
            "Global step: 2013,loss: 0.021983307\n",
            "\n",
            "Global step: 2014,loss: 0.02218645\n",
            "\n",
            "Global step: 2015,loss: 0.022696875\n",
            "\n",
            "Global step: 2016,loss: 0.026817717\n",
            "\n",
            "Global step: 2017,loss: 0.02349932\n",
            "\n",
            "Global step: 2018,loss: 0.02472575\n",
            "\n",
            "Global step: 2019,loss: 0.025413161\n",
            "\n",
            "Global step: 2020,loss: 0.026662484\n",
            "\n",
            "Global step: 2021,loss: 0.03505194\n",
            "\n",
            "Global step: 2022,loss: 0.025141671\n",
            "\n",
            "Global step: 2023,loss: 0.030587561\n",
            "\n",
            "Global step: 2024,loss: 0.034492485\n",
            "\n",
            "Global step: 2025,loss: 0.0351554\n",
            "\n",
            "Global step: 2026,loss: 0.028689712\n",
            "\n",
            "Global step: 2027,loss: 0.027826816\n",
            "\n",
            "Global step: 2028,loss: 0.026110355\n",
            "\n",
            "Global step: 2029,loss: 0.024040509\n",
            "\n",
            "Global step: 2030,loss: 0.022775259\n",
            "\n",
            "Global step: 2031,loss: 0.023905769\n",
            "\n",
            "Global step: 2032,loss: 0.022857573\n",
            "\n",
            "Global step: 2033,loss: 0.027639747\n",
            "\n",
            "Global step: 2034,loss: 0.025740243\n",
            "\n",
            "Global step: 2035,loss: 0.032067865\n",
            "\n",
            "Global step: 2036,loss: 0.027553655\n",
            "\n",
            "Global step: 2037,loss: 0.026196126\n",
            "\n",
            "Global step: 2038,loss: 0.024251122\n",
            "\n",
            "Global step: 2039,loss: 0.023181992\n",
            "\n",
            "Global step: 2040,loss: 0.025084712\n",
            "\n",
            "Global step: 2041,loss: 0.02470385\n",
            "\n",
            "Global step: 2042,loss: 0.029296232\n",
            "\n",
            "Global step: 2043,loss: 0.02113202\n",
            "\n",
            "Global step: 2044,loss: 0.024459997\n",
            "\n",
            "Global step: 2045,loss: 0.04228876\n",
            "\n",
            "Global step: 2046,loss: 0.023329008\n",
            "\n",
            "Global step: 2047,loss: 0.024681974\n",
            "\n",
            "Global step: 2048,loss: 0.024600431\n",
            "\n",
            "Global step: 2049,loss: 0.025317973\n",
            "\n",
            "Global step: 2050,loss: 0.043435857\n",
            "\n",
            "Global step: 2051,loss: 0.029939074\n",
            "\n",
            "Global step: 2052,loss: 0.023802638\n",
            "\n",
            "Global step: 2053,loss: 0.028836815\n",
            "\n",
            "Global step: 2054,loss: 0.04003294\n",
            "\n",
            "Global step: 2055,loss: 0.023036212\n",
            "\n",
            "Global step: 2056,loss: 0.02406831\n",
            "\n",
            "Global step: 2057,loss: 0.025837816\n",
            "\n",
            "Global step: 2058,loss: 0.025292343\n",
            "\n",
            "Global step: 2059,loss: 0.029984396\n",
            "\n",
            "Global step: 2060,loss: 0.022197586\n",
            "\n",
            "Global step: 2061,loss: 0.024699993\n",
            "\n",
            "Global step: 2062,loss: 0.024463898\n",
            "\n",
            "Global step: 2063,loss: 0.021338742\n",
            "\n",
            "Global step: 2064,loss: 0.029254427\n",
            "\n",
            "Global step: 2065,loss: 0.03212325\n",
            "\n",
            "Global step: 2066,loss: 0.022642698\n",
            "\n",
            "Global step: 2067,loss: 0.024494939\n",
            "\n",
            "Global step: 2068,loss: 0.030253924\n",
            "\n",
            "Global step: 2069,loss: 0.026072731\n",
            "\n",
            "Global step: 2070,loss: 0.024319999\n",
            "\n",
            "Global step: 2071,loss: 0.02463046\n",
            "\n",
            "Global step: 2072,loss: 0.024010602\n",
            "\n",
            "Global step: 2073,loss: 0.030155474\n",
            "\n",
            "Global step: 2074,loss: 0.02947232\n",
            "\n",
            "Global step: 2075,loss: 0.031313103\n",
            "\n",
            "Global step: 2076,loss: 0.024439737\n",
            "\n",
            "Global step: 2077,loss: 0.025644664\n",
            "\n",
            "Global step: 2078,loss: 0.027222514\n",
            "\n",
            "Global step: 2079,loss: 0.026134402\n",
            "\n",
            "Global step: 2080,loss: 0.022820808\n",
            "\n",
            "Global step: 2081,loss: 0.0210512\n",
            "\n",
            "Global step: 2082,loss: 0.022700801\n",
            "\n",
            "Global step: 2083,loss: 0.02466458\n",
            "\n",
            "Global step: 2084,loss: 0.024812095\n",
            "\n",
            "Global step: 2085,loss: 0.023103518\n",
            "\n",
            "Global step: 2086,loss: 0.022867156\n",
            "\n",
            "Global step: 2087,loss: 0.02883525\n",
            "\n",
            "Global step: 2088,loss: 0.023568038\n",
            "\n",
            "Global step: 2089,loss: 0.024873197\n",
            "\n",
            "Global step: 2090,loss: 0.022239417\n",
            "\n",
            "Global step: 2091,loss: 0.021622255\n",
            "\n",
            "Global step: 2092,loss: 0.023351103\n",
            "\n",
            "Global step: 2093,loss: 0.02374589\n",
            "\n",
            "Global step: 2094,loss: 0.022276975\n",
            "\n",
            "Global step: 2095,loss: 0.02875626\n",
            "\n",
            "Global step: 2096,loss: 0.025949337\n",
            "\n",
            "Global step: 2097,loss: 0.024532527\n",
            "\n",
            "Global step: 2098,loss: 0.026491757\n",
            "\n",
            "Global step: 2099,loss: 0.024987487\n",
            "\n",
            "Global step: 2100,loss: 0.02386698\n",
            "\n",
            "Global step: 2101,loss: 0.022294018\n",
            "\n",
            "Global step: 2102,loss: 0.026824914\n",
            "\n",
            "Global step: 2103,loss: 0.02423356\n",
            "\n",
            "Global step: 2104,loss: 0.028315527\n",
            "\n",
            "Global step: 2105,loss: 0.025392639\n",
            "\n",
            "Global step: 2106,loss: 0.027881887\n",
            "\n",
            "Global step: 2107,loss: 0.027166735\n",
            "\n",
            "Global step: 2108,loss: 0.023148306\n",
            "\n",
            "Global step: 2109,loss: 0.023692872\n",
            "\n",
            "Global step: 2110,loss: 0.03077028\n",
            "\n",
            "Global step: 2111,loss: 0.023992961\n",
            "\n",
            "Global step: 2112,loss: 0.03912519\n",
            "\n",
            "Global step: 2113,loss: 0.03572847\n",
            "\n",
            "Global step: 2114,loss: 0.030089658\n",
            "\n",
            "Global step: 2115,loss: 0.026039582\n",
            "\n",
            "Global step: 2116,loss: 0.022127343\n",
            "\n",
            "Global step: 2117,loss: 0.022834951\n",
            "\n",
            "Global step: 2118,loss: 0.02399088\n",
            "\n",
            "Global step: 2119,loss: 0.022404773\n",
            "\n",
            "Global step: 2120,loss: 0.033500995\n",
            "\n",
            "Global step: 2121,loss: 0.021933008\n",
            "\n",
            "Global step: 2122,loss: 0.02462471\n",
            "\n",
            "Global step: 2123,loss: 0.025594676\n",
            "\n",
            "Global step: 2124,loss: 0.02587238\n",
            "\n",
            "Global step: 2125,loss: 0.026254484\n",
            "\n",
            "Global step: 2126,loss: 0.027824383\n",
            "\n",
            "Global step: 2127,loss: 0.029747041\n",
            "\n",
            "Global step: 2128,loss: 0.02237126\n",
            "\n",
            "Global step: 2129,loss: 0.031008646\n",
            "\n",
            "Global step: 2130,loss: 0.029183064\n",
            "\n",
            "Global step: 2131,loss: 0.021797834\n",
            "\n",
            "Global step: 2132,loss: 0.029849561\n",
            "\n",
            "Global step: 2133,loss: 0.024083845\n",
            "\n",
            "Global step: 2134,loss: 0.023646615\n",
            "\n",
            "Global step: 2135,loss: 0.027001992\n",
            "\n",
            "Global step: 2136,loss: 0.022115061\n",
            "\n",
            "Global step: 2137,loss: 0.026094109\n",
            "\n",
            "Global step: 2138,loss: 0.023507224\n",
            "\n",
            "Global step: 2139,loss: 0.021563228\n",
            "\n",
            "Global step: 2140,loss: 0.023540175\n",
            "\n",
            "Global step: 2141,loss: 0.030896582\n",
            "\n",
            "Global step: 2142,loss: 0.025938435\n",
            "\n",
            "Global step: 2143,loss: 0.025944006\n",
            "\n",
            "Global step: 2144,loss: 0.02754631\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2144,Val_Loss: 0.02636178606786789,  Val_acc: 0.9973958333333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:24:11.990751 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/15:\n",
            "Global step: 2145,loss: 0.02377498\n",
            "\n",
            "Global step: 2146,loss: 0.026813187\n",
            "\n",
            "Global step: 2147,loss: 0.0252806\n",
            "\n",
            "Global step: 2148,loss: 0.027124338\n",
            "\n",
            "Global step: 2149,loss: 0.021736149\n",
            "\n",
            "Global step: 2150,loss: 0.024027899\n",
            "\n",
            "Global step: 2151,loss: 0.035792317\n",
            "\n",
            "Global step: 2152,loss: 0.02774093\n",
            "\n",
            "Global step: 2153,loss: 0.022624098\n",
            "\n",
            "Global step: 2154,loss: 0.022302216\n",
            "\n",
            "Global step: 2155,loss: 0.022433339\n",
            "\n",
            "Global step: 2156,loss: 0.023326647\n",
            "\n",
            "Global step: 2157,loss: 0.022069648\n",
            "\n",
            "Global step: 2158,loss: 0.029195517\n",
            "\n",
            "Global step: 2159,loss: 0.022926984\n",
            "\n",
            "Global step: 2160,loss: 0.02529287\n",
            "\n",
            "Global step: 2161,loss: 0.022854492\n",
            "\n",
            "Global step: 2162,loss: 0.02350411\n",
            "\n",
            "Global step: 2163,loss: 0.023634085\n",
            "\n",
            "Global step: 2164,loss: 0.021610588\n",
            "\n",
            "Global step: 2165,loss: 0.028823785\n",
            "\n",
            "Global step: 2166,loss: 0.022403678\n",
            "\n",
            "Global step: 2167,loss: 0.022872513\n",
            "\n",
            "Global step: 2168,loss: 0.022290992\n",
            "\n",
            "Global step: 2169,loss: 0.023399279\n",
            "\n",
            "Global step: 2170,loss: 0.02523047\n",
            "\n",
            "Global step: 2171,loss: 0.027285751\n",
            "\n",
            "Global step: 2172,loss: 0.029073138\n",
            "\n",
            "Global step: 2173,loss: 0.02842219\n",
            "\n",
            "Global step: 2174,loss: 0.021619765\n",
            "\n",
            "Global step: 2175,loss: 0.023944942\n",
            "\n",
            "Global step: 2176,loss: 0.02783909\n",
            "\n",
            "Global step: 2177,loss: 0.021715872\n",
            "\n",
            "Global step: 2178,loss: 0.02215042\n",
            "\n",
            "Global step: 2179,loss: 0.03286984\n",
            "\n",
            "Global step: 2180,loss: 0.02387183\n",
            "\n",
            "Global step: 2181,loss: 0.024772944\n",
            "\n",
            "Global step: 2182,loss: 0.023005724\n",
            "\n",
            "Global step: 2183,loss: 0.021714728\n",
            "\n",
            "Global step: 2184,loss: 0.021668218\n",
            "\n",
            "Global step: 2185,loss: 0.030835774\n",
            "\n",
            "Global step: 2186,loss: 0.02241725\n",
            "\n",
            "Global step: 2187,loss: 0.022345874\n",
            "\n",
            "Global step: 2188,loss: 0.022639815\n",
            "\n",
            "Global step: 2189,loss: 0.02294489\n",
            "\n",
            "Global step: 2190,loss: 0.022962356\n",
            "\n",
            "Global step: 2191,loss: 0.022138653\n",
            "\n",
            "Global step: 2192,loss: 0.024498124\n",
            "\n",
            "Global step: 2193,loss: 0.024854824\n",
            "\n",
            "Global step: 2194,loss: 0.023674596\n",
            "\n",
            "Global step: 2195,loss: 0.024234097\n",
            "\n",
            "Global step: 2196,loss: 0.02143902\n",
            "\n",
            "Global step: 2197,loss: 0.023548692\n",
            "\n",
            "Global step: 2198,loss: 0.022875477\n",
            "\n",
            "Global step: 2199,loss: 0.02277119\n",
            "\n",
            "Global step: 2200,loss: 0.022048946\n",
            "\n",
            "Global step: 2201,loss: 0.025345664\n",
            "\n",
            "Global step: 2202,loss: 0.02676226\n",
            "\n",
            "Global step: 2203,loss: 0.02300336\n",
            "\n",
            "Global step: 2204,loss: 0.023507798\n",
            "\n",
            "Global step: 2205,loss: 0.020764988\n",
            "\n",
            "Global step: 2206,loss: 0.023212556\n",
            "\n",
            "Global step: 2207,loss: 0.026352132\n",
            "\n",
            "Global step: 2208,loss: 0.0255204\n",
            "\n",
            "Global step: 2209,loss: 0.021959415\n",
            "\n",
            "Global step: 2210,loss: 0.024143882\n",
            "\n",
            "Global step: 2211,loss: 0.022888733\n",
            "\n",
            "Global step: 2212,loss: 0.022143869\n",
            "\n",
            "Global step: 2213,loss: 0.022912843\n",
            "\n",
            "Global step: 2214,loss: 0.023989113\n",
            "\n",
            "Global step: 2215,loss: 0.024922032\n",
            "\n",
            "Global step: 2216,loss: 0.022364572\n",
            "\n",
            "Global step: 2217,loss: 0.027615283\n",
            "\n",
            "Global step: 2218,loss: 0.02270493\n",
            "\n",
            "Global step: 2219,loss: 0.02102474\n",
            "\n",
            "Global step: 2220,loss: 0.027652353\n",
            "\n",
            "Global step: 2221,loss: 0.022007503\n",
            "\n",
            "Global step: 2222,loss: 0.026064448\n",
            "\n",
            "Global step: 2223,loss: 0.0356047\n",
            "\n",
            "Global step: 2224,loss: 0.031504124\n",
            "\n",
            "Global step: 2225,loss: 0.022003748\n",
            "\n",
            "Global step: 2226,loss: 0.025807777\n",
            "\n",
            "Global step: 2227,loss: 0.021860491\n",
            "\n",
            "Global step: 2228,loss: 0.0226791\n",
            "\n",
            "Global step: 2229,loss: 0.021010213\n",
            "\n",
            "Global step: 2230,loss: 0.021129537\n",
            "\n",
            "Global step: 2231,loss: 0.022275347\n",
            "\n",
            "Global step: 2232,loss: 0.026854476\n",
            "\n",
            "Global step: 2233,loss: 0.025862431\n",
            "\n",
            "Global step: 2234,loss: 0.024503611\n",
            "\n",
            "Global step: 2235,loss: 0.021391528\n",
            "\n",
            "Global step: 2236,loss: 0.021310814\n",
            "\n",
            "Global step: 2237,loss: 0.022767901\n",
            "\n",
            "Global step: 2238,loss: 0.022503478\n",
            "\n",
            "Global step: 2239,loss: 0.027024055\n",
            "\n",
            "Global step: 2240,loss: 0.022736352\n",
            "\n",
            "Global step: 2241,loss: 0.023859788\n",
            "\n",
            "Global step: 2242,loss: 0.023425322\n",
            "\n",
            "Global step: 2243,loss: 0.022869455\n",
            "\n",
            "Global step: 2244,loss: 0.024240572\n",
            "\n",
            "Global step: 2245,loss: 0.031095952\n",
            "\n",
            "Global step: 2246,loss: 0.022202939\n",
            "\n",
            "Global step: 2247,loss: 0.02110783\n",
            "\n",
            "Global step: 2248,loss: 0.02253648\n",
            "\n",
            "Global step: 2249,loss: 0.022213235\n",
            "\n",
            "Global step: 2250,loss: 0.021768704\n",
            "\n",
            "Global step: 2251,loss: 0.033057235\n",
            "\n",
            "Global step: 2252,loss: 0.02375495\n",
            "\n",
            "Global step: 2253,loss: 0.023639943\n",
            "\n",
            "Global step: 2254,loss: 0.022594232\n",
            "\n",
            "Global step: 2255,loss: 0.025544276\n",
            "\n",
            "Global step: 2256,loss: 0.024692308\n",
            "\n",
            "Global step: 2257,loss: 0.023675578\n",
            "\n",
            "Global step: 2258,loss: 0.022087976\n",
            "\n",
            "Global step: 2259,loss: 0.021694552\n",
            "\n",
            "Global step: 2260,loss: 0.021862512\n",
            "\n",
            "Global step: 2261,loss: 0.022968827\n",
            "\n",
            "Global step: 2262,loss: 0.021174489\n",
            "\n",
            "Global step: 2263,loss: 0.027565688\n",
            "\n",
            "Global step: 2264,loss: 0.02657834\n",
            "\n",
            "Global step: 2265,loss: 0.02116587\n",
            "\n",
            "Global step: 2266,loss: 0.02292408\n",
            "\n",
            "Global step: 2267,loss: 0.022678986\n",
            "\n",
            "Global step: 2268,loss: 0.022424854\n",
            "\n",
            "Global step: 2269,loss: 0.024090502\n",
            "\n",
            "Global step: 2270,loss: 0.021078454\n",
            "\n",
            "Global step: 2271,loss: 0.02176221\n",
            "\n",
            "Global step: 2272,loss: 0.03311255\n",
            "\n",
            "Global step: 2273,loss: 0.022103269\n",
            "\n",
            "Global step: 2274,loss: 0.02286795\n",
            "\n",
            "Global step: 2275,loss: 0.025105562\n",
            "\n",
            "Global step: 2276,loss: 0.022847315\n",
            "\n",
            "Global step: 2277,loss: 0.02109894\n",
            "\n",
            "Global step: 2278,loss: 0.021913981\n",
            "\n",
            "Global step: 2279,loss: 0.026252387\n",
            "\n",
            "Global step: 2280,loss: 0.02399896\n",
            "\n",
            "Global step: 2281,loss: 0.022728644\n",
            "\n",
            "Global step: 2282,loss: 0.02311485\n",
            "\n",
            "Global step: 2283,loss: 0.021541277\n",
            "\n",
            "Global step: 2284,loss: 0.021377273\n",
            "\n",
            "Global step: 2285,loss: 0.020787455\n",
            "\n",
            "Global step: 2286,loss: 0.023950819\n",
            "\n",
            "Global step: 2287,loss: 0.021731833\n",
            "\n",
            "Global step: 2288,loss: 0.021573715\n",
            "\n",
            "Global step: 2289,loss: 0.026679086\n",
            "\n",
            "Global step: 2290,loss: 0.022968752\n",
            "\n",
            "Global step: 2291,loss: 0.023001797\n",
            "\n",
            "Global step: 2292,loss: 0.022772677\n",
            "\n",
            "Global step: 2293,loss: 0.03151646\n",
            "\n",
            "Global step: 2294,loss: 0.021199973\n",
            "\n",
            "Global step: 2295,loss: 0.034105383\n",
            "\n",
            "Global step: 2296,loss: 0.023224756\n",
            "\n",
            "Global step: 2297,loss: 0.023769142\n",
            "\n",
            "Global step: 2298,loss: 0.022620484\n",
            "\n",
            "Global step: 2299,loss: 0.021881102\n",
            "\n",
            "Global step: 2300,loss: 0.022972407\n",
            "\n",
            "Global step: 2301,loss: 0.026511246\n",
            "\n",
            "Global step: 2302,loss: 0.02333553\n",
            "\n",
            "Global step: 2303,loss: 0.023854539\n",
            "\n",
            "Global step: 2304,loss: 0.022637615\n",
            "\n",
            "Global step: 2305,loss: 0.029517706\n",
            "\n",
            "Global step: 2306,loss: 0.022063995\n",
            "\n",
            "Global step: 2307,loss: 0.031629823\n",
            "\n",
            "Global step: 2308,loss: 0.02500522\n",
            "\n",
            "Global step: 2309,loss: 0.023780208\n",
            "\n",
            "Global step: 2310,loss: 0.02564382\n",
            "\n",
            "Global step: 2311,loss: 0.022517316\n",
            "\n",
            "Global step: 2312,loss: 0.021375112\n",
            "\n",
            "Global step: 2313,loss: 0.024923196\n",
            "\n",
            "Global step: 2314,loss: 0.028076705\n",
            "\n",
            "Global step: 2315,loss: 0.020448873\n",
            "\n",
            "Global step: 2316,loss: 0.02517802\n",
            "\n",
            "Global step: 2317,loss: 0.028136967\n",
            "\n",
            "Global step: 2318,loss: 0.02227117\n",
            "\n",
            "Global step: 2319,loss: 0.026004523\n",
            "\n",
            "Global step: 2320,loss: 0.022569476\n",
            "\n",
            "Global step: 2321,loss: 0.021951724\n",
            "\n",
            "Global step: 2322,loss: 0.021634234\n",
            "\n",
            "Global step: 2323,loss: 0.021032743\n",
            "\n",
            "Global step: 2324,loss: 0.02086273\n",
            "\n",
            "Global step: 2325,loss: 0.02427985\n",
            "\n",
            "Global step: 2326,loss: 0.022575228\n",
            "\n",
            "Global step: 2327,loss: 0.021992113\n",
            "\n",
            "Global step: 2328,loss: 0.022034284\n",
            "\n",
            "Global step: 2329,loss: 0.022331847\n",
            "\n",
            "Global step: 2330,loss: 0.021496715\n",
            "\n",
            "Global step: 2331,loss: 0.023142414\n",
            "\n",
            "Global step: 2332,loss: 0.023699893\n",
            "\n",
            "Global step: 2333,loss: 0.028324028\n",
            "\n",
            "Global step: 2334,loss: 0.020201676\n",
            "\n",
            "Global step: 2335,loss: 0.023053728\n",
            "\n",
            "Global step: 2336,loss: 0.030654583\n",
            "\n",
            "Global step: 2337,loss: 0.025104959\n",
            "\n",
            "Global step: 2338,loss: 0.027603863\n",
            "\n",
            "Global step: 2339,loss: 0.0352504\n",
            "\n",
            "Global step: 2340,loss: 0.024109056\n",
            "\n",
            "Global step: 2341,loss: 0.021634199\n",
            "\n",
            "Global step: 2342,loss: 0.021887936\n",
            "\n",
            "Global step: 2343,loss: 0.028397053\n",
            "\n",
            "Global step: 2344,loss: 0.020312116\n",
            "\n",
            "Global step: 2345,loss: 0.023290541\n",
            "\n",
            "Global step: 2346,loss: 0.021403268\n",
            "\n",
            "Global step: 2347,loss: 0.023058934\n",
            "\n",
            "Global step: 2348,loss: 0.025488012\n",
            "\n",
            "Global step: 2349,loss: 0.0242907\n",
            "\n",
            "Global step: 2350,loss: 0.025837502\n",
            "\n",
            "Global step: 2351,loss: 0.022919226\n",
            "\n",
            "Global step: 2352,loss: 0.02208689\n",
            "\n",
            "Global step: 2353,loss: 0.022179952\n",
            "\n",
            "Global step: 2354,loss: 0.022813624\n",
            "\n",
            "Global step: 2355,loss: 0.023810934\n",
            "\n",
            "Global step: 2356,loss: 0.028986886\n",
            "\n",
            "Global step: 2357,loss: 0.027718324\n",
            "\n",
            "Global step: 2358,loss: 0.027857326\n",
            "\n",
            "Global step: 2359,loss: 0.025660954\n",
            "\n",
            "Global step: 2360,loss: 0.0251644\n",
            "\n",
            "Global step: 2361,loss: 0.021817392\n",
            "\n",
            "Global step: 2362,loss: 0.020670768\n",
            "\n",
            "Global step: 2363,loss: 0.034602195\n",
            "\n",
            "Global step: 2364,loss: 0.02050561\n",
            "\n",
            "Global step: 2365,loss: 0.022425663\n",
            "\n",
            "Global step: 2366,loss: 0.02246603\n",
            "\n",
            "Global step: 2367,loss: 0.02441133\n",
            "\n",
            "Global step: 2368,loss: 0.026458472\n",
            "\n",
            "Global step: 2369,loss: 0.02072755\n",
            "\n",
            "Global step: 2370,loss: 0.02161482\n",
            "\n",
            "Global step: 2371,loss: 0.021805877\n",
            "\n",
            "Global step: 2372,loss: 0.02514842\n",
            "\n",
            "Global step: 2373,loss: 0.022762991\n",
            "\n",
            "Global step: 2374,loss: 0.022522008\n",
            "\n",
            "Global step: 2375,loss: 0.021192705\n",
            "\n",
            "Global step: 2376,loss: 0.022091351\n",
            "\n",
            "Global step: 2377,loss: 0.027248433\n",
            "\n",
            "Global step: 2378,loss: 0.022349924\n",
            "\n",
            "Global step: 2379,loss: 0.021099102\n",
            "\n",
            "Global step: 2380,loss: 0.02391011\n",
            "\n",
            "Global step: 2381,loss: 0.028383467\n",
            "\n",
            "Global step: 2382,loss: 0.023371994\n",
            "\n",
            "Global step: 2383,loss: 0.021784557\n",
            "\n",
            "Global step: 2384,loss: 0.022079166\n",
            "\n",
            "Global step: 2385,loss: 0.02759744\n",
            "\n",
            "Global step: 2386,loss: 0.021048423\n",
            "\n",
            "Global step: 2387,loss: 0.02064379\n",
            "\n",
            "Global step: 2388,loss: 0.025439277\n",
            "\n",
            "Global step: 2389,loss: 0.020910362\n",
            "\n",
            "Global step: 2390,loss: 0.02251655\n",
            "\n",
            "Global step: 2391,loss: 0.022028143\n",
            "\n",
            "Global step: 2392,loss: 0.023547743\n",
            "\n",
            "Global step: 2393,loss: 0.026113106\n",
            "\n",
            "Global step: 2394,loss: 0.02348039\n",
            "\n",
            "Global step: 2395,loss: 0.021767862\n",
            "\n",
            "Global step: 2396,loss: 0.024668006\n",
            "\n",
            "Global step: 2397,loss: 0.022045508\n",
            "\n",
            "Global step: 2398,loss: 0.024298128\n",
            "\n",
            "Global step: 2399,loss: 0.024294823\n",
            "\n",
            "Global step: 2400,loss: 0.024547037\n",
            "\n",
            "Global step: 2401,loss: 0.022088014\n",
            "\n",
            "Global step: 2402,loss: 0.021994252\n",
            "\n",
            "Global step: 2403,loss: 0.02283187\n",
            "\n",
            "Global step: 2404,loss: 0.02284235\n",
            "\n",
            "Global step: 2405,loss: 0.026790455\n",
            "\n",
            "Global step: 2406,loss: 0.020096865\n",
            "\n",
            "Global step: 2407,loss: 0.022323262\n",
            "\n",
            "Global step: 2408,loss: 0.02509124\n",
            "\n",
            "Global step: 2409,loss: 0.022643186\n",
            "\n",
            "Global step: 2410,loss: 0.021248857\n",
            "\n",
            "Global step: 2411,loss: 0.024774652\n",
            "\n",
            "Global step: 2412,loss: 0.02148836\n",
            "\n",
            "Global step: 2413,loss: 0.022699647\n",
            "\n",
            "Global step: 2414,loss: 0.0226848\n",
            "\n",
            "Global step: 2415,loss: 0.02322955\n",
            "\n",
            "Global step: 2416,loss: 0.02297786\n",
            "\n",
            "Global step: 2417,loss: 0.022510702\n",
            "\n",
            "Global step: 2418,loss: 0.02226618\n",
            "\n",
            "Global step: 2419,loss: 0.024813084\n",
            "\n",
            "Global step: 2420,loss: 0.022263225\n",
            "\n",
            "Global step: 2421,loss: 0.022043861\n",
            "\n",
            "Global step: 2422,loss: 0.02407257\n",
            "\n",
            "Global step: 2423,loss: 0.020589726\n",
            "\n",
            "Global step: 2424,loss: 0.036380272\n",
            "\n",
            "Global step: 2425,loss: 0.023311581\n",
            "\n",
            "Global step: 2426,loss: 0.031991534\n",
            "\n",
            "Global step: 2427,loss: 0.024025127\n",
            "\n",
            "Global step: 2428,loss: 0.029293723\n",
            "\n",
            "Global step: 2429,loss: 0.025003724\n",
            "\n",
            "Global step: 2430,loss: 0.030162245\n",
            "\n",
            "Global step: 2431,loss: 0.029473392\n",
            "\n",
            "Global step: 2432,loss: 0.021065878\n",
            "\n",
            "Global step: 2433,loss: 0.025794484\n",
            "\n",
            "Global step: 2434,loss: 0.02273509\n",
            "\n",
            "Global step: 2435,loss: 0.02390539\n",
            "\n",
            "Global step: 2436,loss: 0.02166609\n",
            "\n",
            "Global step: 2437,loss: 0.020534389\n",
            "\n",
            "Global step: 2438,loss: 0.021074368\n",
            "\n",
            "Global step: 2439,loss: 0.027685702\n",
            "\n",
            "Global step: 2440,loss: 0.020638341\n",
            "\n",
            "Global step: 2441,loss: 0.021177273\n",
            "\n",
            "Global step: 2442,loss: 0.021667732\n",
            "\n",
            "Global step: 2443,loss: 0.023010235\n",
            "\n",
            "Global step: 2444,loss: 0.021507477\n",
            "\n",
            "Global step: 2445,loss: 0.02230409\n",
            "\n",
            "Global step: 2446,loss: 0.02717319\n",
            "\n",
            "Global step: 2447,loss: 0.025865596\n",
            "\n",
            "Global step: 2448,loss: 0.028182264\n",
            "\n",
            "Global step: 2449,loss: 0.024683133\n",
            "\n",
            "Global step: 2450,loss: 0.023381049\n",
            "\n",
            "Global step: 2451,loss: 0.024678944\n",
            "\n",
            "Global step: 2452,loss: 0.021913538\n",
            "\n",
            "Global step: 2453,loss: 0.020256445\n",
            "\n",
            "Global step: 2454,loss: 0.025405247\n",
            "\n",
            "Global step: 2455,loss: 0.032559328\n",
            "\n",
            "Global step: 2456,loss: 0.029568758\n",
            "\n",
            "Global step: 2457,loss: 0.022599911\n",
            "\n",
            "Global step: 2458,loss: 0.02237175\n",
            "\n",
            "Global step: 2459,loss: 0.024341792\n",
            "\n",
            "Global step: 2460,loss: 0.022248771\n",
            "\n",
            "Global step: 2461,loss: 0.021288337\n",
            "\n",
            "Global step: 2462,loss: 0.023422357\n",
            "\n",
            "Global step: 2463,loss: 0.027005387\n",
            "\n",
            "Global step: 2464,loss: 0.021231687\n",
            "\n",
            "Global step: 2465,loss: 0.022644842\n",
            "\n",
            "Global step: 2466,loss: 0.024420612\n",
            "\n",
            "Global step: 2467,loss: 0.020792672\n",
            "\n",
            "Global step: 2468,loss: 0.023579562\n",
            "\n",
            "Global step: 2469,loss: 0.021668661\n",
            "\n",
            "Global step: 2470,loss: 0.023100708\n",
            "\n",
            "Global step: 2471,loss: 0.021681825\n",
            "\n",
            "Global step: 2472,loss: 0.022021042\n",
            "\n",
            "Global step: 2473,loss: 0.024157818\n",
            "\n",
            "Global step: 2474,loss: 0.02615086\n",
            "\n",
            "Global step: 2475,loss: 0.023514023\n",
            "\n",
            "Global step: 2476,loss: 0.022679476\n",
            "\n",
            "Global step: 2477,loss: 0.025881799\n",
            "\n",
            "Global step: 2478,loss: 0.020279273\n",
            "\n",
            "Global step: 2479,loss: 0.025655212\n",
            "\n",
            "Global step: 2480,loss: 0.022375394\n",
            "\n",
            "Global step: 2481,loss: 0.022562534\n",
            "\n",
            "Global step: 2482,loss: 0.021680433\n",
            "\n",
            "Global step: 2483,loss: 0.020700911\n",
            "\n",
            "Global step: 2484,loss: 0.02715334\n",
            "\n",
            "Global step: 2485,loss: 0.027579451\n",
            "\n",
            "Global step: 2486,loss: 0.022739392\n",
            "\n",
            "Global step: 2487,loss: 0.02390584\n",
            "\n",
            "Global step: 2488,loss: 0.02113028\n",
            "\n",
            "Global step: 2489,loss: 0.020862155\n",
            "\n",
            "Global step: 2490,loss: 0.022185404\n",
            "\n",
            "Global step: 2491,loss: 0.022189448\n",
            "\n",
            "Global step: 2492,loss: 0.029585391\n",
            "\n",
            "Global step: 2493,loss: 0.021634497\n",
            "\n",
            "Global step: 2494,loss: 0.02151891\n",
            "\n",
            "Global step: 2495,loss: 0.021432688\n",
            "\n",
            "Global step: 2496,loss: 0.0228739\n",
            "\n",
            "Global step: 2497,loss: 0.030336127\n",
            "\n",
            "Global step: 2498,loss: 0.023391511\n",
            "\n",
            "Global step: 2499,loss: 0.02480349\n",
            "\n",
            "Global step: 2500,loss: 0.022042505\n",
            "\n",
            "Global step: 2501,loss: 0.023556426\n",
            "\n",
            "Global step: 2502,loss: 0.024664696\n",
            "\n",
            "Global step: 2503,loss: 0.020810494\n",
            "\n",
            "Global step: 2504,loss: 0.023699865\n",
            "\n",
            "Global step: 2505,loss: 0.02162802\n",
            "\n",
            "Global step: 2506,loss: 0.021847628\n",
            "\n",
            "Global step: 2507,loss: 0.031378005\n",
            "\n",
            "Global step: 2508,loss: 0.021170277\n",
            "\n",
            "Global step: 2509,loss: 0.021499751\n",
            "\n",
            "Global step: 2510,loss: 0.022777015\n",
            "\n",
            "Global step: 2511,loss: 0.01991477\n",
            "\n",
            "Global step: 2512,loss: 0.020505061\n",
            "\n",
            "Global step: 2513,loss: 0.021436725\n",
            "\n",
            "Global step: 2514,loss: 0.021571554\n",
            "\n",
            "Global step: 2515,loss: 0.02123253\n",
            "\n",
            "Global step: 2516,loss: 0.023712542\n",
            "\n",
            "Global step: 2517,loss: 0.029958602\n",
            "\n",
            "Global step: 2518,loss: 0.022548687\n",
            "\n",
            "Global step: 2519,loss: 0.023918169\n",
            "\n",
            "Global step: 2520,loss: 0.02952771\n",
            "\n",
            "Global step: 2521,loss: 0.024565334\n",
            "\n",
            "Global step: 2522,loss: 0.030278478\n",
            "\n",
            "Global step: 2523,loss: 0.02331402\n",
            "\n",
            "Global step: 2524,loss: 0.02797347\n",
            "\n",
            "Global step: 2525,loss: 0.0219029\n",
            "\n",
            "Global step: 2526,loss: 0.026297316\n",
            "\n",
            "Global step: 2527,loss: 0.022880947\n",
            "\n",
            "Global step: 2528,loss: 0.022886554\n",
            "\n",
            "Global step: 2529,loss: 0.024477778\n",
            "\n",
            "Global step: 2530,loss: 0.03295245\n",
            "\n",
            "Global step: 2531,loss: 0.02954574\n",
            "\n",
            "Global step: 2532,loss: 0.021465233\n",
            "\n",
            "Global step: 2533,loss: 0.022082612\n",
            "\n",
            "Global step: 2534,loss: 0.03051459\n",
            "\n",
            "Global step: 2535,loss: 0.024288313\n",
            "\n",
            "Global step: 2536,loss: 0.033539888\n",
            "\n",
            "Global step: 2537,loss: 0.021043682\n",
            "\n",
            "Global step: 2538,loss: 0.02399456\n",
            "\n",
            "Global step: 2539,loss: 0.03239063\n",
            "\n",
            "Global step: 2540,loss: 0.02524507\n",
            "\n",
            "Global step: 2541,loss: 0.023133013\n",
            "\n",
            "Global step: 2542,loss: 0.022474423\n",
            "\n",
            "Global step: 2543,loss: 0.02329177\n",
            "\n",
            "Global step: 2544,loss: 0.025814075\n",
            "\n",
            "Global step: 2545,loss: 0.030510724\n",
            "\n",
            "Global step: 2546,loss: 0.022558948\n",
            "\n",
            "Global step: 2547,loss: 0.029381853\n",
            "\n",
            "Global step: 2548,loss: 0.035963036\n",
            "\n",
            "Global step: 2549,loss: 0.02453902\n",
            "\n",
            "Global step: 2550,loss: 0.025769006\n",
            "\n",
            "Global step: 2551,loss: 0.02304416\n",
            "\n",
            "Global step: 2552,loss: 0.023465326\n",
            "\n",
            "Global step: 2553,loss: 0.027932532\n",
            "\n",
            "Global step: 2554,loss: 0.0240201\n",
            "\n",
            "Global step: 2555,loss: 0.02280495\n",
            "\n",
            "Global step: 2556,loss: 0.021382375\n",
            "\n",
            "Global step: 2557,loss: 0.022004083\n",
            "\n",
            "Global step: 2558,loss: 0.027596697\n",
            "\n",
            "Global step: 2559,loss: 0.021387976\n",
            "\n",
            "Global step: 2560,loss: 0.020772606\n",
            "\n",
            "Global step: 2561,loss: 0.023370637\n",
            "\n",
            "Global step: 2562,loss: 0.023468014\n",
            "\n",
            "Global step: 2563,loss: 0.02146476\n",
            "\n",
            "Global step: 2564,loss: 0.02153535\n",
            "\n",
            "Global step: 2565,loss: 0.0207597\n",
            "\n",
            "Global step: 2566,loss: 0.022988318\n",
            "\n",
            "Global step: 2567,loss: 0.022649124\n",
            "\n",
            "Global step: 2568,loss: 0.023560468\n",
            "\n",
            "Global step: 2569,loss: 0.026912786\n",
            "\n",
            "Global step: 2570,loss: 0.02092595\n",
            "\n",
            "Global step: 2571,loss: 0.021193154\n",
            "\n",
            "Global step: 2572,loss: 0.022697397\n",
            "\n",
            "Global step: 2573,loss: 0.022271384\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 2573,Val_Loss: 0.025306872211587735,  Val_acc: 0.9971955128205128 Improved\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:25:07.750392 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:25:08.283751 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 7/15:\n",
            "Global step: 2574,loss: 0.027890708\n",
            "\n",
            "Global step: 2575,loss: 0.021231614\n",
            "\n",
            "Global step: 2576,loss: 0.02717496\n",
            "\n",
            "Global step: 2577,loss: 0.023006378\n",
            "\n",
            "Global step: 2578,loss: 0.02587496\n",
            "\n",
            "Global step: 2579,loss: 0.021294963\n",
            "\n",
            "Global step: 2580,loss: 0.027320763\n",
            "\n",
            "Global step: 2581,loss: 0.02348739\n",
            "\n",
            "Global step: 2582,loss: 0.021131901\n",
            "\n",
            "Global step: 2583,loss: 0.022364331\n",
            "\n",
            "Global step: 2584,loss: 0.023424227\n",
            "\n",
            "Global step: 2585,loss: 0.027324555\n",
            "\n",
            "Global step: 2586,loss: 0.03295768\n",
            "\n",
            "Global step: 2587,loss: 0.019513888\n",
            "\n",
            "Global step: 2588,loss: 0.021201385\n",
            "\n",
            "Global step: 2589,loss: 0.026072245\n",
            "\n",
            "Global step: 2590,loss: 0.025422834\n",
            "\n",
            "Global step: 2591,loss: 0.022784548\n",
            "\n",
            "Global step: 2592,loss: 0.022245992\n",
            "\n",
            "Global step: 2593,loss: 0.024221225\n",
            "\n",
            "Global step: 2594,loss: 0.02927361\n",
            "\n",
            "Global step: 2595,loss: 0.027832303\n",
            "\n",
            "Global step: 2596,loss: 0.022207856\n",
            "\n",
            "Global step: 2597,loss: 0.023347337\n",
            "\n",
            "Global step: 2598,loss: 0.026460558\n",
            "\n",
            "Global step: 2599,loss: 0.023860637\n",
            "\n",
            "Global step: 2600,loss: 0.020313825\n",
            "\n",
            "Global step: 2601,loss: 0.020673797\n",
            "\n",
            "Global step: 2602,loss: 0.025398236\n",
            "\n",
            "Global step: 2603,loss: 0.020941138\n",
            "\n",
            "Global step: 2604,loss: 0.022338051\n",
            "\n",
            "Global step: 2605,loss: 0.022772301\n",
            "\n",
            "Global step: 2606,loss: 0.021705743\n",
            "\n",
            "Global step: 2607,loss: 0.020451343\n",
            "\n",
            "Global step: 2608,loss: 0.023728551\n",
            "\n",
            "Global step: 2609,loss: 0.024084514\n",
            "\n",
            "Global step: 2610,loss: 0.021076601\n",
            "\n",
            "Global step: 2611,loss: 0.020237135\n",
            "\n",
            "Global step: 2612,loss: 0.02001531\n",
            "\n",
            "Global step: 2613,loss: 0.021365825\n",
            "\n",
            "Global step: 2614,loss: 0.024333816\n",
            "\n",
            "Global step: 2615,loss: 0.021935612\n",
            "\n",
            "Global step: 2616,loss: 0.020796837\n",
            "\n",
            "Global step: 2617,loss: 0.024065284\n",
            "\n",
            "Global step: 2618,loss: 0.02144087\n",
            "\n",
            "Global step: 2619,loss: 0.023543052\n",
            "\n",
            "Global step: 2620,loss: 0.022000514\n",
            "\n",
            "Global step: 2621,loss: 0.019227805\n",
            "\n",
            "Global step: 2622,loss: 0.025637941\n",
            "\n",
            "Global step: 2623,loss: 0.021931479\n",
            "\n",
            "Global step: 2624,loss: 0.021371705\n",
            "\n",
            "Global step: 2625,loss: 0.036720973\n",
            "\n",
            "Global step: 2626,loss: 0.02010403\n",
            "\n",
            "Global step: 2627,loss: 0.021275798\n",
            "\n",
            "Global step: 2628,loss: 0.021648718\n",
            "\n",
            "Global step: 2629,loss: 0.020923791\n",
            "\n",
            "Global step: 2630,loss: 0.021066932\n",
            "\n",
            "Global step: 2631,loss: 0.021223195\n",
            "\n",
            "Global step: 2632,loss: 0.022062467\n",
            "\n",
            "Global step: 2633,loss: 0.02030873\n",
            "\n",
            "Global step: 2634,loss: 0.021283396\n",
            "\n",
            "Global step: 2635,loss: 0.020688087\n",
            "\n",
            "Global step: 2636,loss: 0.02088943\n",
            "\n",
            "Global step: 2637,loss: 0.020462127\n",
            "\n",
            "Global step: 2638,loss: 0.022471618\n",
            "\n",
            "Global step: 2639,loss: 0.0207868\n",
            "\n",
            "Global step: 2640,loss: 0.02152761\n",
            "\n",
            "Global step: 2641,loss: 0.022840526\n",
            "\n",
            "Global step: 2642,loss: 0.025888663\n",
            "\n",
            "Global step: 2643,loss: 0.019873744\n",
            "\n",
            "Global step: 2644,loss: 0.020917693\n",
            "\n",
            "Global step: 2645,loss: 0.019397838\n",
            "\n",
            "Global step: 2646,loss: 0.026081255\n",
            "\n",
            "Global step: 2647,loss: 0.021746486\n",
            "\n",
            "Global step: 2648,loss: 0.019302683\n",
            "\n",
            "Global step: 2649,loss: 0.020665456\n",
            "\n",
            "Global step: 2650,loss: 0.021478884\n",
            "\n",
            "Global step: 2651,loss: 0.020798879\n",
            "\n",
            "Global step: 2652,loss: 0.025739484\n",
            "\n",
            "Global step: 2653,loss: 0.023515858\n",
            "\n",
            "Global step: 2654,loss: 0.020636566\n",
            "\n",
            "Global step: 2655,loss: 0.021097988\n",
            "\n",
            "Global step: 2656,loss: 0.019132728\n",
            "\n",
            "Global step: 2657,loss: 0.022626162\n",
            "\n",
            "Global step: 2658,loss: 0.023728555\n",
            "\n",
            "Global step: 2659,loss: 0.022777403\n",
            "\n",
            "Global step: 2660,loss: 0.022678772\n",
            "\n",
            "Global step: 2661,loss: 0.019983834\n",
            "\n",
            "Global step: 2662,loss: 0.031563118\n",
            "\n",
            "Global step: 2663,loss: 0.020227147\n",
            "\n",
            "Global step: 2664,loss: 0.021763677\n",
            "\n",
            "Global step: 2665,loss: 0.020403812\n",
            "\n",
            "Global step: 2666,loss: 0.032482456\n",
            "\n",
            "Global step: 2667,loss: 0.021871217\n",
            "\n",
            "Global step: 2668,loss: 0.0323399\n",
            "\n",
            "Global step: 2669,loss: 0.022816775\n",
            "\n",
            "Global step: 2670,loss: 0.02552636\n",
            "\n",
            "Global step: 2671,loss: 0.02062496\n",
            "\n",
            "Global step: 2672,loss: 0.02264394\n",
            "\n",
            "Global step: 2673,loss: 0.024280585\n",
            "\n",
            "Global step: 2674,loss: 0.021489292\n",
            "\n",
            "Global step: 2675,loss: 0.01955699\n",
            "\n",
            "Global step: 2676,loss: 0.020585325\n",
            "\n",
            "Global step: 2677,loss: 0.021306878\n",
            "\n",
            "Global step: 2678,loss: 0.022412786\n",
            "\n",
            "Global step: 2679,loss: 0.025309497\n",
            "\n",
            "Global step: 2680,loss: 0.02370195\n",
            "\n",
            "Global step: 2681,loss: 0.021159317\n",
            "\n",
            "Global step: 2682,loss: 0.022337347\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.63137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:25:22.165510 139648999347968 supervisor.py:1099] global_step/sec: 7.63137\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2683,loss: 0.02228087\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2684.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:25:22.253990 139649007740672 supervisor.py:1050] Recording summary at step 2684.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2684,loss: 0.023657102\n",
            "\n",
            "Global step: 2685,loss: 0.021425866\n",
            "\n",
            "Global step: 2686,loss: 0.020089101\n",
            "\n",
            "Global step: 2687,loss: 0.025891947\n",
            "\n",
            "Global step: 2688,loss: 0.022283604\n",
            "\n",
            "Global step: 2689,loss: 0.030669805\n",
            "\n",
            "Global step: 2690,loss: 0.022796592\n",
            "\n",
            "Global step: 2691,loss: 0.023336016\n",
            "\n",
            "Global step: 2692,loss: 0.01937741\n",
            "\n",
            "Global step: 2693,loss: 0.023952883\n",
            "\n",
            "Global step: 2694,loss: 0.02098265\n",
            "\n",
            "Global step: 2695,loss: 0.02409626\n",
            "\n",
            "Global step: 2696,loss: 0.022885125\n",
            "\n",
            "Global step: 2697,loss: 0.021782164\n",
            "\n",
            "Global step: 2698,loss: 0.019691821\n",
            "\n",
            "Global step: 2699,loss: 0.022730976\n",
            "\n",
            "Global step: 2700,loss: 0.02099232\n",
            "\n",
            "Global step: 2701,loss: 0.020725619\n",
            "\n",
            "Global step: 2702,loss: 0.026758645\n",
            "\n",
            "Global step: 2703,loss: 0.020766178\n",
            "\n",
            "Global step: 2704,loss: 0.020236602\n",
            "\n",
            "Global step: 2705,loss: 0.025181388\n",
            "\n",
            "Global step: 2706,loss: 0.022058249\n",
            "\n",
            "Global step: 2707,loss: 0.029689215\n",
            "\n",
            "Global step: 2708,loss: 0.021020249\n",
            "\n",
            "Global step: 2709,loss: 0.022263099\n",
            "\n",
            "Global step: 2710,loss: 0.02006312\n",
            "\n",
            "Global step: 2711,loss: 0.021292716\n",
            "\n",
            "Global step: 2712,loss: 0.020413924\n",
            "\n",
            "Global step: 2713,loss: 0.026509669\n",
            "\n",
            "Global step: 2714,loss: 0.020576917\n",
            "\n",
            "Global step: 2715,loss: 0.027792165\n",
            "\n",
            "Global step: 2716,loss: 0.021104474\n",
            "\n",
            "Global step: 2717,loss: 0.020593492\n",
            "\n",
            "Global step: 2718,loss: 0.021279115\n",
            "\n",
            "Global step: 2719,loss: 0.023851607\n",
            "\n",
            "Global step: 2720,loss: 0.03076392\n",
            "\n",
            "Global step: 2721,loss: 0.02227275\n",
            "\n",
            "Global step: 2722,loss: 0.035586845\n",
            "\n",
            "Global step: 2723,loss: 0.02027275\n",
            "\n",
            "Global step: 2724,loss: 0.02038039\n",
            "\n",
            "Global step: 2725,loss: 0.02055933\n",
            "\n",
            "Global step: 2726,loss: 0.02157963\n",
            "\n",
            "Global step: 2727,loss: 0.022634018\n",
            "\n",
            "Global step: 2728,loss: 0.025738355\n",
            "\n",
            "Global step: 2729,loss: 0.022218365\n",
            "\n",
            "Global step: 2730,loss: 0.020710554\n",
            "\n",
            "Global step: 2731,loss: 0.020884756\n",
            "\n",
            "Global step: 2732,loss: 0.021162985\n",
            "\n",
            "Global step: 2733,loss: 0.021054078\n",
            "\n",
            "Global step: 2734,loss: 0.020757766\n",
            "\n",
            "Global step: 2735,loss: 0.02081853\n",
            "\n",
            "Global step: 2736,loss: 0.020382209\n",
            "\n",
            "Global step: 2737,loss: 0.020580947\n",
            "\n",
            "Global step: 2738,loss: 0.021957887\n",
            "\n",
            "Global step: 2739,loss: 0.023581274\n",
            "\n",
            "Global step: 2740,loss: 0.021865567\n",
            "\n",
            "Global step: 2741,loss: 0.020118738\n",
            "\n",
            "Global step: 2742,loss: 0.021437429\n",
            "\n",
            "Global step: 2743,loss: 0.03705588\n",
            "\n",
            "Global step: 2744,loss: 0.021230007\n",
            "\n",
            "Global step: 2745,loss: 0.020576663\n",
            "\n",
            "Global step: 2746,loss: 0.02291187\n",
            "\n",
            "Global step: 2747,loss: 0.019700123\n",
            "\n",
            "Global step: 2748,loss: 0.025620477\n",
            "\n",
            "Global step: 2749,loss: 0.020490061\n",
            "\n",
            "Global step: 2750,loss: 0.019864328\n",
            "\n",
            "Global step: 2751,loss: 0.025967704\n",
            "\n",
            "Global step: 2752,loss: 0.02149932\n",
            "\n",
            "Global step: 2753,loss: 0.019734124\n",
            "\n",
            "Global step: 2754,loss: 0.020713931\n",
            "\n",
            "Global step: 2755,loss: 0.020633398\n",
            "\n",
            "Global step: 2756,loss: 0.023868475\n",
            "\n",
            "Global step: 2757,loss: 0.02102752\n",
            "\n",
            "Global step: 2758,loss: 0.020606559\n",
            "\n",
            "Global step: 2759,loss: 0.021835255\n",
            "\n",
            "Global step: 2760,loss: 0.021156976\n",
            "\n",
            "Global step: 2761,loss: 0.020815808\n",
            "\n",
            "Global step: 2762,loss: 0.021972815\n",
            "\n",
            "Global step: 2763,loss: 0.020105751\n",
            "\n",
            "Global step: 2764,loss: 0.020766653\n",
            "\n",
            "Global step: 2765,loss: 0.018650962\n",
            "\n",
            "Global step: 2766,loss: 0.022134433\n",
            "\n",
            "Global step: 2767,loss: 0.020491626\n",
            "\n",
            "Global step: 2768,loss: 0.020433307\n",
            "\n",
            "Global step: 2769,loss: 0.029307261\n",
            "\n",
            "Global step: 2770,loss: 0.022117924\n",
            "\n",
            "Global step: 2771,loss: 0.023341991\n",
            "\n",
            "Global step: 2772,loss: 0.020497033\n",
            "\n",
            "Global step: 2773,loss: 0.021218427\n",
            "\n",
            "Global step: 2774,loss: 0.024438124\n",
            "\n",
            "Global step: 2775,loss: 0.022420203\n",
            "\n",
            "Global step: 2776,loss: 0.024730828\n",
            "\n",
            "Global step: 2777,loss: 0.03262863\n",
            "\n",
            "Global step: 2778,loss: 0.021318957\n",
            "\n",
            "Global step: 2779,loss: 0.021667022\n",
            "\n",
            "Global step: 2780,loss: 0.024547849\n",
            "\n",
            "Global step: 2781,loss: 0.02104047\n",
            "\n",
            "Global step: 2782,loss: 0.029257813\n",
            "\n",
            "Global step: 2783,loss: 0.020778337\n",
            "\n",
            "Global step: 2784,loss: 0.020460144\n",
            "\n",
            "Global step: 2785,loss: 0.025638292\n",
            "\n",
            "Global step: 2786,loss: 0.021517837\n",
            "\n",
            "Global step: 2787,loss: 0.02124903\n",
            "\n",
            "Global step: 2788,loss: 0.022187335\n",
            "\n",
            "Global step: 2789,loss: 0.022672605\n",
            "\n",
            "Global step: 2790,loss: 0.02455264\n",
            "\n",
            "Global step: 2791,loss: 0.020560255\n",
            "\n",
            "Global step: 2792,loss: 0.023390986\n",
            "\n",
            "Global step: 2793,loss: 0.019048901\n",
            "\n",
            "Global step: 2794,loss: 0.024639748\n",
            "\n",
            "Global step: 2795,loss: 0.023501765\n",
            "\n",
            "Global step: 2796,loss: 0.019854091\n",
            "\n",
            "Global step: 2797,loss: 0.025733642\n",
            "\n",
            "Global step: 2798,loss: 0.02495851\n",
            "\n",
            "Global step: 2799,loss: 0.023746908\n",
            "\n",
            "Global step: 2800,loss: 0.021948094\n",
            "\n",
            "Global step: 2801,loss: 0.01892266\n",
            "\n",
            "Global step: 2802,loss: 0.02147359\n",
            "\n",
            "Global step: 2803,loss: 0.023953874\n",
            "\n",
            "Global step: 2804,loss: 0.020286242\n",
            "\n",
            "Global step: 2805,loss: 0.02125381\n",
            "\n",
            "Global step: 2806,loss: 0.020020643\n",
            "\n",
            "Global step: 2807,loss: 0.02086631\n",
            "\n",
            "Global step: 2808,loss: 0.023044862\n",
            "\n",
            "Global step: 2809,loss: 0.022164747\n",
            "\n",
            "Global step: 2810,loss: 0.02274396\n",
            "\n",
            "Global step: 2811,loss: 0.0224446\n",
            "\n",
            "Global step: 2812,loss: 0.020328442\n",
            "\n",
            "Global step: 2813,loss: 0.02101794\n",
            "\n",
            "Global step: 2814,loss: 0.020493176\n",
            "\n",
            "Global step: 2815,loss: 0.019274998\n",
            "\n",
            "Global step: 2816,loss: 0.021844171\n",
            "\n",
            "Global step: 2817,loss: 0.021189312\n",
            "\n",
            "Global step: 2818,loss: 0.019265482\n",
            "\n",
            "Global step: 2819,loss: 0.020457583\n",
            "\n",
            "Global step: 2820,loss: 0.023717424\n",
            "\n",
            "Global step: 2821,loss: 0.023081496\n",
            "\n",
            "Global step: 2822,loss: 0.027981725\n",
            "\n",
            "Global step: 2823,loss: 0.024065262\n",
            "\n",
            "Global step: 2824,loss: 0.019889023\n",
            "\n",
            "Global step: 2825,loss: 0.02007624\n",
            "\n",
            "Global step: 2826,loss: 0.022216097\n",
            "\n",
            "Global step: 2827,loss: 0.021965994\n",
            "\n",
            "Global step: 2828,loss: 0.021454982\n",
            "\n",
            "Global step: 2829,loss: 0.02060282\n",
            "\n",
            "Global step: 2830,loss: 0.019604772\n",
            "\n",
            "Global step: 2831,loss: 0.019497968\n",
            "\n",
            "Global step: 2832,loss: 0.030752622\n",
            "\n",
            "Global step: 2833,loss: 0.020373397\n",
            "\n",
            "Global step: 2834,loss: 0.02174503\n",
            "\n",
            "Global step: 2835,loss: 0.022343434\n",
            "\n",
            "Global step: 2836,loss: 0.027935829\n",
            "\n",
            "Global step: 2837,loss: 0.020054638\n",
            "\n",
            "Global step: 2838,loss: 0.025873173\n",
            "\n",
            "Global step: 2839,loss: 0.019723788\n",
            "\n",
            "Global step: 2840,loss: 0.021535128\n",
            "\n",
            "Global step: 2841,loss: 0.020404171\n",
            "\n",
            "Global step: 2842,loss: 0.026710236\n",
            "\n",
            "Global step: 2843,loss: 0.020908734\n",
            "\n",
            "Global step: 2844,loss: 0.01948082\n",
            "\n",
            "Global step: 2845,loss: 0.020772133\n",
            "\n",
            "Global step: 2846,loss: 0.023890914\n",
            "\n",
            "Global step: 2847,loss: 0.022019252\n",
            "\n",
            "Global step: 2848,loss: 0.025887325\n",
            "\n",
            "Global step: 2849,loss: 0.020844705\n",
            "\n",
            "Global step: 2850,loss: 0.019522425\n",
            "\n",
            "Global step: 2851,loss: 0.02230247\n",
            "\n",
            "Global step: 2852,loss: 0.023497794\n",
            "\n",
            "Global step: 2853,loss: 0.02099122\n",
            "\n",
            "Global step: 2854,loss: 0.01940214\n",
            "\n",
            "Global step: 2855,loss: 0.02049113\n",
            "\n",
            "Global step: 2856,loss: 0.020889109\n",
            "\n",
            "Global step: 2857,loss: 0.01940836\n",
            "\n",
            "Global step: 2858,loss: 0.021386841\n",
            "\n",
            "Global step: 2859,loss: 0.020859556\n",
            "\n",
            "Global step: 2860,loss: 0.020695161\n",
            "\n",
            "Global step: 2861,loss: 0.019617958\n",
            "\n",
            "Global step: 2862,loss: 0.020530494\n",
            "\n",
            "Global step: 2863,loss: 0.019280914\n",
            "\n",
            "Global step: 2864,loss: 0.020415517\n",
            "\n",
            "Global step: 2865,loss: 0.020962471\n",
            "\n",
            "Global step: 2866,loss: 0.019907713\n",
            "\n",
            "Global step: 2867,loss: 0.020402595\n",
            "\n",
            "Global step: 2868,loss: 0.020998023\n",
            "\n",
            "Global step: 2869,loss: 0.021059286\n",
            "\n",
            "Global step: 2870,loss: 0.027860576\n",
            "\n",
            "Global step: 2871,loss: 0.020719882\n",
            "\n",
            "Global step: 2872,loss: 0.020459102\n",
            "\n",
            "Global step: 2873,loss: 0.020574456\n",
            "\n",
            "Global step: 2874,loss: 0.023691906\n",
            "\n",
            "Global step: 2875,loss: 0.024346763\n",
            "\n",
            "Global step: 2876,loss: 0.021934912\n",
            "\n",
            "Global step: 2877,loss: 0.020845449\n",
            "\n",
            "Global step: 2878,loss: 0.021989964\n",
            "\n",
            "Global step: 2879,loss: 0.024243468\n",
            "\n",
            "Global step: 2880,loss: 0.024057709\n",
            "\n",
            "Global step: 2881,loss: 0.02729218\n",
            "\n",
            "Global step: 2882,loss: 0.020817708\n",
            "\n",
            "Global step: 2883,loss: 0.02090251\n",
            "\n",
            "Global step: 2884,loss: 0.02030885\n",
            "\n",
            "Global step: 2885,loss: 0.021696616\n",
            "\n",
            "Global step: 2886,loss: 0.020833593\n",
            "\n",
            "Global step: 2887,loss: 0.018672505\n",
            "\n",
            "Global step: 2888,loss: 0.020431839\n",
            "\n",
            "Global step: 2889,loss: 0.020715922\n",
            "\n",
            "Global step: 2890,loss: 0.01968271\n",
            "\n",
            "Global step: 2891,loss: 0.024713436\n",
            "\n",
            "Global step: 2892,loss: 0.019453717\n",
            "\n",
            "Global step: 2893,loss: 0.019236641\n",
            "\n",
            "Global step: 2894,loss: 0.02684053\n",
            "\n",
            "Global step: 2895,loss: 0.026982103\n",
            "\n",
            "Global step: 2896,loss: 0.020834116\n",
            "\n",
            "Global step: 2897,loss: 0.020024307\n",
            "\n",
            "Global step: 2898,loss: 0.020185426\n",
            "\n",
            "Global step: 2899,loss: 0.020624425\n",
            "\n",
            "Global step: 2900,loss: 0.021618875\n",
            "\n",
            "Global step: 2901,loss: 0.020492187\n",
            "\n",
            "Global step: 2902,loss: 0.020102115\n",
            "\n",
            "Global step: 2903,loss: 0.02014221\n",
            "\n",
            "Global step: 2904,loss: 0.020633176\n",
            "\n",
            "Global step: 2905,loss: 0.02047214\n",
            "\n",
            "Global step: 2906,loss: 0.030073604\n",
            "\n",
            "Global step: 2907,loss: 0.022504792\n",
            "\n",
            "Global step: 2908,loss: 0.021442994\n",
            "\n",
            "Global step: 2909,loss: 0.019064061\n",
            "\n",
            "Global step: 2910,loss: 0.020943426\n",
            "\n",
            "Global step: 2911,loss: 0.026047636\n",
            "\n",
            "Global step: 2912,loss: 0.02662187\n",
            "\n",
            "Global step: 2913,loss: 0.020383189\n",
            "\n",
            "Global step: 2914,loss: 0.018434633\n",
            "\n",
            "Global step: 2915,loss: 0.025085302\n",
            "\n",
            "Global step: 2916,loss: 0.020302212\n",
            "\n",
            "Global step: 2917,loss: 0.02080182\n",
            "\n",
            "Global step: 2918,loss: 0.019334719\n",
            "\n",
            "Global step: 2919,loss: 0.02184887\n",
            "\n",
            "Global step: 2920,loss: 0.021380816\n",
            "\n",
            "Global step: 2921,loss: 0.018795941\n",
            "\n",
            "Global step: 2922,loss: 0.020117793\n",
            "\n",
            "Global step: 2923,loss: 0.02004397\n",
            "\n",
            "Global step: 2924,loss: 0.019486515\n",
            "\n",
            "Global step: 2925,loss: 0.020687424\n",
            "\n",
            "Global step: 2926,loss: 0.021146746\n",
            "\n",
            "Global step: 2927,loss: 0.019002365\n",
            "\n",
            "Global step: 2928,loss: 0.020721663\n",
            "\n",
            "Global step: 2929,loss: 0.022148013\n",
            "\n",
            "Global step: 2930,loss: 0.02657891\n",
            "\n",
            "Global step: 2931,loss: 0.021849088\n",
            "\n",
            "Global step: 2932,loss: 0.020162571\n",
            "\n",
            "Global step: 2933,loss: 0.02470816\n",
            "\n",
            "Global step: 2934,loss: 0.025276672\n",
            "\n",
            "Global step: 2935,loss: 0.021769097\n",
            "\n",
            "Global step: 2936,loss: 0.020137595\n",
            "\n",
            "Global step: 2937,loss: 0.02423048\n",
            "\n",
            "Global step: 2938,loss: 0.019909227\n",
            "\n",
            "Global step: 2939,loss: 0.019123781\n",
            "\n",
            "Global step: 2940,loss: 0.018803459\n",
            "\n",
            "Global step: 2941,loss: 0.019995002\n",
            "\n",
            "Global step: 2942,loss: 0.020248527\n",
            "\n",
            "Global step: 2943,loss: 0.021476703\n",
            "\n",
            "Global step: 2944,loss: 0.024867134\n",
            "\n",
            "Global step: 2945,loss: 0.024556264\n",
            "\n",
            "Global step: 2946,loss: 0.018552797\n",
            "\n",
            "Global step: 2947,loss: 0.021873042\n",
            "\n",
            "Global step: 2948,loss: 0.021472441\n",
            "\n",
            "Global step: 2949,loss: 0.023655122\n",
            "\n",
            "Global step: 2950,loss: 0.02022724\n",
            "\n",
            "Global step: 2951,loss: 0.019367883\n",
            "\n",
            "Global step: 2952,loss: 0.028913781\n",
            "\n",
            "Global step: 2953,loss: 0.025113584\n",
            "\n",
            "Global step: 2954,loss: 0.025064625\n",
            "\n",
            "Global step: 2955,loss: 0.020678658\n",
            "\n",
            "Global step: 2956,loss: 0.021014325\n",
            "\n",
            "Global step: 2957,loss: 0.025402796\n",
            "\n",
            "Global step: 2958,loss: 0.019797252\n",
            "\n",
            "Global step: 2959,loss: 0.01935407\n",
            "\n",
            "Global step: 2960,loss: 0.020564198\n",
            "\n",
            "Global step: 2961,loss: 0.021007292\n",
            "\n",
            "Global step: 2962,loss: 0.02602987\n",
            "\n",
            "Global step: 2963,loss: 0.035108056\n",
            "\n",
            "Global step: 2964,loss: 0.039239198\n",
            "\n",
            "Global step: 2965,loss: 0.02180513\n",
            "\n",
            "Global step: 2966,loss: 0.020897968\n",
            "\n",
            "Global step: 2967,loss: 0.01964364\n",
            "\n",
            "Global step: 2968,loss: 0.018760819\n",
            "\n",
            "Global step: 2969,loss: 0.019928858\n",
            "\n",
            "Global step: 2970,loss: 0.025186542\n",
            "\n",
            "Global step: 2971,loss: 0.027985018\n",
            "\n",
            "Global step: 2972,loss: 0.020676905\n",
            "\n",
            "Global step: 2973,loss: 0.021240482\n",
            "\n",
            "Global step: 2974,loss: 0.019547967\n",
            "\n",
            "Global step: 2975,loss: 0.019738987\n",
            "\n",
            "Global step: 2976,loss: 0.022601796\n",
            "\n",
            "Global step: 2977,loss: 0.022960125\n",
            "\n",
            "Global step: 2978,loss: 0.018684477\n",
            "\n",
            "Global step: 2979,loss: 0.020042771\n",
            "\n",
            "Global step: 2980,loss: 0.01884943\n",
            "\n",
            "Global step: 2981,loss: 0.021330046\n",
            "\n",
            "Global step: 2982,loss: 0.020954056\n",
            "\n",
            "Global step: 2983,loss: 0.019619007\n",
            "\n",
            "Global step: 2984,loss: 0.022966161\n",
            "\n",
            "Global step: 2985,loss: 0.020775748\n",
            "\n",
            "Global step: 2986,loss: 0.01897276\n",
            "\n",
            "Global step: 2987,loss: 0.020542162\n",
            "\n",
            "Global step: 2988,loss: 0.022874516\n",
            "\n",
            "Global step: 2989,loss: 0.023971807\n",
            "\n",
            "Global step: 2990,loss: 0.02279302\n",
            "\n",
            "Global step: 2991,loss: 0.021987785\n",
            "\n",
            "Global step: 2992,loss: 0.019747775\n",
            "\n",
            "Global step: 2993,loss: 0.023722496\n",
            "\n",
            "Global step: 2994,loss: 0.02006337\n",
            "\n",
            "Global step: 2995,loss: 0.02155442\n",
            "\n",
            "Global step: 2996,loss: 0.018889872\n",
            "\n",
            "Global step: 2997,loss: 0.020385027\n",
            "\n",
            "Global step: 2998,loss: 0.019856606\n",
            "\n",
            "Global step: 2999,loss: 0.022479353\n",
            "\n",
            "Global step: 3000,loss: 0.023048444\n",
            "\n",
            "Global step: 3001,loss: 0.020599943\n",
            "\n",
            "Global step: 3002,loss: 0.019929646\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3002,Val_Loss: 0.02439499694185379,  Val_acc: 0.9971955128205128 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:26:04.355097 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 8/15:\n",
            "Global step: 3003,loss: 0.019166099\n",
            "\n",
            "Global step: 3004,loss: 0.020396646\n",
            "\n",
            "Global step: 3005,loss: 0.01979824\n",
            "\n",
            "Global step: 3006,loss: 0.01959454\n",
            "\n",
            "Global step: 3007,loss: 0.019812088\n",
            "\n",
            "Global step: 3008,loss: 0.020986304\n",
            "\n",
            "Global step: 3009,loss: 0.019430546\n",
            "\n",
            "Global step: 3010,loss: 0.022074029\n",
            "\n",
            "Global step: 3011,loss: 0.019573668\n",
            "\n",
            "Global step: 3012,loss: 0.02052212\n",
            "\n",
            "Global step: 3013,loss: 0.020207275\n",
            "\n",
            "Global step: 3014,loss: 0.023475079\n",
            "\n",
            "Global step: 3015,loss: 0.019399067\n",
            "\n",
            "Global step: 3016,loss: 0.02133606\n",
            "\n",
            "Global step: 3017,loss: 0.01954389\n",
            "\n",
            "Global step: 3018,loss: 0.021138923\n",
            "\n",
            "Global step: 3019,loss: 0.019538542\n",
            "\n",
            "Global step: 3020,loss: 0.018875377\n",
            "\n",
            "Global step: 3021,loss: 0.018693944\n",
            "\n",
            "Global step: 3022,loss: 0.019006645\n",
            "\n",
            "Global step: 3023,loss: 0.019646322\n",
            "\n",
            "Global step: 3024,loss: 0.026610617\n",
            "\n",
            "Global step: 3025,loss: 0.018684907\n",
            "\n",
            "Global step: 3026,loss: 0.021272656\n",
            "\n",
            "Global step: 3027,loss: 0.019179828\n",
            "\n",
            "Global step: 3028,loss: 0.025376994\n",
            "\n",
            "Global step: 3029,loss: 0.021082163\n",
            "\n",
            "Global step: 3030,loss: 0.02073967\n",
            "\n",
            "Global step: 3031,loss: 0.024339704\n",
            "\n",
            "Global step: 3032,loss: 0.019048655\n",
            "\n",
            "Global step: 3033,loss: 0.019115042\n",
            "\n",
            "Global step: 3034,loss: 0.019421717\n",
            "\n",
            "Global step: 3035,loss: 0.02507153\n",
            "\n",
            "Global step: 3036,loss: 0.02123172\n",
            "\n",
            "Global step: 3037,loss: 0.02043348\n",
            "\n",
            "Global step: 3038,loss: 0.01947761\n",
            "\n",
            "Global step: 3039,loss: 0.020118179\n",
            "\n",
            "Global step: 3040,loss: 0.01825836\n",
            "\n",
            "Global step: 3041,loss: 0.018441303\n",
            "\n",
            "Global step: 3042,loss: 0.020708824\n",
            "\n",
            "Global step: 3043,loss: 0.020414425\n",
            "\n",
            "Global step: 3044,loss: 0.019906113\n",
            "\n",
            "Global step: 3045,loss: 0.023605045\n",
            "\n",
            "Global step: 3046,loss: 0.019222526\n",
            "\n",
            "Global step: 3047,loss: 0.019944146\n",
            "\n",
            "Global step: 3048,loss: 0.0259864\n",
            "\n",
            "Global step: 3049,loss: 0.01951529\n",
            "\n",
            "Global step: 3050,loss: 0.020192131\n",
            "\n",
            "Global step: 3051,loss: 0.018469077\n",
            "\n",
            "Global step: 3052,loss: 0.023006514\n",
            "\n",
            "Global step: 3053,loss: 0.020932755\n",
            "\n",
            "Global step: 3054,loss: 0.019597013\n",
            "\n",
            "Global step: 3055,loss: 0.021049876\n",
            "\n",
            "Global step: 3056,loss: 0.019957552\n",
            "\n",
            "Global step: 3057,loss: 0.019175807\n",
            "\n",
            "Global step: 3058,loss: 0.020433929\n",
            "\n",
            "Global step: 3059,loss: 0.018935945\n",
            "\n",
            "Global step: 3060,loss: 0.02006534\n",
            "\n",
            "Global step: 3061,loss: 0.018820694\n",
            "\n",
            "Global step: 3062,loss: 0.019587342\n",
            "\n",
            "Global step: 3063,loss: 0.029624343\n",
            "\n",
            "Global step: 3064,loss: 0.019158516\n",
            "\n",
            "Global step: 3065,loss: 0.0230942\n",
            "\n",
            "Global step: 3066,loss: 0.022084119\n",
            "\n",
            "Global step: 3067,loss: 0.02696049\n",
            "\n",
            "Global step: 3068,loss: 0.023955077\n",
            "\n",
            "Global step: 3069,loss: 0.022394631\n",
            "\n",
            "Global step: 3070,loss: 0.020236759\n",
            "\n",
            "Global step: 3071,loss: 0.01961895\n",
            "\n",
            "Global step: 3072,loss: 0.020108262\n",
            "\n",
            "Global step: 3073,loss: 0.018136501\n",
            "\n",
            "Global step: 3074,loss: 0.019926433\n",
            "\n",
            "Global step: 3075,loss: 0.020003038\n",
            "\n",
            "Global step: 3076,loss: 0.01970794\n",
            "\n",
            "Global step: 3077,loss: 0.020419927\n",
            "\n",
            "Global step: 3078,loss: 0.018595567\n",
            "\n",
            "Global step: 3079,loss: 0.020760713\n",
            "\n",
            "Global step: 3080,loss: 0.018917862\n",
            "\n",
            "Global step: 3081,loss: 0.02154326\n",
            "\n",
            "Global step: 3082,loss: 0.019935748\n",
            "\n",
            "Global step: 3083,loss: 0.02751641\n",
            "\n",
            "Global step: 3084,loss: 0.023624692\n",
            "\n",
            "Global step: 3085,loss: 0.023456108\n",
            "\n",
            "Global step: 3086,loss: 0.019087363\n",
            "\n",
            "Global step: 3087,loss: 0.018647425\n",
            "\n",
            "Global step: 3088,loss: 0.018943027\n",
            "\n",
            "Global step: 3089,loss: 0.019328136\n",
            "\n",
            "Global step: 3090,loss: 0.028461423\n",
            "\n",
            "Global step: 3091,loss: 0.019884955\n",
            "\n",
            "Global step: 3092,loss: 0.018823862\n",
            "\n",
            "Global step: 3093,loss: 0.021386253\n",
            "\n",
            "Global step: 3094,loss: 0.019881308\n",
            "\n",
            "Global step: 3095,loss: 0.01971926\n",
            "\n",
            "Global step: 3096,loss: 0.028256662\n",
            "\n",
            "Global step: 3097,loss: 0.020025002\n",
            "\n",
            "Global step: 3098,loss: 0.019411352\n",
            "\n",
            "Global step: 3099,loss: 0.020089358\n",
            "\n",
            "Global step: 3100,loss: 0.019100841\n",
            "\n",
            "Global step: 3101,loss: 0.019625656\n",
            "\n",
            "Global step: 3102,loss: 0.018879816\n",
            "\n",
            "Global step: 3103,loss: 0.017964346\n",
            "\n",
            "Global step: 3104,loss: 0.02006398\n",
            "\n",
            "Global step: 3105,loss: 0.020735707\n",
            "\n",
            "Global step: 3106,loss: 0.020025708\n",
            "\n",
            "Global step: 3107,loss: 0.020725377\n",
            "\n",
            "Global step: 3108,loss: 0.024604999\n",
            "\n",
            "Global step: 3109,loss: 0.02024659\n",
            "\n",
            "Global step: 3110,loss: 0.019751102\n",
            "\n",
            "Global step: 3111,loss: 0.020623777\n",
            "\n",
            "Global step: 3112,loss: 0.021363374\n",
            "\n",
            "Global step: 3113,loss: 0.019474713\n",
            "\n",
            "Global step: 3114,loss: 0.021135319\n",
            "\n",
            "Global step: 3115,loss: 0.021570241\n",
            "\n",
            "Global step: 3116,loss: 0.023400193\n",
            "\n",
            "Global step: 3117,loss: 0.019764759\n",
            "\n",
            "Global step: 3118,loss: 0.019465163\n",
            "\n",
            "Global step: 3119,loss: 0.019534033\n",
            "\n",
            "Global step: 3120,loss: 0.023188187\n",
            "\n",
            "Global step: 3121,loss: 0.025399933\n",
            "\n",
            "Global step: 3122,loss: 0.019622004\n",
            "\n",
            "Global step: 3123,loss: 0.020028217\n",
            "\n",
            "Global step: 3124,loss: 0.020071365\n",
            "\n",
            "Global step: 3125,loss: 0.018971931\n",
            "\n",
            "Global step: 3126,loss: 0.020553842\n",
            "\n",
            "Global step: 3127,loss: 0.019728122\n",
            "\n",
            "Global step: 3128,loss: 0.021741994\n",
            "\n",
            "Global step: 3129,loss: 0.020150464\n",
            "\n",
            "Global step: 3130,loss: 0.019143654\n",
            "\n",
            "Global step: 3131,loss: 0.018851701\n",
            "\n",
            "Global step: 3132,loss: 0.026129443\n",
            "\n",
            "Global step: 3133,loss: 0.020802926\n",
            "\n",
            "Global step: 3134,loss: 0.022290472\n",
            "\n",
            "Global step: 3135,loss: 0.018698566\n",
            "\n",
            "Global step: 3136,loss: 0.019559786\n",
            "\n",
            "Global step: 3137,loss: 0.019803345\n",
            "\n",
            "Global step: 3138,loss: 0.018447328\n",
            "\n",
            "Global step: 3139,loss: 0.01939722\n",
            "\n",
            "Global step: 3140,loss: 0.021050038\n",
            "\n",
            "Global step: 3141,loss: 0.02320303\n",
            "\n",
            "Global step: 3142,loss: 0.0197801\n",
            "\n",
            "Global step: 3143,loss: 0.019763565\n",
            "\n",
            "Global step: 3144,loss: 0.019066311\n",
            "\n",
            "Global step: 3145,loss: 0.024100753\n",
            "\n",
            "Global step: 3146,loss: 0.024213292\n",
            "\n",
            "Global step: 3147,loss: 0.018270265\n",
            "\n",
            "Global step: 3148,loss: 0.019048939\n",
            "\n",
            "Global step: 3149,loss: 0.018474752\n",
            "\n",
            "Global step: 3150,loss: 0.019734938\n",
            "\n",
            "Global step: 3151,loss: 0.01949794\n",
            "\n",
            "Global step: 3152,loss: 0.023294602\n",
            "\n",
            "Global step: 3153,loss: 0.019619199\n",
            "\n",
            "Global step: 3154,loss: 0.025012108\n",
            "\n",
            "Global step: 3155,loss: 0.020157708\n",
            "\n",
            "Global step: 3156,loss: 0.018956799\n",
            "\n",
            "Global step: 3157,loss: 0.020183675\n",
            "\n",
            "Global step: 3158,loss: 0.023270084\n",
            "\n",
            "Global step: 3159,loss: 0.019079031\n",
            "\n",
            "Global step: 3160,loss: 0.02070995\n",
            "\n",
            "Global step: 3161,loss: 0.017677944\n",
            "\n",
            "Global step: 3162,loss: 0.019698225\n",
            "\n",
            "Global step: 3163,loss: 0.019143824\n",
            "\n",
            "Global step: 3164,loss: 0.01959251\n",
            "\n",
            "Global step: 3165,loss: 0.018450566\n",
            "\n",
            "Global step: 3166,loss: 0.019846767\n",
            "\n",
            "Global step: 3167,loss: 0.019780166\n",
            "\n",
            "Global step: 3168,loss: 0.018535467\n",
            "\n",
            "Global step: 3169,loss: 0.018611083\n",
            "\n",
            "Global step: 3170,loss: 0.01984754\n",
            "\n",
            "Global step: 3171,loss: 0.022881206\n",
            "\n",
            "Global step: 3172,loss: 0.018754803\n",
            "\n",
            "Global step: 3173,loss: 0.018591259\n",
            "\n",
            "Global step: 3174,loss: 0.019654214\n",
            "\n",
            "Global step: 3175,loss: 0.01919061\n",
            "\n",
            "Global step: 3176,loss: 0.017765598\n",
            "\n",
            "Global step: 3177,loss: 0.019775083\n",
            "\n",
            "Global step: 3178,loss: 0.020582031\n",
            "\n",
            "Global step: 3179,loss: 0.020093456\n",
            "\n",
            "Global step: 3180,loss: 0.040467747\n",
            "\n",
            "Global step: 3181,loss: 0.018848855\n",
            "\n",
            "Global step: 3182,loss: 0.019759389\n",
            "\n",
            "Global step: 3183,loss: 0.020446744\n",
            "\n",
            "Global step: 3184,loss: 0.019473078\n",
            "\n",
            "Global step: 3185,loss: 0.02453351\n",
            "\n",
            "Global step: 3186,loss: 0.021029122\n",
            "\n",
            "Global step: 3187,loss: 0.018353643\n",
            "\n",
            "Global step: 3188,loss: 0.020222107\n",
            "\n",
            "Global step: 3189,loss: 0.021386202\n",
            "\n",
            "Global step: 3190,loss: 0.02197512\n",
            "\n",
            "Global step: 3191,loss: 0.03227713\n",
            "\n",
            "Global step: 3192,loss: 0.018893711\n",
            "\n",
            "Global step: 3193,loss: 0.019424388\n",
            "\n",
            "Global step: 3194,loss: 0.020082524\n",
            "\n",
            "Global step: 3195,loss: 0.019705199\n",
            "\n",
            "Global step: 3196,loss: 0.01857026\n",
            "\n",
            "Global step: 3197,loss: 0.020146772\n",
            "\n",
            "Global step: 3198,loss: 0.019940551\n",
            "\n",
            "Global step: 3199,loss: 0.02088245\n",
            "\n",
            "Global step: 3200,loss: 0.020085754\n",
            "\n",
            "Global step: 3201,loss: 0.019126484\n",
            "\n",
            "Global step: 3202,loss: 0.024397898\n",
            "\n",
            "Global step: 3203,loss: 0.020426031\n",
            "\n",
            "Global step: 3204,loss: 0.020148486\n",
            "\n",
            "Global step: 3205,loss: 0.020088254\n",
            "\n",
            "Global step: 3206,loss: 0.020617075\n",
            "\n",
            "Global step: 3207,loss: 0.017749134\n",
            "\n",
            "Global step: 3208,loss: 0.019066075\n",
            "\n",
            "Global step: 3209,loss: 0.020108746\n",
            "\n",
            "Global step: 3210,loss: 0.021246456\n",
            "\n",
            "Global step: 3211,loss: 0.019076793\n",
            "\n",
            "Global step: 3212,loss: 0.019481964\n",
            "\n",
            "Global step: 3213,loss: 0.021001706\n",
            "\n",
            "Global step: 3214,loss: 0.018986333\n",
            "\n",
            "Global step: 3215,loss: 0.01908433\n",
            "\n",
            "Global step: 3216,loss: 0.019242262\n",
            "\n",
            "Global step: 3217,loss: 0.020020988\n",
            "\n",
            "Global step: 3218,loss: 0.019151945\n",
            "\n",
            "Global step: 3219,loss: 0.018542817\n",
            "\n",
            "Global step: 3220,loss: 0.020015268\n",
            "\n",
            "Global step: 3221,loss: 0.017890323\n",
            "\n",
            "Global step: 3222,loss: 0.017802622\n",
            "\n",
            "Global step: 3223,loss: 0.01901545\n",
            "\n",
            "Global step: 3224,loss: 0.018597389\n",
            "\n",
            "Global step: 3225,loss: 0.023272704\n",
            "\n",
            "Global step: 3226,loss: 0.019595249\n",
            "\n",
            "Global step: 3227,loss: 0.020800482\n",
            "\n",
            "Global step: 3228,loss: 0.021112906\n",
            "\n",
            "Global step: 3229,loss: 0.019820865\n",
            "\n",
            "Global step: 3230,loss: 0.018839642\n",
            "\n",
            "Global step: 3231,loss: 0.019556193\n",
            "\n",
            "Global step: 3232,loss: 0.020184427\n",
            "\n",
            "Global step: 3233,loss: 0.019521434\n",
            "\n",
            "Global step: 3234,loss: 0.018952299\n",
            "\n",
            "Global step: 3235,loss: 0.018266516\n",
            "\n",
            "Global step: 3236,loss: 0.023582688\n",
            "\n",
            "Global step: 3237,loss: 0.01873388\n",
            "\n",
            "Global step: 3238,loss: 0.020568969\n",
            "\n",
            "Global step: 3239,loss: 0.01919176\n",
            "\n",
            "Global step: 3240,loss: 0.018656906\n",
            "\n",
            "Global step: 3241,loss: 0.019901706\n",
            "\n",
            "Global step: 3242,loss: 0.01957159\n",
            "\n",
            "Global step: 3243,loss: 0.019199012\n",
            "\n",
            "Global step: 3244,loss: 0.019490434\n",
            "\n",
            "Global step: 3245,loss: 0.019240255\n",
            "\n",
            "Global step: 3246,loss: 0.02237292\n",
            "\n",
            "Global step: 3247,loss: 0.019437954\n",
            "\n",
            "Global step: 3248,loss: 0.021974448\n",
            "\n",
            "Global step: 3249,loss: 0.022640735\n",
            "\n",
            "Global step: 3250,loss: 0.018135848\n",
            "\n",
            "Global step: 3251,loss: 0.018619074\n",
            "\n",
            "Global step: 3252,loss: 0.018977035\n",
            "\n",
            "Global step: 3253,loss: 0.02698268\n",
            "\n",
            "Global step: 3254,loss: 0.020027315\n",
            "\n",
            "Global step: 3255,loss: 0.020132268\n",
            "\n",
            "Global step: 3256,loss: 0.02112277\n",
            "\n",
            "Global step: 3257,loss: 0.020006344\n",
            "\n",
            "Global step: 3258,loss: 0.018955817\n",
            "\n",
            "Global step: 3259,loss: 0.018899862\n",
            "\n",
            "Global step: 3260,loss: 0.018159814\n",
            "\n",
            "Global step: 3261,loss: 0.020225871\n",
            "\n",
            "Global step: 3262,loss: 0.018412277\n",
            "\n",
            "Global step: 3263,loss: 0.018824227\n",
            "\n",
            "Global step: 3264,loss: 0.02056251\n",
            "\n",
            "Global step: 3265,loss: 0.020977737\n",
            "\n",
            "Global step: 3266,loss: 0.020656873\n",
            "\n",
            "Global step: 3267,loss: 0.020339362\n",
            "\n",
            "Global step: 3268,loss: 0.01893703\n",
            "\n",
            "Global step: 3269,loss: 0.019444669\n",
            "\n",
            "Global step: 3270,loss: 0.018835614\n",
            "\n",
            "Global step: 3271,loss: 0.021228943\n",
            "\n",
            "Global step: 3272,loss: 0.018225655\n",
            "\n",
            "Global step: 3273,loss: 0.019207554\n",
            "\n",
            "Global step: 3274,loss: 0.01986639\n",
            "\n",
            "Global step: 3275,loss: 0.01892688\n",
            "\n",
            "Global step: 3276,loss: 0.020002943\n",
            "\n",
            "Global step: 3277,loss: 0.019287985\n",
            "\n",
            "Global step: 3278,loss: 0.02221285\n",
            "\n",
            "Global step: 3279,loss: 0.024513122\n",
            "\n",
            "Global step: 3280,loss: 0.018921718\n",
            "\n",
            "Global step: 3281,loss: 0.019085642\n",
            "\n",
            "Global step: 3282,loss: 0.022323234\n",
            "\n",
            "Global step: 3283,loss: 0.019231908\n",
            "\n",
            "Global step: 3284,loss: 0.019787401\n",
            "\n",
            "Global step: 3285,loss: 0.021961534\n",
            "\n",
            "Global step: 3286,loss: 0.022066168\n",
            "\n",
            "Global step: 3287,loss: 0.019298013\n",
            "\n",
            "Global step: 3288,loss: 0.020213112\n",
            "\n",
            "Global step: 3289,loss: 0.019872364\n",
            "\n",
            "Global step: 3290,loss: 0.020987289\n",
            "\n",
            "Global step: 3291,loss: 0.020125836\n",
            "\n",
            "Global step: 3292,loss: 0.018818716\n",
            "\n",
            "Global step: 3293,loss: 0.018667785\n",
            "\n",
            "Global step: 3294,loss: 0.018466957\n",
            "\n",
            "Global step: 3295,loss: 0.018397145\n",
            "\n",
            "Global step: 3296,loss: 0.026127959\n",
            "\n",
            "Global step: 3297,loss: 0.01943714\n",
            "\n",
            "Global step: 3298,loss: 0.020294443\n",
            "\n",
            "Global step: 3299,loss: 0.0254095\n",
            "\n",
            "Global step: 3300,loss: 0.01957316\n",
            "\n",
            "Global step: 3301,loss: 0.01883792\n",
            "\n",
            "Global step: 3302,loss: 0.019528076\n",
            "\n",
            "Global step: 3303,loss: 0.022096219\n",
            "\n",
            "Global step: 3304,loss: 0.019777713\n",
            "\n",
            "Global step: 3305,loss: 0.01943887\n",
            "\n",
            "Global step: 3306,loss: 0.019614581\n",
            "\n",
            "Global step: 3307,loss: 0.019405747\n",
            "\n",
            "Global step: 3308,loss: 0.02506846\n",
            "\n",
            "Global step: 3309,loss: 0.020069191\n",
            "\n",
            "Global step: 3310,loss: 0.019598693\n",
            "\n",
            "Global step: 3311,loss: 0.0201257\n",
            "\n",
            "Global step: 3312,loss: 0.018873367\n",
            "\n",
            "Global step: 3313,loss: 0.021345934\n",
            "\n",
            "Global step: 3314,loss: 0.01804198\n",
            "\n",
            "Global step: 3315,loss: 0.019442711\n",
            "\n",
            "Global step: 3316,loss: 0.019560736\n",
            "\n",
            "Global step: 3317,loss: 0.01916861\n",
            "\n",
            "Global step: 3318,loss: 0.024995662\n",
            "\n",
            "Global step: 3319,loss: 0.019622073\n",
            "\n",
            "Global step: 3320,loss: 0.01755284\n",
            "\n",
            "Global step: 3321,loss: 0.021657405\n",
            "\n",
            "Global step: 3322,loss: 0.020149808\n",
            "\n",
            "Global step: 3323,loss: 0.020096917\n",
            "\n",
            "Global step: 3324,loss: 0.018457634\n",
            "\n",
            "Global step: 3325,loss: 0.019049253\n",
            "\n",
            "Global step: 3326,loss: 0.019182818\n",
            "\n",
            "Global step: 3327,loss: 0.020183159\n",
            "\n",
            "Global step: 3328,loss: 0.021501698\n",
            "\n",
            "Global step: 3329,loss: 0.019945255\n",
            "\n",
            "Global step: 3330,loss: 0.019674271\n",
            "\n",
            "Global step: 3331,loss: 0.020066041\n",
            "\n",
            "Global step: 3332,loss: 0.018711297\n",
            "\n",
            "Global step: 3333,loss: 0.019191194\n",
            "\n",
            "Global step: 3334,loss: 0.026800046\n",
            "\n",
            "Global step: 3335,loss: 0.01883692\n",
            "\n",
            "Global step: 3336,loss: 0.020838333\n",
            "\n",
            "Global step: 3337,loss: 0.021309251\n",
            "\n",
            "Global step: 3338,loss: 0.018666029\n",
            "\n",
            "Global step: 3339,loss: 0.01920447\n",
            "\n",
            "Global step: 3340,loss: 0.019033665\n",
            "\n",
            "Global step: 3341,loss: 0.019794067\n",
            "\n",
            "Global step: 3342,loss: 0.027664496\n",
            "\n",
            "Global step: 3343,loss: 0.019043863\n",
            "\n",
            "Global step: 3344,loss: 0.019286448\n",
            "\n",
            "Global step: 3345,loss: 0.018963706\n",
            "\n",
            "Global step: 3346,loss: 0.019375622\n",
            "\n",
            "Global step: 3347,loss: 0.018749846\n",
            "\n",
            "Global step: 3348,loss: 0.018681433\n",
            "\n",
            "Global step: 3349,loss: 0.024713522\n",
            "\n",
            "Global step: 3350,loss: 0.018790366\n",
            "\n",
            "Global step: 3351,loss: 0.024955183\n",
            "\n",
            "Global step: 3352,loss: 0.019315345\n",
            "\n",
            "Global step: 3353,loss: 0.03103023\n",
            "\n",
            "Global step: 3354,loss: 0.02131266\n",
            "\n",
            "Global step: 3355,loss: 0.019443244\n",
            "\n",
            "Global step: 3356,loss: 0.018587736\n",
            "\n",
            "Global step: 3357,loss: 0.027385373\n",
            "\n",
            "Global step: 3358,loss: 0.02303609\n",
            "\n",
            "Global step: 3359,loss: 0.019605143\n",
            "\n",
            "Global step: 3360,loss: 0.0184747\n",
            "\n",
            "Global step: 3361,loss: 0.017519902\n",
            "\n",
            "Global step: 3362,loss: 0.019212909\n",
            "\n",
            "Global step: 3363,loss: 0.020791322\n",
            "\n",
            "Global step: 3364,loss: 0.02019468\n",
            "\n",
            "Global step: 3365,loss: 0.019321574\n",
            "\n",
            "Global step: 3366,loss: 0.021240067\n",
            "\n",
            "Global step: 3367,loss: 0.019549474\n",
            "\n",
            "Global step: 3368,loss: 0.018518118\n",
            "\n",
            "Global step: 3369,loss: 0.021025429\n",
            "\n",
            "Global step: 3370,loss: 0.019010853\n",
            "\n",
            "Global step: 3371,loss: 0.019305138\n",
            "\n",
            "Global step: 3372,loss: 0.020012992\n",
            "\n",
            "Global step: 3373,loss: 0.020903742\n",
            "\n",
            "Global step: 3374,loss: 0.022351613\n",
            "\n",
            "Global step: 3375,loss: 0.03140797\n",
            "\n",
            "Global step: 3376,loss: 0.017618706\n",
            "\n",
            "Global step: 3377,loss: 0.02169595\n",
            "\n",
            "Global step: 3378,loss: 0.023072338\n",
            "\n",
            "Global step: 3379,loss: 0.019538188\n",
            "\n",
            "Global step: 3380,loss: 0.021149008\n",
            "\n",
            "Global step: 3381,loss: 0.019225962\n",
            "\n",
            "Global step: 3382,loss: 0.025119402\n",
            "\n",
            "Global step: 3383,loss: 0.018889692\n",
            "\n",
            "Global step: 3384,loss: 0.019463433\n",
            "\n",
            "Global step: 3385,loss: 0.019785542\n",
            "\n",
            "Global step: 3386,loss: 0.019514713\n",
            "\n",
            "Global step: 3387,loss: 0.019791808\n",
            "\n",
            "Global step: 3388,loss: 0.018471597\n",
            "\n",
            "Global step: 3389,loss: 0.019616883\n",
            "\n",
            "Global step: 3390,loss: 0.020110881\n",
            "\n",
            "Global step: 3391,loss: 0.024789918\n",
            "\n",
            "Global step: 3392,loss: 0.023319066\n",
            "\n",
            "Global step: 3393,loss: 0.018824413\n",
            "\n",
            "Global step: 3394,loss: 0.018963562\n",
            "\n",
            "Global step: 3395,loss: 0.019625958\n",
            "\n",
            "Global step: 3396,loss: 0.019068105\n",
            "\n",
            "Global step: 3397,loss: 0.031406492\n",
            "\n",
            "Global step: 3398,loss: 0.018728077\n",
            "\n",
            "Global step: 3399,loss: 0.018690052\n",
            "\n",
            "Global step: 3400,loss: 0.019481136\n",
            "\n",
            "Global step: 3401,loss: 0.018919189\n",
            "\n",
            "Global step: 3402,loss: 0.019202322\n",
            "\n",
            "Global step: 3403,loss: 0.02583555\n",
            "\n",
            "Global step: 3404,loss: 0.018365538\n",
            "\n",
            "Global step: 3405,loss: 0.018474538\n",
            "\n",
            "Global step: 3406,loss: 0.020716406\n",
            "\n",
            "Global step: 3407,loss: 0.019209487\n",
            "\n",
            "Global step: 3408,loss: 0.020117842\n",
            "\n",
            "Global step: 3409,loss: 0.01996829\n",
            "\n",
            "Global step: 3410,loss: 0.020695115\n",
            "\n",
            "Global step: 3411,loss: 0.018870449\n",
            "\n",
            "Global step: 3412,loss: 0.018846674\n",
            "\n",
            "Global step: 3413,loss: 0.01904169\n",
            "\n",
            "Global step: 3414,loss: 0.019482454\n",
            "\n",
            "Global step: 3415,loss: 0.018266879\n",
            "\n",
            "Global step: 3416,loss: 0.021687513\n",
            "\n",
            "Global step: 3417,loss: 0.02059802\n",
            "\n",
            "Global step: 3418,loss: 0.024484348\n",
            "\n",
            "Global step: 3419,loss: 0.018266099\n",
            "\n",
            "Global step: 3420,loss: 0.01848925\n",
            "\n",
            "Global step: 3421,loss: 0.018244456\n",
            "\n",
            "Global step: 3422,loss: 0.019941304\n",
            "\n",
            "Global step: 3423,loss: 0.018782325\n",
            "\n",
            "Global step: 3424,loss: 0.022675699\n",
            "\n",
            "Global step: 3425,loss: 0.019248622\n",
            "\n",
            "Global step: 3426,loss: 0.02368964\n",
            "\n",
            "Global step: 3427,loss: 0.018009288\n",
            "\n",
            "Global step: 3428,loss: 0.018105632\n",
            "\n",
            "Global step: 3429,loss: 0.019137098\n",
            "\n",
            "Global step: 3430,loss: 0.018924594\n",
            "\n",
            "Global step: 3431,loss: 0.017974032\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3431,Val_Loss: 0.02279065014460148,  Val_acc: 0.9981971153846154 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:27:00.102536 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 9/15:\n",
            "Global step: 3432,loss: 0.017956788\n",
            "\n",
            "Global step: 3433,loss: 0.018159134\n",
            "\n",
            "Global step: 3434,loss: 0.01963008\n",
            "\n",
            "Global step: 3435,loss: 0.02206031\n",
            "\n",
            "Global step: 3436,loss: 0.019414216\n",
            "\n",
            "Global step: 3437,loss: 0.0182056\n",
            "\n",
            "Global step: 3438,loss: 0.018694684\n",
            "\n",
            "Global step: 3439,loss: 0.018817458\n",
            "\n",
            "Global step: 3440,loss: 0.025729436\n",
            "\n",
            "Global step: 3441,loss: 0.017976673\n",
            "\n",
            "Global step: 3442,loss: 0.01812597\n",
            "\n",
            "Global step: 3443,loss: 0.019235687\n",
            "\n",
            "Global step: 3444,loss: 0.018183136\n",
            "\n",
            "Global step: 3445,loss: 0.02095465\n",
            "\n",
            "Global step: 3446,loss: 0.019528989\n",
            "\n",
            "Global step: 3447,loss: 0.018835226\n",
            "\n",
            "Global step: 3448,loss: 0.019022383\n",
            "\n",
            "Global step: 3449,loss: 0.020010063\n",
            "\n",
            "Global step: 3450,loss: 0.018309169\n",
            "\n",
            "Global step: 3451,loss: 0.019891266\n",
            "\n",
            "Global step: 3452,loss: 0.017945427\n",
            "\n",
            "Global step: 3453,loss: 0.023663938\n",
            "\n",
            "Global step: 3454,loss: 0.017724134\n",
            "\n",
            "Global step: 3455,loss: 0.018024465\n",
            "\n",
            "Global step: 3456,loss: 0.018872473\n",
            "\n",
            "Global step: 3457,loss: 0.018024426\n",
            "\n",
            "Global step: 3458,loss: 0.019470587\n",
            "\n",
            "Global step: 3459,loss: 0.020163553\n",
            "\n",
            "Global step: 3460,loss: 0.017975017\n",
            "\n",
            "Global step: 3461,loss: 0.018032309\n",
            "\n",
            "Global step: 3462,loss: 0.023054622\n",
            "\n",
            "Global step: 3463,loss: 0.018254753\n",
            "\n",
            "Global step: 3464,loss: 0.019817099\n",
            "\n",
            "Global step: 3465,loss: 0.018444922\n",
            "\n",
            "Global step: 3466,loss: 0.01774015\n",
            "\n",
            "Global step: 3467,loss: 0.017557727\n",
            "\n",
            "Global step: 3468,loss: 0.020935033\n",
            "\n",
            "Global step: 3469,loss: 0.017737564\n",
            "\n",
            "Global step: 3470,loss: 0.022023385\n",
            "\n",
            "Global step: 3471,loss: 0.01834408\n",
            "\n",
            "Global step: 3472,loss: 0.018882796\n",
            "\n",
            "Global step: 3473,loss: 0.01964091\n",
            "\n",
            "Global step: 3474,loss: 0.019660885\n",
            "\n",
            "Global step: 3475,loss: 0.017779548\n",
            "\n",
            "Global step: 3476,loss: 0.018453963\n",
            "\n",
            "Global step: 3477,loss: 0.019247096\n",
            "\n",
            "Global step: 3478,loss: 0.018844342\n",
            "\n",
            "Global step: 3479,loss: 0.018458935\n",
            "\n",
            "Global step: 3480,loss: 0.017749503\n",
            "\n",
            "Global step: 3481,loss: 0.019624189\n",
            "\n",
            "Global step: 3482,loss: 0.01877833\n",
            "\n",
            "Global step: 3483,loss: 0.019788738\n",
            "\n",
            "Global step: 3484,loss: 0.022576615\n",
            "\n",
            "Global step: 3485,loss: 0.01867575\n",
            "\n",
            "Global step: 3486,loss: 0.018497115\n",
            "\n",
            "Global step: 3487,loss: 0.01891889\n",
            "\n",
            "Global step: 3488,loss: 0.020090668\n",
            "\n",
            "Global step: 3489,loss: 0.01825576\n",
            "\n",
            "Global step: 3490,loss: 0.025133613\n",
            "\n",
            "Global step: 3491,loss: 0.019521818\n",
            "\n",
            "Global step: 3492,loss: 0.020472646\n",
            "\n",
            "Global step: 3493,loss: 0.019049391\n",
            "\n",
            "Global step: 3494,loss: 0.01799645\n",
            "\n",
            "Global step: 3495,loss: 0.018111357\n",
            "\n",
            "Global step: 3496,loss: 0.023357458\n",
            "\n",
            "Global step: 3497,loss: 0.018438522\n",
            "\n",
            "Global step: 3498,loss: 0.021569885\n",
            "\n",
            "Global step: 3499,loss: 0.019822024\n",
            "\n",
            "Global step: 3500,loss: 0.018807508\n",
            "\n",
            "Global step: 3501,loss: 0.025911272\n",
            "\n",
            "Global step: 3502,loss: 0.028433638\n",
            "\n",
            "Global step: 3503,loss: 0.02905165\n",
            "\n",
            "Global step: 3504,loss: 0.021684527\n",
            "\n",
            "Global step: 3505,loss: 0.021413676\n",
            "\n",
            "Global step: 3506,loss: 0.020073684\n",
            "\n",
            "Global step: 3507,loss: 0.01944205\n",
            "\n",
            "Global step: 3508,loss: 0.019709803\n",
            "\n",
            "Global step: 3509,loss: 0.019420486\n",
            "\n",
            "Global step: 3510,loss: 0.018308476\n",
            "\n",
            "Global step: 3511,loss: 0.018460184\n",
            "\n",
            "Global step: 3512,loss: 0.018683642\n",
            "\n",
            "Global step: 3513,loss: 0.019541804\n",
            "\n",
            "Global step: 3514,loss: 0.018149832\n",
            "\n",
            "Global step: 3515,loss: 0.019367443\n",
            "\n",
            "Global step: 3516,loss: 0.01977777\n",
            "\n",
            "Global step: 3517,loss: 0.019436937\n",
            "\n",
            "Global step: 3518,loss: 0.01846874\n",
            "\n",
            "Global step: 3519,loss: 0.018927185\n",
            "\n",
            "Global step: 3520,loss: 0.017730033\n",
            "\n",
            "Global step: 3521,loss: 0.018324114\n",
            "\n",
            "Global step: 3522,loss: 0.020681603\n",
            "\n",
            "Global step: 3523,loss: 0.018077323\n",
            "\n",
            "Global step: 3524,loss: 0.025031537\n",
            "\n",
            "Global step: 3525,loss: 0.020993661\n",
            "\n",
            "Global step: 3526,loss: 0.022244563\n",
            "\n",
            "Global step: 3527,loss: 0.020275455\n",
            "\n",
            "Global step: 3528,loss: 0.019970011\n",
            "\n",
            "Global step: 3529,loss: 0.018797308\n",
            "\n",
            "Global step: 3530,loss: 0.01831667\n",
            "\n",
            "Global step: 3531,loss: 0.01937929\n",
            "\n",
            "Global step: 3532,loss: 0.019258738\n",
            "\n",
            "Global step: 3533,loss: 0.018672239\n",
            "\n",
            "Global step: 3534,loss: 0.01949129\n",
            "\n",
            "Global step: 3535,loss: 0.01873554\n",
            "\n",
            "Global step: 3536,loss: 0.019536871\n",
            "\n",
            "Global step: 3537,loss: 0.019514687\n",
            "\n",
            "Global step: 3538,loss: 0.019652791\n",
            "\n",
            "Global step: 3539,loss: 0.019340154\n",
            "\n",
            "Global step: 3540,loss: 0.01870648\n",
            "\n",
            "Global step: 3541,loss: 0.02224563\n",
            "\n",
            "Global step: 3542,loss: 0.018432079\n",
            "\n",
            "Global step: 3543,loss: 0.017479446\n",
            "\n",
            "Global step: 3544,loss: 0.019913388\n",
            "\n",
            "Global step: 3545,loss: 0.018707598\n",
            "\n",
            "Global step: 3546,loss: 0.017959008\n",
            "\n",
            "Global step: 3547,loss: 0.017839635\n",
            "\n",
            "Global step: 3548,loss: 0.018012175\n",
            "\n",
            "Global step: 3549,loss: 0.018225066\n",
            "\n",
            "Global step: 3550,loss: 0.023425521\n",
            "\n",
            "Global step: 3551,loss: 0.01890474\n",
            "\n",
            "Global step: 3552,loss: 0.01941737\n",
            "\n",
            "Global step: 3553,loss: 0.019285731\n",
            "\n",
            "Global step: 3554,loss: 0.019058978\n",
            "\n",
            "Global step: 3555,loss: 0.018638533\n",
            "\n",
            "Global step: 3556,loss: 0.01820273\n",
            "\n",
            "Global step: 3557,loss: 0.019193772\n",
            "\n",
            "Global step: 3558,loss: 0.019034266\n",
            "\n",
            "Global step: 3559,loss: 0.019902132\n",
            "\n",
            "Global step: 3560,loss: 0.018367093\n",
            "\n",
            "Global step: 3561,loss: 0.018599847\n",
            "\n",
            "Global step: 3562,loss: 0.018744854\n",
            "\n",
            "Global step: 3563,loss: 0.023592966\n",
            "\n",
            "Global step: 3564,loss: 0.021405017\n",
            "\n",
            "Global step: 3565,loss: 0.017803127\n",
            "\n",
            "Global step: 3566,loss: 0.01876312\n",
            "\n",
            "Global step: 3567,loss: 0.018689642\n",
            "\n",
            "Global step: 3568,loss: 0.021673353\n",
            "\n",
            "Global step: 3569,loss: 0.022044333\n",
            "\n",
            "Global step: 3570,loss: 0.01922952\n",
            "\n",
            "Global step: 3571,loss: 0.018923374\n",
            "\n",
            "Global step: 3572,loss: 0.017509034\n",
            "\n",
            "Global step: 3573,loss: 0.018016402\n",
            "\n",
            "Global step: 3574,loss: 0.017367134\n",
            "\n",
            "Global step: 3575,loss: 0.02190416\n",
            "\n",
            "Global step: 3576,loss: 0.01837353\n",
            "\n",
            "Global step: 3577,loss: 0.018369077\n",
            "\n",
            "Global step: 3578,loss: 0.018055514\n",
            "\n",
            "Global step: 3579,loss: 0.017883247\n",
            "\n",
            "Global step: 3580,loss: 0.018161915\n",
            "\n",
            "Global step: 3581,loss: 0.01624404\n",
            "\n",
            "Global step: 3582,loss: 0.017501036\n",
            "\n",
            "Global step: 3583,loss: 0.018236563\n",
            "\n",
            "Global step: 3584,loss: 0.019197775\n",
            "\n",
            "Global step: 3585,loss: 0.017638372\n",
            "\n",
            "Global step: 3586,loss: 0.017995121\n",
            "\n",
            "Global step: 3587,loss: 0.01693672\n",
            "\n",
            "Global step: 3588,loss: 0.018314593\n",
            "\n",
            "Global step: 3589,loss: 0.019187938\n",
            "\n",
            "Global step: 3590,loss: 0.019009273\n",
            "\n",
            "Global step: 3591,loss: 0.023311619\n",
            "\n",
            "Global step: 3592,loss: 0.023163226\n",
            "\n",
            "Global step: 3593,loss: 0.017574161\n",
            "\n",
            "Global step: 3594,loss: 0.019554073\n",
            "\n",
            "Global step: 3595,loss: 0.018919682\n",
            "\n",
            "Global step: 3596,loss: 0.019106874\n",
            "\n",
            "Global step: 3597,loss: 0.01841954\n",
            "\n",
            "Global step: 3598,loss: 0.018034339\n",
            "\n",
            "Global step: 3599,loss: 0.019482026\n",
            "\n",
            "Global step: 3600,loss: 0.019241426\n",
            "\n",
            "Global step: 3601,loss: 0.018362276\n",
            "\n",
            "Global step: 3602,loss: 0.018662317\n",
            "\n",
            "Global step: 3603,loss: 0.018054795\n",
            "\n",
            "Global step: 3604,loss: 0.01845235\n",
            "\n",
            "Global step: 3605,loss: 0.019829188\n",
            "\n",
            "Global step: 3606,loss: 0.017676506\n",
            "\n",
            "Global step: 3607,loss: 0.018634222\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.7104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:27:22.133245 139648999347968 supervisor.py:1099] global_step/sec: 7.7104\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3608,loss: 0.018068807\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3609.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:27:22.326594 139649007740672 supervisor.py:1050] Recording summary at step 3609.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3609,loss: 0.02046964\n",
            "\n",
            "Global step: 3610,loss: 0.01753969\n",
            "\n",
            "Global step: 3611,loss: 0.0191659\n",
            "\n",
            "Global step: 3612,loss: 0.019591274\n",
            "\n",
            "Global step: 3613,loss: 0.019874586\n",
            "\n",
            "Global step: 3614,loss: 0.018004214\n",
            "\n",
            "Global step: 3615,loss: 0.030218393\n",
            "\n",
            "Global step: 3616,loss: 0.017862804\n",
            "\n",
            "Global step: 3617,loss: 0.019170575\n",
            "\n",
            "Global step: 3618,loss: 0.017149556\n",
            "\n",
            "Global step: 3619,loss: 0.018106947\n",
            "\n",
            "Global step: 3620,loss: 0.024589587\n",
            "\n",
            "Global step: 3621,loss: 0.017657876\n",
            "\n",
            "Global step: 3622,loss: 0.018182976\n",
            "\n",
            "Global step: 3623,loss: 0.018826718\n",
            "\n",
            "Global step: 3624,loss: 0.018153016\n",
            "\n",
            "Global step: 3625,loss: 0.018493203\n",
            "\n",
            "Global step: 3626,loss: 0.019404572\n",
            "\n",
            "Global step: 3627,loss: 0.019139867\n",
            "\n",
            "Global step: 3628,loss: 0.020430923\n",
            "\n",
            "Global step: 3629,loss: 0.017757844\n",
            "\n",
            "Global step: 3630,loss: 0.019080283\n",
            "\n",
            "Global step: 3631,loss: 0.017674115\n",
            "\n",
            "Global step: 3632,loss: 0.023404859\n",
            "\n",
            "Global step: 3633,loss: 0.019646022\n",
            "\n",
            "Global step: 3634,loss: 0.018665617\n",
            "\n",
            "Global step: 3635,loss: 0.017934177\n",
            "\n",
            "Global step: 3636,loss: 0.017613864\n",
            "\n",
            "Global step: 3637,loss: 0.019568764\n",
            "\n",
            "Global step: 3638,loss: 0.01747219\n",
            "\n",
            "Global step: 3639,loss: 0.017288463\n",
            "\n",
            "Global step: 3640,loss: 0.019711794\n",
            "\n",
            "Global step: 3641,loss: 0.025050974\n",
            "\n",
            "Global step: 3642,loss: 0.019059325\n",
            "\n",
            "Global step: 3643,loss: 0.018787494\n",
            "\n",
            "Global step: 3644,loss: 0.017136505\n",
            "\n",
            "Global step: 3645,loss: 0.01733023\n",
            "\n",
            "Global step: 3646,loss: 0.016831785\n",
            "\n",
            "Global step: 3647,loss: 0.01840726\n",
            "\n",
            "Global step: 3648,loss: 0.020648565\n",
            "\n",
            "Global step: 3649,loss: 0.020896737\n",
            "\n",
            "Global step: 3650,loss: 0.027911618\n",
            "\n",
            "Global step: 3651,loss: 0.019146357\n",
            "\n",
            "Global step: 3652,loss: 0.018494105\n",
            "\n",
            "Global step: 3653,loss: 0.01962836\n",
            "\n",
            "Global step: 3654,loss: 0.023523968\n",
            "\n",
            "Global step: 3655,loss: 0.017956095\n",
            "\n",
            "Global step: 3656,loss: 0.017624542\n",
            "\n",
            "Global step: 3657,loss: 0.01832855\n",
            "\n",
            "Global step: 3658,loss: 0.027895235\n",
            "\n",
            "Global step: 3659,loss: 0.017820304\n",
            "\n",
            "Global step: 3660,loss: 0.01824035\n",
            "\n",
            "Global step: 3661,loss: 0.018564321\n",
            "\n",
            "Global step: 3662,loss: 0.019186763\n",
            "\n",
            "Global step: 3663,loss: 0.019174758\n",
            "\n",
            "Global step: 3664,loss: 0.018269053\n",
            "\n",
            "Global step: 3665,loss: 0.017603174\n",
            "\n",
            "Global step: 3666,loss: 0.0188808\n",
            "\n",
            "Global step: 3667,loss: 0.018661592\n",
            "\n",
            "Global step: 3668,loss: 0.018517923\n",
            "\n",
            "Global step: 3669,loss: 0.017823985\n",
            "\n",
            "Global step: 3670,loss: 0.01972961\n",
            "\n",
            "Global step: 3671,loss: 0.017739914\n",
            "\n",
            "Global step: 3672,loss: 0.019033095\n",
            "\n",
            "Global step: 3673,loss: 0.030079417\n",
            "\n",
            "Global step: 3674,loss: 0.019634262\n",
            "\n",
            "Global step: 3675,loss: 0.019887\n",
            "\n",
            "Global step: 3676,loss: 0.0174302\n",
            "\n",
            "Global step: 3677,loss: 0.018128892\n",
            "\n",
            "Global step: 3678,loss: 0.017340397\n",
            "\n",
            "Global step: 3679,loss: 0.018824197\n",
            "\n",
            "Global step: 3680,loss: 0.018874526\n",
            "\n",
            "Global step: 3681,loss: 0.018121066\n",
            "\n",
            "Global step: 3682,loss: 0.018161831\n",
            "\n",
            "Global step: 3683,loss: 0.017282508\n",
            "\n",
            "Global step: 3684,loss: 0.01715847\n",
            "\n",
            "Global step: 3685,loss: 0.018186962\n",
            "\n",
            "Global step: 3686,loss: 0.018150004\n",
            "\n",
            "Global step: 3687,loss: 0.018541656\n",
            "\n",
            "Global step: 3688,loss: 0.018890403\n",
            "\n",
            "Global step: 3689,loss: 0.018817032\n",
            "\n",
            "Global step: 3690,loss: 0.018351702\n",
            "\n",
            "Global step: 3691,loss: 0.017851682\n",
            "\n",
            "Global step: 3692,loss: 0.018107781\n",
            "\n",
            "Global step: 3693,loss: 0.018526714\n",
            "\n",
            "Global step: 3694,loss: 0.019229691\n",
            "\n",
            "Global step: 3695,loss: 0.020966668\n",
            "\n",
            "Global step: 3696,loss: 0.017576316\n",
            "\n",
            "Global step: 3697,loss: 0.018062884\n",
            "\n",
            "Global step: 3698,loss: 0.029087475\n",
            "\n",
            "Global step: 3699,loss: 0.019980546\n",
            "\n",
            "Global step: 3700,loss: 0.016873116\n",
            "\n",
            "Global step: 3701,loss: 0.020255128\n",
            "\n",
            "Global step: 3702,loss: 0.017358012\n",
            "\n",
            "Global step: 3703,loss: 0.019638218\n",
            "\n",
            "Global step: 3704,loss: 0.017450277\n",
            "\n",
            "Global step: 3705,loss: 0.020594392\n",
            "\n",
            "Global step: 3706,loss: 0.023997774\n",
            "\n",
            "Global step: 3707,loss: 0.019386059\n",
            "\n",
            "Global step: 3708,loss: 0.019650927\n",
            "\n",
            "Global step: 3709,loss: 0.019047804\n",
            "\n",
            "Global step: 3710,loss: 0.020466672\n",
            "\n",
            "Global step: 3711,loss: 0.017663918\n",
            "\n",
            "Global step: 3712,loss: 0.024962291\n",
            "\n",
            "Global step: 3713,loss: 0.018039577\n",
            "\n",
            "Global step: 3714,loss: 0.018919969\n",
            "\n",
            "Global step: 3715,loss: 0.018398216\n",
            "\n",
            "Global step: 3716,loss: 0.020587068\n",
            "\n",
            "Global step: 3717,loss: 0.018897675\n",
            "\n",
            "Global step: 3718,loss: 0.018197408\n",
            "\n",
            "Global step: 3719,loss: 0.019714877\n",
            "\n",
            "Global step: 3720,loss: 0.019647352\n",
            "\n",
            "Global step: 3721,loss: 0.01785357\n",
            "\n",
            "Global step: 3722,loss: 0.017684394\n",
            "\n",
            "Global step: 3723,loss: 0.017985586\n",
            "\n",
            "Global step: 3724,loss: 0.020697203\n",
            "\n",
            "Global step: 3725,loss: 0.01867968\n",
            "\n",
            "Global step: 3726,loss: 0.018072989\n",
            "\n",
            "Global step: 3727,loss: 0.017071456\n",
            "\n",
            "Global step: 3728,loss: 0.018003112\n",
            "\n",
            "Global step: 3729,loss: 0.017829482\n",
            "\n",
            "Global step: 3730,loss: 0.018659173\n",
            "\n",
            "Global step: 3731,loss: 0.01798352\n",
            "\n",
            "Global step: 3732,loss: 0.022174437\n",
            "\n",
            "Global step: 3733,loss: 0.01886218\n",
            "\n",
            "Global step: 3734,loss: 0.018512174\n",
            "\n",
            "Global step: 3735,loss: 0.018459192\n",
            "\n",
            "Global step: 3736,loss: 0.01796143\n",
            "\n",
            "Global step: 3737,loss: 0.02001134\n",
            "\n",
            "Global step: 3738,loss: 0.018299028\n",
            "\n",
            "Global step: 3739,loss: 0.01824855\n",
            "\n",
            "Global step: 3740,loss: 0.017796792\n",
            "\n",
            "Global step: 3741,loss: 0.017473912\n",
            "\n",
            "Global step: 3742,loss: 0.017378319\n",
            "\n",
            "Global step: 3743,loss: 0.018659169\n",
            "\n",
            "Global step: 3744,loss: 0.018396495\n",
            "\n",
            "Global step: 3745,loss: 0.017456472\n",
            "\n",
            "Global step: 3746,loss: 0.017088635\n",
            "\n",
            "Global step: 3747,loss: 0.019134492\n",
            "\n",
            "Global step: 3748,loss: 0.018430399\n",
            "\n",
            "Global step: 3749,loss: 0.018406836\n",
            "\n",
            "Global step: 3750,loss: 0.018870108\n",
            "\n",
            "Global step: 3751,loss: 0.019223344\n",
            "\n",
            "Global step: 3752,loss: 0.01784681\n",
            "\n",
            "Global step: 3753,loss: 0.018757362\n",
            "\n",
            "Global step: 3754,loss: 0.016936181\n",
            "\n",
            "Global step: 3755,loss: 0.02166288\n",
            "\n",
            "Global step: 3756,loss: 0.017533412\n",
            "\n",
            "Global step: 3757,loss: 0.018930672\n",
            "\n",
            "Global step: 3758,loss: 0.018535929\n",
            "\n",
            "Global step: 3759,loss: 0.018922891\n",
            "\n",
            "Global step: 3760,loss: 0.019431554\n",
            "\n",
            "Global step: 3761,loss: 0.018911859\n",
            "\n",
            "Global step: 3762,loss: 0.022073876\n",
            "\n",
            "Global step: 3763,loss: 0.019793397\n",
            "\n",
            "Global step: 3764,loss: 0.017498946\n",
            "\n",
            "Global step: 3765,loss: 0.019793902\n",
            "\n",
            "Global step: 3766,loss: 0.017608883\n",
            "\n",
            "Global step: 3767,loss: 0.017404329\n",
            "\n",
            "Global step: 3768,loss: 0.018591825\n",
            "\n",
            "Global step: 3769,loss: 0.0213007\n",
            "\n",
            "Global step: 3770,loss: 0.025908798\n",
            "\n",
            "Global step: 3771,loss: 0.018022917\n",
            "\n",
            "Global step: 3772,loss: 0.01815585\n",
            "\n",
            "Global step: 3773,loss: 0.01993197\n",
            "\n",
            "Global step: 3774,loss: 0.018324811\n",
            "\n",
            "Global step: 3775,loss: 0.019758865\n",
            "\n",
            "Global step: 3776,loss: 0.018401302\n",
            "\n",
            "Global step: 3777,loss: 0.01819839\n",
            "\n",
            "Global step: 3778,loss: 0.017016705\n",
            "\n",
            "Global step: 3779,loss: 0.018380798\n",
            "\n",
            "Global step: 3780,loss: 0.018193908\n",
            "\n",
            "Global step: 3781,loss: 0.021566516\n",
            "\n",
            "Global step: 3782,loss: 0.019095602\n",
            "\n",
            "Global step: 3783,loss: 0.017077034\n",
            "\n",
            "Global step: 3784,loss: 0.016758906\n",
            "\n",
            "Global step: 3785,loss: 0.017843131\n",
            "\n",
            "Global step: 3786,loss: 0.017129041\n",
            "\n",
            "Global step: 3787,loss: 0.017939432\n",
            "\n",
            "Global step: 3788,loss: 0.022281898\n",
            "\n",
            "Global step: 3789,loss: 0.017810341\n",
            "\n",
            "Global step: 3790,loss: 0.018316822\n",
            "\n",
            "Global step: 3791,loss: 0.017435096\n",
            "\n",
            "Global step: 3792,loss: 0.019239973\n",
            "\n",
            "Global step: 3793,loss: 0.019196453\n",
            "\n",
            "Global step: 3794,loss: 0.017808093\n",
            "\n",
            "Global step: 3795,loss: 0.017518936\n",
            "\n",
            "Global step: 3796,loss: 0.020651057\n",
            "\n",
            "Global step: 3797,loss: 0.019243293\n",
            "\n",
            "Global step: 3798,loss: 0.016499609\n",
            "\n",
            "Global step: 3799,loss: 0.025192542\n",
            "\n",
            "Global step: 3800,loss: 0.018372394\n",
            "\n",
            "Global step: 3801,loss: 0.018280867\n",
            "\n",
            "Global step: 3802,loss: 0.01843607\n",
            "\n",
            "Global step: 3803,loss: 0.022987448\n",
            "\n",
            "Global step: 3804,loss: 0.019651867\n",
            "\n",
            "Global step: 3805,loss: 0.019294083\n",
            "\n",
            "Global step: 3806,loss: 0.018827412\n",
            "\n",
            "Global step: 3807,loss: 0.020915601\n",
            "\n",
            "Global step: 3808,loss: 0.019445494\n",
            "\n",
            "Global step: 3809,loss: 0.020205764\n",
            "\n",
            "Global step: 3810,loss: 0.019954853\n",
            "\n",
            "Global step: 3811,loss: 0.01920285\n",
            "\n",
            "Global step: 3812,loss: 0.0195005\n",
            "\n",
            "Global step: 3813,loss: 0.022030218\n",
            "\n",
            "Global step: 3814,loss: 0.019455079\n",
            "\n",
            "Global step: 3815,loss: 0.020883925\n",
            "\n",
            "Global step: 3816,loss: 0.022602275\n",
            "\n",
            "Global step: 3817,loss: 0.019228324\n",
            "\n",
            "Global step: 3818,loss: 0.018006263\n",
            "\n",
            "Global step: 3819,loss: 0.01957387\n",
            "\n",
            "Global step: 3820,loss: 0.021553691\n",
            "\n",
            "Global step: 3821,loss: 0.017648846\n",
            "\n",
            "Global step: 3822,loss: 0.019277195\n",
            "\n",
            "Global step: 3823,loss: 0.019315483\n",
            "\n",
            "Global step: 3824,loss: 0.021762583\n",
            "\n",
            "Global step: 3825,loss: 0.026920572\n",
            "\n",
            "Global step: 3826,loss: 0.018245798\n",
            "\n",
            "Global step: 3827,loss: 0.020134592\n",
            "\n",
            "Global step: 3828,loss: 0.018932883\n",
            "\n",
            "Global step: 3829,loss: 0.020087648\n",
            "\n",
            "Global step: 3830,loss: 0.01726325\n",
            "\n",
            "Global step: 3831,loss: 0.01943587\n",
            "\n",
            "Global step: 3832,loss: 0.022069106\n",
            "\n",
            "Global step: 3833,loss: 0.019913761\n",
            "\n",
            "Global step: 3834,loss: 0.019282432\n",
            "\n",
            "Global step: 3835,loss: 0.020417819\n",
            "\n",
            "Global step: 3836,loss: 0.019622816\n",
            "\n",
            "Global step: 3837,loss: 0.018436043\n",
            "\n",
            "Global step: 3838,loss: 0.01800169\n",
            "\n",
            "Global step: 3839,loss: 0.022009747\n",
            "\n",
            "Global step: 3840,loss: 0.017654419\n",
            "\n",
            "Global step: 3841,loss: 0.02104534\n",
            "\n",
            "Global step: 3842,loss: 0.020098401\n",
            "\n",
            "Global step: 3843,loss: 0.023242872\n",
            "\n",
            "Global step: 3844,loss: 0.023481097\n",
            "\n",
            "Global step: 3845,loss: 0.017112847\n",
            "\n",
            "Global step: 3846,loss: 0.01722807\n",
            "\n",
            "Global step: 3847,loss: 0.018857682\n",
            "\n",
            "Global step: 3848,loss: 0.017276872\n",
            "\n",
            "Global step: 3849,loss: 0.017853422\n",
            "\n",
            "Global step: 3850,loss: 0.017940618\n",
            "\n",
            "Global step: 3851,loss: 0.018017313\n",
            "\n",
            "Global step: 3852,loss: 0.016950462\n",
            "\n",
            "Global step: 3853,loss: 0.019101067\n",
            "\n",
            "Global step: 3854,loss: 0.01728205\n",
            "\n",
            "Global step: 3855,loss: 0.017290417\n",
            "\n",
            "Global step: 3856,loss: 0.017859971\n",
            "\n",
            "Global step: 3857,loss: 0.018278195\n",
            "\n",
            "Global step: 3858,loss: 0.01721486\n",
            "\n",
            "Global step: 3859,loss: 0.017420612\n",
            "\n",
            "Global step: 3860,loss: 0.018569183\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 3860,Val_Loss: 0.022381703870800827,  Val_acc: 0.9973958333333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:27:56.215529 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 10/15:\n",
            "Global step: 3861,loss: 0.017834011\n",
            "\n",
            "Global step: 3862,loss: 0.016823662\n",
            "\n",
            "Global step: 3863,loss: 0.016885122\n",
            "\n",
            "Global step: 3864,loss: 0.019276839\n",
            "\n",
            "Global step: 3865,loss: 0.017489206\n",
            "\n",
            "Global step: 3866,loss: 0.01767603\n",
            "\n",
            "Global step: 3867,loss: 0.020559417\n",
            "\n",
            "Global step: 3868,loss: 0.019008955\n",
            "\n",
            "Global step: 3869,loss: 0.018210871\n",
            "\n",
            "Global step: 3870,loss: 0.019886892\n",
            "\n",
            "Global step: 3871,loss: 0.016893566\n",
            "\n",
            "Global step: 3872,loss: 0.017360222\n",
            "\n",
            "Global step: 3873,loss: 0.017573548\n",
            "\n",
            "Global step: 3874,loss: 0.017851347\n",
            "\n",
            "Global step: 3875,loss: 0.017455013\n",
            "\n",
            "Global step: 3876,loss: 0.018173635\n",
            "\n",
            "Global step: 3877,loss: 0.017162668\n",
            "\n",
            "Global step: 3878,loss: 0.020156851\n",
            "\n",
            "Global step: 3879,loss: 0.017924124\n",
            "\n",
            "Global step: 3880,loss: 0.0179295\n",
            "\n",
            "Global step: 3881,loss: 0.018259319\n",
            "\n",
            "Global step: 3882,loss: 0.016375137\n",
            "\n",
            "Global step: 3883,loss: 0.018808287\n",
            "\n",
            "Global step: 3884,loss: 0.017433375\n",
            "\n",
            "Global step: 3885,loss: 0.022070458\n",
            "\n",
            "Global step: 3886,loss: 0.018012073\n",
            "\n",
            "Global step: 3887,loss: 0.018192807\n",
            "\n",
            "Global step: 3888,loss: 0.020599158\n",
            "\n",
            "Global step: 3889,loss: 0.01862528\n",
            "\n",
            "Global step: 3890,loss: 0.018150711\n",
            "\n",
            "Global step: 3891,loss: 0.018456578\n",
            "\n",
            "Global step: 3892,loss: 0.019406563\n",
            "\n",
            "Global step: 3893,loss: 0.01728582\n",
            "\n",
            "Global step: 3894,loss: 0.016509717\n",
            "\n",
            "Global step: 3895,loss: 0.017196577\n",
            "\n",
            "Global step: 3896,loss: 0.019030822\n",
            "\n",
            "Global step: 3897,loss: 0.017373865\n",
            "\n",
            "Global step: 3898,loss: 0.017052172\n",
            "\n",
            "Global step: 3899,loss: 0.01900361\n",
            "\n",
            "Global step: 3900,loss: 0.017825032\n",
            "\n",
            "Global step: 3901,loss: 0.017683\n",
            "\n",
            "Global step: 3902,loss: 0.017098624\n",
            "\n",
            "Global step: 3903,loss: 0.017890168\n",
            "\n",
            "Global step: 3904,loss: 0.017639574\n",
            "\n",
            "Global step: 3905,loss: 0.01784376\n",
            "\n",
            "Global step: 3906,loss: 0.019034557\n",
            "\n",
            "Global step: 3907,loss: 0.01733847\n",
            "\n",
            "Global step: 3908,loss: 0.017708981\n",
            "\n",
            "Global step: 3909,loss: 0.018836264\n",
            "\n",
            "Global step: 3910,loss: 0.017112954\n",
            "\n",
            "Global step: 3911,loss: 0.017781254\n",
            "\n",
            "Global step: 3912,loss: 0.018430319\n",
            "\n",
            "Global step: 3913,loss: 0.017821157\n",
            "\n",
            "Global step: 3914,loss: 0.017342892\n",
            "\n",
            "Global step: 3915,loss: 0.017253675\n",
            "\n",
            "Global step: 3916,loss: 0.017240768\n",
            "\n",
            "Global step: 3917,loss: 0.02155712\n",
            "\n",
            "Global step: 3918,loss: 0.018151045\n",
            "\n",
            "Global step: 3919,loss: 0.016972812\n",
            "\n",
            "Global step: 3920,loss: 0.017431138\n",
            "\n",
            "Global step: 3921,loss: 0.017248366\n",
            "\n",
            "Global step: 3922,loss: 0.016833527\n",
            "\n",
            "Global step: 3923,loss: 0.017429471\n",
            "\n",
            "Global step: 3924,loss: 0.017115427\n",
            "\n",
            "Global step: 3925,loss: 0.017713672\n",
            "\n",
            "Global step: 3926,loss: 0.018819973\n",
            "\n",
            "Global step: 3927,loss: 0.016916508\n",
            "\n",
            "Global step: 3928,loss: 0.01732188\n",
            "\n",
            "Global step: 3929,loss: 0.02482764\n",
            "\n",
            "Global step: 3930,loss: 0.016281791\n",
            "\n",
            "Global step: 3931,loss: 0.017747495\n",
            "\n",
            "Global step: 3932,loss: 0.0179084\n",
            "\n",
            "Global step: 3933,loss: 0.017129723\n",
            "\n",
            "Global step: 3934,loss: 0.018064994\n",
            "\n",
            "Global step: 3935,loss: 0.017148599\n",
            "\n",
            "Global step: 3936,loss: 0.0175484\n",
            "\n",
            "Global step: 3937,loss: 0.01676663\n",
            "\n",
            "Global step: 3938,loss: 0.018118754\n",
            "\n",
            "Global step: 3939,loss: 0.01713318\n",
            "\n",
            "Global step: 3940,loss: 0.017862778\n",
            "\n",
            "Global step: 3941,loss: 0.01676509\n",
            "\n",
            "Global step: 3942,loss: 0.016342128\n",
            "\n",
            "Global step: 3943,loss: 0.017336478\n",
            "\n",
            "Global step: 3944,loss: 0.019286696\n",
            "\n",
            "Global step: 3945,loss: 0.017892754\n",
            "\n",
            "Global step: 3946,loss: 0.01688104\n",
            "\n",
            "Global step: 3947,loss: 0.018123211\n",
            "\n",
            "Global step: 3948,loss: 0.017881086\n",
            "\n",
            "Global step: 3949,loss: 0.017719796\n",
            "\n",
            "Global step: 3950,loss: 0.016535118\n",
            "\n",
            "Global step: 3951,loss: 0.016875938\n",
            "\n",
            "Global step: 3952,loss: 0.018478958\n",
            "\n",
            "Global step: 3953,loss: 0.017608617\n",
            "\n",
            "Global step: 3954,loss: 0.021419259\n",
            "\n",
            "Global step: 3955,loss: 0.019251239\n",
            "\n",
            "Global step: 3956,loss: 0.021736115\n",
            "\n",
            "Global step: 3957,loss: 0.018021982\n",
            "\n",
            "Global step: 3958,loss: 0.018203536\n",
            "\n",
            "Global step: 3959,loss: 0.01743662\n",
            "\n",
            "Global step: 3960,loss: 0.017497165\n",
            "\n",
            "Global step: 3961,loss: 0.017316243\n",
            "\n",
            "Global step: 3962,loss: 0.01613658\n",
            "\n",
            "Global step: 3963,loss: 0.02332643\n",
            "\n",
            "Global step: 3964,loss: 0.01767177\n",
            "\n",
            "Global step: 3965,loss: 0.01733202\n",
            "\n",
            "Global step: 3966,loss: 0.017376712\n",
            "\n",
            "Global step: 3967,loss: 0.017434541\n",
            "\n",
            "Global step: 3968,loss: 0.018296521\n",
            "\n",
            "Global step: 3969,loss: 0.018812375\n",
            "\n",
            "Global step: 3970,loss: 0.018842328\n",
            "\n",
            "Global step: 3971,loss: 0.018439438\n",
            "\n",
            "Global step: 3972,loss: 0.022437597\n",
            "\n",
            "Global step: 3973,loss: 0.018319646\n",
            "\n",
            "Global step: 3974,loss: 0.018589519\n",
            "\n",
            "Global step: 3975,loss: 0.017868452\n",
            "\n",
            "Global step: 3976,loss: 0.018239764\n",
            "\n",
            "Global step: 3977,loss: 0.01735322\n",
            "\n",
            "Global step: 3978,loss: 0.01735317\n",
            "\n",
            "Global step: 3979,loss: 0.017216874\n",
            "\n",
            "Global step: 3980,loss: 0.01666545\n",
            "\n",
            "Global step: 3981,loss: 0.022580586\n",
            "\n",
            "Global step: 3982,loss: 0.017034119\n",
            "\n",
            "Global step: 3983,loss: 0.016068747\n",
            "\n",
            "Global step: 3984,loss: 0.019599853\n",
            "\n",
            "Global step: 3985,loss: 0.018647347\n",
            "\n",
            "Global step: 3986,loss: 0.017014865\n",
            "\n",
            "Global step: 3987,loss: 0.016898869\n",
            "\n",
            "Global step: 3988,loss: 0.018345064\n",
            "\n",
            "Global step: 3989,loss: 0.017494865\n",
            "\n",
            "Global step: 3990,loss: 0.016337873\n",
            "\n",
            "Global step: 3991,loss: 0.017630102\n",
            "\n",
            "Global step: 3992,loss: 0.017480973\n",
            "\n",
            "Global step: 3993,loss: 0.021842716\n",
            "\n",
            "Global step: 3994,loss: 0.016945092\n",
            "\n",
            "Global step: 3995,loss: 0.017153226\n",
            "\n",
            "Global step: 3996,loss: 0.01780115\n",
            "\n",
            "Global step: 3997,loss: 0.018210223\n",
            "\n",
            "Global step: 3998,loss: 0.016954811\n",
            "\n",
            "Global step: 3999,loss: 0.017540613\n",
            "\n",
            "Global step: 4000,loss: 0.016864225\n",
            "\n",
            "Global step: 4001,loss: 0.01616633\n",
            "\n",
            "Global step: 4002,loss: 0.017697243\n",
            "\n",
            "Global step: 4003,loss: 0.016493795\n",
            "\n",
            "Global step: 4004,loss: 0.017672472\n",
            "\n",
            "Global step: 4005,loss: 0.016970785\n",
            "\n",
            "Global step: 4006,loss: 0.017288322\n",
            "\n",
            "Global step: 4007,loss: 0.018075064\n",
            "\n",
            "Global step: 4008,loss: 0.017979318\n",
            "\n",
            "Global step: 4009,loss: 0.017400574\n",
            "\n",
            "Global step: 4010,loss: 0.016127082\n",
            "\n",
            "Global step: 4011,loss: 0.017127495\n",
            "\n",
            "Global step: 4012,loss: 0.017327776\n",
            "\n",
            "Global step: 4013,loss: 0.025933886\n",
            "\n",
            "Global step: 4014,loss: 0.02707417\n",
            "\n",
            "Global step: 4015,loss: 0.019504726\n",
            "\n",
            "Global step: 4016,loss: 0.019136848\n",
            "\n",
            "Global step: 4017,loss: 0.017288111\n",
            "\n",
            "Global step: 4018,loss: 0.016919551\n",
            "\n",
            "Global step: 4019,loss: 0.017690998\n",
            "\n",
            "Global step: 4020,loss: 0.016951429\n",
            "\n",
            "Global step: 4021,loss: 0.017274367\n",
            "\n",
            "Global step: 4022,loss: 0.018861232\n",
            "\n",
            "Global step: 4023,loss: 0.0178559\n",
            "\n",
            "Global step: 4024,loss: 0.017198816\n",
            "\n",
            "Global step: 4025,loss: 0.01632845\n",
            "\n",
            "Global step: 4026,loss: 0.023406155\n",
            "\n",
            "Global step: 4027,loss: 0.0163938\n",
            "\n",
            "Global step: 4028,loss: 0.01688132\n",
            "\n",
            "Global step: 4029,loss: 0.016867248\n",
            "\n",
            "Global step: 4030,loss: 0.01685809\n",
            "\n",
            "Global step: 4031,loss: 0.016532011\n",
            "\n",
            "Global step: 4032,loss: 0.016376385\n",
            "\n",
            "Global step: 4033,loss: 0.017877517\n",
            "\n",
            "Global step: 4034,loss: 0.017478053\n",
            "\n",
            "Global step: 4035,loss: 0.017265007\n",
            "\n",
            "Global step: 4036,loss: 0.017647086\n",
            "\n",
            "Global step: 4037,loss: 0.018464243\n",
            "\n",
            "Global step: 4038,loss: 0.017094547\n",
            "\n",
            "Global step: 4039,loss: 0.017973892\n",
            "\n",
            "Global step: 4040,loss: 0.016443795\n",
            "\n",
            "Global step: 4041,loss: 0.016113592\n",
            "\n",
            "Global step: 4042,loss: 0.018582495\n",
            "\n",
            "Global step: 4043,loss: 0.02430055\n",
            "\n",
            "Global step: 4044,loss: 0.018483948\n",
            "\n",
            "Global step: 4045,loss: 0.021293504\n",
            "\n",
            "Global step: 4046,loss: 0.01740383\n",
            "\n",
            "Global step: 4047,loss: 0.019034525\n",
            "\n",
            "Global step: 4048,loss: 0.018750884\n",
            "\n",
            "Global step: 4049,loss: 0.018426672\n",
            "\n",
            "Global step: 4050,loss: 0.017932726\n",
            "\n",
            "Global step: 4051,loss: 0.01828431\n",
            "\n",
            "Global step: 4052,loss: 0.017539803\n",
            "\n",
            "Global step: 4053,loss: 0.017914783\n",
            "\n",
            "Global step: 4054,loss: 0.01690797\n",
            "\n",
            "Global step: 4055,loss: 0.018710487\n",
            "\n",
            "Global step: 4056,loss: 0.016787698\n",
            "\n",
            "Global step: 4057,loss: 0.017472422\n",
            "\n",
            "Global step: 4058,loss: 0.01733219\n",
            "\n",
            "Global step: 4059,loss: 0.017400615\n",
            "\n",
            "Global step: 4060,loss: 0.018546047\n",
            "\n",
            "Global step: 4061,loss: 0.018005192\n",
            "\n",
            "Global step: 4062,loss: 0.019894665\n",
            "\n",
            "Global step: 4063,loss: 0.017188003\n",
            "\n",
            "Global step: 4064,loss: 0.017691378\n",
            "\n",
            "Global step: 4065,loss: 0.017964037\n",
            "\n",
            "Global step: 4066,loss: 0.021834167\n",
            "\n",
            "Global step: 4067,loss: 0.016431795\n",
            "\n",
            "Global step: 4068,loss: 0.021097373\n",
            "\n",
            "Global step: 4069,loss: 0.018910065\n",
            "\n",
            "Global step: 4070,loss: 0.029307403\n",
            "\n",
            "Global step: 4071,loss: 0.017142825\n",
            "\n",
            "Global step: 4072,loss: 0.017177895\n",
            "\n",
            "Global step: 4073,loss: 0.01771751\n",
            "\n",
            "Global step: 4074,loss: 0.016966196\n",
            "\n",
            "Global step: 4075,loss: 0.016407402\n",
            "\n",
            "Global step: 4076,loss: 0.016691023\n",
            "\n",
            "Global step: 4077,loss: 0.016258033\n",
            "\n",
            "Global step: 4078,loss: 0.016940124\n",
            "\n",
            "Global step: 4079,loss: 0.01754838\n",
            "\n",
            "Global step: 4080,loss: 0.020423269\n",
            "\n",
            "Global step: 4081,loss: 0.017217679\n",
            "\n",
            "Global step: 4082,loss: 0.018333687\n",
            "\n",
            "Global step: 4083,loss: 0.016677259\n",
            "\n",
            "Global step: 4084,loss: 0.019155255\n",
            "\n",
            "Global step: 4085,loss: 0.016572839\n",
            "\n",
            "Global step: 4086,loss: 0.0175607\n",
            "\n",
            "Global step: 4087,loss: 0.016736168\n",
            "\n",
            "Global step: 4088,loss: 0.017053302\n",
            "\n",
            "Global step: 4089,loss: 0.017499004\n",
            "\n",
            "Global step: 4090,loss: 0.02015432\n",
            "\n",
            "Global step: 4091,loss: 0.016984787\n",
            "\n",
            "Global step: 4092,loss: 0.019225182\n",
            "\n",
            "Global step: 4093,loss: 0.01782411\n",
            "\n",
            "Global step: 4094,loss: 0.019549632\n",
            "\n",
            "Global step: 4095,loss: 0.017461488\n",
            "\n",
            "Global step: 4096,loss: 0.017799186\n",
            "\n",
            "Global step: 4097,loss: 0.0196236\n",
            "\n",
            "Global step: 4098,loss: 0.016809283\n",
            "\n",
            "Global step: 4099,loss: 0.017369462\n",
            "\n",
            "Global step: 4100,loss: 0.016877811\n",
            "\n",
            "Global step: 4101,loss: 0.017723856\n",
            "\n",
            "Global step: 4102,loss: 0.021370515\n",
            "\n",
            "Global step: 4103,loss: 0.017435307\n",
            "\n",
            "Global step: 4104,loss: 0.017700665\n",
            "\n",
            "Global step: 4105,loss: 0.018796297\n",
            "\n",
            "Global step: 4106,loss: 0.022402577\n",
            "\n",
            "Global step: 4107,loss: 0.017489113\n",
            "\n",
            "Global step: 4108,loss: 0.018330319\n",
            "\n",
            "Global step: 4109,loss: 0.016260043\n",
            "\n",
            "Global step: 4110,loss: 0.018841527\n",
            "\n",
            "Global step: 4111,loss: 0.016601907\n",
            "\n",
            "Global step: 4112,loss: 0.016647592\n",
            "\n",
            "Global step: 4113,loss: 0.018370695\n",
            "\n",
            "Global step: 4114,loss: 0.018585121\n",
            "\n",
            "Global step: 4115,loss: 0.017008735\n",
            "\n",
            "Global step: 4116,loss: 0.016502686\n",
            "\n",
            "Global step: 4117,loss: 0.019513719\n",
            "\n",
            "Global step: 4118,loss: 0.016625015\n",
            "\n",
            "Global step: 4119,loss: 0.018869257\n",
            "\n",
            "Global step: 4120,loss: 0.022812262\n",
            "\n",
            "Global step: 4121,loss: 0.018572886\n",
            "\n",
            "Global step: 4122,loss: 0.016588006\n",
            "\n",
            "Global step: 4123,loss: 0.015980083\n",
            "\n",
            "Global step: 4124,loss: 0.016925648\n",
            "\n",
            "Global step: 4125,loss: 0.019612698\n",
            "\n",
            "Global step: 4126,loss: 0.017316734\n",
            "\n",
            "Global step: 4127,loss: 0.018000642\n",
            "\n",
            "Global step: 4128,loss: 0.017261067\n",
            "\n",
            "Global step: 4129,loss: 0.016280694\n",
            "\n",
            "Global step: 4130,loss: 0.01666239\n",
            "\n",
            "Global step: 4131,loss: 0.017641023\n",
            "\n",
            "Global step: 4132,loss: 0.022999708\n",
            "\n",
            "Global step: 4133,loss: 0.017751876\n",
            "\n",
            "Global step: 4134,loss: 0.016503401\n",
            "\n",
            "Global step: 4135,loss: 0.020422092\n",
            "\n",
            "Global step: 4136,loss: 0.017811889\n",
            "\n",
            "Global step: 4137,loss: 0.017175077\n",
            "\n",
            "Global step: 4138,loss: 0.017278641\n",
            "\n",
            "Global step: 4139,loss: 0.017014146\n",
            "\n",
            "Global step: 4140,loss: 0.018611562\n",
            "\n",
            "Global step: 4141,loss: 0.017820675\n",
            "\n",
            "Global step: 4142,loss: 0.018126339\n",
            "\n",
            "Global step: 4143,loss: 0.022308314\n",
            "\n",
            "Global step: 4144,loss: 0.017219368\n",
            "\n",
            "Global step: 4145,loss: 0.017673885\n",
            "\n",
            "Global step: 4146,loss: 0.015994886\n",
            "\n",
            "Global step: 4147,loss: 0.0171351\n",
            "\n",
            "Global step: 4148,loss: 0.02021185\n",
            "\n",
            "Global step: 4149,loss: 0.015686968\n",
            "\n",
            "Global step: 4150,loss: 0.017369121\n",
            "\n",
            "Global step: 4151,loss: 0.017483795\n",
            "\n",
            "Global step: 4152,loss: 0.023490254\n",
            "\n",
            "Global step: 4153,loss: 0.01753041\n",
            "\n",
            "Global step: 4154,loss: 0.017616242\n",
            "\n",
            "Global step: 4155,loss: 0.018185327\n",
            "\n",
            "Global step: 4156,loss: 0.017799277\n",
            "\n",
            "Global step: 4157,loss: 0.01841613\n",
            "\n",
            "Global step: 4158,loss: 0.018236611\n",
            "\n",
            "Global step: 4159,loss: 0.017180825\n",
            "\n",
            "Global step: 4160,loss: 0.018679418\n",
            "\n",
            "Global step: 4161,loss: 0.017138956\n",
            "\n",
            "Global step: 4162,loss: 0.017579308\n",
            "\n",
            "Global step: 4163,loss: 0.017570917\n",
            "\n",
            "Global step: 4164,loss: 0.017724877\n",
            "\n",
            "Global step: 4165,loss: 0.017125195\n",
            "\n",
            "Global step: 4166,loss: 0.017597018\n",
            "\n",
            "Global step: 4167,loss: 0.022714898\n",
            "\n",
            "Global step: 4168,loss: 0.019308543\n",
            "\n",
            "Global step: 4169,loss: 0.02385706\n",
            "\n",
            "Global step: 4170,loss: 0.018538594\n",
            "\n",
            "Global step: 4171,loss: 0.017905626\n",
            "\n",
            "Global step: 4172,loss: 0.018914595\n",
            "\n",
            "Global step: 4173,loss: 0.017712072\n",
            "\n",
            "Global step: 4174,loss: 0.016757851\n",
            "\n",
            "Global step: 4175,loss: 0.017950658\n",
            "\n",
            "Global step: 4176,loss: 0.018308802\n",
            "\n",
            "Global step: 4177,loss: 0.01730987\n",
            "\n",
            "Global step: 4178,loss: 0.018274352\n",
            "\n",
            "Global step: 4179,loss: 0.017734498\n",
            "\n",
            "Global step: 4180,loss: 0.017053239\n",
            "\n",
            "Global step: 4181,loss: 0.017119924\n",
            "\n",
            "Global step: 4182,loss: 0.018673606\n",
            "\n",
            "Global step: 4183,loss: 0.018432556\n",
            "\n",
            "Global step: 4184,loss: 0.017757112\n",
            "\n",
            "Global step: 4185,loss: 0.0174224\n",
            "\n",
            "Global step: 4186,loss: 0.018139929\n",
            "\n",
            "Global step: 4187,loss: 0.016835738\n",
            "\n",
            "Global step: 4188,loss: 0.017508782\n",
            "\n",
            "Global step: 4189,loss: 0.018035918\n",
            "\n",
            "Global step: 4190,loss: 0.01730419\n",
            "\n",
            "Global step: 4191,loss: 0.017380385\n",
            "\n",
            "Global step: 4192,loss: 0.019081578\n",
            "\n",
            "Global step: 4193,loss: 0.018095639\n",
            "\n",
            "Global step: 4194,loss: 0.017371591\n",
            "\n",
            "Global step: 4195,loss: 0.018662034\n",
            "\n",
            "Global step: 4196,loss: 0.018468173\n",
            "\n",
            "Global step: 4197,loss: 0.018529853\n",
            "\n",
            "Global step: 4198,loss: 0.016978491\n",
            "\n",
            "Global step: 4199,loss: 0.023309566\n",
            "\n",
            "Global step: 4200,loss: 0.0166907\n",
            "\n",
            "Global step: 4201,loss: 0.015488076\n",
            "\n",
            "Global step: 4202,loss: 0.017529517\n",
            "\n",
            "Global step: 4203,loss: 0.017264327\n",
            "\n",
            "Global step: 4204,loss: 0.017345518\n",
            "\n",
            "Global step: 4205,loss: 0.017710302\n",
            "\n",
            "Global step: 4206,loss: 0.017439287\n",
            "\n",
            "Global step: 4207,loss: 0.016845524\n",
            "\n",
            "Global step: 4208,loss: 0.015996128\n",
            "\n",
            "Global step: 4209,loss: 0.017604968\n",
            "\n",
            "Global step: 4210,loss: 0.016551208\n",
            "\n",
            "Global step: 4211,loss: 0.022461612\n",
            "\n",
            "Global step: 4212,loss: 0.016726442\n",
            "\n",
            "Global step: 4213,loss: 0.020048477\n",
            "\n",
            "Global step: 4214,loss: 0.016944991\n",
            "\n",
            "Global step: 4215,loss: 0.01710612\n",
            "\n",
            "Global step: 4216,loss: 0.021861251\n",
            "\n",
            "Global step: 4217,loss: 0.016505623\n",
            "\n",
            "Global step: 4218,loss: 0.020949831\n",
            "\n",
            "Global step: 4219,loss: 0.017896205\n",
            "\n",
            "Global step: 4220,loss: 0.019692754\n",
            "\n",
            "Global step: 4221,loss: 0.01914933\n",
            "\n",
            "Global step: 4222,loss: 0.018276243\n",
            "\n",
            "Global step: 4223,loss: 0.016749565\n",
            "\n",
            "Global step: 4224,loss: 0.017438458\n",
            "\n",
            "Global step: 4225,loss: 0.01704414\n",
            "\n",
            "Global step: 4226,loss: 0.01641884\n",
            "\n",
            "Global step: 4227,loss: 0.017226174\n",
            "\n",
            "Global step: 4228,loss: 0.019355992\n",
            "\n",
            "Global step: 4229,loss: 0.02040238\n",
            "\n",
            "Global step: 4230,loss: 0.017107457\n",
            "\n",
            "Global step: 4231,loss: 0.017919095\n",
            "\n",
            "Global step: 4232,loss: 0.018029038\n",
            "\n",
            "Global step: 4233,loss: 0.016419316\n",
            "\n",
            "Global step: 4234,loss: 0.019994393\n",
            "\n",
            "Global step: 4235,loss: 0.018977396\n",
            "\n",
            "Global step: 4236,loss: 0.017695578\n",
            "\n",
            "Global step: 4237,loss: 0.018388199\n",
            "\n",
            "Global step: 4238,loss: 0.01713222\n",
            "\n",
            "Global step: 4239,loss: 0.017148258\n",
            "\n",
            "Global step: 4240,loss: 0.017833404\n",
            "\n",
            "Global step: 4241,loss: 0.016521823\n",
            "\n",
            "Global step: 4242,loss: 0.01773242\n",
            "\n",
            "Global step: 4243,loss: 0.017213576\n",
            "\n",
            "Global step: 4244,loss: 0.017752161\n",
            "\n",
            "Global step: 4245,loss: 0.018633556\n",
            "\n",
            "Global step: 4246,loss: 0.016979137\n",
            "\n",
            "Global step: 4247,loss: 0.018560775\n",
            "\n",
            "Global step: 4248,loss: 0.018068537\n",
            "\n",
            "Global step: 4249,loss: 0.016063916\n",
            "\n",
            "Global step: 4250,loss: 0.017092397\n",
            "\n",
            "Global step: 4251,loss: 0.0174832\n",
            "\n",
            "Global step: 4252,loss: 0.01741749\n",
            "\n",
            "Global step: 4253,loss: 0.016183162\n",
            "\n",
            "Global step: 4254,loss: 0.01768044\n",
            "\n",
            "Global step: 4255,loss: 0.017308878\n",
            "\n",
            "Global step: 4256,loss: 0.01676854\n",
            "\n",
            "Global step: 4257,loss: 0.017342988\n",
            "\n",
            "Global step: 4258,loss: 0.018482774\n",
            "\n",
            "Global step: 4259,loss: 0.016252855\n",
            "\n",
            "Global step: 4260,loss: 0.017395066\n",
            "\n",
            "Global step: 4261,loss: 0.017185267\n",
            "\n",
            "Global step: 4262,loss: 0.01702771\n",
            "\n",
            "Global step: 4263,loss: 0.024134282\n",
            "\n",
            "Global step: 4264,loss: 0.01901299\n",
            "\n",
            "Global step: 4265,loss: 0.0179136\n",
            "\n",
            "Global step: 4266,loss: 0.017424189\n",
            "\n",
            "Global step: 4267,loss: 0.016241005\n",
            "\n",
            "Global step: 4268,loss: 0.017672244\n",
            "\n",
            "Global step: 4269,loss: 0.018276641\n",
            "\n",
            "Global step: 4270,loss: 0.016653081\n",
            "\n",
            "Global step: 4271,loss: 0.016519504\n",
            "\n",
            "Global step: 4272,loss: 0.01757238\n",
            "\n",
            "Global step: 4273,loss: 0.015717836\n",
            "\n",
            "Global step: 4274,loss: 0.0155618265\n",
            "\n",
            "Global step: 4275,loss: 0.019342883\n",
            "\n",
            "Global step: 4276,loss: 0.017932534\n",
            "\n",
            "Global step: 4277,loss: 0.018031016\n",
            "\n",
            "Global step: 4278,loss: 0.024541914\n",
            "\n",
            "Global step: 4279,loss: 0.017920492\n",
            "\n",
            "Global step: 4280,loss: 0.017076576\n",
            "\n",
            "Global step: 4281,loss: 0.016730309\n",
            "\n",
            "Global step: 4282,loss: 0.016561987\n",
            "\n",
            "Global step: 4283,loss: 0.016600432\n",
            "\n",
            "Global step: 4284,loss: 0.01737987\n",
            "\n",
            "Global step: 4285,loss: 0.01559287\n",
            "\n",
            "Global step: 4286,loss: 0.016264489\n",
            "\n",
            "Global step: 4287,loss: 0.016488831\n",
            "\n",
            "Global step: 4288,loss: 0.016787501\n",
            "\n",
            "Global step: 4289,loss: 0.015801294\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4289,Val_Loss: 0.021995258875764333,  Val_acc: 0.9969951923076923 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:28:51.933097 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 11/15:\n",
            "Global step: 4290,loss: 0.017548833\n",
            "\n",
            "Global step: 4291,loss: 0.016371595\n",
            "\n",
            "Global step: 4292,loss: 0.01666151\n",
            "\n",
            "Global step: 4293,loss: 0.016385304\n",
            "\n",
            "Global step: 4294,loss: 0.017560272\n",
            "\n",
            "Global step: 4295,loss: 0.016566876\n",
            "\n",
            "Global step: 4296,loss: 0.01763647\n",
            "\n",
            "Global step: 4297,loss: 0.022555828\n",
            "\n",
            "Global step: 4298,loss: 0.016229972\n",
            "\n",
            "Global step: 4299,loss: 0.019210206\n",
            "\n",
            "Global step: 4300,loss: 0.016011992\n",
            "\n",
            "Global step: 4301,loss: 0.017136924\n",
            "\n",
            "Global step: 4302,loss: 0.016509056\n",
            "\n",
            "Global step: 4303,loss: 0.016438877\n",
            "\n",
            "Global step: 4304,loss: 0.015739108\n",
            "\n",
            "Global step: 4305,loss: 0.01647925\n",
            "\n",
            "Global step: 4306,loss: 0.016261527\n",
            "\n",
            "Global step: 4307,loss: 0.016892852\n",
            "\n",
            "Global step: 4308,loss: 0.016888589\n",
            "\n",
            "Global step: 4309,loss: 0.017054887\n",
            "\n",
            "Global step: 4310,loss: 0.016424002\n",
            "\n",
            "Global step: 4311,loss: 0.017058458\n",
            "\n",
            "Global step: 4312,loss: 0.015845668\n",
            "\n",
            "Global step: 4313,loss: 0.016921619\n",
            "\n",
            "Global step: 4314,loss: 0.019658722\n",
            "\n",
            "Global step: 4315,loss: 0.016823936\n",
            "\n",
            "Global step: 4316,loss: 0.015855277\n",
            "\n",
            "Global step: 4317,loss: 0.016073043\n",
            "\n",
            "Global step: 4318,loss: 0.016896943\n",
            "\n",
            "Global step: 4319,loss: 0.016095605\n",
            "\n",
            "Global step: 4320,loss: 0.0165896\n",
            "\n",
            "Global step: 4321,loss: 0.016318155\n",
            "\n",
            "Global step: 4322,loss: 0.015839782\n",
            "\n",
            "Global step: 4323,loss: 0.01684788\n",
            "\n",
            "Global step: 4324,loss: 0.0154537475\n",
            "\n",
            "Global step: 4325,loss: 0.01600547\n",
            "\n",
            "Global step: 4326,loss: 0.017953401\n",
            "\n",
            "Global step: 4327,loss: 0.017021367\n",
            "\n",
            "Global step: 4328,loss: 0.016682144\n",
            "\n",
            "Global step: 4329,loss: 0.015980281\n",
            "\n",
            "Global step: 4330,loss: 0.016422568\n",
            "\n",
            "Global step: 4331,loss: 0.016007725\n",
            "\n",
            "Global step: 4332,loss: 0.01739407\n",
            "\n",
            "Global step: 4333,loss: 0.016130116\n",
            "\n",
            "Global step: 4334,loss: 0.01661104\n",
            "\n",
            "Global step: 4335,loss: 0.01696543\n",
            "\n",
            "Global step: 4336,loss: 0.016400935\n",
            "\n",
            "Global step: 4337,loss: 0.015824549\n",
            "\n",
            "Global step: 4338,loss: 0.015888013\n",
            "\n",
            "Global step: 4339,loss: 0.01644264\n",
            "\n",
            "Global step: 4340,loss: 0.016650766\n",
            "\n",
            "Global step: 4341,loss: 0.017171867\n",
            "\n",
            "Global step: 4342,loss: 0.01753768\n",
            "\n",
            "Global step: 4343,loss: 0.016960217\n",
            "\n",
            "Global step: 4344,loss: 0.01633746\n",
            "\n",
            "Global step: 4345,loss: 0.023653261\n",
            "\n",
            "Global step: 4346,loss: 0.016253427\n",
            "\n",
            "Global step: 4347,loss: 0.016064074\n",
            "\n",
            "Global step: 4348,loss: 0.01662557\n",
            "\n",
            "Global step: 4349,loss: 0.015683278\n",
            "\n",
            "Global step: 4350,loss: 0.016506188\n",
            "\n",
            "Global step: 4351,loss: 0.016480673\n",
            "\n",
            "Global step: 4352,loss: 0.01754206\n",
            "\n",
            "Global step: 4353,loss: 0.019846546\n",
            "\n",
            "Global step: 4354,loss: 0.021763587\n",
            "\n",
            "Global step: 4355,loss: 0.016418263\n",
            "\n",
            "Global step: 4356,loss: 0.019101216\n",
            "\n",
            "Global step: 4357,loss: 0.02257896\n",
            "\n",
            "Global step: 4358,loss: 0.016712261\n",
            "\n",
            "Global step: 4359,loss: 0.015655652\n",
            "\n",
            "Global step: 4360,loss: 0.017519005\n",
            "\n",
            "Global step: 4361,loss: 0.016830843\n",
            "\n",
            "Global step: 4362,loss: 0.018300718\n",
            "\n",
            "Global step: 4363,loss: 0.016686805\n",
            "\n",
            "Global step: 4364,loss: 0.01713678\n",
            "\n",
            "Global step: 4365,loss: 0.02076112\n",
            "\n",
            "Global step: 4366,loss: 0.016558586\n",
            "\n",
            "Global step: 4367,loss: 0.01772568\n",
            "\n",
            "Global step: 4368,loss: 0.016303508\n",
            "\n",
            "Global step: 4369,loss: 0.017129343\n",
            "\n",
            "Global step: 4370,loss: 0.016561246\n",
            "\n",
            "Global step: 4371,loss: 0.016906088\n",
            "\n",
            "Global step: 4372,loss: 0.021161068\n",
            "\n",
            "Global step: 4373,loss: 0.016916476\n",
            "\n",
            "Global step: 4374,loss: 0.016434422\n",
            "\n",
            "Global step: 4375,loss: 0.01659026\n",
            "\n",
            "Global step: 4376,loss: 0.017225865\n",
            "\n",
            "Global step: 4377,loss: 0.021510415\n",
            "\n",
            "Global step: 4378,loss: 0.01643268\n",
            "\n",
            "Global step: 4379,loss: 0.016952299\n",
            "\n",
            "Global step: 4380,loss: 0.017119043\n",
            "\n",
            "Global step: 4381,loss: 0.019924419\n",
            "\n",
            "Global step: 4382,loss: 0.017373195\n",
            "\n",
            "Global step: 4383,loss: 0.016805667\n",
            "\n",
            "Global step: 4384,loss: 0.015759414\n",
            "\n",
            "Global step: 4385,loss: 0.01704777\n",
            "\n",
            "Global step: 4386,loss: 0.015472369\n",
            "\n",
            "Global step: 4387,loss: 0.016846271\n",
            "\n",
            "Global step: 4388,loss: 0.017012333\n",
            "\n",
            "Global step: 4389,loss: 0.01627855\n",
            "\n",
            "Global step: 4390,loss: 0.0162168\n",
            "\n",
            "Global step: 4391,loss: 0.015764918\n",
            "\n",
            "Global step: 4392,loss: 0.016141882\n",
            "\n",
            "Global step: 4393,loss: 0.016207442\n",
            "\n",
            "Global step: 4394,loss: 0.015976615\n",
            "\n",
            "Global step: 4395,loss: 0.015650006\n",
            "\n",
            "Global step: 4396,loss: 0.016166423\n",
            "\n",
            "Global step: 4397,loss: 0.016262682\n",
            "\n",
            "Global step: 4398,loss: 0.01675329\n",
            "\n",
            "Global step: 4399,loss: 0.016513838\n",
            "\n",
            "Global step: 4400,loss: 0.01556548\n",
            "\n",
            "Global step: 4401,loss: 0.016337365\n",
            "\n",
            "Global step: 4402,loss: 0.016800048\n",
            "\n",
            "Global step: 4403,loss: 0.015795326\n",
            "\n",
            "Global step: 4404,loss: 0.016314244\n",
            "\n",
            "Global step: 4405,loss: 0.01712589\n",
            "\n",
            "Global step: 4406,loss: 0.016520731\n",
            "\n",
            "Global step: 4407,loss: 0.01603365\n",
            "\n",
            "Global step: 4408,loss: 0.015881216\n",
            "\n",
            "Global step: 4409,loss: 0.020556092\n",
            "\n",
            "Global step: 4410,loss: 0.016385997\n",
            "\n",
            "Global step: 4411,loss: 0.016739551\n",
            "\n",
            "Global step: 4412,loss: 0.017504178\n",
            "\n",
            "Global step: 4413,loss: 0.017092116\n",
            "\n",
            "Global step: 4414,loss: 0.017182581\n",
            "\n",
            "Global step: 4415,loss: 0.01807053\n",
            "\n",
            "Global step: 4416,loss: 0.01586851\n",
            "\n",
            "Global step: 4417,loss: 0.022950724\n",
            "\n",
            "Global step: 4418,loss: 0.017548986\n",
            "\n",
            "Global step: 4419,loss: 0.016801853\n",
            "\n",
            "Global step: 4420,loss: 0.016522752\n",
            "\n",
            "Global step: 4421,loss: 0.018096222\n",
            "\n",
            "Global step: 4422,loss: 0.017616471\n",
            "\n",
            "Global step: 4423,loss: 0.017996445\n",
            "\n",
            "Global step: 4424,loss: 0.016292995\n",
            "\n",
            "Global step: 4425,loss: 0.015851416\n",
            "\n",
            "Global step: 4426,loss: 0.019269405\n",
            "\n",
            "Global step: 4427,loss: 0.016819108\n",
            "\n",
            "Global step: 4428,loss: 0.016724885\n",
            "\n",
            "Global step: 4429,loss: 0.016898014\n",
            "\n",
            "Global step: 4430,loss: 0.02133245\n",
            "\n",
            "Global step: 4431,loss: 0.016404804\n",
            "\n",
            "Global step: 4432,loss: 0.016090013\n",
            "\n",
            "Global step: 4433,loss: 0.0151652675\n",
            "\n",
            "Global step: 4434,loss: 0.0160084\n",
            "\n",
            "Global step: 4435,loss: 0.016972767\n",
            "\n",
            "Global step: 4436,loss: 0.01679329\n",
            "\n",
            "Global step: 4437,loss: 0.016053185\n",
            "\n",
            "Global step: 4438,loss: 0.016466755\n",
            "\n",
            "Global step: 4439,loss: 0.016341744\n",
            "\n",
            "Global step: 4440,loss: 0.02595963\n",
            "\n",
            "Global step: 4441,loss: 0.015211371\n",
            "\n",
            "Global step: 4442,loss: 0.016726447\n",
            "\n",
            "Global step: 4443,loss: 0.016135845\n",
            "\n",
            "Global step: 4444,loss: 0.01595134\n",
            "\n",
            "Global step: 4445,loss: 0.016270211\n",
            "\n",
            "Global step: 4446,loss: 0.016738689\n",
            "\n",
            "Global step: 4447,loss: 0.016736362\n",
            "\n",
            "Global step: 4448,loss: 0.017094666\n",
            "\n",
            "Global step: 4449,loss: 0.01595242\n",
            "\n",
            "Global step: 4450,loss: 0.015881749\n",
            "\n",
            "Global step: 4451,loss: 0.01731141\n",
            "\n",
            "Global step: 4452,loss: 0.01706056\n",
            "\n",
            "Global step: 4453,loss: 0.017630504\n",
            "\n",
            "Global step: 4454,loss: 0.016442088\n",
            "\n",
            "Global step: 4455,loss: 0.017104536\n",
            "\n",
            "Global step: 4456,loss: 0.018427042\n",
            "\n",
            "Global step: 4457,loss: 0.016656732\n",
            "\n",
            "Global step: 4458,loss: 0.022280512\n",
            "\n",
            "Global step: 4459,loss: 0.01527849\n",
            "\n",
            "Global step: 4460,loss: 0.01678416\n",
            "\n",
            "Global step: 4461,loss: 0.022716474\n",
            "\n",
            "Global step: 4462,loss: 0.016544072\n",
            "\n",
            "Global step: 4463,loss: 0.017165171\n",
            "\n",
            "Global step: 4464,loss: 0.020696914\n",
            "\n",
            "Global step: 4465,loss: 0.020033821\n",
            "\n",
            "Global step: 4466,loss: 0.01727195\n",
            "\n",
            "Global step: 4467,loss: 0.017346643\n",
            "\n",
            "Global step: 4468,loss: 0.016014073\n",
            "\n",
            "Global step: 4469,loss: 0.018111888\n",
            "\n",
            "Global step: 4470,loss: 0.017133476\n",
            "\n",
            "Global step: 4471,loss: 0.01666158\n",
            "\n",
            "Global step: 4472,loss: 0.015649281\n",
            "\n",
            "Global step: 4473,loss: 0.01597466\n",
            "\n",
            "Global step: 4474,loss: 0.01690242\n",
            "\n",
            "Global step: 4475,loss: 0.016493909\n",
            "\n",
            "Global step: 4476,loss: 0.017498158\n",
            "\n",
            "Global step: 4477,loss: 0.015761932\n",
            "\n",
            "Global step: 4478,loss: 0.016176594\n",
            "\n",
            "Global step: 4479,loss: 0.017506002\n",
            "\n",
            "Global step: 4480,loss: 0.0159018\n",
            "\n",
            "Global step: 4481,loss: 0.015585401\n",
            "\n",
            "Global step: 4482,loss: 0.016813662\n",
            "\n",
            "Global step: 4483,loss: 0.018617498\n",
            "\n",
            "Global step: 4484,loss: 0.016491974\n",
            "\n",
            "Global step: 4485,loss: 0.016305596\n",
            "\n",
            "Global step: 4486,loss: 0.015266342\n",
            "\n",
            "Global step: 4487,loss: 0.01666286\n",
            "\n",
            "Global step: 4488,loss: 0.016936645\n",
            "\n",
            "Global step: 4489,loss: 0.015803332\n",
            "\n",
            "Global step: 4490,loss: 0.016075594\n",
            "\n",
            "Global step: 4491,loss: 0.017687837\n",
            "\n",
            "Global step: 4492,loss: 0.016672054\n",
            "\n",
            "Global step: 4493,loss: 0.020898238\n",
            "\n",
            "Global step: 4494,loss: 0.018810146\n",
            "\n",
            "Global step: 4495,loss: 0.02472828\n",
            "\n",
            "Global step: 4496,loss: 0.017927175\n",
            "\n",
            "Global step: 4497,loss: 0.017030919\n",
            "\n",
            "Global step: 4498,loss: 0.016127959\n",
            "\n",
            "Global step: 4499,loss: 0.016711175\n",
            "\n",
            "Global step: 4500,loss: 0.015588871\n",
            "\n",
            "Global step: 4501,loss: 0.016388612\n",
            "\n",
            "Global step: 4502,loss: 0.017206961\n",
            "\n",
            "Global step: 4503,loss: 0.017526783\n",
            "\n",
            "Global step: 4504,loss: 0.017184852\n",
            "\n",
            "Global step: 4505,loss: 0.01636068\n",
            "\n",
            "Global step: 4506,loss: 0.016377551\n",
            "\n",
            "Global step: 4507,loss: 0.016952831\n",
            "\n",
            "Global step: 4508,loss: 0.01600946\n",
            "\n",
            "Global step: 4509,loss: 0.020513257\n",
            "\n",
            "Global step: 4510,loss: 0.015713321\n",
            "\n",
            "Global step: 4511,loss: 0.016501846\n",
            "\n",
            "Global step: 4512,loss: 0.01569584\n",
            "\n",
            "Global step: 4513,loss: 0.015868839\n",
            "\n",
            "Global step: 4514,loss: 0.015653506\n",
            "\n",
            "Global step: 4515,loss: 0.016042694\n",
            "\n",
            "Global step: 4516,loss: 0.016075619\n",
            "\n",
            "Global step: 4517,loss: 0.016778717\n",
            "\n",
            "Global step: 4518,loss: 0.016464848\n",
            "\n",
            "Global step: 4519,loss: 0.017414091\n",
            "\n",
            "Global step: 4520,loss: 0.015872203\n",
            "\n",
            "Global step: 4521,loss: 0.0176122\n",
            "\n",
            "Global step: 4522,loss: 0.016475137\n",
            "\n",
            "Global step: 4523,loss: 0.0198239\n",
            "\n",
            "Global step: 4524,loss: 0.017015425\n",
            "\n",
            "Global step: 4525,loss: 0.0157718\n",
            "\n",
            "Global step: 4526,loss: 0.020885173\n",
            "\n",
            "Global step: 4527,loss: 0.017479802\n",
            "\n",
            "Global step: 4528,loss: 0.01732314\n",
            "\n",
            "Global step: 4529,loss: 0.01579731\n",
            "\n",
            "Global step: 4530,loss: 0.015722444\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.69132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:29:22.138682 139648999347968 supervisor.py:1099] global_step/sec: 7.69132\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4531,loss: 0.016996378\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 4532.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:29:22.252227 139649007740672 supervisor.py:1050] Recording summary at step 4532.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 4532,loss: 0.016098466\n",
            "\n",
            "Global step: 4533,loss: 0.016492559\n",
            "\n",
            "Global step: 4534,loss: 0.016854078\n",
            "\n",
            "Global step: 4535,loss: 0.016207075\n",
            "\n",
            "Global step: 4536,loss: 0.016588656\n",
            "\n",
            "Global step: 4537,loss: 0.018349433\n",
            "\n",
            "Global step: 4538,loss: 0.016738974\n",
            "\n",
            "Global step: 4539,loss: 0.020403817\n",
            "\n",
            "Global step: 4540,loss: 0.016078843\n",
            "\n",
            "Global step: 4541,loss: 0.016353657\n",
            "\n",
            "Global step: 4542,loss: 0.017583027\n",
            "\n",
            "Global step: 4543,loss: 0.017199222\n",
            "\n",
            "Global step: 4544,loss: 0.01803483\n",
            "\n",
            "Global step: 4545,loss: 0.015992038\n",
            "\n",
            "Global step: 4546,loss: 0.01668501\n",
            "\n",
            "Global step: 4547,loss: 0.017487118\n",
            "\n",
            "Global step: 4548,loss: 0.016466573\n",
            "\n",
            "Global step: 4549,loss: 0.016856514\n",
            "\n",
            "Global step: 4550,loss: 0.022978844\n",
            "\n",
            "Global step: 4551,loss: 0.016384965\n",
            "\n",
            "Global step: 4552,loss: 0.015027529\n",
            "\n",
            "Global step: 4553,loss: 0.017514499\n",
            "\n",
            "Global step: 4554,loss: 0.017744526\n",
            "\n",
            "Global step: 4555,loss: 0.016916893\n",
            "\n",
            "Global step: 4556,loss: 0.015248118\n",
            "\n",
            "Global step: 4557,loss: 0.016406165\n",
            "\n",
            "Global step: 4558,loss: 0.01682694\n",
            "\n",
            "Global step: 4559,loss: 0.015675554\n",
            "\n",
            "Global step: 4560,loss: 0.01544414\n",
            "\n",
            "Global step: 4561,loss: 0.018897166\n",
            "\n",
            "Global step: 4562,loss: 0.016656484\n",
            "\n",
            "Global step: 4563,loss: 0.017718753\n",
            "\n",
            "Global step: 4564,loss: 0.016573751\n",
            "\n",
            "Global step: 4565,loss: 0.016438017\n",
            "\n",
            "Global step: 4566,loss: 0.016007017\n",
            "\n",
            "Global step: 4567,loss: 0.016170653\n",
            "\n",
            "Global step: 4568,loss: 0.015377898\n",
            "\n",
            "Global step: 4569,loss: 0.01666918\n",
            "\n",
            "Global step: 4570,loss: 0.016950592\n",
            "\n",
            "Global step: 4571,loss: 0.01719847\n",
            "\n",
            "Global step: 4572,loss: 0.01701943\n",
            "\n",
            "Global step: 4573,loss: 0.017033989\n",
            "\n",
            "Global step: 4574,loss: 0.015879922\n",
            "\n",
            "Global step: 4575,loss: 0.019045992\n",
            "\n",
            "Global step: 4576,loss: 0.015872255\n",
            "\n",
            "Global step: 4577,loss: 0.016348884\n",
            "\n",
            "Global step: 4578,loss: 0.027110526\n",
            "\n",
            "Global step: 4579,loss: 0.016230352\n",
            "\n",
            "Global step: 4580,loss: 0.016445316\n",
            "\n",
            "Global step: 4581,loss: 0.017193697\n",
            "\n",
            "Global step: 4582,loss: 0.017632414\n",
            "\n",
            "Global step: 4583,loss: 0.016277997\n",
            "\n",
            "Global step: 4584,loss: 0.017030919\n",
            "\n",
            "Global step: 4585,loss: 0.017121136\n",
            "\n",
            "Global step: 4586,loss: 0.017127136\n",
            "\n",
            "Global step: 4587,loss: 0.017597051\n",
            "\n",
            "Global step: 4588,loss: 0.016053965\n",
            "\n",
            "Global step: 4589,loss: 0.0153410025\n",
            "\n",
            "Global step: 4590,loss: 0.016875114\n",
            "\n",
            "Global step: 4591,loss: 0.016347557\n",
            "\n",
            "Global step: 4592,loss: 0.015377441\n",
            "\n",
            "Global step: 4593,loss: 0.016019072\n",
            "\n",
            "Global step: 4594,loss: 0.016221197\n",
            "\n",
            "Global step: 4595,loss: 0.016267676\n",
            "\n",
            "Global step: 4596,loss: 0.015764438\n",
            "\n",
            "Global step: 4597,loss: 0.014845281\n",
            "\n",
            "Global step: 4598,loss: 0.015510817\n",
            "\n",
            "Global step: 4599,loss: 0.016832018\n",
            "\n",
            "Global step: 4600,loss: 0.016593663\n",
            "\n",
            "Global step: 4601,loss: 0.015061645\n",
            "\n",
            "Global step: 4602,loss: 0.015610288\n",
            "\n",
            "Global step: 4603,loss: 0.01671548\n",
            "\n",
            "Global step: 4604,loss: 0.016819749\n",
            "\n",
            "Global step: 4605,loss: 0.017754933\n",
            "\n",
            "Global step: 4606,loss: 0.015968174\n",
            "\n",
            "Global step: 4607,loss: 0.018817998\n",
            "\n",
            "Global step: 4608,loss: 0.018990528\n",
            "\n",
            "Global step: 4609,loss: 0.018113641\n",
            "\n",
            "Global step: 4610,loss: 0.016048823\n",
            "\n",
            "Global step: 4611,loss: 0.016883103\n",
            "\n",
            "Global step: 4612,loss: 0.015653256\n",
            "\n",
            "Global step: 4613,loss: 0.015674347\n",
            "\n",
            "Global step: 4614,loss: 0.01608656\n",
            "\n",
            "Global step: 4615,loss: 0.015775902\n",
            "\n",
            "Global step: 4616,loss: 0.017735776\n",
            "\n",
            "Global step: 4617,loss: 0.016432459\n",
            "\n",
            "Global step: 4618,loss: 0.017009825\n",
            "\n",
            "Global step: 4619,loss: 0.01645441\n",
            "\n",
            "Global step: 4620,loss: 0.019101372\n",
            "\n",
            "Global step: 4621,loss: 0.016691351\n",
            "\n",
            "Global step: 4622,loss: 0.016375639\n",
            "\n",
            "Global step: 4623,loss: 0.015343893\n",
            "\n",
            "Global step: 4624,loss: 0.015698679\n",
            "\n",
            "Global step: 4625,loss: 0.01608096\n",
            "\n",
            "Global step: 4626,loss: 0.016958077\n",
            "\n",
            "Global step: 4627,loss: 0.01911351\n",
            "\n",
            "Global step: 4628,loss: 0.024985181\n",
            "\n",
            "Global step: 4629,loss: 0.02043602\n",
            "\n",
            "Global step: 4630,loss: 0.017495414\n",
            "\n",
            "Global step: 4631,loss: 0.017339353\n",
            "\n",
            "Global step: 4632,loss: 0.015943762\n",
            "\n",
            "Global step: 4633,loss: 0.019417988\n",
            "\n",
            "Global step: 4634,loss: 0.017956618\n",
            "\n",
            "Global step: 4635,loss: 0.017272228\n",
            "\n",
            "Global step: 4636,loss: 0.015724685\n",
            "\n",
            "Global step: 4637,loss: 0.015741233\n",
            "\n",
            "Global step: 4638,loss: 0.015780438\n",
            "\n",
            "Global step: 4639,loss: 0.01602299\n",
            "\n",
            "Global step: 4640,loss: 0.018314194\n",
            "\n",
            "Global step: 4641,loss: 0.015745414\n",
            "\n",
            "Global step: 4642,loss: 0.017499683\n",
            "\n",
            "Global step: 4643,loss: 0.015194029\n",
            "\n",
            "Global step: 4644,loss: 0.018961243\n",
            "\n",
            "Global step: 4645,loss: 0.016235847\n",
            "\n",
            "Global step: 4646,loss: 0.017387167\n",
            "\n",
            "Global step: 4647,loss: 0.016585058\n",
            "\n",
            "Global step: 4648,loss: 0.01623856\n",
            "\n",
            "Global step: 4649,loss: 0.015435623\n",
            "\n",
            "Global step: 4650,loss: 0.016725024\n",
            "\n",
            "Global step: 4651,loss: 0.01633936\n",
            "\n",
            "Global step: 4652,loss: 0.015463639\n",
            "\n",
            "Global step: 4653,loss: 0.015932748\n",
            "\n",
            "Global step: 4654,loss: 0.015457047\n",
            "\n",
            "Global step: 4655,loss: 0.015922418\n",
            "\n",
            "Global step: 4656,loss: 0.016207429\n",
            "\n",
            "Global step: 4657,loss: 0.015655164\n",
            "\n",
            "Global step: 4658,loss: 0.015302533\n",
            "\n",
            "Global step: 4659,loss: 0.015811263\n",
            "\n",
            "Global step: 4660,loss: 0.016061174\n",
            "\n",
            "Global step: 4661,loss: 0.015289113\n",
            "\n",
            "Global step: 4662,loss: 0.016622467\n",
            "\n",
            "Global step: 4663,loss: 0.016971892\n",
            "\n",
            "Global step: 4664,loss: 0.015876994\n",
            "\n",
            "Global step: 4665,loss: 0.01518582\n",
            "\n",
            "Global step: 4666,loss: 0.016249504\n",
            "\n",
            "Global step: 4667,loss: 0.017590716\n",
            "\n",
            "Global step: 4668,loss: 0.016233217\n",
            "\n",
            "Global step: 4669,loss: 0.027293127\n",
            "\n",
            "Global step: 4670,loss: 0.017028388\n",
            "\n",
            "Global step: 4671,loss: 0.015225451\n",
            "\n",
            "Global step: 4672,loss: 0.016583316\n",
            "\n",
            "Global step: 4673,loss: 0.014917733\n",
            "\n",
            "Global step: 4674,loss: 0.015873965\n",
            "\n",
            "Global step: 4675,loss: 0.01622315\n",
            "\n",
            "Global step: 4676,loss: 0.016331352\n",
            "\n",
            "Global step: 4677,loss: 0.01571393\n",
            "\n",
            "Global step: 4678,loss: 0.016444862\n",
            "\n",
            "Global step: 4679,loss: 0.017103981\n",
            "\n",
            "Global step: 4680,loss: 0.017197661\n",
            "\n",
            "Global step: 4681,loss: 0.016727839\n",
            "\n",
            "Global step: 4682,loss: 0.015889948\n",
            "\n",
            "Global step: 4683,loss: 0.016709927\n",
            "\n",
            "Global step: 4684,loss: 0.017770248\n",
            "\n",
            "Global step: 4685,loss: 0.016464181\n",
            "\n",
            "Global step: 4686,loss: 0.016162654\n",
            "\n",
            "Global step: 4687,loss: 0.017330816\n",
            "\n",
            "Global step: 4688,loss: 0.016077237\n",
            "\n",
            "Global step: 4689,loss: 0.02054678\n",
            "\n",
            "Global step: 4690,loss: 0.016407598\n",
            "\n",
            "Global step: 4691,loss: 0.015477496\n",
            "\n",
            "Global step: 4692,loss: 0.019923054\n",
            "\n",
            "Global step: 4693,loss: 0.016820833\n",
            "\n",
            "Global step: 4694,loss: 0.015739692\n",
            "\n",
            "Global step: 4695,loss: 0.015295966\n",
            "\n",
            "Global step: 4696,loss: 0.016206972\n",
            "\n",
            "Global step: 4697,loss: 0.015909327\n",
            "\n",
            "Global step: 4698,loss: 0.018695466\n",
            "\n",
            "Global step: 4699,loss: 0.015704218\n",
            "\n",
            "Global step: 4700,loss: 0.015708456\n",
            "\n",
            "Global step: 4701,loss: 0.015098472\n",
            "\n",
            "Global step: 4702,loss: 0.015565059\n",
            "\n",
            "Global step: 4703,loss: 0.020719677\n",
            "\n",
            "Global step: 4704,loss: 0.016429465\n",
            "\n",
            "Global step: 4705,loss: 0.014496372\n",
            "\n",
            "Global step: 4706,loss: 0.0156538\n",
            "\n",
            "Global step: 4707,loss: 0.015651891\n",
            "\n",
            "Global step: 4708,loss: 0.016417952\n",
            "\n",
            "Global step: 4709,loss: 0.016160594\n",
            "\n",
            "Global step: 4710,loss: 0.014813532\n",
            "\n",
            "Global step: 4711,loss: 0.016444765\n",
            "\n",
            "Global step: 4712,loss: 0.015603893\n",
            "\n",
            "Global step: 4713,loss: 0.01612972\n",
            "\n",
            "Global step: 4714,loss: 0.015559769\n",
            "\n",
            "Global step: 4715,loss: 0.015790094\n",
            "\n",
            "Global step: 4716,loss: 0.015084543\n",
            "\n",
            "Global step: 4717,loss: 0.0175059\n",
            "\n",
            "Global step: 4718,loss: 0.015158111\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 4718,Val_Loss: 0.02039383225238476,  Val_acc: 0.9981971153846154 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:29:48.099824 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 12/15:\n",
            "Global step: 4719,loss: 0.015689831\n",
            "\n",
            "Global step: 4720,loss: 0.015835483\n",
            "\n",
            "Global step: 4721,loss: 0.0153785255\n",
            "\n",
            "Global step: 4722,loss: 0.01554744\n",
            "\n",
            "Global step: 4723,loss: 0.017927235\n",
            "\n",
            "Global step: 4724,loss: 0.017320681\n",
            "\n",
            "Global step: 4725,loss: 0.015025502\n",
            "\n",
            "Global step: 4726,loss: 0.020817796\n",
            "\n",
            "Global step: 4727,loss: 0.016240127\n",
            "\n",
            "Global step: 4728,loss: 0.017686578\n",
            "\n",
            "Global step: 4729,loss: 0.016537279\n",
            "\n",
            "Global step: 4730,loss: 0.015502093\n",
            "\n",
            "Global step: 4731,loss: 0.015434708\n",
            "\n",
            "Global step: 4732,loss: 0.016635768\n",
            "\n",
            "Global step: 4733,loss: 0.015944695\n",
            "\n",
            "Global step: 4734,loss: 0.01652242\n",
            "\n",
            "Global step: 4735,loss: 0.018901622\n",
            "\n",
            "Global step: 4736,loss: 0.016507171\n",
            "\n",
            "Global step: 4737,loss: 0.015369039\n",
            "\n",
            "Global step: 4738,loss: 0.0155431675\n",
            "\n",
            "Global step: 4739,loss: 0.01688918\n",
            "\n",
            "Global step: 4740,loss: 0.015427421\n",
            "\n",
            "Global step: 4741,loss: 0.017391013\n",
            "\n",
            "Global step: 4742,loss: 0.015888726\n",
            "\n",
            "Global step: 4743,loss: 0.016039\n",
            "\n",
            "Global step: 4744,loss: 0.015287102\n",
            "\n",
            "Global step: 4745,loss: 0.017502597\n",
            "\n",
            "Global step: 4746,loss: 0.01496903\n",
            "\n",
            "Global step: 4747,loss: 0.01665409\n",
            "\n",
            "Global step: 4748,loss: 0.01620936\n",
            "\n",
            "Global step: 4749,loss: 0.015846943\n",
            "\n",
            "Global step: 4750,loss: 0.016776254\n",
            "\n",
            "Global step: 4751,loss: 0.019385576\n",
            "\n",
            "Global step: 4752,loss: 0.016532123\n",
            "\n",
            "Global step: 4753,loss: 0.016532872\n",
            "\n",
            "Global step: 4754,loss: 0.016117709\n",
            "\n",
            "Global step: 4755,loss: 0.015417267\n",
            "\n",
            "Global step: 4756,loss: 0.014705098\n",
            "\n",
            "Global step: 4757,loss: 0.01414476\n",
            "\n",
            "Global step: 4758,loss: 0.015741905\n",
            "\n",
            "Global step: 4759,loss: 0.0156207215\n",
            "\n",
            "Global step: 4760,loss: 0.016370727\n",
            "\n",
            "Global step: 4761,loss: 0.01647931\n",
            "\n",
            "Global step: 4762,loss: 0.015944162\n",
            "\n",
            "Global step: 4763,loss: 0.014723034\n",
            "\n",
            "Global step: 4764,loss: 0.016597016\n",
            "\n",
            "Global step: 4765,loss: 0.017001202\n",
            "\n",
            "Global step: 4766,loss: 0.020218171\n",
            "\n",
            "Global step: 4767,loss: 0.014581574\n",
            "\n",
            "Global step: 4768,loss: 0.015344734\n",
            "\n",
            "Global step: 4769,loss: 0.016224407\n",
            "\n",
            "Global step: 4770,loss: 0.015886905\n",
            "\n",
            "Global step: 4771,loss: 0.01534942\n",
            "\n",
            "Global step: 4772,loss: 0.014568674\n",
            "\n",
            "Global step: 4773,loss: 0.016044153\n",
            "\n",
            "Global step: 4774,loss: 0.015944878\n",
            "\n",
            "Global step: 4775,loss: 0.016035328\n",
            "\n",
            "Global step: 4776,loss: 0.01892781\n",
            "\n",
            "Global step: 4777,loss: 0.016646788\n",
            "\n",
            "Global step: 4778,loss: 0.020416189\n",
            "\n",
            "Global step: 4779,loss: 0.015595899\n",
            "\n",
            "Global step: 4780,loss: 0.015246687\n",
            "\n",
            "Global step: 4781,loss: 0.01643712\n",
            "\n",
            "Global step: 4782,loss: 0.016578672\n",
            "\n",
            "Global step: 4783,loss: 0.01513901\n",
            "\n",
            "Global step: 4784,loss: 0.014682888\n",
            "\n",
            "Global step: 4785,loss: 0.01604031\n",
            "\n",
            "Global step: 4786,loss: 0.015548475\n",
            "\n",
            "Global step: 4787,loss: 0.015525639\n",
            "\n",
            "Global step: 4788,loss: 0.015216971\n",
            "\n",
            "Global step: 4789,loss: 0.015404032\n",
            "\n",
            "Global step: 4790,loss: 0.018739026\n",
            "\n",
            "Global step: 4791,loss: 0.016794292\n",
            "\n",
            "Global step: 4792,loss: 0.01565082\n",
            "\n",
            "Global step: 4793,loss: 0.014968461\n",
            "\n",
            "Global step: 4794,loss: 0.015049427\n",
            "\n",
            "Global step: 4795,loss: 0.015260242\n",
            "\n",
            "Global step: 4796,loss: 0.015178691\n",
            "\n",
            "Global step: 4797,loss: 0.016148113\n",
            "\n",
            "Global step: 4798,loss: 0.015136019\n",
            "\n",
            "Global step: 4799,loss: 0.016829329\n",
            "\n",
            "Global step: 4800,loss: 0.016325729\n",
            "\n",
            "Global step: 4801,loss: 0.01581827\n",
            "\n",
            "Global step: 4802,loss: 0.016986316\n",
            "\n",
            "Global step: 4803,loss: 0.018227415\n",
            "\n",
            "Global step: 4804,loss: 0.016464699\n",
            "\n",
            "Global step: 4805,loss: 0.015126881\n",
            "\n",
            "Global step: 4806,loss: 0.015103364\n",
            "\n",
            "Global step: 4807,loss: 0.01885107\n",
            "\n",
            "Global step: 4808,loss: 0.015249593\n",
            "\n",
            "Global step: 4809,loss: 0.014846811\n",
            "\n",
            "Global step: 4810,loss: 0.015975293\n",
            "\n",
            "Global step: 4811,loss: 0.015109054\n",
            "\n",
            "Global step: 4812,loss: 0.016708277\n",
            "\n",
            "Global step: 4813,loss: 0.015279606\n",
            "\n",
            "Global step: 4814,loss: 0.01683747\n",
            "\n",
            "Global step: 4815,loss: 0.02018855\n",
            "\n",
            "Global step: 4816,loss: 0.016111936\n",
            "\n",
            "Global step: 4817,loss: 0.02002229\n",
            "\n",
            "Global step: 4818,loss: 0.01610174\n",
            "\n",
            "Global step: 4819,loss: 0.015613207\n",
            "\n",
            "Global step: 4820,loss: 0.016716994\n",
            "\n",
            "Global step: 4821,loss: 0.015824322\n",
            "\n",
            "Global step: 4822,loss: 0.015044828\n",
            "\n",
            "Global step: 4823,loss: 0.014993505\n",
            "\n",
            "Global step: 4824,loss: 0.016372046\n",
            "\n",
            "Global step: 4825,loss: 0.015228667\n",
            "\n",
            "Global step: 4826,loss: 0.015470953\n",
            "\n",
            "Global step: 4827,loss: 0.015260251\n",
            "\n",
            "Global step: 4828,loss: 0.01672462\n",
            "\n",
            "Global step: 4829,loss: 0.016674062\n",
            "\n",
            "Global step: 4830,loss: 0.01597788\n",
            "\n",
            "Global step: 4831,loss: 0.020804096\n",
            "\n",
            "Global step: 4832,loss: 0.015618395\n",
            "\n",
            "Global step: 4833,loss: 0.015735693\n",
            "\n",
            "Global step: 4834,loss: 0.016342623\n",
            "\n",
            "Global step: 4835,loss: 0.015841521\n",
            "\n",
            "Global step: 4836,loss: 0.01653483\n",
            "\n",
            "Global step: 4837,loss: 0.01601566\n",
            "\n",
            "Global step: 4838,loss: 0.015351304\n",
            "\n",
            "Global step: 4839,loss: 0.015585343\n",
            "\n",
            "Global step: 4840,loss: 0.015024754\n",
            "\n",
            "Global step: 4841,loss: 0.016058158\n",
            "\n",
            "Global step: 4842,loss: 0.016028613\n",
            "\n",
            "Global step: 4843,loss: 0.015937325\n",
            "\n",
            "Global step: 4844,loss: 0.01573298\n",
            "\n",
            "Global step: 4845,loss: 0.015828663\n",
            "\n",
            "Global step: 4846,loss: 0.014215229\n",
            "\n",
            "Global step: 4847,loss: 0.016282625\n",
            "\n",
            "Global step: 4848,loss: 0.015338767\n",
            "\n",
            "Global step: 4849,loss: 0.015027959\n",
            "\n",
            "Global step: 4850,loss: 0.015775373\n",
            "\n",
            "Global step: 4851,loss: 0.015061243\n",
            "\n",
            "Global step: 4852,loss: 0.015672356\n",
            "\n",
            "Global step: 4853,loss: 0.015129429\n",
            "\n",
            "Global step: 4854,loss: 0.015470567\n",
            "\n",
            "Global step: 4855,loss: 0.015651086\n",
            "\n",
            "Global step: 4856,loss: 0.015940325\n",
            "\n",
            "Global step: 4857,loss: 0.02096192\n",
            "\n",
            "Global step: 4858,loss: 0.015025835\n",
            "\n",
            "Global step: 4859,loss: 0.015249678\n",
            "\n",
            "Global step: 4860,loss: 0.014791326\n",
            "\n",
            "Global step: 4861,loss: 0.01659655\n",
            "\n",
            "Global step: 4862,loss: 0.015821537\n",
            "\n",
            "Global step: 4863,loss: 0.016190683\n",
            "\n",
            "Global step: 4864,loss: 0.015571334\n",
            "\n",
            "Global step: 4865,loss: 0.015318482\n",
            "\n",
            "Global step: 4866,loss: 0.016054003\n",
            "\n",
            "Global step: 4867,loss: 0.015284211\n",
            "\n",
            "Global step: 4868,loss: 0.016467547\n",
            "\n",
            "Global step: 4869,loss: 0.016583845\n",
            "\n",
            "Global step: 4870,loss: 0.015807187\n",
            "\n",
            "Global step: 4871,loss: 0.019040234\n",
            "\n",
            "Global step: 4872,loss: 0.015288209\n",
            "\n",
            "Global step: 4873,loss: 0.01528011\n",
            "\n",
            "Global step: 4874,loss: 0.016574949\n",
            "\n",
            "Global step: 4875,loss: 0.015549755\n",
            "\n",
            "Global step: 4876,loss: 0.018317789\n",
            "\n",
            "Global step: 4877,loss: 0.015091136\n",
            "\n",
            "Global step: 4878,loss: 0.014757085\n",
            "\n",
            "Global step: 4879,loss: 0.01605222\n",
            "\n",
            "Global step: 4880,loss: 0.01703985\n",
            "\n",
            "Global step: 4881,loss: 0.015842643\n",
            "\n",
            "Global step: 4882,loss: 0.01523821\n",
            "\n",
            "Global step: 4883,loss: 0.014159551\n",
            "\n",
            "Global step: 4884,loss: 0.015690302\n",
            "\n",
            "Global step: 4885,loss: 0.014621211\n",
            "\n",
            "Global step: 4886,loss: 0.01561118\n",
            "\n",
            "Global step: 4887,loss: 0.015913177\n",
            "\n",
            "Global step: 4888,loss: 0.015418762\n",
            "\n",
            "Global step: 4889,loss: 0.01720556\n",
            "\n",
            "Global step: 4890,loss: 0.01606124\n",
            "\n",
            "Global step: 4891,loss: 0.01586313\n",
            "\n",
            "Global step: 4892,loss: 0.015971731\n",
            "\n",
            "Global step: 4893,loss: 0.015894582\n",
            "\n",
            "Global step: 4894,loss: 0.015193712\n",
            "\n",
            "Global step: 4895,loss: 0.016270125\n",
            "\n",
            "Global step: 4896,loss: 0.014780433\n",
            "\n",
            "Global step: 4897,loss: 0.016730208\n",
            "\n",
            "Global step: 4898,loss: 0.020208506\n",
            "\n",
            "Global step: 4899,loss: 0.016024044\n",
            "\n",
            "Global step: 4900,loss: 0.017045384\n",
            "\n",
            "Global step: 4901,loss: 0.016673591\n",
            "\n",
            "Global step: 4902,loss: 0.020685155\n",
            "\n",
            "Global step: 4903,loss: 0.018198758\n",
            "\n",
            "Global step: 4904,loss: 0.014792872\n",
            "\n",
            "Global step: 4905,loss: 0.016196385\n",
            "\n",
            "Global step: 4906,loss: 0.015687333\n",
            "\n",
            "Global step: 4907,loss: 0.0159905\n",
            "\n",
            "Global step: 4908,loss: 0.015555633\n",
            "\n",
            "Global step: 4909,loss: 0.014203934\n",
            "\n",
            "Global step: 4910,loss: 0.016055202\n",
            "\n",
            "Global step: 4911,loss: 0.015504626\n",
            "\n",
            "Global step: 4912,loss: 0.016270632\n",
            "\n",
            "Global step: 4913,loss: 0.014704546\n",
            "\n",
            "Global step: 4914,loss: 0.015967196\n",
            "\n",
            "Global step: 4915,loss: 0.014995674\n",
            "\n",
            "Global step: 4916,loss: 0.015095315\n",
            "\n",
            "Global step: 4917,loss: 0.014833687\n",
            "\n",
            "Global step: 4918,loss: 0.014550492\n",
            "\n",
            "Global step: 4919,loss: 0.015051653\n",
            "\n",
            "Global step: 4920,loss: 0.016554397\n",
            "\n",
            "Global step: 4921,loss: 0.017766481\n",
            "\n",
            "Global step: 4922,loss: 0.0154303275\n",
            "\n",
            "Global step: 4923,loss: 0.016051402\n",
            "\n",
            "Global step: 4924,loss: 0.015266588\n",
            "\n",
            "Global step: 4925,loss: 0.01770938\n",
            "\n",
            "Global step: 4926,loss: 0.016784767\n",
            "\n",
            "Global step: 4927,loss: 0.020254627\n",
            "\n",
            "Global step: 4928,loss: 0.015912537\n",
            "\n",
            "Global step: 4929,loss: 0.015138029\n",
            "\n",
            "Global step: 4930,loss: 0.015963126\n",
            "\n",
            "Global step: 4931,loss: 0.01670345\n",
            "\n",
            "Global step: 4932,loss: 0.014176321\n",
            "\n",
            "Global step: 4933,loss: 0.015325828\n",
            "\n",
            "Global step: 4934,loss: 0.016389988\n",
            "\n",
            "Global step: 4935,loss: 0.015218218\n",
            "\n",
            "Global step: 4936,loss: 0.016276417\n",
            "\n",
            "Global step: 4937,loss: 0.015465194\n",
            "\n",
            "Global step: 4938,loss: 0.016193345\n",
            "\n",
            "Global step: 4939,loss: 0.01581746\n",
            "\n",
            "Global step: 4940,loss: 0.014140629\n",
            "\n",
            "Global step: 4941,loss: 0.016069915\n",
            "\n",
            "Global step: 4942,loss: 0.01560148\n",
            "\n",
            "Global step: 4943,loss: 0.016325573\n",
            "\n",
            "Global step: 4944,loss: 0.015080077\n",
            "\n",
            "Global step: 4945,loss: 0.015303196\n",
            "\n",
            "Global step: 4946,loss: 0.017276937\n",
            "\n",
            "Global step: 4947,loss: 0.015638707\n",
            "\n",
            "Global step: 4948,loss: 0.015375055\n",
            "\n",
            "Global step: 4949,loss: 0.017939055\n",
            "\n",
            "Global step: 4950,loss: 0.015205503\n",
            "\n",
            "Global step: 4951,loss: 0.01645261\n",
            "\n",
            "Global step: 4952,loss: 0.015169748\n",
            "\n",
            "Global step: 4953,loss: 0.016401177\n",
            "\n",
            "Global step: 4954,loss: 0.015680337\n",
            "\n",
            "Global step: 4955,loss: 0.01689605\n",
            "\n",
            "Global step: 4956,loss: 0.015403076\n",
            "\n",
            "Global step: 4957,loss: 0.016007895\n",
            "\n",
            "Global step: 4958,loss: 0.01575919\n",
            "\n",
            "Global step: 4959,loss: 0.015142551\n",
            "\n",
            "Global step: 4960,loss: 0.015906557\n",
            "\n",
            "Global step: 4961,loss: 0.015187997\n",
            "\n",
            "Global step: 4962,loss: 0.018618457\n",
            "\n",
            "Global step: 4963,loss: 0.015806142\n",
            "\n",
            "Global step: 4964,loss: 0.014877178\n",
            "\n",
            "Global step: 4965,loss: 0.016480854\n",
            "\n",
            "Global step: 4966,loss: 0.015682153\n",
            "\n",
            "Global step: 4967,loss: 0.015937362\n",
            "\n",
            "Global step: 4968,loss: 0.020506267\n",
            "\n",
            "Global step: 4969,loss: 0.01497782\n",
            "\n",
            "Global step: 4970,loss: 0.014987007\n",
            "\n",
            "Global step: 4971,loss: 0.015955089\n",
            "\n",
            "Global step: 4972,loss: 0.01742899\n",
            "\n",
            "Global step: 4973,loss: 0.015864596\n",
            "\n",
            "Global step: 4974,loss: 0.015577766\n",
            "\n",
            "Global step: 4975,loss: 0.014559593\n",
            "\n",
            "Global step: 4976,loss: 0.015595659\n",
            "\n",
            "Global step: 4977,loss: 0.015949009\n",
            "\n",
            "Global step: 4978,loss: 0.016465956\n",
            "\n",
            "Global step: 4979,loss: 0.015637781\n",
            "\n",
            "Global step: 4980,loss: 0.015689788\n",
            "\n",
            "Global step: 4981,loss: 0.015015817\n",
            "\n",
            "Global step: 4982,loss: 0.015552078\n",
            "\n",
            "Global step: 4983,loss: 0.015730731\n",
            "\n",
            "Global step: 4984,loss: 0.01511774\n",
            "\n",
            "Global step: 4985,loss: 0.015200993\n",
            "\n",
            "Global step: 4986,loss: 0.015808105\n",
            "\n",
            "Global step: 4987,loss: 0.014117553\n",
            "\n",
            "Global step: 4988,loss: 0.018565187\n",
            "\n",
            "Global step: 4989,loss: 0.015129302\n",
            "\n",
            "Global step: 4990,loss: 0.015565752\n",
            "\n",
            "Global step: 4991,loss: 0.015251269\n",
            "\n",
            "Global step: 4992,loss: 0.016666424\n",
            "\n",
            "Global step: 4993,loss: 0.016819574\n",
            "\n",
            "Global step: 4994,loss: 0.015670856\n",
            "\n",
            "Global step: 4995,loss: 0.016892895\n",
            "\n",
            "Global step: 4996,loss: 0.015469312\n",
            "\n",
            "Global step: 4997,loss: 0.01581594\n",
            "\n",
            "Global step: 4998,loss: 0.02047914\n",
            "\n",
            "Global step: 4999,loss: 0.015409443\n",
            "\n",
            "Global step: 5000,loss: 0.014879892\n",
            "\n",
            "Global step: 5001,loss: 0.021823186\n",
            "\n",
            "Global step: 5002,loss: 0.016694622\n",
            "\n",
            "Global step: 5003,loss: 0.018100576\n",
            "\n",
            "Global step: 5004,loss: 0.015705321\n",
            "\n",
            "Global step: 5005,loss: 0.016436502\n",
            "\n",
            "Global step: 5006,loss: 0.018345144\n",
            "\n",
            "Global step: 5007,loss: 0.017606935\n",
            "\n",
            "Global step: 5008,loss: 0.01603024\n",
            "\n",
            "Global step: 5009,loss: 0.019495957\n",
            "\n",
            "Global step: 5010,loss: 0.016904254\n",
            "\n",
            "Global step: 5011,loss: 0.017047837\n",
            "\n",
            "Global step: 5012,loss: 0.015396553\n",
            "\n",
            "Global step: 5013,loss: 0.015724557\n",
            "\n",
            "Global step: 5014,loss: 0.017584302\n",
            "\n",
            "Global step: 5015,loss: 0.021447198\n",
            "\n",
            "Global step: 5016,loss: 0.015695127\n",
            "\n",
            "Global step: 5017,loss: 0.016669556\n",
            "\n",
            "Global step: 5018,loss: 0.022292532\n",
            "\n",
            "Global step: 5019,loss: 0.014857425\n",
            "\n",
            "Global step: 5020,loss: 0.015677398\n",
            "\n",
            "Global step: 5021,loss: 0.015849583\n",
            "\n",
            "Global step: 5022,loss: 0.016616272\n",
            "\n",
            "Global step: 5023,loss: 0.015243062\n",
            "\n",
            "Global step: 5024,loss: 0.015381675\n",
            "\n",
            "Global step: 5025,loss: 0.015293044\n",
            "\n",
            "Global step: 5026,loss: 0.015087118\n",
            "\n",
            "Global step: 5027,loss: 0.015633453\n",
            "\n",
            "Global step: 5028,loss: 0.016607845\n",
            "\n",
            "Global step: 5029,loss: 0.015049699\n",
            "\n",
            "Global step: 5030,loss: 0.016104901\n",
            "\n",
            "Global step: 5031,loss: 0.01677876\n",
            "\n",
            "Global step: 5032,loss: 0.018316805\n",
            "\n",
            "Global step: 5033,loss: 0.01600147\n",
            "\n",
            "Global step: 5034,loss: 0.015491072\n",
            "\n",
            "Global step: 5035,loss: 0.015090325\n",
            "\n",
            "Global step: 5036,loss: 0.015353273\n",
            "\n",
            "Global step: 5037,loss: 0.015780281\n",
            "\n",
            "Global step: 5038,loss: 0.016266836\n",
            "\n",
            "Global step: 5039,loss: 0.018014329\n",
            "\n",
            "Global step: 5040,loss: 0.015822787\n",
            "\n",
            "Global step: 5041,loss: 0.016604125\n",
            "\n",
            "Global step: 5042,loss: 0.016430214\n",
            "\n",
            "Global step: 5043,loss: 0.021114305\n",
            "\n",
            "Global step: 5044,loss: 0.015924195\n",
            "\n",
            "Global step: 5045,loss: 0.016271695\n",
            "\n",
            "Global step: 5046,loss: 0.014973895\n",
            "\n",
            "Global step: 5047,loss: 0.015771713\n",
            "\n",
            "Global step: 5048,loss: 0.016080577\n",
            "\n",
            "Global step: 5049,loss: 0.015267507\n",
            "\n",
            "Global step: 5050,loss: 0.015072834\n",
            "\n",
            "Global step: 5051,loss: 0.01590531\n",
            "\n",
            "Global step: 5052,loss: 0.015281509\n",
            "\n",
            "Global step: 5053,loss: 0.014793034\n",
            "\n",
            "Global step: 5054,loss: 0.017873183\n",
            "\n",
            "Global step: 5055,loss: 0.016299585\n",
            "\n",
            "Global step: 5056,loss: 0.01770769\n",
            "\n",
            "Global step: 5057,loss: 0.016753385\n",
            "\n",
            "Global step: 5058,loss: 0.01586336\n",
            "\n",
            "Global step: 5059,loss: 0.014956245\n",
            "\n",
            "Global step: 5060,loss: 0.014990602\n",
            "\n",
            "Global step: 5061,loss: 0.015049303\n",
            "\n",
            "Global step: 5062,loss: 0.017805144\n",
            "\n",
            "Global step: 5063,loss: 0.014867529\n",
            "\n",
            "Global step: 5064,loss: 0.013920166\n",
            "\n",
            "Global step: 5065,loss: 0.01575987\n",
            "\n",
            "Global step: 5066,loss: 0.015496108\n",
            "\n",
            "Global step: 5067,loss: 0.015244496\n",
            "\n",
            "Global step: 5068,loss: 0.015155495\n",
            "\n",
            "Global step: 5069,loss: 0.016563922\n",
            "\n",
            "Global step: 5070,loss: 0.016202156\n",
            "\n",
            "Global step: 5071,loss: 0.015161544\n",
            "\n",
            "Global step: 5072,loss: 0.015586651\n",
            "\n",
            "Global step: 5073,loss: 0.0138507085\n",
            "\n",
            "Global step: 5074,loss: 0.019110216\n",
            "\n",
            "Global step: 5075,loss: 0.01588024\n",
            "\n",
            "Global step: 5076,loss: 0.014732062\n",
            "\n",
            "Global step: 5077,loss: 0.025849644\n",
            "\n",
            "Global step: 5078,loss: 0.016135585\n",
            "\n",
            "Global step: 5079,loss: 0.016805898\n",
            "\n",
            "Global step: 5080,loss: 0.015707323\n",
            "\n",
            "Global step: 5081,loss: 0.016098537\n",
            "\n",
            "Global step: 5082,loss: 0.016935594\n",
            "\n",
            "Global step: 5083,loss: 0.017318439\n",
            "\n",
            "Global step: 5084,loss: 0.016281474\n",
            "\n",
            "Global step: 5085,loss: 0.016170837\n",
            "\n",
            "Global step: 5086,loss: 0.01522213\n",
            "\n",
            "Global step: 5087,loss: 0.015002499\n",
            "\n",
            "Global step: 5088,loss: 0.015800131\n",
            "\n",
            "Global step: 5089,loss: 0.015519648\n",
            "\n",
            "Global step: 5090,loss: 0.014594217\n",
            "\n",
            "Global step: 5091,loss: 0.015477763\n",
            "\n",
            "Global step: 5092,loss: 0.018772576\n",
            "\n",
            "Global step: 5093,loss: 0.02676494\n",
            "\n",
            "Global step: 5094,loss: 0.016120687\n",
            "\n",
            "Global step: 5095,loss: 0.01779896\n",
            "\n",
            "Global step: 5096,loss: 0.016370326\n",
            "\n",
            "Global step: 5097,loss: 0.015570577\n",
            "\n",
            "Global step: 5098,loss: 0.015472877\n",
            "\n",
            "Global step: 5099,loss: 0.015482463\n",
            "\n",
            "Global step: 5100,loss: 0.015486887\n",
            "\n",
            "Global step: 5101,loss: 0.014779737\n",
            "\n",
            "Global step: 5102,loss: 0.014157725\n",
            "\n",
            "Global step: 5103,loss: 0.015186858\n",
            "\n",
            "Global step: 5104,loss: 0.01586487\n",
            "\n",
            "Global step: 5105,loss: 0.015278549\n",
            "\n",
            "Global step: 5106,loss: 0.015495002\n",
            "\n",
            "Global step: 5107,loss: 0.0150459455\n",
            "\n",
            "Global step: 5108,loss: 0.016060552\n",
            "\n",
            "Global step: 5109,loss: 0.014649101\n",
            "\n",
            "Global step: 5110,loss: 0.016808461\n",
            "\n",
            "Global step: 5111,loss: 0.01645091\n",
            "\n",
            "Global step: 5112,loss: 0.015632868\n",
            "\n",
            "Global step: 5113,loss: 0.015682237\n",
            "\n",
            "Global step: 5114,loss: 0.01682041\n",
            "\n",
            "Global step: 5115,loss: 0.015104589\n",
            "\n",
            "Global step: 5116,loss: 0.015645314\n",
            "\n",
            "Global step: 5117,loss: 0.015455941\n",
            "\n",
            "Global step: 5118,loss: 0.0148458285\n",
            "\n",
            "Global step: 5119,loss: 0.016055351\n",
            "\n",
            "Global step: 5120,loss: 0.015892046\n",
            "\n",
            "Global step: 5121,loss: 0.015005694\n",
            "\n",
            "Global step: 5122,loss: 0.014856663\n",
            "\n",
            "Global step: 5123,loss: 0.01589972\n",
            "\n",
            "Global step: 5124,loss: 0.014876664\n",
            "\n",
            "Global step: 5125,loss: 0.015329083\n",
            "\n",
            "Global step: 5126,loss: 0.017255388\n",
            "\n",
            "Global step: 5127,loss: 0.016522441\n",
            "\n",
            "Global step: 5128,loss: 0.014848104\n",
            "\n",
            "Global step: 5129,loss: 0.01614511\n",
            "\n",
            "Global step: 5130,loss: 0.014224072\n",
            "\n",
            "Global step: 5131,loss: 0.014351347\n",
            "\n",
            "Global step: 5132,loss: 0.015126238\n",
            "\n",
            "Global step: 5133,loss: 0.016011992\n",
            "\n",
            "Global step: 5134,loss: 0.014954301\n",
            "\n",
            "Global step: 5135,loss: 0.015149965\n",
            "\n",
            "Global step: 5136,loss: 0.015824538\n",
            "\n",
            "Global step: 5137,loss: 0.014510054\n",
            "\n",
            "Global step: 5138,loss: 0.014941684\n",
            "\n",
            "Global step: 5139,loss: 0.014906817\n",
            "\n",
            "Global step: 5140,loss: 0.015995886\n",
            "\n",
            "Global step: 5141,loss: 0.016191073\n",
            "\n",
            "Global step: 5142,loss: 0.015394548\n",
            "\n",
            "Global step: 5143,loss: 0.019108642\n",
            "\n",
            "Global step: 5144,loss: 0.0151729975\n",
            "\n",
            "Global step: 5145,loss: 0.01397448\n",
            "\n",
            "Global step: 5146,loss: 0.018545218\n",
            "\n",
            "Global step: 5147,loss: 0.016032085\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5147,Val_Loss: 0.02028798097028182,  Val_acc: 0.9973958333333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:30:43.943853 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 13/15:\n",
            "Global step: 5148,loss: 0.015398366\n",
            "\n",
            "Global step: 5149,loss: 0.014615433\n",
            "\n",
            "Global step: 5150,loss: 0.015201408\n",
            "\n",
            "Global step: 5151,loss: 0.014996384\n",
            "\n",
            "Global step: 5152,loss: 0.015425719\n",
            "\n",
            "Global step: 5153,loss: 0.014225066\n",
            "\n",
            "Global step: 5154,loss: 0.014193478\n",
            "\n",
            "Global step: 5155,loss: 0.015061839\n",
            "\n",
            "Global step: 5156,loss: 0.015831562\n",
            "\n",
            "Global step: 5157,loss: 0.01436844\n",
            "\n",
            "Global step: 5158,loss: 0.01544785\n",
            "\n",
            "Global step: 5159,loss: 0.01463886\n",
            "\n",
            "Global step: 5160,loss: 0.015596438\n",
            "\n",
            "Global step: 5161,loss: 0.014651581\n",
            "\n",
            "Global step: 5162,loss: 0.01532826\n",
            "\n",
            "Global step: 5163,loss: 0.0163696\n",
            "\n",
            "Global step: 5164,loss: 0.019918008\n",
            "\n",
            "Global step: 5165,loss: 0.015397199\n",
            "\n",
            "Global step: 5166,loss: 0.01570815\n",
            "\n",
            "Global step: 5167,loss: 0.015404868\n",
            "\n",
            "Global step: 5168,loss: 0.017845122\n",
            "\n",
            "Global step: 5169,loss: 0.015263357\n",
            "\n",
            "Global step: 5170,loss: 0.015605206\n",
            "\n",
            "Global step: 5171,loss: 0.016722912\n",
            "\n",
            "Global step: 5172,loss: 0.019364692\n",
            "\n",
            "Global step: 5173,loss: 0.015737142\n",
            "\n",
            "Global step: 5174,loss: 0.014225811\n",
            "\n",
            "Global step: 5175,loss: 0.016102273\n",
            "\n",
            "Global step: 5176,loss: 0.015226466\n",
            "\n",
            "Global step: 5177,loss: 0.015994659\n",
            "\n",
            "Global step: 5178,loss: 0.015249379\n",
            "\n",
            "Global step: 5179,loss: 0.015656035\n",
            "\n",
            "Global step: 5180,loss: 0.015068525\n",
            "\n",
            "Global step: 5181,loss: 0.015539445\n",
            "\n",
            "Global step: 5182,loss: 0.015061748\n",
            "\n",
            "Global step: 5183,loss: 0.015908275\n",
            "\n",
            "Global step: 5184,loss: 0.015076592\n",
            "\n",
            "Global step: 5185,loss: 0.014685193\n",
            "\n",
            "Global step: 5186,loss: 0.014636423\n",
            "\n",
            "Global step: 5187,loss: 0.014424957\n",
            "\n",
            "Global step: 5188,loss: 0.014645238\n",
            "\n",
            "Global step: 5189,loss: 0.01533192\n",
            "\n",
            "Global step: 5190,loss: 0.015157141\n",
            "\n",
            "Global step: 5191,loss: 0.015273251\n",
            "\n",
            "Global step: 5192,loss: 0.015577458\n",
            "\n",
            "Global step: 5193,loss: 0.0142822275\n",
            "\n",
            "Global step: 5194,loss: 0.014236753\n",
            "\n",
            "Global step: 5195,loss: 0.015149721\n",
            "\n",
            "Global step: 5196,loss: 0.015140215\n",
            "\n",
            "Global step: 5197,loss: 0.015055235\n",
            "\n",
            "Global step: 5198,loss: 0.016164638\n",
            "\n",
            "Global step: 5199,loss: 0.015892155\n",
            "\n",
            "Global step: 5200,loss: 0.015589155\n",
            "\n",
            "Global step: 5201,loss: 0.014251864\n",
            "\n",
            "Global step: 5202,loss: 0.020105107\n",
            "\n",
            "Global step: 5203,loss: 0.013920085\n",
            "\n",
            "Global step: 5204,loss: 0.015047673\n",
            "\n",
            "Global step: 5205,loss: 0.015202995\n",
            "\n",
            "Global step: 5206,loss: 0.017489577\n",
            "\n",
            "Global step: 5207,loss: 0.020265723\n",
            "\n",
            "Global step: 5208,loss: 0.014478213\n",
            "\n",
            "Global step: 5209,loss: 0.015857115\n",
            "\n",
            "Global step: 5210,loss: 0.017042965\n",
            "\n",
            "Global step: 5211,loss: 0.014082843\n",
            "\n",
            "Global step: 5212,loss: 0.016937885\n",
            "\n",
            "Global step: 5213,loss: 0.015333736\n",
            "\n",
            "Global step: 5214,loss: 0.01596209\n",
            "\n",
            "Global step: 5215,loss: 0.01524516\n",
            "\n",
            "Global step: 5216,loss: 0.015847322\n",
            "\n",
            "Global step: 5217,loss: 0.014303925\n",
            "\n",
            "Global step: 5218,loss: 0.014842002\n",
            "\n",
            "Global step: 5219,loss: 0.015397597\n",
            "\n",
            "Global step: 5220,loss: 0.020782324\n",
            "\n",
            "Global step: 5221,loss: 0.017388068\n",
            "\n",
            "Global step: 5222,loss: 0.01474729\n",
            "\n",
            "Global step: 5223,loss: 0.01538095\n",
            "\n",
            "Global step: 5224,loss: 0.014846064\n",
            "\n",
            "Global step: 5225,loss: 0.015172576\n",
            "\n",
            "Global step: 5226,loss: 0.015350454\n",
            "\n",
            "Global step: 5227,loss: 0.014842791\n",
            "\n",
            "Global step: 5228,loss: 0.014259788\n",
            "\n",
            "Global step: 5229,loss: 0.01576287\n",
            "\n",
            "Global step: 5230,loss: 0.01442409\n",
            "\n",
            "Global step: 5231,loss: 0.014331078\n",
            "\n",
            "Global step: 5232,loss: 0.014198642\n",
            "\n",
            "Global step: 5233,loss: 0.016024152\n",
            "\n",
            "Global step: 5234,loss: 0.014478586\n",
            "\n",
            "Global step: 5235,loss: 0.014629412\n",
            "\n",
            "Global step: 5236,loss: 0.01500696\n",
            "\n",
            "Global step: 5237,loss: 0.014652556\n",
            "\n",
            "Global step: 5238,loss: 0.014915364\n",
            "\n",
            "Global step: 5239,loss: 0.016420985\n",
            "\n",
            "Global step: 5240,loss: 0.015614962\n",
            "\n",
            "Global step: 5241,loss: 0.014771408\n",
            "\n",
            "Global step: 5242,loss: 0.014423405\n",
            "\n",
            "Global step: 5243,loss: 0.015461743\n",
            "\n",
            "Global step: 5244,loss: 0.018369282\n",
            "\n",
            "Global step: 5245,loss: 0.015316738\n",
            "\n",
            "Global step: 5246,loss: 0.016266773\n",
            "\n",
            "Global step: 5247,loss: 0.017440746\n",
            "\n",
            "Global step: 5248,loss: 0.015060421\n",
            "\n",
            "Global step: 5249,loss: 0.015117016\n",
            "\n",
            "Global step: 5250,loss: 0.016104795\n",
            "\n",
            "Global step: 5251,loss: 0.01707415\n",
            "\n",
            "Global step: 5252,loss: 0.014430433\n",
            "\n",
            "Global step: 5253,loss: 0.014860951\n",
            "\n",
            "Global step: 5254,loss: 0.015231965\n",
            "\n",
            "Global step: 5255,loss: 0.014512267\n",
            "\n",
            "Global step: 5256,loss: 0.01468266\n",
            "\n",
            "Global step: 5257,loss: 0.014768388\n",
            "\n",
            "Global step: 5258,loss: 0.014249291\n",
            "\n",
            "Global step: 5259,loss: 0.015426188\n",
            "\n",
            "Global step: 5260,loss: 0.015390284\n",
            "\n",
            "Global step: 5261,loss: 0.015780102\n",
            "\n",
            "Global step: 5262,loss: 0.017617993\n",
            "\n",
            "Global step: 5263,loss: 0.015187046\n",
            "\n",
            "Global step: 5264,loss: 0.015811158\n",
            "\n",
            "Global step: 5265,loss: 0.015796138\n",
            "\n",
            "Global step: 5266,loss: 0.01541095\n",
            "\n",
            "Global step: 5267,loss: 0.015061708\n",
            "\n",
            "Global step: 5268,loss: 0.015718106\n",
            "\n",
            "Global step: 5269,loss: 0.014851574\n",
            "\n",
            "Global step: 5270,loss: 0.014885528\n",
            "\n",
            "Global step: 5271,loss: 0.015833983\n",
            "\n",
            "Global step: 5272,loss: 0.01737625\n",
            "\n",
            "Global step: 5273,loss: 0.014153764\n",
            "\n",
            "Global step: 5274,loss: 0.0150654195\n",
            "\n",
            "Global step: 5275,loss: 0.014651339\n",
            "\n",
            "Global step: 5276,loss: 0.015054059\n",
            "\n",
            "Global step: 5277,loss: 0.016641073\n",
            "\n",
            "Global step: 5278,loss: 0.01571573\n",
            "\n",
            "Global step: 5279,loss: 0.01430068\n",
            "\n",
            "Global step: 5280,loss: 0.015038157\n",
            "\n",
            "Global step: 5281,loss: 0.014782901\n",
            "\n",
            "Global step: 5282,loss: 0.014582373\n",
            "\n",
            "Global step: 5283,loss: 0.015241674\n",
            "\n",
            "Global step: 5284,loss: 0.015132265\n",
            "\n",
            "Global step: 5285,loss: 0.015012246\n",
            "\n",
            "Global step: 5286,loss: 0.014585753\n",
            "\n",
            "Global step: 5287,loss: 0.017673282\n",
            "\n",
            "Global step: 5288,loss: 0.015516105\n",
            "\n",
            "Global step: 5289,loss: 0.015322618\n",
            "\n",
            "Global step: 5290,loss: 0.016482271\n",
            "\n",
            "Global step: 5291,loss: 0.014145425\n",
            "\n",
            "Global step: 5292,loss: 0.014633749\n",
            "\n",
            "Global step: 5293,loss: 0.014766896\n",
            "\n",
            "Global step: 5294,loss: 0.013876448\n",
            "\n",
            "Global step: 5295,loss: 0.01579889\n",
            "\n",
            "Global step: 5296,loss: 0.016048022\n",
            "\n",
            "Global step: 5297,loss: 0.01605271\n",
            "\n",
            "Global step: 5298,loss: 0.015174743\n",
            "\n",
            "Global step: 5299,loss: 0.01465909\n",
            "\n",
            "Global step: 5300,loss: 0.015402962\n",
            "\n",
            "Global step: 5301,loss: 0.014608551\n",
            "\n",
            "Global step: 5302,loss: 0.0150450645\n",
            "\n",
            "Global step: 5303,loss: 0.015215763\n",
            "\n",
            "Global step: 5304,loss: 0.013501545\n",
            "\n",
            "Global step: 5305,loss: 0.014839453\n",
            "\n",
            "Global step: 5306,loss: 0.013818811\n",
            "\n",
            "Global step: 5307,loss: 0.015645133\n",
            "\n",
            "Global step: 5308,loss: 0.01429804\n",
            "\n",
            "Global step: 5309,loss: 0.014640024\n",
            "\n",
            "Global step: 5310,loss: 0.014687797\n",
            "\n",
            "Global step: 5311,loss: 0.014328151\n",
            "\n",
            "Global step: 5312,loss: 0.014705535\n",
            "\n",
            "Global step: 5313,loss: 0.0144396\n",
            "\n",
            "Global step: 5314,loss: 0.015314626\n",
            "\n",
            "Global step: 5315,loss: 0.014223149\n",
            "\n",
            "Global step: 5316,loss: 0.015602915\n",
            "\n",
            "Global step: 5317,loss: 0.015434932\n",
            "\n",
            "Global step: 5318,loss: 0.013903418\n",
            "\n",
            "Global step: 5319,loss: 0.01482834\n",
            "\n",
            "Global step: 5320,loss: 0.014279234\n",
            "\n",
            "Global step: 5321,loss: 0.015679188\n",
            "\n",
            "Global step: 5322,loss: 0.014621141\n",
            "\n",
            "Global step: 5323,loss: 0.019529175\n",
            "\n",
            "Global step: 5324,loss: 0.014566274\n",
            "\n",
            "Global step: 5325,loss: 0.015811864\n",
            "\n",
            "Global step: 5326,loss: 0.015050109\n",
            "\n",
            "Global step: 5327,loss: 0.015099472\n",
            "\n",
            "Global step: 5328,loss: 0.015047266\n",
            "\n",
            "Global step: 5329,loss: 0.015701916\n",
            "\n",
            "Global step: 5330,loss: 0.014913844\n",
            "\n",
            "Global step: 5331,loss: 0.015360613\n",
            "\n",
            "Global step: 5332,loss: 0.018355247\n",
            "\n",
            "Global step: 5333,loss: 0.015397552\n",
            "\n",
            "Global step: 5334,loss: 0.015531756\n",
            "\n",
            "Global step: 5335,loss: 0.015549666\n",
            "\n",
            "Global step: 5336,loss: 0.014535955\n",
            "\n",
            "Global step: 5337,loss: 0.0145414015\n",
            "\n",
            "Global step: 5338,loss: 0.015003052\n",
            "\n",
            "Global step: 5339,loss: 0.015791137\n",
            "\n",
            "Global step: 5340,loss: 0.015242013\n",
            "\n",
            "Global step: 5341,loss: 0.014922784\n",
            "\n",
            "Global step: 5342,loss: 0.019621849\n",
            "\n",
            "Global step: 5343,loss: 0.0146703\n",
            "\n",
            "Global step: 5344,loss: 0.014679921\n",
            "\n",
            "Global step: 5345,loss: 0.014393457\n",
            "\n",
            "Global step: 5346,loss: 0.017522622\n",
            "\n",
            "Global step: 5347,loss: 0.015490918\n",
            "\n",
            "Global step: 5348,loss: 0.014751953\n",
            "\n",
            "Global step: 5349,loss: 0.014938689\n",
            "\n",
            "Global step: 5350,loss: 0.014531137\n",
            "\n",
            "Global step: 5351,loss: 0.014769186\n",
            "\n",
            "Global step: 5352,loss: 0.014829218\n",
            "\n",
            "Global step: 5353,loss: 0.014886694\n",
            "\n",
            "Global step: 5354,loss: 0.016349735\n",
            "\n",
            "Global step: 5355,loss: 0.015354776\n",
            "\n",
            "Global step: 5356,loss: 0.014129931\n",
            "\n",
            "Global step: 5357,loss: 0.015399486\n",
            "\n",
            "Global step: 5358,loss: 0.014222051\n",
            "\n",
            "Global step: 5359,loss: 0.014673773\n",
            "\n",
            "Global step: 5360,loss: 0.016101116\n",
            "\n",
            "Global step: 5361,loss: 0.015154817\n",
            "\n",
            "Global step: 5362,loss: 0.01519599\n",
            "\n",
            "Global step: 5363,loss: 0.014715085\n",
            "\n",
            "Global step: 5364,loss: 0.015908927\n",
            "\n",
            "Global step: 5365,loss: 0.016051322\n",
            "\n",
            "Global step: 5366,loss: 0.014165459\n",
            "\n",
            "Global step: 5367,loss: 0.015026864\n",
            "\n",
            "Global step: 5368,loss: 0.014991085\n",
            "\n",
            "Global step: 5369,loss: 0.01397074\n",
            "\n",
            "Global step: 5370,loss: 0.015652046\n",
            "\n",
            "Global step: 5371,loss: 0.015323046\n",
            "\n",
            "Global step: 5372,loss: 0.015113926\n",
            "\n",
            "Global step: 5373,loss: 0.01473481\n",
            "\n",
            "Global step: 5374,loss: 0.01716057\n",
            "\n",
            "Global step: 5375,loss: 0.0157778\n",
            "\n",
            "Global step: 5376,loss: 0.01526068\n",
            "\n",
            "Global step: 5377,loss: 0.014621018\n",
            "\n",
            "Global step: 5378,loss: 0.015723193\n",
            "\n",
            "Global step: 5379,loss: 0.013773156\n",
            "\n",
            "Global step: 5380,loss: 0.015540916\n",
            "\n",
            "Global step: 5381,loss: 0.015323183\n",
            "\n",
            "Global step: 5382,loss: 0.014458195\n",
            "\n",
            "Global step: 5383,loss: 0.015754037\n",
            "\n",
            "Global step: 5384,loss: 0.015287399\n",
            "\n",
            "Global step: 5385,loss: 0.015408907\n",
            "\n",
            "Global step: 5386,loss: 0.014863048\n",
            "\n",
            "Global step: 5387,loss: 0.014249522\n",
            "\n",
            "Global step: 5388,loss: 0.014758579\n",
            "\n",
            "Global step: 5389,loss: 0.0142733\n",
            "\n",
            "Global step: 5390,loss: 0.016333623\n",
            "\n",
            "Global step: 5391,loss: 0.014326573\n",
            "\n",
            "Global step: 5392,loss: 0.015606241\n",
            "\n",
            "Global step: 5393,loss: 0.015227877\n",
            "\n",
            "Global step: 5394,loss: 0.013987222\n",
            "\n",
            "Global step: 5395,loss: 0.015030789\n",
            "\n",
            "Global step: 5396,loss: 0.015063132\n",
            "\n",
            "Global step: 5397,loss: 0.014784073\n",
            "\n",
            "Global step: 5398,loss: 0.01409083\n",
            "\n",
            "Global step: 5399,loss: 0.015232133\n",
            "\n",
            "Global step: 5400,loss: 0.014302643\n",
            "\n",
            "Global step: 5401,loss: 0.016174484\n",
            "\n",
            "Global step: 5402,loss: 0.015138245\n",
            "\n",
            "Global step: 5403,loss: 0.020934995\n",
            "\n",
            "Global step: 5404,loss: 0.021056253\n",
            "\n",
            "Global step: 5405,loss: 0.015068939\n",
            "\n",
            "Global step: 5406,loss: 0.01578074\n",
            "\n",
            "Global step: 5407,loss: 0.01457746\n",
            "\n",
            "Global step: 5408,loss: 0.013183014\n",
            "\n",
            "Global step: 5409,loss: 0.015393245\n",
            "\n",
            "Global step: 5410,loss: 0.01368151\n",
            "\n",
            "Global step: 5411,loss: 0.016653867\n",
            "\n",
            "Global step: 5412,loss: 0.014072417\n",
            "\n",
            "Global step: 5413,loss: 0.014939473\n",
            "\n",
            "Global step: 5414,loss: 0.014168967\n",
            "\n",
            "Global step: 5415,loss: 0.014432041\n",
            "\n",
            "Global step: 5416,loss: 0.015204154\n",
            "\n",
            "Global step: 5417,loss: 0.014909519\n",
            "\n",
            "Global step: 5418,loss: 0.015014475\n",
            "\n",
            "Global step: 5419,loss: 0.015359718\n",
            "\n",
            "Global step: 5420,loss: 0.014766322\n",
            "\n",
            "Global step: 5421,loss: 0.01436191\n",
            "\n",
            "Global step: 5422,loss: 0.013106006\n",
            "\n",
            "Global step: 5423,loss: 0.014478966\n",
            "\n",
            "Global step: 5424,loss: 0.015496364\n",
            "\n",
            "Global step: 5425,loss: 0.015937332\n",
            "\n",
            "Global step: 5426,loss: 0.014891016\n",
            "\n",
            "Global step: 5427,loss: 0.013976548\n",
            "\n",
            "Global step: 5428,loss: 0.015323526\n",
            "\n",
            "Global step: 5429,loss: 0.015581846\n",
            "\n",
            "Global step: 5430,loss: 0.014685937\n",
            "\n",
            "Global step: 5431,loss: 0.015228041\n",
            "\n",
            "Global step: 5432,loss: 0.014611144\n",
            "\n",
            "Global step: 5433,loss: 0.015525078\n",
            "\n",
            "Global step: 5434,loss: 0.018985469\n",
            "\n",
            "Global step: 5435,loss: 0.015213132\n",
            "\n",
            "Global step: 5436,loss: 0.014349368\n",
            "\n",
            "Global step: 5437,loss: 0.014881529\n",
            "\n",
            "Global step: 5438,loss: 0.014795179\n",
            "\n",
            "Global step: 5439,loss: 0.01521477\n",
            "\n",
            "Global step: 5440,loss: 0.014068353\n",
            "\n",
            "Global step: 5441,loss: 0.014272027\n",
            "\n",
            "Global step: 5442,loss: 0.015009654\n",
            "\n",
            "Global step: 5443,loss: 0.015341331\n",
            "\n",
            "Global step: 5444,loss: 0.014795644\n",
            "\n",
            "Global step: 5445,loss: 0.01360423\n",
            "\n",
            "Global step: 5446,loss: 0.016284982\n",
            "\n",
            "Global step: 5447,loss: 0.016810285\n",
            "\n",
            "Global step: 5448,loss: 0.014477559\n",
            "\n",
            "Global step: 5449,loss: 0.014190977\n",
            "\n",
            "Global step: 5450,loss: 0.015017267\n",
            "\n",
            "Global step: 5451,loss: 0.015355127\n",
            "\n",
            "Global step: 5452,loss: 0.0147550665\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.67741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:31:22.231291 139648999347968 supervisor.py:1099] global_step/sec: 7.67741\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5453,loss: 0.017456785\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 5454.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:31:22.316735 139649007740672 supervisor.py:1050] Recording summary at step 5454.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 5454,loss: 0.014580205\n",
            "\n",
            "Global step: 5455,loss: 0.013673166\n",
            "\n",
            "Global step: 5456,loss: 0.015372936\n",
            "\n",
            "Global step: 5457,loss: 0.014920357\n",
            "\n",
            "Global step: 5458,loss: 0.015282044\n",
            "\n",
            "Global step: 5459,loss: 0.014169709\n",
            "\n",
            "Global step: 5460,loss: 0.015080629\n",
            "\n",
            "Global step: 5461,loss: 0.014953405\n",
            "\n",
            "Global step: 5462,loss: 0.01530329\n",
            "\n",
            "Global step: 5463,loss: 0.013799343\n",
            "\n",
            "Global step: 5464,loss: 0.01417698\n",
            "\n",
            "Global step: 5465,loss: 0.014643675\n",
            "\n",
            "Global step: 5466,loss: 0.015613014\n",
            "\n",
            "Global step: 5467,loss: 0.014405221\n",
            "\n",
            "Global step: 5468,loss: 0.015099108\n",
            "\n",
            "Global step: 5469,loss: 0.015352154\n",
            "\n",
            "Global step: 5470,loss: 0.015365307\n",
            "\n",
            "Global step: 5471,loss: 0.01536507\n",
            "\n",
            "Global step: 5472,loss: 0.023995208\n",
            "\n",
            "Global step: 5473,loss: 0.015300754\n",
            "\n",
            "Global step: 5474,loss: 0.014269687\n",
            "\n",
            "Global step: 5475,loss: 0.015882324\n",
            "\n",
            "Global step: 5476,loss: 0.01628182\n",
            "\n",
            "Global step: 5477,loss: 0.014520568\n",
            "\n",
            "Global step: 5478,loss: 0.0151905\n",
            "\n",
            "Global step: 5479,loss: 0.015790386\n",
            "\n",
            "Global step: 5480,loss: 0.014832865\n",
            "\n",
            "Global step: 5481,loss: 0.013756359\n",
            "\n",
            "Global step: 5482,loss: 0.01397139\n",
            "\n",
            "Global step: 5483,loss: 0.014872195\n",
            "\n",
            "Global step: 5484,loss: 0.01396285\n",
            "\n",
            "Global step: 5485,loss: 0.014756935\n",
            "\n",
            "Global step: 5486,loss: 0.016227458\n",
            "\n",
            "Global step: 5487,loss: 0.014484148\n",
            "\n",
            "Global step: 5488,loss: 0.015145345\n",
            "\n",
            "Global step: 5489,loss: 0.014629828\n",
            "\n",
            "Global step: 5490,loss: 0.014037723\n",
            "\n",
            "Global step: 5491,loss: 0.014685084\n",
            "\n",
            "Global step: 5492,loss: 0.014711935\n",
            "\n",
            "Global step: 5493,loss: 0.014998803\n",
            "\n",
            "Global step: 5494,loss: 0.014698781\n",
            "\n",
            "Global step: 5495,loss: 0.014033308\n",
            "\n",
            "Global step: 5496,loss: 0.014463378\n",
            "\n",
            "Global step: 5497,loss: 0.014279334\n",
            "\n",
            "Global step: 5498,loss: 0.013829214\n",
            "\n",
            "Global step: 5499,loss: 0.015112117\n",
            "\n",
            "Global step: 5500,loss: 0.016015396\n",
            "\n",
            "Global step: 5501,loss: 0.015986793\n",
            "\n",
            "Global step: 5502,loss: 0.014335455\n",
            "\n",
            "Global step: 5503,loss: 0.013853321\n",
            "\n",
            "Global step: 5504,loss: 0.015633935\n",
            "\n",
            "Global step: 5505,loss: 0.01489437\n",
            "\n",
            "Global step: 5506,loss: 0.013842302\n",
            "\n",
            "Global step: 5507,loss: 0.015475795\n",
            "\n",
            "Global step: 5508,loss: 0.014517452\n",
            "\n",
            "Global step: 5509,loss: 0.014778884\n",
            "\n",
            "Global step: 5510,loss: 0.01414442\n",
            "\n",
            "Global step: 5511,loss: 0.0147068\n",
            "\n",
            "Global step: 5512,loss: 0.01913742\n",
            "\n",
            "Global step: 5513,loss: 0.014488834\n",
            "\n",
            "Global step: 5514,loss: 0.014370722\n",
            "\n",
            "Global step: 5515,loss: 0.013758693\n",
            "\n",
            "Global step: 5516,loss: 0.014954199\n",
            "\n",
            "Global step: 5517,loss: 0.014556401\n",
            "\n",
            "Global step: 5518,loss: 0.013833593\n",
            "\n",
            "Global step: 5519,loss: 0.0145609025\n",
            "\n",
            "Global step: 5520,loss: 0.013926505\n",
            "\n",
            "Global step: 5521,loss: 0.015029438\n",
            "\n",
            "Global step: 5522,loss: 0.014629865\n",
            "\n",
            "Global step: 5523,loss: 0.015700404\n",
            "\n",
            "Global step: 5524,loss: 0.014974476\n",
            "\n",
            "Global step: 5525,loss: 0.015506081\n",
            "\n",
            "Global step: 5526,loss: 0.0144568905\n",
            "\n",
            "Global step: 5527,loss: 0.018546611\n",
            "\n",
            "Global step: 5528,loss: 0.018126113\n",
            "\n",
            "Global step: 5529,loss: 0.014775499\n",
            "\n",
            "Global step: 5530,loss: 0.014727522\n",
            "\n",
            "Global step: 5531,loss: 0.014871543\n",
            "\n",
            "Global step: 5532,loss: 0.014061234\n",
            "\n",
            "Global step: 5533,loss: 0.015288822\n",
            "\n",
            "Global step: 5534,loss: 0.013843769\n",
            "\n",
            "Global step: 5535,loss: 0.013988326\n",
            "\n",
            "Global step: 5536,loss: 0.014965179\n",
            "\n",
            "Global step: 5537,loss: 0.015076182\n",
            "\n",
            "Global step: 5538,loss: 0.013596454\n",
            "\n",
            "Global step: 5539,loss: 0.015379507\n",
            "\n",
            "Global step: 5540,loss: 0.015563136\n",
            "\n",
            "Global step: 5541,loss: 0.01477092\n",
            "\n",
            "Global step: 5542,loss: 0.014259344\n",
            "\n",
            "Global step: 5543,loss: 0.015463223\n",
            "\n",
            "Global step: 5544,loss: 0.014947525\n",
            "\n",
            "Global step: 5545,loss: 0.01672315\n",
            "\n",
            "Global step: 5546,loss: 0.014408286\n",
            "\n",
            "Global step: 5547,loss: 0.014391557\n",
            "\n",
            "Global step: 5548,loss: 0.014078929\n",
            "\n",
            "Global step: 5549,loss: 0.014066047\n",
            "\n",
            "Global step: 5550,loss: 0.015282188\n",
            "\n",
            "Global step: 5551,loss: 0.014625726\n",
            "\n",
            "Global step: 5552,loss: 0.015232395\n",
            "\n",
            "Global step: 5553,loss: 0.013864772\n",
            "\n",
            "Global step: 5554,loss: 0.015059447\n",
            "\n",
            "Global step: 5555,loss: 0.01371301\n",
            "\n",
            "Global step: 5556,loss: 0.015065378\n",
            "\n",
            "Global step: 5557,loss: 0.014903024\n",
            "\n",
            "Global step: 5558,loss: 0.01612536\n",
            "\n",
            "Global step: 5559,loss: 0.014286015\n",
            "\n",
            "Global step: 5560,loss: 0.014895915\n",
            "\n",
            "Global step: 5561,loss: 0.015090419\n",
            "\n",
            "Global step: 5562,loss: 0.014408008\n",
            "\n",
            "Global step: 5563,loss: 0.014428561\n",
            "\n",
            "Global step: 5564,loss: 0.01416057\n",
            "\n",
            "Global step: 5565,loss: 0.014528248\n",
            "\n",
            "Global step: 5566,loss: 0.014664991\n",
            "\n",
            "Global step: 5567,loss: 0.01566797\n",
            "\n",
            "Global step: 5568,loss: 0.014353743\n",
            "\n",
            "Global step: 5569,loss: 0.014416005\n",
            "\n",
            "Global step: 5570,loss: 0.014170906\n",
            "\n",
            "Global step: 5571,loss: 0.014676946\n",
            "\n",
            "Global step: 5572,loss: 0.014510863\n",
            "\n",
            "Global step: 5573,loss: 0.014493502\n",
            "\n",
            "Global step: 5574,loss: 0.013797107\n",
            "\n",
            "Global step: 5575,loss: 0.014617741\n",
            "\n",
            "Global step: 5576,loss: 0.014903541\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 5576,Val_Loss: 0.01872780966835144,  Val_acc: 0.9973958333333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:31:40.153495 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 14/15:\n",
            "Global step: 5577,loss: 0.013809276\n",
            "\n",
            "Global step: 5578,loss: 0.014552993\n",
            "\n",
            "Global step: 5579,loss: 0.0142094875\n",
            "\n",
            "Global step: 5580,loss: 0.014580396\n",
            "\n",
            "Global step: 5581,loss: 0.014493458\n",
            "\n",
            "Global step: 5582,loss: 0.013864118\n",
            "\n",
            "Global step: 5583,loss: 0.014572266\n",
            "\n",
            "Global step: 5584,loss: 0.014990267\n",
            "\n",
            "Global step: 5585,loss: 0.013755894\n",
            "\n",
            "Global step: 5586,loss: 0.013952427\n",
            "\n",
            "Global step: 5587,loss: 0.013525703\n",
            "\n",
            "Global step: 5588,loss: 0.015724996\n",
            "\n",
            "Global step: 5589,loss: 0.01324193\n",
            "\n",
            "Global step: 5590,loss: 0.016207632\n",
            "\n",
            "Global step: 5591,loss: 0.014176282\n",
            "\n",
            "Global step: 5592,loss: 0.013535445\n",
            "\n",
            "Global step: 5593,loss: 0.018687556\n",
            "\n",
            "Global step: 5594,loss: 0.013248725\n",
            "\n",
            "Global step: 5595,loss: 0.014467869\n",
            "\n",
            "Global step: 5596,loss: 0.013920337\n",
            "\n",
            "Global step: 5597,loss: 0.014929416\n",
            "\n",
            "Global step: 5598,loss: 0.0140608065\n",
            "\n",
            "Global step: 5599,loss: 0.015346972\n",
            "\n",
            "Global step: 5600,loss: 0.014573644\n",
            "\n",
            "Global step: 5601,loss: 0.017732153\n",
            "\n",
            "Global step: 5602,loss: 0.0136976885\n",
            "\n",
            "Global step: 5603,loss: 0.013560814\n",
            "\n",
            "Global step: 5604,loss: 0.014799977\n",
            "\n",
            "Global step: 5605,loss: 0.014482525\n",
            "\n",
            "Global step: 5606,loss: 0.0140869925\n",
            "\n",
            "Global step: 5607,loss: 0.015609604\n",
            "\n",
            "Global step: 5608,loss: 0.014612429\n",
            "\n",
            "Global step: 5609,loss: 0.015123963\n",
            "\n",
            "Global step: 5610,loss: 0.014601199\n",
            "\n",
            "Global step: 5611,loss: 0.014713939\n",
            "\n",
            "Global step: 5612,loss: 0.015681196\n",
            "\n",
            "Global step: 5613,loss: 0.015173253\n",
            "\n",
            "Global step: 5614,loss: 0.013360842\n",
            "\n",
            "Global step: 5615,loss: 0.015806623\n",
            "\n",
            "Global step: 5616,loss: 0.013658697\n",
            "\n",
            "Global step: 5617,loss: 0.014219419\n",
            "\n",
            "Global step: 5618,loss: 0.015045248\n",
            "\n",
            "Global step: 5619,loss: 0.015340262\n",
            "\n",
            "Global step: 5620,loss: 0.0139210615\n",
            "\n",
            "Global step: 5621,loss: 0.021110088\n",
            "\n",
            "Global step: 5622,loss: 0.0143400105\n",
            "\n",
            "Global step: 5623,loss: 0.013919344\n",
            "\n",
            "Global step: 5624,loss: 0.0152087305\n",
            "\n",
            "Global step: 5625,loss: 0.01562697\n",
            "\n",
            "Global step: 5626,loss: 0.014788756\n",
            "\n",
            "Global step: 5627,loss: 0.017731607\n",
            "\n",
            "Global step: 5628,loss: 0.014196036\n",
            "\n",
            "Global step: 5629,loss: 0.01530123\n",
            "\n",
            "Global step: 5630,loss: 0.01665705\n",
            "\n",
            "Global step: 5631,loss: 0.015338265\n",
            "\n",
            "Global step: 5632,loss: 0.013926212\n",
            "\n",
            "Global step: 5633,loss: 0.016172232\n",
            "\n",
            "Global step: 5634,loss: 0.015834592\n",
            "\n",
            "Global step: 5635,loss: 0.01657649\n",
            "\n",
            "Global step: 5636,loss: 0.014558631\n",
            "\n",
            "Global step: 5637,loss: 0.014750409\n",
            "\n",
            "Global step: 5638,loss: 0.015166153\n",
            "\n",
            "Global step: 5639,loss: 0.018488994\n",
            "\n",
            "Global step: 5640,loss: 0.017876739\n",
            "\n",
            "Global step: 5641,loss: 0.015347607\n",
            "\n",
            "Global step: 5642,loss: 0.014282645\n",
            "\n",
            "Global step: 5643,loss: 0.014465977\n",
            "\n",
            "Global step: 5644,loss: 0.014324403\n",
            "\n",
            "Global step: 5645,loss: 0.014645639\n",
            "\n",
            "Global step: 5646,loss: 0.013813006\n",
            "\n",
            "Global step: 5647,loss: 0.014890273\n",
            "\n",
            "Global step: 5648,loss: 0.014934169\n",
            "\n",
            "Global step: 5649,loss: 0.015142914\n",
            "\n",
            "Global step: 5650,loss: 0.018518716\n",
            "\n",
            "Global step: 5651,loss: 0.015446139\n",
            "\n",
            "Global step: 5652,loss: 0.014923655\n",
            "\n",
            "Global step: 5653,loss: 0.014420263\n",
            "\n",
            "Global step: 5654,loss: 0.013992273\n",
            "\n",
            "Global step: 5655,loss: 0.017931802\n",
            "\n",
            "Global step: 5656,loss: 0.020019652\n",
            "\n",
            "Global step: 5657,loss: 0.014692761\n",
            "\n",
            "Global step: 5658,loss: 0.02038452\n",
            "\n",
            "Global step: 5659,loss: 0.014094598\n",
            "\n",
            "Global step: 5660,loss: 0.016298048\n",
            "\n",
            "Global step: 5661,loss: 0.015083981\n",
            "\n",
            "Global step: 5662,loss: 0.01411554\n",
            "\n",
            "Global step: 5663,loss: 0.015826067\n",
            "\n",
            "Global step: 5664,loss: 0.014593688\n",
            "\n",
            "Global step: 5665,loss: 0.014256304\n",
            "\n",
            "Global step: 5666,loss: 0.014853653\n",
            "\n",
            "Global step: 5667,loss: 0.014289595\n",
            "\n",
            "Global step: 5668,loss: 0.015146103\n",
            "\n",
            "Global step: 5669,loss: 0.014404075\n",
            "\n",
            "Global step: 5670,loss: 0.015325356\n",
            "\n",
            "Global step: 5671,loss: 0.015204851\n",
            "\n",
            "Global step: 5672,loss: 0.014322732\n",
            "\n",
            "Global step: 5673,loss: 0.01538553\n",
            "\n",
            "Global step: 5674,loss: 0.014540499\n",
            "\n",
            "Global step: 5675,loss: 0.0144079225\n",
            "\n",
            "Global step: 5676,loss: 0.013028464\n",
            "\n",
            "Global step: 5677,loss: 0.014373789\n",
            "\n",
            "Global step: 5678,loss: 0.015558169\n",
            "\n",
            "Global step: 5679,loss: 0.0141254105\n",
            "\n",
            "Global step: 5680,loss: 0.01408076\n",
            "\n",
            "Global step: 5681,loss: 0.01438863\n",
            "\n",
            "Global step: 5682,loss: 0.013575756\n",
            "\n",
            "Global step: 5683,loss: 0.013466975\n",
            "\n",
            "Global step: 5684,loss: 0.014003504\n",
            "\n",
            "Global step: 5685,loss: 0.013543122\n",
            "\n",
            "Global step: 5686,loss: 0.014060374\n",
            "\n",
            "Global step: 5687,loss: 0.014806069\n",
            "\n",
            "Global step: 5688,loss: 0.016174031\n",
            "\n",
            "Global step: 5689,loss: 0.015047984\n",
            "\n",
            "Global step: 5690,loss: 0.013964503\n",
            "\n",
            "Global step: 5691,loss: 0.014257777\n",
            "\n",
            "Global step: 5692,loss: 0.014496986\n",
            "\n",
            "Global step: 5693,loss: 0.014175111\n",
            "\n",
            "Global step: 5694,loss: 0.01557488\n",
            "\n",
            "Global step: 5695,loss: 0.014035101\n",
            "\n",
            "Global step: 5696,loss: 0.0141712865\n",
            "\n",
            "Global step: 5697,loss: 0.013621293\n",
            "\n",
            "Global step: 5698,loss: 0.015325107\n",
            "\n",
            "Global step: 5699,loss: 0.014727888\n",
            "\n",
            "Global step: 5700,loss: 0.014718195\n",
            "\n",
            "Global step: 5701,loss: 0.016521536\n",
            "\n",
            "Global step: 5702,loss: 0.01393116\n",
            "\n",
            "Global step: 5703,loss: 0.01450039\n",
            "\n",
            "Global step: 5704,loss: 0.01413243\n",
            "\n",
            "Global step: 5705,loss: 0.016530465\n",
            "\n",
            "Global step: 5706,loss: 0.013499713\n",
            "\n",
            "Global step: 5707,loss: 0.017101146\n",
            "\n",
            "Global step: 5708,loss: 0.014095993\n",
            "\n",
            "Global step: 5709,loss: 0.014727693\n",
            "\n",
            "Global step: 5710,loss: 0.013579928\n",
            "\n",
            "Global step: 5711,loss: 0.013412677\n",
            "\n",
            "Global step: 5712,loss: 0.014690945\n",
            "\n",
            "Global step: 5713,loss: 0.0149905905\n",
            "\n",
            "Global step: 5714,loss: 0.015381556\n",
            "\n",
            "Global step: 5715,loss: 0.017573416\n",
            "\n",
            "Global step: 5716,loss: 0.0140967965\n",
            "\n",
            "Global step: 5717,loss: 0.013825667\n",
            "\n",
            "Global step: 5718,loss: 0.014316818\n",
            "\n",
            "Global step: 5719,loss: 0.014530177\n",
            "\n",
            "Global step: 5720,loss: 0.014240634\n",
            "\n",
            "Global step: 5721,loss: 0.013875593\n",
            "\n",
            "Global step: 5722,loss: 0.0139652295\n",
            "\n",
            "Global step: 5723,loss: 0.01492532\n",
            "\n",
            "Global step: 5724,loss: 0.016248511\n",
            "\n",
            "Global step: 5725,loss: 0.013434209\n",
            "\n",
            "Global step: 5726,loss: 0.014226732\n",
            "\n",
            "Global step: 5727,loss: 0.014392619\n",
            "\n",
            "Global step: 5728,loss: 0.014608982\n",
            "\n",
            "Global step: 5729,loss: 0.020763129\n",
            "\n",
            "Global step: 5730,loss: 0.013887366\n",
            "\n",
            "Global step: 5731,loss: 0.013873881\n",
            "\n",
            "Global step: 5732,loss: 0.015989637\n",
            "\n",
            "Global step: 5733,loss: 0.0138659645\n",
            "\n",
            "Global step: 5734,loss: 0.014461232\n",
            "\n",
            "Global step: 5735,loss: 0.015019716\n",
            "\n",
            "Global step: 5736,loss: 0.015340233\n",
            "\n",
            "Global step: 5737,loss: 0.0142363105\n",
            "\n",
            "Global step: 5738,loss: 0.014630489\n",
            "\n",
            "Global step: 5739,loss: 0.017219074\n",
            "\n",
            "Global step: 5740,loss: 0.02149674\n",
            "\n",
            "Global step: 5741,loss: 0.016321227\n",
            "\n",
            "Global step: 5742,loss: 0.022329157\n",
            "\n",
            "Global step: 5743,loss: 0.015026858\n",
            "\n",
            "Global step: 5744,loss: 0.014500748\n",
            "\n",
            "Global step: 5745,loss: 0.014560913\n",
            "\n",
            "Global step: 5746,loss: 0.014107235\n",
            "\n",
            "Global step: 5747,loss: 0.014278271\n",
            "\n",
            "Global step: 5748,loss: 0.01531979\n",
            "\n",
            "Global step: 5749,loss: 0.020143246\n",
            "\n",
            "Global step: 5750,loss: 0.014083017\n",
            "\n",
            "Global step: 5751,loss: 0.01623019\n",
            "\n",
            "Global step: 5752,loss: 0.0146938\n",
            "\n",
            "Global step: 5753,loss: 0.014010535\n",
            "\n",
            "Global step: 5754,loss: 0.015048728\n",
            "\n",
            "Global step: 5755,loss: 0.013923755\n",
            "\n",
            "Global step: 5756,loss: 0.014325694\n",
            "\n",
            "Global step: 5757,loss: 0.013510793\n",
            "\n",
            "Global step: 5758,loss: 0.0148769785\n",
            "\n",
            "Global step: 5759,loss: 0.013141241\n",
            "\n",
            "Global step: 5760,loss: 0.013860782\n",
            "\n",
            "Global step: 5761,loss: 0.013654871\n",
            "\n",
            "Global step: 5762,loss: 0.016010819\n",
            "\n",
            "Global step: 5763,loss: 0.013345844\n",
            "\n",
            "Global step: 5764,loss: 0.013422482\n",
            "\n",
            "Global step: 5765,loss: 0.014196003\n",
            "\n",
            "Global step: 5766,loss: 0.0140467845\n",
            "\n",
            "Global step: 5767,loss: 0.015693247\n",
            "\n",
            "Global step: 5768,loss: 0.015645025\n",
            "\n",
            "Global step: 5769,loss: 0.013825003\n",
            "\n",
            "Global step: 5770,loss: 0.014179348\n",
            "\n",
            "Global step: 5771,loss: 0.01339763\n",
            "\n",
            "Global step: 5772,loss: 0.01384545\n",
            "\n",
            "Global step: 5773,loss: 0.01355196\n",
            "\n",
            "Global step: 5774,loss: 0.014973611\n",
            "\n",
            "Global step: 5775,loss: 0.014261634\n",
            "\n",
            "Global step: 5776,loss: 0.014149415\n",
            "\n",
            "Global step: 5777,loss: 0.015046776\n",
            "\n",
            "Global step: 5778,loss: 0.01293089\n",
            "\n",
            "Global step: 5779,loss: 0.014535241\n",
            "\n",
            "Global step: 5780,loss: 0.01610703\n",
            "\n",
            "Global step: 5781,loss: 0.014500732\n",
            "\n",
            "Global step: 5782,loss: 0.013738806\n",
            "\n",
            "Global step: 5783,loss: 0.016179832\n",
            "\n",
            "Global step: 5784,loss: 0.013737551\n",
            "\n",
            "Global step: 5785,loss: 0.014622258\n",
            "\n",
            "Global step: 5786,loss: 0.013409307\n",
            "\n",
            "Global step: 5787,loss: 0.018504951\n",
            "\n",
            "Global step: 5788,loss: 0.0148021355\n",
            "\n",
            "Global step: 5789,loss: 0.014631267\n",
            "\n",
            "Global step: 5790,loss: 0.014000805\n",
            "\n",
            "Global step: 5791,loss: 0.018960807\n",
            "\n",
            "Global step: 5792,loss: 0.014033254\n",
            "\n",
            "Global step: 5793,loss: 0.014162324\n",
            "\n",
            "Global step: 5794,loss: 0.014375211\n",
            "\n",
            "Global step: 5795,loss: 0.014085907\n",
            "\n",
            "Global step: 5796,loss: 0.015331573\n",
            "\n",
            "Global step: 5797,loss: 0.014082005\n",
            "\n",
            "Global step: 5798,loss: 0.014384984\n",
            "\n",
            "Global step: 5799,loss: 0.019980034\n",
            "\n",
            "Global step: 5800,loss: 0.014434182\n",
            "\n",
            "Global step: 5801,loss: 0.013650787\n",
            "\n",
            "Global step: 5802,loss: 0.014236222\n",
            "\n",
            "Global step: 5803,loss: 0.015034998\n",
            "\n",
            "Global step: 5804,loss: 0.01420789\n",
            "\n",
            "Global step: 5805,loss: 0.014476875\n",
            "\n",
            "Global step: 5806,loss: 0.015003571\n",
            "\n",
            "Global step: 5807,loss: 0.013508415\n",
            "\n",
            "Global step: 5808,loss: 0.014847955\n",
            "\n",
            "Global step: 5809,loss: 0.014330393\n",
            "\n",
            "Global step: 5810,loss: 0.014276105\n",
            "\n",
            "Global step: 5811,loss: 0.015418241\n",
            "\n",
            "Global step: 5812,loss: 0.013252919\n",
            "\n",
            "Global step: 5813,loss: 0.014674596\n",
            "\n",
            "Global step: 5814,loss: 0.013773242\n",
            "\n",
            "Global step: 5815,loss: 0.013766298\n",
            "\n",
            "Global step: 5816,loss: 0.015626231\n",
            "\n",
            "Global step: 5817,loss: 0.014303839\n",
            "\n",
            "Global step: 5818,loss: 0.013541852\n",
            "\n",
            "Global step: 5819,loss: 0.014422823\n",
            "\n",
            "Global step: 5820,loss: 0.01457457\n",
            "\n",
            "Global step: 5821,loss: 0.013988263\n",
            "\n",
            "Global step: 5822,loss: 0.0149215115\n",
            "\n",
            "Global step: 5823,loss: 0.013249228\n",
            "\n",
            "Global step: 5824,loss: 0.015365157\n",
            "\n",
            "Global step: 5825,loss: 0.015147446\n",
            "\n",
            "Global step: 5826,loss: 0.014425243\n",
            "\n",
            "Global step: 5827,loss: 0.014293975\n",
            "\n",
            "Global step: 5828,loss: 0.014589986\n",
            "\n",
            "Global step: 5829,loss: 0.014927838\n",
            "\n",
            "Global step: 5830,loss: 0.01471721\n",
            "\n",
            "Global step: 5831,loss: 0.014603103\n",
            "\n",
            "Global step: 5832,loss: 0.01388584\n",
            "\n",
            "Global step: 5833,loss: 0.014977345\n",
            "\n",
            "Global step: 5834,loss: 0.013674437\n",
            "\n",
            "Global step: 5835,loss: 0.014977449\n",
            "\n",
            "Global step: 5836,loss: 0.013551254\n",
            "\n",
            "Global step: 5837,loss: 0.014519821\n",
            "\n",
            "Global step: 5838,loss: 0.01398336\n",
            "\n",
            "Global step: 5839,loss: 0.014638557\n",
            "\n",
            "Global step: 5840,loss: 0.013831092\n",
            "\n",
            "Global step: 5841,loss: 0.013960482\n",
            "\n",
            "Global step: 5842,loss: 0.014778558\n",
            "\n",
            "Global step: 5843,loss: 0.014211673\n",
            "\n",
            "Global step: 5844,loss: 0.014173238\n",
            "\n",
            "Global step: 5845,loss: 0.017672569\n",
            "\n",
            "Global step: 5846,loss: 0.014722338\n",
            "\n",
            "Global step: 5847,loss: 0.01369221\n",
            "\n",
            "Global step: 5848,loss: 0.014005347\n",
            "\n",
            "Global step: 5849,loss: 0.019030312\n",
            "\n",
            "Global step: 5850,loss: 0.014840728\n",
            "\n",
            "Global step: 5851,loss: 0.014467567\n",
            "\n",
            "Global step: 5852,loss: 0.014549515\n",
            "\n",
            "Global step: 5853,loss: 0.014445468\n",
            "\n",
            "Global step: 5854,loss: 0.013444023\n",
            "\n",
            "Global step: 5855,loss: 0.018492354\n",
            "\n",
            "Global step: 5856,loss: 0.014344185\n",
            "\n",
            "Global step: 5857,loss: 0.01567461\n",
            "\n",
            "Global step: 5858,loss: 0.014070414\n",
            "\n",
            "Global step: 5859,loss: 0.015189634\n",
            "\n",
            "Global step: 5860,loss: 0.015420898\n",
            "\n",
            "Global step: 5861,loss: 0.014606109\n",
            "\n",
            "Global step: 5862,loss: 0.018659668\n",
            "\n",
            "Global step: 5863,loss: 0.015396322\n",
            "\n",
            "Global step: 5864,loss: 0.014266131\n",
            "\n",
            "Global step: 5865,loss: 0.013766617\n",
            "\n",
            "Global step: 5866,loss: 0.01392728\n",
            "\n",
            "Global step: 5867,loss: 0.013842915\n",
            "\n",
            "Global step: 5868,loss: 0.012889244\n",
            "\n",
            "Global step: 5869,loss: 0.014154439\n",
            "\n",
            "Global step: 5870,loss: 0.013882079\n",
            "\n",
            "Global step: 5871,loss: 0.014743093\n",
            "\n",
            "Global step: 5872,loss: 0.013651576\n",
            "\n",
            "Global step: 5873,loss: 0.014823382\n",
            "\n",
            "Global step: 5874,loss: 0.013841883\n",
            "\n",
            "Global step: 5875,loss: 0.01421219\n",
            "\n",
            "Global step: 5876,loss: 0.014182227\n",
            "\n",
            "Global step: 5877,loss: 0.013214918\n",
            "\n",
            "Global step: 5878,loss: 0.01304652\n",
            "\n",
            "Global step: 5879,loss: 0.016803596\n",
            "\n",
            "Global step: 5880,loss: 0.01328933\n",
            "\n",
            "Global step: 5881,loss: 0.0137488\n",
            "\n",
            "Global step: 5882,loss: 0.014915999\n",
            "\n",
            "Global step: 5883,loss: 0.014544323\n",
            "\n",
            "Global step: 5884,loss: 0.015428944\n",
            "\n",
            "Global step: 5885,loss: 0.014516755\n",
            "\n",
            "Global step: 5886,loss: 0.014533113\n",
            "\n",
            "Global step: 5887,loss: 0.0139442505\n",
            "\n",
            "Global step: 5888,loss: 0.015321921\n",
            "\n",
            "Global step: 5889,loss: 0.019767527\n",
            "\n",
            "Global step: 5890,loss: 0.013264969\n",
            "\n",
            "Global step: 5891,loss: 0.014097289\n",
            "\n",
            "Global step: 5892,loss: 0.014220043\n",
            "\n",
            "Global step: 5893,loss: 0.013520793\n",
            "\n",
            "Global step: 5894,loss: 0.015368077\n",
            "\n",
            "Global step: 5895,loss: 0.014045343\n",
            "\n",
            "Global step: 5896,loss: 0.014776518\n",
            "\n",
            "Global step: 5897,loss: 0.014934387\n",
            "\n",
            "Global step: 5898,loss: 0.013849002\n",
            "\n",
            "Global step: 5899,loss: 0.013596694\n",
            "\n",
            "Global step: 5900,loss: 0.014295774\n",
            "\n",
            "Global step: 5901,loss: 0.014314187\n",
            "\n",
            "Global step: 5902,loss: 0.013963741\n",
            "\n",
            "Global step: 5903,loss: 0.014114526\n",
            "\n",
            "Global step: 5904,loss: 0.013530366\n",
            "\n",
            "Global step: 5905,loss: 0.013356378\n",
            "\n",
            "Global step: 5906,loss: 0.013308507\n",
            "\n",
            "Global step: 5907,loss: 0.014050099\n",
            "\n",
            "Global step: 5908,loss: 0.014405237\n",
            "\n",
            "Global step: 5909,loss: 0.013860358\n",
            "\n",
            "Global step: 5910,loss: 0.013873345\n",
            "\n",
            "Global step: 5911,loss: 0.014276254\n",
            "\n",
            "Global step: 5912,loss: 0.014754527\n",
            "\n",
            "Global step: 5913,loss: 0.016775081\n",
            "\n",
            "Global step: 5914,loss: 0.013943333\n",
            "\n",
            "Global step: 5915,loss: 0.013210159\n",
            "\n",
            "Global step: 5916,loss: 0.0147363\n",
            "\n",
            "Global step: 5917,loss: 0.013788281\n",
            "\n",
            "Global step: 5918,loss: 0.013509586\n",
            "\n",
            "Global step: 5919,loss: 0.014881456\n",
            "\n",
            "Global step: 5920,loss: 0.013675261\n",
            "\n",
            "Global step: 5921,loss: 0.013482231\n",
            "\n",
            "Global step: 5922,loss: 0.014734479\n",
            "\n",
            "Global step: 5923,loss: 0.014172065\n",
            "\n",
            "Global step: 5924,loss: 0.0151629485\n",
            "\n",
            "Global step: 5925,loss: 0.014850639\n",
            "\n",
            "Global step: 5926,loss: 0.015389184\n",
            "\n",
            "Global step: 5927,loss: 0.014442348\n",
            "\n",
            "Global step: 5928,loss: 0.014242663\n",
            "\n",
            "Global step: 5929,loss: 0.013500367\n",
            "\n",
            "Global step: 5930,loss: 0.012994088\n",
            "\n",
            "Global step: 5931,loss: 0.014352348\n",
            "\n",
            "Global step: 5932,loss: 0.014939919\n",
            "\n",
            "Global step: 5933,loss: 0.015133695\n",
            "\n",
            "Global step: 5934,loss: 0.014541294\n",
            "\n",
            "Global step: 5935,loss: 0.014702467\n",
            "\n",
            "Global step: 5936,loss: 0.013668415\n",
            "\n",
            "Global step: 5937,loss: 0.014177872\n",
            "\n",
            "Global step: 5938,loss: 0.014098834\n",
            "\n",
            "Global step: 5939,loss: 0.013523281\n",
            "\n",
            "Global step: 5940,loss: 0.015047179\n",
            "\n",
            "Global step: 5941,loss: 0.015050797\n",
            "\n",
            "Global step: 5942,loss: 0.013825393\n",
            "\n",
            "Global step: 5943,loss: 0.0148724485\n",
            "\n",
            "Global step: 5944,loss: 0.014887716\n",
            "\n",
            "Global step: 5945,loss: 0.013759199\n",
            "\n",
            "Global step: 5946,loss: 0.014865977\n",
            "\n",
            "Global step: 5947,loss: 0.014488777\n",
            "\n",
            "Global step: 5948,loss: 0.014990404\n",
            "\n",
            "Global step: 5949,loss: 0.013759954\n",
            "\n",
            "Global step: 5950,loss: 0.019597262\n",
            "\n",
            "Global step: 5951,loss: 0.013748948\n",
            "\n",
            "Global step: 5952,loss: 0.014630632\n",
            "\n",
            "Global step: 5953,loss: 0.016063243\n",
            "\n",
            "Global step: 5954,loss: 0.014359024\n",
            "\n",
            "Global step: 5955,loss: 0.015150352\n",
            "\n",
            "Global step: 5956,loss: 0.014369482\n",
            "\n",
            "Global step: 5957,loss: 0.014027247\n",
            "\n",
            "Global step: 5958,loss: 0.014524987\n",
            "\n",
            "Global step: 5959,loss: 0.013288314\n",
            "\n",
            "Global step: 5960,loss: 0.014304277\n",
            "\n",
            "Global step: 5961,loss: 0.014526809\n",
            "\n",
            "Global step: 5962,loss: 0.015283085\n",
            "\n",
            "Global step: 5963,loss: 0.01352036\n",
            "\n",
            "Global step: 5964,loss: 0.013983967\n",
            "\n",
            "Global step: 5965,loss: 0.015387636\n",
            "\n",
            "Global step: 5966,loss: 0.013364253\n",
            "\n",
            "Global step: 5967,loss: 0.013305182\n",
            "\n",
            "Global step: 5968,loss: 0.015448418\n",
            "\n",
            "Global step: 5969,loss: 0.014220402\n",
            "\n",
            "Global step: 5970,loss: 0.013334126\n",
            "\n",
            "Global step: 5971,loss: 0.014108838\n",
            "\n",
            "Global step: 5972,loss: 0.014094058\n",
            "\n",
            "Global step: 5973,loss: 0.014592342\n",
            "\n",
            "Global step: 5974,loss: 0.013617549\n",
            "\n",
            "Global step: 5975,loss: 0.014980595\n",
            "\n",
            "Global step: 5976,loss: 0.014881908\n",
            "\n",
            "Global step: 5977,loss: 0.014301462\n",
            "\n",
            "Global step: 5978,loss: 0.013318178\n",
            "\n",
            "Global step: 5979,loss: 0.013393178\n",
            "\n",
            "Global step: 5980,loss: 0.014255943\n",
            "\n",
            "Global step: 5981,loss: 0.014495561\n",
            "\n",
            "Global step: 5982,loss: 0.013846487\n",
            "\n",
            "Global step: 5983,loss: 0.014799459\n",
            "\n",
            "Global step: 5984,loss: 0.014121836\n",
            "\n",
            "Global step: 5985,loss: 0.013957353\n",
            "\n",
            "Global step: 5986,loss: 0.013344476\n",
            "\n",
            "Global step: 5987,loss: 0.012909233\n",
            "\n",
            "Global step: 5988,loss: 0.015672399\n",
            "\n",
            "Global step: 5989,loss: 0.018568065\n",
            "\n",
            "Global step: 5990,loss: 0.01507511\n",
            "\n",
            "Global step: 5991,loss: 0.013940833\n",
            "\n",
            "Global step: 5992,loss: 0.0140897315\n",
            "\n",
            "Global step: 5993,loss: 0.014194874\n",
            "\n",
            "Global step: 5994,loss: 0.0133981425\n",
            "\n",
            "Global step: 5995,loss: 0.014671327\n",
            "\n",
            "Global step: 5996,loss: 0.014375124\n",
            "\n",
            "Global step: 5997,loss: 0.014464162\n",
            "\n",
            "Global step: 5998,loss: 0.0138719315\n",
            "\n",
            "Global step: 5999,loss: 0.013804288\n",
            "\n",
            "Global step: 6000,loss: 0.013429733\n",
            "\n",
            "Global step: 6001,loss: 0.013394437\n",
            "\n",
            "Global step: 6002,loss: 0.01376068\n",
            "\n",
            "Global step: 6003,loss: 0.014042054\n",
            "\n",
            "Global step: 6004,loss: 0.013729396\n",
            "\n",
            "Global step: 6005,loss: 0.014065525\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 6005,Val_Loss: 0.018659834248515274,  Val_acc: 0.9973958333333334 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:32:36.155557 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 15/15:\n",
            "Global step: 6006,loss: 0.013463025\n",
            "\n",
            "Global step: 6007,loss: 0.014296902\n",
            "\n",
            "Global step: 6008,loss: 0.014237134\n",
            "\n",
            "Global step: 6009,loss: 0.013802791\n",
            "\n",
            "Global step: 6010,loss: 0.014328578\n",
            "\n",
            "Global step: 6011,loss: 0.014483038\n",
            "\n",
            "Global step: 6012,loss: 0.013986634\n",
            "\n",
            "Global step: 6013,loss: 0.015011017\n",
            "\n",
            "Global step: 6014,loss: 0.016428087\n",
            "\n",
            "Global step: 6015,loss: 0.013751738\n",
            "\n",
            "Global step: 6016,loss: 0.013788607\n",
            "\n",
            "Global step: 6017,loss: 0.014823035\n",
            "\n",
            "Global step: 6018,loss: 0.013948937\n",
            "\n",
            "Global step: 6019,loss: 0.013406073\n",
            "\n",
            "Global step: 6020,loss: 0.01498952\n",
            "\n",
            "Global step: 6021,loss: 0.013823704\n",
            "\n",
            "Global step: 6022,loss: 0.013588246\n",
            "\n",
            "Global step: 6023,loss: 0.014248581\n",
            "\n",
            "Global step: 6024,loss: 0.013484887\n",
            "\n",
            "Global step: 6025,loss: 0.014588583\n",
            "\n",
            "Global step: 6026,loss: 0.013864376\n",
            "\n",
            "Global step: 6027,loss: 0.013398321\n",
            "\n",
            "Global step: 6028,loss: 0.01416282\n",
            "\n",
            "Global step: 6029,loss: 0.013369498\n",
            "\n",
            "Global step: 6030,loss: 0.014338741\n",
            "\n",
            "Global step: 6031,loss: 0.013832758\n",
            "\n",
            "Global step: 6032,loss: 0.015287277\n",
            "\n",
            "Global step: 6033,loss: 0.013399428\n",
            "\n",
            "Global step: 6034,loss: 0.014236134\n",
            "\n",
            "Global step: 6035,loss: 0.014099832\n",
            "\n",
            "Global step: 6036,loss: 0.013785235\n",
            "\n",
            "Global step: 6037,loss: 0.013836055\n",
            "\n",
            "Global step: 6038,loss: 0.014287053\n",
            "\n",
            "Global step: 6039,loss: 0.013274951\n",
            "\n",
            "Global step: 6040,loss: 0.01863943\n",
            "\n",
            "Global step: 6041,loss: 0.014674967\n",
            "\n",
            "Global step: 6042,loss: 0.014011971\n",
            "\n",
            "Global step: 6043,loss: 0.013477196\n",
            "\n",
            "Global step: 6044,loss: 0.013080131\n",
            "\n",
            "Global step: 6045,loss: 0.013572167\n",
            "\n",
            "Global step: 6046,loss: 0.014297168\n",
            "\n",
            "Global step: 6047,loss: 0.014665591\n",
            "\n",
            "Global step: 6048,loss: 0.014056201\n",
            "\n",
            "Global step: 6049,loss: 0.0144990925\n",
            "\n",
            "Global step: 6050,loss: 0.013849105\n",
            "\n",
            "Global step: 6051,loss: 0.013454463\n",
            "\n",
            "Global step: 6052,loss: 0.013780791\n",
            "\n",
            "Global step: 6053,loss: 0.013539446\n",
            "\n",
            "Global step: 6054,loss: 0.013409703\n",
            "\n",
            "Global step: 6055,loss: 0.013320201\n",
            "\n",
            "Global step: 6056,loss: 0.014586848\n",
            "\n",
            "Global step: 6057,loss: 0.020508563\n",
            "\n",
            "Global step: 6058,loss: 0.013668211\n",
            "\n",
            "Global step: 6059,loss: 0.013474272\n",
            "\n",
            "Global step: 6060,loss: 0.014621267\n",
            "\n",
            "Global step: 6061,loss: 0.014394147\n",
            "\n",
            "Global step: 6062,loss: 0.013887232\n",
            "\n",
            "Global step: 6063,loss: 0.012942371\n",
            "\n",
            "Global step: 6064,loss: 0.014468527\n",
            "\n",
            "Global step: 6065,loss: 0.013349062\n",
            "\n",
            "Global step: 6066,loss: 0.013525183\n",
            "\n",
            "Global step: 6067,loss: 0.013408921\n",
            "\n",
            "Global step: 6068,loss: 0.013636656\n",
            "\n",
            "Global step: 6069,loss: 0.014270243\n",
            "\n",
            "Global step: 6070,loss: 0.015028398\n",
            "\n",
            "Global step: 6071,loss: 0.014297302\n",
            "\n",
            "Global step: 6072,loss: 0.013949827\n",
            "\n",
            "Global step: 6073,loss: 0.013019081\n",
            "\n",
            "Global step: 6074,loss: 0.015703216\n",
            "\n",
            "Global step: 6075,loss: 0.014508868\n",
            "\n",
            "Global step: 6076,loss: 0.013927143\n",
            "\n",
            "Global step: 6077,loss: 0.015494754\n",
            "\n",
            "Global step: 6078,loss: 0.014883698\n",
            "\n",
            "Global step: 6079,loss: 0.013607414\n",
            "\n",
            "Global step: 6080,loss: 0.013597131\n",
            "\n",
            "Global step: 6081,loss: 0.0133073535\n",
            "\n",
            "Global step: 6082,loss: 0.012715946\n",
            "\n",
            "Global step: 6083,loss: 0.013445985\n",
            "\n",
            "Global step: 6084,loss: 0.0137212295\n",
            "\n",
            "Global step: 6085,loss: 0.014153019\n",
            "\n",
            "Global step: 6086,loss: 0.013118732\n",
            "\n",
            "Global step: 6087,loss: 0.015847199\n",
            "\n",
            "Global step: 6088,loss: 0.013813509\n",
            "\n",
            "Global step: 6089,loss: 0.01365502\n",
            "\n",
            "Global step: 6090,loss: 0.014105058\n",
            "\n",
            "Global step: 6091,loss: 0.014250468\n",
            "\n",
            "Global step: 6092,loss: 0.014514244\n",
            "\n",
            "Global step: 6093,loss: 0.014876396\n",
            "\n",
            "Global step: 6094,loss: 0.013909187\n",
            "\n",
            "Global step: 6095,loss: 0.014222043\n",
            "\n",
            "Global step: 6096,loss: 0.013676024\n",
            "\n",
            "Global step: 6097,loss: 0.015106603\n",
            "\n",
            "Global step: 6098,loss: 0.014194128\n",
            "\n",
            "Global step: 6099,loss: 0.014380112\n",
            "\n",
            "Global step: 6100,loss: 0.015087276\n",
            "\n",
            "Global step: 6101,loss: 0.013733466\n",
            "\n",
            "Global step: 6102,loss: 0.014269764\n",
            "\n",
            "Global step: 6103,loss: 0.014191602\n",
            "\n",
            "Global step: 6104,loss: 0.013248765\n",
            "\n",
            "Global step: 6105,loss: 0.015663624\n",
            "\n",
            "Global step: 6106,loss: 0.014319397\n",
            "\n",
            "Global step: 6107,loss: 0.013575905\n",
            "\n",
            "Global step: 6108,loss: 0.013531707\n",
            "\n",
            "Global step: 6109,loss: 0.013951546\n",
            "\n",
            "Global step: 6110,loss: 0.01513334\n",
            "\n",
            "Global step: 6111,loss: 0.016268779\n",
            "\n",
            "Global step: 6112,loss: 0.0145883495\n",
            "\n",
            "Global step: 6113,loss: 0.01320755\n",
            "\n",
            "Global step: 6114,loss: 0.014042856\n",
            "\n",
            "Global step: 6115,loss: 0.014088963\n",
            "\n",
            "Global step: 6116,loss: 0.014471088\n",
            "\n",
            "Global step: 6117,loss: 0.013960068\n",
            "\n",
            "Global step: 6118,loss: 0.013497673\n",
            "\n",
            "Global step: 6119,loss: 0.015382883\n",
            "\n",
            "Global step: 6120,loss: 0.013744431\n",
            "\n",
            "Global step: 6121,loss: 0.023586687\n",
            "\n",
            "Global step: 6122,loss: 0.013950547\n",
            "\n",
            "Global step: 6123,loss: 0.01329398\n",
            "\n",
            "Global step: 6124,loss: 0.013271341\n",
            "\n",
            "Global step: 6125,loss: 0.014233498\n",
            "\n",
            "Global step: 6126,loss: 0.013547356\n",
            "\n",
            "Global step: 6127,loss: 0.012454707\n",
            "\n",
            "Global step: 6128,loss: 0.013388314\n",
            "\n",
            "Global step: 6129,loss: 0.013661664\n",
            "\n",
            "Global step: 6130,loss: 0.014204672\n",
            "\n",
            "Global step: 6131,loss: 0.014203469\n",
            "\n",
            "Global step: 6132,loss: 0.014160074\n",
            "\n",
            "Global step: 6133,loss: 0.013991019\n",
            "\n",
            "Global step: 6134,loss: 0.013519087\n",
            "\n",
            "Global step: 6135,loss: 0.013793608\n",
            "\n",
            "Global step: 6136,loss: 0.013946636\n",
            "\n",
            "Global step: 6137,loss: 0.014328936\n",
            "\n",
            "Global step: 6138,loss: 0.014904459\n",
            "\n",
            "Global step: 6139,loss: 0.014914592\n",
            "\n",
            "Global step: 6140,loss: 0.014488297\n",
            "\n",
            "Global step: 6141,loss: 0.013256487\n",
            "\n",
            "Global step: 6142,loss: 0.014645458\n",
            "\n",
            "Global step: 6143,loss: 0.013243115\n",
            "\n",
            "Global step: 6144,loss: 0.0133326845\n",
            "\n",
            "Global step: 6145,loss: 0.014643289\n",
            "\n",
            "Global step: 6146,loss: 0.013511348\n",
            "\n",
            "Global step: 6147,loss: 0.014252532\n",
            "\n",
            "Global step: 6148,loss: 0.014543682\n",
            "\n",
            "Global step: 6149,loss: 0.01288348\n",
            "\n",
            "Global step: 6150,loss: 0.013134714\n",
            "\n",
            "Global step: 6151,loss: 0.013906014\n",
            "\n",
            "Global step: 6152,loss: 0.013534697\n",
            "\n",
            "Global step: 6153,loss: 0.014027928\n",
            "\n",
            "Global step: 6154,loss: 0.014188128\n",
            "\n",
            "Global step: 6155,loss: 0.013887258\n",
            "\n",
            "Global step: 6156,loss: 0.0138962725\n",
            "\n",
            "Global step: 6157,loss: 0.013990802\n",
            "\n",
            "Global step: 6158,loss: 0.014282388\n",
            "\n",
            "Global step: 6159,loss: 0.0134324655\n",
            "\n",
            "Global step: 6160,loss: 0.017589536\n",
            "\n",
            "Global step: 6161,loss: 0.014500086\n",
            "\n",
            "Global step: 6162,loss: 0.013567169\n",
            "\n",
            "Global step: 6163,loss: 0.0135370735\n",
            "\n",
            "Global step: 6164,loss: 0.014561421\n",
            "\n",
            "Global step: 6165,loss: 0.013681881\n",
            "\n",
            "Global step: 6166,loss: 0.012889127\n",
            "\n",
            "Global step: 6167,loss: 0.014715413\n",
            "\n",
            "Global step: 6168,loss: 0.013439615\n",
            "\n",
            "Global step: 6169,loss: 0.013647144\n",
            "\n",
            "Global step: 6170,loss: 0.013295997\n",
            "\n",
            "Global step: 6171,loss: 0.014940714\n",
            "\n",
            "Global step: 6172,loss: 0.0139867915\n",
            "\n",
            "Global step: 6173,loss: 0.013871617\n",
            "\n",
            "Global step: 6174,loss: 0.013381347\n",
            "\n",
            "Global step: 6175,loss: 0.0141842645\n",
            "\n",
            "Global step: 6176,loss: 0.014944008\n",
            "\n",
            "Global step: 6177,loss: 0.013300785\n",
            "\n",
            "Global step: 6178,loss: 0.014502359\n",
            "\n",
            "Global step: 6179,loss: 0.014261305\n",
            "\n",
            "Global step: 6180,loss: 0.014054295\n",
            "\n",
            "Global step: 6181,loss: 0.012244964\n",
            "\n",
            "Global step: 6182,loss: 0.013752387\n",
            "\n",
            "Global step: 6183,loss: 0.014612451\n",
            "\n",
            "Global step: 6184,loss: 0.012777311\n",
            "\n",
            "Global step: 6185,loss: 0.013800678\n",
            "\n",
            "Global step: 6186,loss: 0.014004868\n",
            "\n",
            "Global step: 6187,loss: 0.01405553\n",
            "\n",
            "Global step: 6188,loss: 0.013075002\n",
            "\n",
            "Global step: 6189,loss: 0.013192867\n",
            "\n",
            "Global step: 6190,loss: 0.013631755\n",
            "\n",
            "Global step: 6191,loss: 0.013389724\n",
            "\n",
            "Global step: 6192,loss: 0.0139510995\n",
            "\n",
            "Global step: 6193,loss: 0.015084701\n",
            "\n",
            "Global step: 6194,loss: 0.013301807\n",
            "\n",
            "Global step: 6195,loss: 0.012511128\n",
            "\n",
            "Global step: 6196,loss: 0.014116515\n",
            "\n",
            "Global step: 6197,loss: 0.0145763075\n",
            "\n",
            "Global step: 6198,loss: 0.013334384\n",
            "\n",
            "Global step: 6199,loss: 0.012905095\n",
            "\n",
            "Global step: 6200,loss: 0.014873303\n",
            "\n",
            "Global step: 6201,loss: 0.013268269\n",
            "\n",
            "Global step: 6202,loss: 0.012960429\n",
            "\n",
            "Global step: 6203,loss: 0.013732755\n",
            "\n",
            "Global step: 6204,loss: 0.014750844\n",
            "\n",
            "Global step: 6205,loss: 0.014316984\n",
            "\n",
            "Global step: 6206,loss: 0.016366297\n",
            "\n",
            "Global step: 6207,loss: 0.014707499\n",
            "\n",
            "Global step: 6208,loss: 0.01358797\n",
            "\n",
            "Global step: 6209,loss: 0.013134897\n",
            "\n",
            "Global step: 6210,loss: 0.018188471\n",
            "\n",
            "Global step: 6211,loss: 0.01406246\n",
            "\n",
            "Global step: 6212,loss: 0.018725466\n",
            "\n",
            "Global step: 6213,loss: 0.013872376\n",
            "\n",
            "Global step: 6214,loss: 0.013891044\n",
            "\n",
            "Global step: 6215,loss: 0.013753556\n",
            "\n",
            "Global step: 6216,loss: 0.013888101\n",
            "\n",
            "Global step: 6217,loss: 0.01558966\n",
            "\n",
            "Global step: 6218,loss: 0.015656797\n",
            "\n",
            "Global step: 6219,loss: 0.013210188\n",
            "\n",
            "Global step: 6220,loss: 0.013546611\n",
            "\n",
            "Global step: 6221,loss: 0.014794461\n",
            "\n",
            "Global step: 6222,loss: 0.013295928\n",
            "\n",
            "Global step: 6223,loss: 0.014757496\n",
            "\n",
            "Global step: 6224,loss: 0.012986049\n",
            "\n",
            "Global step: 6225,loss: 0.013729945\n",
            "\n",
            "Global step: 6226,loss: 0.013897942\n",
            "\n",
            "Global step: 6227,loss: 0.014336957\n",
            "\n",
            "Global step: 6228,loss: 0.014221822\n",
            "\n",
            "Global step: 6229,loss: 0.013615633\n",
            "\n",
            "Global step: 6230,loss: 0.014170943\n",
            "\n",
            "Global step: 6231,loss: 0.013711127\n",
            "\n",
            "Global step: 6232,loss: 0.013437199\n",
            "\n",
            "Global step: 6233,loss: 0.013196651\n",
            "\n",
            "Global step: 6234,loss: 0.014788832\n",
            "\n",
            "Global step: 6235,loss: 0.013761668\n",
            "\n",
            "Global step: 6236,loss: 0.013059534\n",
            "\n",
            "Global step: 6237,loss: 0.013218492\n",
            "\n",
            "Global step: 6238,loss: 0.013358871\n",
            "\n",
            "Global step: 6239,loss: 0.014830862\n",
            "\n",
            "Global step: 6240,loss: 0.013834126\n",
            "\n",
            "Global step: 6241,loss: 0.013196519\n",
            "\n",
            "Global step: 6242,loss: 0.013891545\n",
            "\n",
            "Global step: 6243,loss: 0.013578728\n",
            "\n",
            "Global step: 6244,loss: 0.013198605\n",
            "\n",
            "Global step: 6245,loss: 0.012621387\n",
            "\n",
            "Global step: 6246,loss: 0.013700544\n",
            "\n",
            "Global step: 6247,loss: 0.013730353\n",
            "\n",
            "Global step: 6248,loss: 0.01366833\n",
            "\n",
            "Global step: 6249,loss: 0.013053445\n",
            "\n",
            "Global step: 6250,loss: 0.020292502\n",
            "\n",
            "Global step: 6251,loss: 0.012461407\n",
            "\n",
            "Global step: 6252,loss: 0.013094251\n",
            "\n",
            "Global step: 6253,loss: 0.014572495\n",
            "\n",
            "Global step: 6254,loss: 0.012689776\n",
            "\n",
            "Global step: 6255,loss: 0.018123614\n",
            "\n",
            "Global step: 6256,loss: 0.0141581055\n",
            "\n",
            "Global step: 6257,loss: 0.013510693\n",
            "\n",
            "Global step: 6258,loss: 0.013652068\n",
            "\n",
            "Global step: 6259,loss: 0.0138322385\n",
            "\n",
            "Global step: 6260,loss: 0.013081829\n",
            "\n",
            "Global step: 6261,loss: 0.013616914\n",
            "\n",
            "Global step: 6262,loss: 0.01442972\n",
            "\n",
            "Global step: 6263,loss: 0.013769842\n",
            "\n",
            "Global step: 6264,loss: 0.014585322\n",
            "\n",
            "Global step: 6265,loss: 0.013460133\n",
            "\n",
            "Global step: 6266,loss: 0.014202266\n",
            "\n",
            "Global step: 6267,loss: 0.014287164\n",
            "\n",
            "Global step: 6268,loss: 0.014111987\n",
            "\n",
            "Global step: 6269,loss: 0.012561939\n",
            "\n",
            "Global step: 6270,loss: 0.015222858\n",
            "\n",
            "Global step: 6271,loss: 0.013381072\n",
            "\n",
            "Global step: 6272,loss: 0.01332083\n",
            "\n",
            "Global step: 6273,loss: 0.013722743\n",
            "\n",
            "Global step: 6274,loss: 0.013478914\n",
            "\n",
            "Global step: 6275,loss: 0.013200241\n",
            "\n",
            "Global step: 6276,loss: 0.012970797\n",
            "\n",
            "Global step: 6277,loss: 0.012864859\n",
            "\n",
            "Global step: 6278,loss: 0.012548909\n",
            "\n",
            "Global step: 6279,loss: 0.013308261\n",
            "\n",
            "Global step: 6280,loss: 0.015317494\n",
            "\n",
            "Global step: 6281,loss: 0.014173403\n",
            "\n",
            "Global step: 6282,loss: 0.014207943\n",
            "\n",
            "Global step: 6283,loss: 0.014084939\n",
            "\n",
            "Global step: 6284,loss: 0.014953522\n",
            "\n",
            "Global step: 6285,loss: 0.013773985\n",
            "\n",
            "Global step: 6286,loss: 0.013613114\n",
            "\n",
            "Global step: 6287,loss: 0.013260979\n",
            "\n",
            "Global step: 6288,loss: 0.0137607185\n",
            "\n",
            "Global step: 6289,loss: 0.013529998\n",
            "\n",
            "Global step: 6290,loss: 0.014175785\n",
            "\n",
            "Global step: 6291,loss: 0.013815074\n",
            "\n",
            "Global step: 6292,loss: 0.012904312\n",
            "\n",
            "Global step: 6293,loss: 0.013346686\n",
            "\n",
            "Global step: 6294,loss: 0.013508904\n",
            "\n",
            "Global step: 6295,loss: 0.013405626\n",
            "\n",
            "Global step: 6296,loss: 0.014653857\n",
            "\n",
            "Global step: 6297,loss: 0.013853237\n",
            "\n",
            "Global step: 6298,loss: 0.013520937\n",
            "\n",
            "Global step: 6299,loss: 0.014092553\n",
            "\n",
            "Global step: 6300,loss: 0.013076633\n",
            "\n",
            "Global step: 6301,loss: 0.013580123\n",
            "\n",
            "Global step: 6302,loss: 0.020147154\n",
            "\n",
            "Global step: 6303,loss: 0.013474423\n",
            "\n",
            "Global step: 6304,loss: 0.014297368\n",
            "\n",
            "Global step: 6305,loss: 0.013998172\n",
            "\n",
            "Global step: 6306,loss: 0.014044623\n",
            "\n",
            "Global step: 6307,loss: 0.012571617\n",
            "\n",
            "Global step: 6308,loss: 0.013354798\n",
            "\n",
            "Global step: 6309,loss: 0.014257628\n",
            "\n",
            "Global step: 6310,loss: 0.013304519\n",
            "\n",
            "Global step: 6311,loss: 0.0134095\n",
            "\n",
            "Global step: 6312,loss: 0.01363148\n",
            "\n",
            "Global step: 6313,loss: 0.014764866\n",
            "\n",
            "Global step: 6314,loss: 0.013607422\n",
            "\n",
            "Global step: 6315,loss: 0.013660493\n",
            "\n",
            "Global step: 6316,loss: 0.014088232\n",
            "\n",
            "Global step: 6317,loss: 0.0133417165\n",
            "\n",
            "Global step: 6318,loss: 0.014688491\n",
            "\n",
            "Global step: 6319,loss: 0.012985001\n",
            "\n",
            "Global step: 6320,loss: 0.014262088\n",
            "\n",
            "Global step: 6321,loss: 0.013363232\n",
            "\n",
            "Global step: 6322,loss: 0.019011952\n",
            "\n",
            "Global step: 6323,loss: 0.014907473\n",
            "\n",
            "Global step: 6324,loss: 0.014186678\n",
            "\n",
            "Global step: 6325,loss: 0.0129196085\n",
            "\n",
            "Global step: 6326,loss: 0.013128169\n",
            "\n",
            "Global step: 6327,loss: 0.014360913\n",
            "\n",
            "Global step: 6328,loss: 0.013573305\n",
            "\n",
            "Global step: 6329,loss: 0.013164176\n",
            "\n",
            "Global step: 6330,loss: 0.014078905\n",
            "\n",
            "Global step: 6331,loss: 0.013530957\n",
            "\n",
            "Global step: 6332,loss: 0.014300484\n",
            "\n",
            "Global step: 6333,loss: 0.013967197\n",
            "\n",
            "Global step: 6334,loss: 0.013363565\n",
            "\n",
            "Global step: 6335,loss: 0.013959567\n",
            "\n",
            "Global step: 6336,loss: 0.013882133\n",
            "\n",
            "Global step: 6337,loss: 0.0133673465\n",
            "\n",
            "Global step: 6338,loss: 0.0135849845\n",
            "\n",
            "Global step: 6339,loss: 0.014582898\n",
            "\n",
            "Global step: 6340,loss: 0.014544011\n",
            "\n",
            "Global step: 6341,loss: 0.013825194\n",
            "\n",
            "Global step: 6342,loss: 0.01337004\n",
            "\n",
            "Global step: 6343,loss: 0.013441684\n",
            "\n",
            "Global step: 6344,loss: 0.013153221\n",
            "\n",
            "Global step: 6345,loss: 0.014509902\n",
            "\n",
            "Global step: 6346,loss: 0.01424277\n",
            "\n",
            "Global step: 6347,loss: 0.013286449\n",
            "\n",
            "Global step: 6348,loss: 0.014394354\n",
            "\n",
            "Global step: 6349,loss: 0.012835952\n",
            "\n",
            "Global step: 6350,loss: 0.0137471\n",
            "\n",
            "Global step: 6351,loss: 0.018321466\n",
            "\n",
            "Global step: 6352,loss: 0.014123919\n",
            "\n",
            "Global step: 6353,loss: 0.012382737\n",
            "\n",
            "Global step: 6354,loss: 0.013699599\n",
            "\n",
            "Global step: 6355,loss: 0.012950651\n",
            "\n",
            "Global step: 6356,loss: 0.01568608\n",
            "\n",
            "Global step: 6357,loss: 0.013914681\n",
            "\n",
            "Global step: 6358,loss: 0.013870929\n",
            "\n",
            "Global step: 6359,loss: 0.014090884\n",
            "\n",
            "Global step: 6360,loss: 0.013479689\n",
            "\n",
            "Global step: 6361,loss: 0.014682677\n",
            "\n",
            "Global step: 6362,loss: 0.013535439\n",
            "\n",
            "Global step: 6363,loss: 0.013709784\n",
            "\n",
            "Global step: 6364,loss: 0.013146786\n",
            "\n",
            "Global step: 6365,loss: 0.013514853\n",
            "\n",
            "Global step: 6366,loss: 0.0138468\n",
            "\n",
            "Global step: 6367,loss: 0.013902216\n",
            "\n",
            "Global step: 6368,loss: 0.015511728\n",
            "\n",
            "Global step: 6369,loss: 0.013231483\n",
            "\n",
            "Global step: 6370,loss: 0.014297851\n",
            "\n",
            "Global step: 6371,loss: 0.014380154\n",
            "\n",
            "Global step: 6372,loss: 0.013548997\n",
            "\n",
            "Global step: 6373,loss: 0.013665646\n",
            "\n",
            "Global step: 6374,loss: 0.01360711\n",
            "\n",
            "Global step: 6375,loss: 0.014229113\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 7.69576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:33:22.167469 139648999347968 supervisor.py:1099] global_step/sec: 7.69576\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 6376,loss: 0.018821388\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 6377.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:33:22.290773 139649007740672 supervisor.py:1050] Recording summary at step 6377.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 6377,loss: 0.012733133\n",
            "\n",
            "Global step: 6378,loss: 0.015659694\n",
            "\n",
            "Global step: 6379,loss: 0.014924721\n",
            "\n",
            "Global step: 6380,loss: 0.014108801\n",
            "\n",
            "Global step: 6381,loss: 0.014698744\n",
            "\n",
            "Global step: 6382,loss: 0.013961528\n",
            "\n",
            "Global step: 6383,loss: 0.014368711\n",
            "\n",
            "Global step: 6384,loss: 0.015074227\n",
            "\n",
            "Global step: 6385,loss: 0.013570946\n",
            "\n",
            "Global step: 6386,loss: 0.014315502\n",
            "\n",
            "Global step: 6387,loss: 0.0146703655\n",
            "\n",
            "Global step: 6388,loss: 0.013567651\n",
            "\n",
            "Global step: 6389,loss: 0.013802628\n",
            "\n",
            "Global step: 6390,loss: 0.019978957\n",
            "\n",
            "Global step: 6391,loss: 0.020064175\n",
            "\n",
            "Global step: 6392,loss: 0.015508031\n",
            "\n",
            "Global step: 6393,loss: 0.017397972\n",
            "\n",
            "Global step: 6394,loss: 0.013634512\n",
            "\n",
            "Global step: 6395,loss: 0.018508216\n",
            "\n",
            "Global step: 6396,loss: 0.013668435\n",
            "\n",
            "Global step: 6397,loss: 0.022353334\n",
            "\n",
            "Global step: 6398,loss: 0.0135922935\n",
            "\n",
            "Global step: 6399,loss: 0.015994113\n",
            "\n",
            "Global step: 6400,loss: 0.014398253\n",
            "\n",
            "Global step: 6401,loss: 0.01462193\n",
            "\n",
            "Global step: 6402,loss: 0.014797804\n",
            "\n",
            "Global step: 6403,loss: 0.014400287\n",
            "\n",
            "Global step: 6404,loss: 0.014432818\n",
            "\n",
            "Global step: 6405,loss: 0.016059993\n",
            "\n",
            "Global step: 6406,loss: 0.014810272\n",
            "\n",
            "Global step: 6407,loss: 0.014861329\n",
            "\n",
            "Global step: 6408,loss: 0.014531671\n",
            "\n",
            "Global step: 6409,loss: 0.014973771\n",
            "\n",
            "Global step: 6410,loss: 0.014814171\n",
            "\n",
            "Global step: 6411,loss: 0.014107121\n",
            "\n",
            "Global step: 6412,loss: 0.014841233\n",
            "\n",
            "Global step: 6413,loss: 0.013691188\n",
            "\n",
            "Global step: 6414,loss: 0.0139384605\n",
            "\n",
            "Global step: 6415,loss: 0.013210724\n",
            "\n",
            "Global step: 6416,loss: 0.01373719\n",
            "\n",
            "Global step: 6417,loss: 0.0143275615\n",
            "\n",
            "Global step: 6418,loss: 0.014911629\n",
            "\n",
            "Global step: 6419,loss: 0.014175752\n",
            "\n",
            "Global step: 6420,loss: 0.016309507\n",
            "\n",
            "Global step: 6421,loss: 0.012486048\n",
            "\n",
            "Global step: 6422,loss: 0.016718287\n",
            "\n",
            "Global step: 6423,loss: 0.016346712\n",
            "\n",
            "Global step: 6424,loss: 0.013610477\n",
            "\n",
            "Global step: 6425,loss: 0.014586312\n",
            "\n",
            "Global step: 6426,loss: 0.014806237\n",
            "\n",
            "Global step: 6427,loss: 0.014196531\n",
            "\n",
            "Global step: 6428,loss: 0.014184627\n",
            "\n",
            "Global step: 6429,loss: 0.014028132\n",
            "\n",
            "Global step: 6430,loss: 0.013466176\n",
            "\n",
            "Global step: 6431,loss: 0.015543036\n",
            "\n",
            "Global step: 6432,loss: 0.015254111\n",
            "\n",
            "Global step: 6433,loss: 0.01529265\n",
            "\n",
            "Global step: 6434,loss: 0.01421697\n",
            "\n",
            "\n",
            "##################### Saving Model ############################\n",
            "\n",
            "Global Step: 6434,Val_Loss: 0.01854775163034598,  Val_acc: 0.9981971153846154 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:33:32.148157 139651229792128 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:33:33.182763 139651229792128 <ipython-input-9-72518a14b1af>:16] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "outputId": "d7ac23f2-3a5f-4852-f1e5-046eab7a63c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "cfg.is_training=False\n",
        "\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:46.696281 139651229792128 <ipython-input-10-2d49ea40900d>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:47.896312 139651229792128 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:47.897973 139651229792128 <ipython-input-10-2d49ea40900d>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0014_step_6434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:54.346555 139651229792128 saver.py:1284] Restoring parameters from logdir/model_epoch_0014_step_6434\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1110 21:34:54.644233 139651229792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:54.657469 139651229792128 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:34:54.682682 139651229792128 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:25.924193 139651229792128 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:26.560143 139651229792128 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0014_step_6434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:26.582731 139651229792128 saver.py:1284] Restoring parameters from logdir/model_epoch_0014_step_6434\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Model restored!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:28.457292 139651229792128 <ipython-input-8-04cc3fa079aa>:117] Model restored!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:28.820175 139647237748480 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy is 0.7939453125:\n",
            "\n",
            "Test accuracy has been saved to results/test_acc\n",
            "INFO:tensorflow:Recording summary at step 6435.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1110 21:35:33.231996 139647229355776 supervisor.py:1050] Recording summary at step 6435.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W8Z-9sVTMtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}