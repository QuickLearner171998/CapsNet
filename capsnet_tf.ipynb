{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsnet_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/CapsNet/blob/master/capsnet_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Ny1TiIq_Mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed76faf7-9045-43a8-9689-d3ebf4570f04"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad8N1QAq_Lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "95dec07f-fd90-4ad8-cd35-fabba9622908"
      },
      "source": [
        "%cd gdrive\n",
        "%cd 'My Drive'\n",
        "%cd 'MY Projects'\n",
        "%cd 'EEE lop'\n",
        "%cd 'tensorflow_implementation'\n",
        "!ls\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/MY Projects\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop\n",
            "/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation\n",
            "'capsnet tf.ipynb'   data     results\t\t  Try-1\n",
            " capsnet_tf.py\t     logdir   tensorboard.ipynb   Try-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHekIQBo9Sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "25acc27a-89a2-4dc7-e8dc-a4621de2c36c"
      },
      "source": [
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_1fObfoM4V",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp6y0HhQoDyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_kannada(batch_size, is_training=True):\n",
        "    if is_training:\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_train-idx3-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
        "\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_train-idx1-ubyte')\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n",
        "\n",
        "        trX = trainX[:55000] / 255.\n",
        "        trY = trainY[:55000]\n",
        "\n",
        "        valX = trainX[55000:, ] / 255.\n",
        "        valY = trainY[55000:]\n",
        "\n",
        "        num_tr_batch = 55000 // batch_size\n",
        "        num_val_batch = 5000 // batch_size\n",
        "\n",
        "        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n",
        "    else:\n",
        "        \n",
        "        # test on 60K dataset\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/X_kannada_MNIST_test-idx3-ubyte')\n",
        "        \n",
        "        # test on DIG 10K \n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/X_dig_MNIST-idx3-ubyte.gz (Unzipped Files)/X_dig_MNIST-idx3-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        n_test_img = ((len(loaded)-16))//(28*28)\n",
        "        teX = loaded[16:].reshape((n_test_img, 28, 28, 1)).astype(np.float)\n",
        "        \n",
        "        # test on 60K\n",
        "        #fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Kannada_MNIST/y_kannada_MNIST_test-idx1-ubyte')\n",
        "        \n",
        "        # test on 10K\n",
        "        fd = open('/content/gdrive/My Drive/MY Projects/EEE lop/tensorflow_implementation/data/Kannada_MNIST_datataset_paper/Kannada_MNIST_Ubyte_gz/Dig_MNIST/y_dig_MNIST-idx1-ubyte.gz (Unzipped Files)/y_dig_MNIST-idx1-ubyte')\n",
        "        \n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((n_test_img)).astype(np.int32)\n",
        "\n",
        "        num_te_batch = n_test_img // batch_size\n",
        "        return teX / 255., teY, num_te_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(batch_size, is_training=True, one_hot=False):\n",
        "    return load_mnist_kannada(batch_size, is_training)\n",
        "    \n",
        "\n",
        "def get_batch_data(batch_size, num_threads):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist_kannada(batch_size, is_training=True)\n",
        "    data_queues = tf.train.slice_input_producer([trX, trY])\n",
        "    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n",
        "                                  batch_size=batch_size,\n",
        "                                  capacity=batch_size * 64,\n",
        "                                  min_after_dequeue=batch_size * 32,\n",
        "                                  allow_smaller_final_batch=False)\n",
        "\n",
        "    return(X, Y)\n",
        "\n",
        "\n",
        "def save_images(imgs, size, path):\n",
        "    '''\n",
        "    Args:\n",
        "        imgs: [batch_size, image_height, image_width]\n",
        "        size: a list with tow int elements, [image_height, image_width]\n",
        "        path: the path to save images\n",
        "    '''\n",
        "    imgs = (imgs + 1.) / 2  # inverse_transform\n",
        "    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n",
        "\n",
        "\n",
        "def mergeImgs(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    imgs = np.zeros((h * size[0], w * size[1], 3))\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
        "    except:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n",
        "\n",
        "\n",
        "# For version compatibility\n",
        "def softmax(logits, axis=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, axis=axis)\n",
        "    except:\n",
        "        return tf.nn.softmax(logits, dim=axis)\n",
        "\n",
        "\n",
        "def get_shape(inputs, name=None):\n",
        "    name = \"shape\" if name is None else name\n",
        "    with tf.name_scope(name):\n",
        "        static_shape = inputs.get_shape().as_list()\n",
        "        dynamic_shape = tf.shape(inputs)\n",
        "        shape = []\n",
        "        for i, dim in enumerate(static_shape):\n",
        "            dim = dim if dim is not None else dynamic_shape[i]\n",
        "            shape.append(dim)\n",
        "        return(shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpCvqBGoWLL",
        "colab_type": "text"
      },
      "source": [
        "# CapsLayer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKkxQriobw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsLayer(object):\n",
        "    ''' Capsule layer.\n",
        "    Args:\n",
        "        input: A 4-D tensor.\n",
        "        num_outputs: the number of capsule in this layer.\n",
        "        vec_len: integer, the length of the output vector of a capsule.\n",
        "        layer_type: string, one of 'FC' or \"CONV\", the type of this layer,\n",
        "            fully connected or convolution, for the future expansion capability\n",
        "        with_routing: boolean, this capsule is routing with the\n",
        "                      lower-level layer capsule.\n",
        "\n",
        "    Returns:\n",
        "        A 4-D tensor.\n",
        "    '''\n",
        "    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type='FC'):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.vec_len = vec_len\n",
        "        self.with_routing = with_routing\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def __call__(self, input, kernel_size=None, stride=None):\n",
        "        '''\n",
        "        The parameters 'kernel_size' and 'stride' will be used while 'layer_type' equal 'CONV'\n",
        "        '''\n",
        "        if self.layer_type == 'CONV':\n",
        "            self.kernel_size = kernel_size\n",
        "            self.stride = stride\n",
        "\n",
        "            if not self.with_routing:\n",
        "                # the PrimaryCaps layer, a convolutional layer\n",
        "                # input: [batch_size, 20, 20, 256]\n",
        "                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n",
        "\n",
        "                # NOTE: I can't find out any words from the paper whether the\n",
        "                # PrimaryCap convolution does a ReLU activation or not before\n",
        "                # squashing function, but experiment show that using ReLU get a\n",
        "                # higher test accuracy. So, which one to use will be your choice\n",
        "                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                                                    self.kernel_size, self.stride, padding=\"VALID\",\n",
        "                                                    activation_fn=tf.nn.relu)\n",
        "                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n",
        "                #                                    self.kernel_size, self.stride,padding=\"VALID\",\n",
        "                #                                    activation_fn=None)\n",
        "                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n",
        "\n",
        "                # return tensor with shape [batch_size, 1152, 8, 1]\n",
        "                capsules = squash(capsules)\n",
        "                return(capsules)\n",
        "\n",
        "        if self.layer_type == 'FC':\n",
        "            if self.with_routing:\n",
        "                # the DigitCaps layer, a fully connected layer\n",
        "                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n",
        "                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n",
        "\n",
        "                with tf.variable_scope('routing'):\n",
        "                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n",
        "                    # about the reason of using 'batch_size', see issue #21\n",
        "                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n",
        "                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n",
        "                    capsules = tf.squeeze(capsules, axis=1)\n",
        "\n",
        "            return(capsules)\n",
        "\n",
        "\n",
        "def routing(input, b_IJ, num_outputs=10, num_dims=16):\n",
        "    ''' The routing algorithm.\n",
        "\n",
        "    Args:\n",
        "        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n",
        "               shape, num_caps_l meaning the number of capsule in the layer l.\n",
        "        num_outputs: the number of output capsules.\n",
        "        num_dims: the number of dimensions for output capsule.\n",
        "    Returns:\n",
        "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
        "        representing the vector output `v_j` in the layer l+1\n",
        "    Notes:\n",
        "        u_i represents the vector output of capsule i in the layer l, and\n",
        "        v_j the vector output of capsule j in the layer l+1.\n",
        "     '''\n",
        "\n",
        "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
        "    input_shape = get_shape(input)\n",
        "    W = tf.get_variable('Weight', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n",
        "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n",
        "    biases = tf.get_variable('bias', shape=(1, 1, num_outputs, num_dims, 1))\n",
        "\n",
        "    # Eq.2, calc u_hat\n",
        "    # Since tf.matmul is a time-consuming op,\n",
        "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
        "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
        "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
        "    # reshape to [a, c]\n",
        "    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n",
        "    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n",
        "\n",
        "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
        "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n",
        "    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n",
        "\n",
        "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
        "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
        "\n",
        "    # line 3,for r iterations do\n",
        "    for r_iter in range(cfg.iter_routing):\n",
        "        with tf.variable_scope('iter_' + str(r_iter)):\n",
        "            # line 4:\n",
        "            # => [batch_size, 1152, 10, 1, 1]\n",
        "            c_IJ = softmax(b_IJ, axis=2)\n",
        "\n",
        "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
        "            if r_iter == cfg.iter_routing - 1:\n",
        "                # line 5:\n",
        "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
        "                # => [batch_size, 1152, 10, 16, 1]\n",
        "                s_J = tf.multiply(c_IJ, u_hat)\n",
        "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n",
        "\n",
        "                # line 6:\n",
        "                # squash using Eq.1,\n",
        "                v_J = squash(s_J)\n",
        "                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
        "            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
        "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
        "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
        "                v_J = squash(s_J)\n",
        "\n",
        "                # line 7:\n",
        "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
        "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
        "                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
        "                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
        "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
        "                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n",
        "\n",
        "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
        "                b_IJ += u_produce_v\n",
        "\n",
        "    return(v_J)\n",
        "\n",
        "\n",
        "def squash(vector):\n",
        "    '''Squashing function corresponding to Eq. 1\n",
        "    Args:\n",
        "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
        "    Returns:\n",
        "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
        "    '''\n",
        "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
        "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
        "    vec_squashed = scalar_factor * vector  # element-wise\n",
        "    return(vec_squashed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANQ7MNaoeX5",
        "colab_type": "text"
      },
      "source": [
        "# capsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkHCMdwoj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "License: Apache-2.0\n",
        "Author: Huadong Liao\n",
        "E-mail: naturomics.liao@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "epsilon = 1e-9\n",
        "\n",
        "\n",
        "class CapsNet(object):\n",
        "    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            height: Integer, the height of inputs.\n",
        "            width: Integer, the width of inputs.\n",
        "            channels: Integer, the channels of inputs.\n",
        "            num_label: Integer, the category number.\n",
        "        \"\"\"\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.num_label = num_label\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.X, self.labels = get_batch_data(cfg.batch_size, cfg.num_threads)\n",
        "                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n",
        "\n",
        "                self.build_arch()\n",
        "                self.loss()\n",
        "                self._summary()\n",
        "\n",
        "                # t_vars = tf.trainable_variables()\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer()\n",
        "                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
        "            else:\n",
        "                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n",
        "                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n",
        "                self.build_arch()\n",
        "\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "    def build_arch(self):\n",
        "        with tf.variable_scope('Conv1_layer'):\n",
        "            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n",
        "            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n",
        "                                             kernel_size=9, stride=1,\n",
        "                                             padding='VALID')\n",
        "\n",
        "        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n",
        "        with tf.variable_scope('PrimaryCaps_layer'):\n",
        "            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
        "            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
        "\n",
        "        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n",
        "        with tf.variable_scope('DigitCaps_layer'):\n",
        "            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type='FC')\n",
        "            self.caps2 = digitCaps(caps1)\n",
        "\n",
        "        # Decoder structure in Fig. 2\n",
        "        # 1. Do masking, how:\n",
        "        with tf.variable_scope('Masking'):\n",
        "            # a). calc ||v_c||, then do softmax(||v_c||)\n",
        "            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
        "            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n",
        "                                               axis=2, keepdims=True) + epsilon)\n",
        "            self.softmax_v = softmax(self.v_length, axis=1)\n",
        "            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "            # b). pick out the index of max softmax val of the 10 caps\n",
        "            # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
        "            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n",
        "            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n",
        "            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n",
        "\n",
        "            # Method 1.\n",
        "            if not cfg.mask_with_y:\n",
        "                # c). indexing\n",
        "                # It's not easy to understand the indexing process with argmax_idx\n",
        "                # as we are 3-dim animal\n",
        "                masked_v = []\n",
        "                for batch_size in range(cfg.batch_size):\n",
        "                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n",
        "                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
        "\n",
        "                self.masked_v = tf.concat(masked_v, axis=0)\n",
        "                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n",
        "            # Method 2. masking with true label, default mode\n",
        "            else:\n",
        "                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n",
        "                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n",
        "\n",
        "        # 2. Reconstructe the MNIST images with 3 FC layers\n",
        "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
        "        with tf.variable_scope('Decoder'):\n",
        "            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n",
        "            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
        "            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
        "            self.decoded = tf.contrib.layers.fully_connected(fc2,\n",
        "                                                             num_outputs=self.height * self.width * self.channels,\n",
        "                                                             activation_fn=tf.sigmoid)\n",
        "\n",
        "    def loss(self):\n",
        "        # 1. The margin loss\n",
        "\n",
        "        # [batch_size, 10, 1, 1]\n",
        "        # max_l = max(0, m_plus-||v_c||)^2\n",
        "        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n",
        "        # max_r = max(0, ||v_c||-m_minus)^2\n",
        "        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n",
        "        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n",
        "\n",
        "        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
        "        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n",
        "        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n",
        "\n",
        "        # calc T_c: [batch_size, 10]\n",
        "        # T_c = Y, is my understanding correct? Try it.\n",
        "        T_c = self.Y\n",
        "        # [batch_size, 10], element-wise multiply\n",
        "        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n",
        "\n",
        "        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
        "\n",
        "        # 2. The reconstruction loss\n",
        "        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n",
        "        squared = tf.square(self.decoded - orgin)\n",
        "        self.reconstruction_err = tf.reduce_mean(squared)\n",
        "\n",
        "        # 3. Total loss\n",
        "        # The paper uses sum of squared error as reconstruction error, but we\n",
        "        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n",
        "        # mean squared error. In order to keep in line with the paper,the\n",
        "        # regularization scale should be 0.0005*784=0.392\n",
        "        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n",
        "\n",
        "    # Summary\n",
        "    def _summary(self):\n",
        "        train_summary = []\n",
        "        train_summary.append(tf.summary.scalar('train/margin_loss', self.margin_loss))\n",
        "        train_summary.append(tf.summary.scalar('train/reconstruction_loss', self.reconstruction_err))\n",
        "        train_summary.append(tf.summary.scalar('train/total_loss', self.total_loss))\n",
        "        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n",
        "        train_summary.append(tf.summary.image('reconstruction_img', recon_img))\n",
        "        self.train_summary = tf.summary.merge(train_summary)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2c47XQtVjMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Delete All flags before running\n",
        "\n",
        "# def del_all_flags(FLAGS):\n",
        "#     flags_dict = FLAGS._flags()    \n",
        "#     keys_list = [keys for keys in flags_dict]    \n",
        "#     for keys in keys_list:\n",
        "#         FLAGS.__delattr__(keys)\n",
        "\n",
        "# del_all_flags(tf.flags.FLAGS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdsP4XjooMy",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJfR97Roq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "\n",
        "############################\n",
        "#    hyper parameters      #\n",
        "############################\n",
        "\n",
        "# For separate margin loss\n",
        "flags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\n",
        "flags.DEFINE_float('m_minus', 0.1, 'the parameter of m minus')\n",
        "flags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n",
        "\n",
        "# for training\n",
        "flags.DEFINE_integer('batch_size', 256, 'batch size')\n",
        "flags.DEFINE_integer('epoch', 15, 'epoch')\n",
        "flags.DEFINE_integer('iter_routing', 5, 'number of iterations in routing algorithm')\n",
        "flags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n",
        "\n",
        "flags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\n",
        "flags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n",
        "\n",
        "\n",
        "############################\n",
        "#   environment setting    #\n",
        "############################\n",
        "flags.DEFINE_boolean('is_training', True, 'train or predict phase')\n",
        "flags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\n",
        "flags.DEFINE_string('logdir', 'logdir', 'logs directory')\n",
        "flags.DEFINE_integer('train_sum_freq', 1, 'the frequency of saving train summary(step)')\n",
        "#flags.DEFINE_integer('val_sum_freq', 429, 'the frequency of saving valuation summary(step)')\n",
        "flags.DEFINE_integer('save_freq', 1, 'the frequency of saving model(epoch)')\n",
        "flags.DEFINE_string('results', 'results', 'path for saving results')\n",
        "\n",
        "############################\n",
        "#   distributed setting    #\n",
        "############################\n",
        "flags.DEFINE_integer('num_gpu', 8, 'number of gpus for distributed training')\n",
        "flags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\n",
        "flags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n",
        "\n",
        "cfg = tf.app.flags.FLAGS\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzFdMr1Wo1AX",
        "colab_type": "text"
      },
      "source": [
        "# Main Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAxG1zdo0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to():\n",
        "    if not os.path.exists(cfg.results):\n",
        "        os.mkdir(cfg.results)\n",
        "    if cfg.is_training:\n",
        "        loss = cfg.results + '/loss.csv'\n",
        "        train_acc = cfg.results + '/train_acc.csv'\n",
        "        val_acc = cfg.results + '/val_acc.csv'\n",
        "\n",
        "        if os.path.exists(val_acc):\n",
        "            os.remove(val_acc)\n",
        "        if os.path.exists(loss):\n",
        "            os.remove(loss)\n",
        "        if os.path.exists(train_acc):\n",
        "            os.remove(train_acc)\n",
        "\n",
        "        fd_train_acc = open(train_acc, 'w')\n",
        "        fd_train_acc.write('step,train_acc\\n')\n",
        "        fd_loss = open(loss, 'w')\n",
        "        fd_loss.write('step,loss\\n')\n",
        "        fd_val_acc = open(val_acc, 'w')\n",
        "        fd_val_acc.write('step,val_acc\\n')\n",
        "        return(fd_train_acc, fd_loss, fd_val_acc)\n",
        "    else:\n",
        "        test_acc = cfg.results + '/test_acc.csv'\n",
        "        if os.path.exists(test_acc):\n",
        "            os.remove(test_acc)\n",
        "        fd_test_acc = open(test_acc, 'w')\n",
        "        fd_test_acc.write('test_acc\\n')\n",
        "        return(fd_test_acc)\n",
        "\n",
        "\n",
        "def train(model, supervisor, num_label):\n",
        "    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.batch_size, is_training=True)\n",
        "    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n",
        "\n",
        "    fd_train_acc, fd_loss, fd_val_acc = save_to()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    with supervisor.managed_session(config=config) as sess:\n",
        "        print(\"\\nNote: all of results will be saved to directory: \" + cfg.results)\n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_loss = 100000000\n",
        "        for epoch in range(cfg.epoch):\n",
        "            print(\"Training for epoch %d/%d:\" % (epoch, cfg.epoch))\n",
        "            if supervisor.should_stop():\n",
        "                print('supervisor stoped!')\n",
        "                break\n",
        "            for step in range(num_tr_batch):\n",
        "                start = step * cfg.batch_size\n",
        "                end = start + cfg.batch_size\n",
        "                global_step = epoch * num_tr_batch + step\n",
        "\n",
        "                if global_step % cfg.train_sum_freq == 0:\n",
        "                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n",
        "                    assert not np.isnan(loss), 'Something wrong! loss is nan...'\n",
        "                    supervisor.summary_writer.add_summary(summary_str, global_step)\n",
        "\n",
        "                    print(\"Global step: {}\".format(str(global_step)) + ',' + \"loss: {}\".format(str(loss)) + \"\\n\")\n",
        "\n",
        "\n",
        "                    fd_loss.write(str(global_step) + ',' + str(loss) + \"\\n\")\n",
        "                    fd_loss.flush()\n",
        "                    fd_train_acc.write(str(global_step) + ',' + str(train_acc / cfg.batch_size) + \"\\n\")\n",
        "                    fd_train_acc.flush()\n",
        "                else:\n",
        "                    sess.run(model.train_op)\n",
        "                \n",
        "                \n",
        "\n",
        "                # if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n",
        "                #     val_acc = 0\n",
        "                #     for i in range(num_val_batch):\n",
        "                #         start = i * cfg.batch_size\n",
        "                #         end = start + cfg.batch_size\n",
        "                #         acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                #         val_acc += acc\n",
        "                #     val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                #     print(\"Global Step: \"+str(global_step) + ',' + \"val_acc: \"+ str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                #     fd_val_acc.flush()\n",
        "\n",
        "            if (epoch + 1) % cfg.save_freq == 0:\n",
        "                val_acc = 0\n",
        "                val_loss =0\n",
        "                for i in range(num_val_batch):\n",
        "                    start = i * cfg.batch_size\n",
        "                    end = start + cfg.batch_size\n",
        "                    acc, loss = sess.run([model.accuracy,model.total_loss], {model.X: valX[start:end], model.labels: valY[start:end]})\n",
        "                    val_acc += acc\n",
        "                    val_loss  += loss \n",
        "\n",
        "                val_acc = val_acc / (cfg.batch_size * num_val_batch)\n",
        "                val_loss = val_loss / (cfg.batch_size * num_val_batch)\n",
        "\n",
        "                if (val_loss < best_val_loss):\n",
        "                  best_val_loss = val_loss\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"Val_Loss: \"+ str(val_loss)+ \", \" +\" Val_acc: \"+ str(val_acc) + \" Improved\"+'\\n')\n",
        "                  fd_val_acc.write(str(global_step) + ',' + str(val_acc) + '\\n')\n",
        "                  fd_val_acc.flush()\n",
        "                  supervisor.saver.save(sess, cfg.logdir + '/model_epoch_%04d_step_%02d' % (epoch, global_step))\n",
        "                else:\n",
        "                  print(\"\\nNOT SAVING MODEL!!\")\n",
        "                  print(\"Global Step: \"+str(global_step) + ',' + \"val_loss: \"+ str(val_loss) +'\\n')\n",
        "\n",
        "        fd_val_acc.close()\n",
        "        fd_train_acc.close()\n",
        "        fd_loss.close()\n",
        "\n",
        "\n",
        "def evaluation(model, supervisor, num_label):\n",
        "    teX, teY, num_te_batch = load_data(cfg.batch_size, is_training=False)\n",
        "    fd_test_acc = save_to()\n",
        "    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n",
        "        tf.logging.info('Model restored!')\n",
        "\n",
        "        test_acc = 0\n",
        "        for i in range(num_te_batch):\n",
        "            start = i * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n",
        "            test_acc += acc\n",
        "        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n",
        "        fd_test_acc.write(str(test_acc))\n",
        "        fd_test_acc.close()\n",
        "        print('\\nTest Accuracy is {}:'.format(test_acc))\n",
        "        print('\\nTest accuracy has been saved to ' + cfg.results + '/test_acc')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKoYiZzjaohA",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwTLpHcJ0Q9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf8f967f-59fb-420f-e308-375d60dd8049"
      },
      "source": [
        "cfg.is_training=True\n",
        "\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:47.189853 140676699535232 <ipython-input-10-fda5935e9628>:2]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.425192 140676699535232 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:54: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.720556 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.728111 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.732151 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.737655 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.742253 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.759216 140676699535232 deprecation.py:323] From <ipython-input-4-cfce60f6a6b7>:59: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:47.780719 140676699535232 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I1104 08:07:48.477559 140676699535232 utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:48.874253 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:49.154112 140676699535232 deprecation.py:323] From <ipython-input-6-4752a8a30df1>:77: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:49.329081 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:50.263278 140676699535232 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:50.265867 140676699535232 <ipython-input-10-fda5935e9628>:5]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-fda5935e9628>:9: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:07:50.268450 140676699535232 deprecation.py:323] From <ipython-input-10-fda5935e9628>:9: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:50.668924 140676699535232 <ipython-input-10-fda5935e9628>:12]  Start training...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:51.800993 140676699535232 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:07:51.832546 140676699535232 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:08:28.749619 140676699535232 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:08:29.413377 140676699535232 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Note: all of results will be saved to directory: results\n",
            "Training for epoch 0/15:\n",
            "INFO:tensorflow:global_step/sec: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:08:29.646210 140673493931776 supervisor.py:1099] global_step/sec: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:08:36.371082 140673502324480 supervisor.py:1050] Recording summary at step 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 0,loss: 0.6633157\n",
            "\n",
            "Global step: 1,loss: 0.6502937\n",
            "\n",
            "Global step: 2,loss: 0.70932317\n",
            "\n",
            "Global step: 3,loss: 0.6236581\n",
            "\n",
            "Global step: 4,loss: 0.6362325\n",
            "\n",
            "Global step: 5,loss: 0.6482961\n",
            "\n",
            "Global step: 6,loss: 0.63384527\n",
            "\n",
            "Global step: 7,loss: 0.626844\n",
            "\n",
            "Global step: 8,loss: 0.6056714\n",
            "\n",
            "Global step: 9,loss: 0.5768939\n",
            "\n",
            "Global step: 10,loss: 0.54354\n",
            "\n",
            "Global step: 11,loss: 0.53478193\n",
            "\n",
            "Global step: 12,loss: 0.5081878\n",
            "\n",
            "Global step: 13,loss: 0.47205466\n",
            "\n",
            "Global step: 14,loss: 0.44202077\n",
            "\n",
            "Global step: 15,loss: 0.4195737\n",
            "\n",
            "Global step: 16,loss: 0.38075894\n",
            "\n",
            "Global step: 17,loss: 0.36274529\n",
            "\n",
            "Global step: 18,loss: 0.33431786\n",
            "\n",
            "Global step: 19,loss: 0.30162978\n",
            "\n",
            "Global step: 20,loss: 0.30422452\n",
            "\n",
            "Global step: 21,loss: 0.28570542\n",
            "\n",
            "Global step: 22,loss: 0.26904416\n",
            "\n",
            "Global step: 23,loss: 0.25831407\n",
            "\n",
            "Global step: 24,loss: 0.24147995\n",
            "\n",
            "Global step: 25,loss: 0.23173963\n",
            "\n",
            "Global step: 26,loss: 0.2104584\n",
            "\n",
            "Global step: 27,loss: 0.21239386\n",
            "\n",
            "Global step: 28,loss: 0.21532753\n",
            "\n",
            "Global step: 29,loss: 0.19613798\n",
            "\n",
            "Global step: 30,loss: 0.20106326\n",
            "\n",
            "Global step: 31,loss: 0.19031174\n",
            "\n",
            "Global step: 32,loss: 0.18324466\n",
            "\n",
            "Global step: 33,loss: 0.17773727\n",
            "\n",
            "Global step: 34,loss: 0.17537229\n",
            "\n",
            "Global step: 35,loss: 0.1689767\n",
            "\n",
            "Global step: 36,loss: 0.16880941\n",
            "\n",
            "Global step: 37,loss: 0.1665058\n",
            "\n",
            "Global step: 38,loss: 0.16007914\n",
            "\n",
            "Global step: 39,loss: 0.15889162\n",
            "\n",
            "Global step: 40,loss: 0.1576207\n",
            "\n",
            "Global step: 41,loss: 0.16086647\n",
            "\n",
            "Global step: 42,loss: 0.14764826\n",
            "\n",
            "Global step: 43,loss: 0.14256212\n",
            "\n",
            "Global step: 44,loss: 0.12952828\n",
            "\n",
            "Global step: 45,loss: 0.13335975\n",
            "\n",
            "Global step: 46,loss: 0.123966604\n",
            "\n",
            "Global step: 47,loss: 0.12018614\n",
            "\n",
            "Global step: 48,loss: 0.1271508\n",
            "\n",
            "Global step: 49,loss: 0.121536046\n",
            "\n",
            "Global step: 50,loss: 0.10915915\n",
            "\n",
            "Global step: 51,loss: 0.10264512\n",
            "\n",
            "Global step: 52,loss: 0.11635816\n",
            "\n",
            "Global step: 53,loss: 0.11787731\n",
            "\n",
            "Global step: 54,loss: 0.109735444\n",
            "\n",
            "Global step: 55,loss: 0.10603967\n",
            "\n",
            "Global step: 56,loss: 0.11302531\n",
            "\n",
            "Global step: 57,loss: 0.09788506\n",
            "\n",
            "Global step: 58,loss: 0.103754535\n",
            "\n",
            "Global step: 59,loss: 0.108275674\n",
            "\n",
            "Global step: 60,loss: 0.09817908\n",
            "\n",
            "Global step: 61,loss: 0.11473183\n",
            "\n",
            "Global step: 62,loss: 0.10781936\n",
            "\n",
            "Global step: 63,loss: 0.09275226\n",
            "\n",
            "Global step: 64,loss: 0.10078491\n",
            "\n",
            "Global step: 65,loss: 0.0888944\n",
            "\n",
            "Global step: 66,loss: 0.088802844\n",
            "\n",
            "Global step: 67,loss: 0.08221251\n",
            "\n",
            "Global step: 68,loss: 0.08633602\n",
            "\n",
            "Global step: 69,loss: 0.075849\n",
            "\n",
            "Global step: 70,loss: 0.08442052\n",
            "\n",
            "Global step: 71,loss: 0.08800612\n",
            "\n",
            "Global step: 72,loss: 0.08546372\n",
            "\n",
            "Global step: 73,loss: 0.07409886\n",
            "\n",
            "Global step: 74,loss: 0.08667364\n",
            "\n",
            "Global step: 75,loss: 0.07245796\n",
            "\n",
            "Global step: 76,loss: 0.07232721\n",
            "\n",
            "Global step: 77,loss: 0.072583884\n",
            "\n",
            "Global step: 78,loss: 0.062747784\n",
            "\n",
            "Global step: 79,loss: 0.076254815\n",
            "\n",
            "Global step: 80,loss: 0.070014715\n",
            "\n",
            "Global step: 81,loss: 0.060093246\n",
            "\n",
            "Global step: 82,loss: 0.06124103\n",
            "\n",
            "Global step: 83,loss: 0.070962615\n",
            "\n",
            "Global step: 84,loss: 0.07396329\n",
            "\n",
            "Global step: 85,loss: 0.060505256\n",
            "\n",
            "Global step: 86,loss: 0.0619763\n",
            "\n",
            "Global step: 87,loss: 0.07068665\n",
            "\n",
            "Global step: 88,loss: 0.07486676\n",
            "\n",
            "Global step: 89,loss: 0.06495276\n",
            "\n",
            "Global step: 90,loss: 0.05902499\n",
            "\n",
            "Global step: 91,loss: 0.05942267\n",
            "\n",
            "Global step: 92,loss: 0.0620531\n",
            "\n",
            "Global step: 93,loss: 0.068900205\n",
            "\n",
            "Global step: 94,loss: 0.06752156\n",
            "\n",
            "Global step: 95,loss: 0.051450096\n",
            "\n",
            "Global step: 96,loss: 0.060757328\n",
            "\n",
            "Global step: 97,loss: 0.06110093\n",
            "\n",
            "Global step: 98,loss: 0.05526474\n",
            "\n",
            "Global step: 99,loss: 0.06165642\n",
            "\n",
            "Global step: 100,loss: 0.054487683\n",
            "\n",
            "Global step: 101,loss: 0.05641684\n",
            "\n",
            "Global step: 102,loss: 0.051363245\n",
            "\n",
            "Global step: 103,loss: 0.0557777\n",
            "\n",
            "Global step: 104,loss: 0.05720957\n",
            "\n",
            "Global step: 105,loss: 0.060115024\n",
            "\n",
            "Global step: 106,loss: 0.06358534\n",
            "\n",
            "Global step: 107,loss: 0.056100518\n",
            "\n",
            "Global step: 108,loss: 0.053338673\n",
            "\n",
            "Global step: 109,loss: 0.048885144\n",
            "\n",
            "Global step: 110,loss: 0.050870582\n",
            "\n",
            "Global step: 111,loss: 0.055300042\n",
            "\n",
            "Global step: 112,loss: 0.046536185\n",
            "\n",
            "Global step: 113,loss: 0.05451632\n",
            "\n",
            "Global step: 114,loss: 0.050775014\n",
            "\n",
            "Global step: 115,loss: 0.041341368\n",
            "\n",
            "Global step: 116,loss: 0.041984513\n",
            "\n",
            "Global step: 117,loss: 0.048972037\n",
            "\n",
            "Global step: 118,loss: 0.05700506\n",
            "\n",
            "Global step: 119,loss: 0.04355066\n",
            "\n",
            "Global step: 120,loss: 0.047662668\n",
            "\n",
            "Global step: 121,loss: 0.04622646\n",
            "\n",
            "Global step: 122,loss: 0.050691515\n",
            "\n",
            "Global step: 123,loss: 0.048723422\n",
            "\n",
            "Global step: 124,loss: 0.049757235\n",
            "\n",
            "Global step: 125,loss: 0.048375472\n",
            "\n",
            "Global step: 126,loss: 0.053512186\n",
            "\n",
            "Global step: 127,loss: 0.05048357\n",
            "\n",
            "Global step: 128,loss: 0.04893653\n",
            "\n",
            "Global step: 129,loss: 0.03929895\n",
            "\n",
            "Global step: 130,loss: 0.043555062\n",
            "\n",
            "Global step: 131,loss: 0.049257718\n",
            "\n",
            "Global step: 132,loss: 0.0614597\n",
            "\n",
            "Global step: 133,loss: 0.043391995\n",
            "\n",
            "Global step: 134,loss: 0.0563818\n",
            "\n",
            "Global step: 135,loss: 0.047695115\n",
            "\n",
            "Global step: 136,loss: 0.047112964\n",
            "\n",
            "Global step: 137,loss: 0.04381311\n",
            "\n",
            "Global step: 138,loss: 0.04273861\n",
            "\n",
            "Global step: 139,loss: 0.050779067\n",
            "\n",
            "Global step: 140,loss: 0.047728103\n",
            "\n",
            "Global step: 141,loss: 0.04453656\n",
            "\n",
            "Global step: 142,loss: 0.050349332\n",
            "\n",
            "Global step: 143,loss: 0.038536828\n",
            "\n",
            "Global step: 144,loss: 0.041547257\n",
            "\n",
            "Global step: 145,loss: 0.03882733\n",
            "\n",
            "Global step: 146,loss: 0.042252205\n",
            "\n",
            "Global step: 147,loss: 0.03785105\n",
            "\n",
            "Global step: 148,loss: 0.042360593\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.24976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:10:29.669174 140673493931776 supervisor.py:1099] global_step/sec: 1.24976\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 149,loss: 0.03675359\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 150.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:10:30.065403 140673502324480 supervisor.py:1050] Recording summary at step 150.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 150,loss: 0.042707272\n",
            "\n",
            "Global step: 151,loss: 0.042791076\n",
            "\n",
            "Global step: 152,loss: 0.047049947\n",
            "\n",
            "Global step: 153,loss: 0.036942143\n",
            "\n",
            "Global step: 154,loss: 0.038993664\n",
            "\n",
            "Global step: 155,loss: 0.032691285\n",
            "\n",
            "Global step: 156,loss: 0.04350749\n",
            "\n",
            "Global step: 157,loss: 0.03957154\n",
            "\n",
            "Global step: 158,loss: 0.04385239\n",
            "\n",
            "Global step: 159,loss: 0.039540533\n",
            "\n",
            "Global step: 160,loss: 0.040916637\n",
            "\n",
            "Global step: 161,loss: 0.03880751\n",
            "\n",
            "Global step: 162,loss: 0.033137515\n",
            "\n",
            "Global step: 163,loss: 0.034006234\n",
            "\n",
            "Global step: 164,loss: 0.041514833\n",
            "\n",
            "Global step: 165,loss: 0.039373867\n",
            "\n",
            "Global step: 166,loss: 0.03917691\n",
            "\n",
            "Global step: 167,loss: 0.030219132\n",
            "\n",
            "Global step: 168,loss: 0.04578864\n",
            "\n",
            "Global step: 169,loss: 0.034655787\n",
            "\n",
            "Global step: 170,loss: 0.039203916\n",
            "\n",
            "Global step: 171,loss: 0.033127684\n",
            "\n",
            "Global step: 172,loss: 0.03743379\n",
            "\n",
            "Global step: 173,loss: 0.040817067\n",
            "\n",
            "Global step: 174,loss: 0.040708475\n",
            "\n",
            "Global step: 175,loss: 0.03389761\n",
            "\n",
            "Global step: 176,loss: 0.040265016\n",
            "\n",
            "Global step: 177,loss: 0.03816724\n",
            "\n",
            "Global step: 178,loss: 0.040395834\n",
            "\n",
            "Global step: 179,loss: 0.036088787\n",
            "\n",
            "Global step: 180,loss: 0.040753894\n",
            "\n",
            "Global step: 181,loss: 0.037357062\n",
            "\n",
            "Global step: 182,loss: 0.034768663\n",
            "\n",
            "Global step: 183,loss: 0.034324817\n",
            "\n",
            "Global step: 184,loss: 0.041182697\n",
            "\n",
            "Global step: 185,loss: 0.036366854\n",
            "\n",
            "Global step: 186,loss: 0.040555745\n",
            "\n",
            "Global step: 187,loss: 0.034985777\n",
            "\n",
            "Global step: 188,loss: 0.042333566\n",
            "\n",
            "Global step: 189,loss: 0.032392316\n",
            "\n",
            "Global step: 190,loss: 0.030040829\n",
            "\n",
            "Global step: 191,loss: 0.034756623\n",
            "\n",
            "Global step: 192,loss: 0.044553787\n",
            "\n",
            "Global step: 193,loss: 0.032146767\n",
            "\n",
            "Global step: 194,loss: 0.036042195\n",
            "\n",
            "Global step: 195,loss: 0.03307931\n",
            "\n",
            "Global step: 196,loss: 0.03007736\n",
            "\n",
            "Global step: 197,loss: 0.03626448\n",
            "\n",
            "Global step: 198,loss: 0.03585306\n",
            "\n",
            "Global step: 199,loss: 0.04058806\n",
            "\n",
            "Global step: 200,loss: 0.04069513\n",
            "\n",
            "Global step: 201,loss: 0.03887321\n",
            "\n",
            "Global step: 202,loss: 0.037537124\n",
            "\n",
            "Global step: 203,loss: 0.032618307\n",
            "\n",
            "Global step: 204,loss: 0.031520084\n",
            "\n",
            "Global step: 205,loss: 0.032939903\n",
            "\n",
            "Global step: 206,loss: 0.032237124\n",
            "\n",
            "Global step: 207,loss: 0.03663863\n",
            "\n",
            "Global step: 208,loss: 0.036973096\n",
            "\n",
            "Global step: 209,loss: 0.041262135\n",
            "\n",
            "Global step: 210,loss: 0.033141308\n",
            "\n",
            "Global step: 211,loss: 0.028647177\n",
            "\n",
            "Global step: 212,loss: 0.03739718\n",
            "\n",
            "Global step: 213,loss: 0.02913053\n",
            "\n",
            "Global Step: 213,Val_Loss: 0.00011033421929153662,  Val_acc: 0.9942434210526315 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:11:25.453466 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 1/15:\n",
            "Global step: 214,loss: 0.033555612\n",
            "\n",
            "Global step: 215,loss: 0.034307882\n",
            "\n",
            "Global step: 216,loss: 0.037433736\n",
            "\n",
            "Global step: 217,loss: 0.025885895\n",
            "\n",
            "Global step: 218,loss: 0.0310569\n",
            "\n",
            "Global step: 219,loss: 0.033860717\n",
            "\n",
            "Global step: 220,loss: 0.029182397\n",
            "\n",
            "Global step: 221,loss: 0.037452325\n",
            "\n",
            "Global step: 222,loss: 0.030296102\n",
            "\n",
            "Global step: 223,loss: 0.031246709\n",
            "\n",
            "Global step: 224,loss: 0.029588614\n",
            "\n",
            "Global step: 225,loss: 0.031118456\n",
            "\n",
            "Global step: 226,loss: 0.036161467\n",
            "\n",
            "Global step: 227,loss: 0.032472476\n",
            "\n",
            "Global step: 228,loss: 0.030546993\n",
            "\n",
            "Global step: 229,loss: 0.029281128\n",
            "\n",
            "Global step: 230,loss: 0.029850703\n",
            "\n",
            "Global step: 231,loss: 0.037595514\n",
            "\n",
            "Global step: 232,loss: 0.025678298\n",
            "\n",
            "Global step: 233,loss: 0.026824731\n",
            "\n",
            "Global step: 234,loss: 0.039043196\n",
            "\n",
            "Global step: 235,loss: 0.031032233\n",
            "\n",
            "Global step: 236,loss: 0.027125003\n",
            "\n",
            "Global step: 237,loss: 0.04086584\n",
            "\n",
            "Global step: 238,loss: 0.03311176\n",
            "\n",
            "Global step: 239,loss: 0.035841655\n",
            "\n",
            "Global step: 240,loss: 0.031088337\n",
            "\n",
            "Global step: 241,loss: 0.040935542\n",
            "\n",
            "Global step: 242,loss: 0.032754406\n",
            "\n",
            "Global step: 243,loss: 0.039814185\n",
            "\n",
            "Global step: 244,loss: 0.033704974\n",
            "\n",
            "Global step: 245,loss: 0.03065693\n",
            "\n",
            "Global step: 246,loss: 0.025621662\n",
            "\n",
            "Global step: 247,loss: 0.025224585\n",
            "\n",
            "Global step: 248,loss: 0.032708287\n",
            "\n",
            "Global step: 249,loss: 0.02808368\n",
            "\n",
            "Global step: 250,loss: 0.023343004\n",
            "\n",
            "Global step: 251,loss: 0.028881999\n",
            "\n",
            "Global step: 252,loss: 0.02824299\n",
            "\n",
            "Global step: 253,loss: 0.029896948\n",
            "\n",
            "Global step: 254,loss: 0.030941453\n",
            "\n",
            "Global step: 255,loss: 0.029988233\n",
            "\n",
            "Global step: 256,loss: 0.033849005\n",
            "\n",
            "Global step: 257,loss: 0.031395927\n",
            "\n",
            "Global step: 258,loss: 0.030254206\n",
            "\n",
            "Global step: 259,loss: 0.029599402\n",
            "\n",
            "Global step: 260,loss: 0.036017187\n",
            "\n",
            "Global step: 261,loss: 0.030754983\n",
            "\n",
            "Global step: 262,loss: 0.029202994\n",
            "\n",
            "Global step: 263,loss: 0.037294768\n",
            "\n",
            "Global step: 264,loss: 0.029852225\n",
            "\n",
            "Global step: 265,loss: 0.0329145\n",
            "\n",
            "Global step: 266,loss: 0.033249684\n",
            "\n",
            "Global step: 267,loss: 0.02796318\n",
            "\n",
            "Global step: 268,loss: 0.033378653\n",
            "\n",
            "Global step: 269,loss: 0.030228274\n",
            "\n",
            "Global step: 270,loss: 0.026602693\n",
            "\n",
            "Global step: 271,loss: 0.02379362\n",
            "\n",
            "Global step: 272,loss: 0.024742486\n",
            "\n",
            "Global step: 273,loss: 0.028802339\n",
            "\n",
            "Global step: 274,loss: 0.027528524\n",
            "\n",
            "Global step: 275,loss: 0.030845301\n",
            "\n",
            "Global step: 276,loss: 0.03213083\n",
            "\n",
            "Global step: 277,loss: 0.028532956\n",
            "\n",
            "Global step: 278,loss: 0.03131852\n",
            "\n",
            "Global step: 279,loss: 0.030636095\n",
            "\n",
            "Global step: 280,loss: 0.030558892\n",
            "\n",
            "Global step: 281,loss: 0.029321445\n",
            "\n",
            "Global step: 282,loss: 0.029832931\n",
            "\n",
            "Global step: 283,loss: 0.026022134\n",
            "\n",
            "Global step: 284,loss: 0.0325643\n",
            "\n",
            "Global step: 285,loss: 0.026300665\n",
            "\n",
            "Global step: 286,loss: 0.026043905\n",
            "\n",
            "Global step: 287,loss: 0.036535364\n",
            "\n",
            "Global step: 288,loss: 0.027435219\n",
            "\n",
            "Global step: 289,loss: 0.03201922\n",
            "\n",
            "Global step: 290,loss: 0.028088007\n",
            "\n",
            "Global step: 291,loss: 0.02362011\n",
            "\n",
            "Global step: 292,loss: 0.036042485\n",
            "\n",
            "Global step: 293,loss: 0.027903683\n",
            "\n",
            "Global step: 294,loss: 0.02974024\n",
            "\n",
            "Global step: 295,loss: 0.041546315\n",
            "\n",
            "Global step: 296,loss: 0.023267651\n",
            "\n",
            "Global step: 297,loss: 0.026601586\n",
            "\n",
            "Global step: 298,loss: 0.032616388\n",
            "\n",
            "Global step: 299,loss: 0.026922576\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.25039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:12:29.631608 140673493931776 supervisor.py:1099] global_step/sec: 1.25039\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 300,loss: 0.028433729\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 301.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:12:30.358217 140673502324480 supervisor.py:1050] Recording summary at step 301.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 301,loss: 0.026742717\n",
            "\n",
            "Global step: 302,loss: 0.030884936\n",
            "\n",
            "Global step: 303,loss: 0.026045697\n",
            "\n",
            "Global step: 304,loss: 0.026053097\n",
            "\n",
            "Global step: 305,loss: 0.025523454\n",
            "\n",
            "Global step: 306,loss: 0.024412187\n",
            "\n",
            "Global step: 307,loss: 0.030654335\n",
            "\n",
            "Global step: 308,loss: 0.03236728\n",
            "\n",
            "Global step: 309,loss: 0.022876844\n",
            "\n",
            "Global step: 310,loss: 0.027647717\n",
            "\n",
            "Global step: 311,loss: 0.03085084\n",
            "\n",
            "Global step: 312,loss: 0.025655612\n",
            "\n",
            "Global step: 313,loss: 0.02798421\n",
            "\n",
            "Global step: 314,loss: 0.030959558\n",
            "\n",
            "Global step: 315,loss: 0.026709605\n",
            "\n",
            "Global step: 316,loss: 0.031396672\n",
            "\n",
            "Global step: 317,loss: 0.028203767\n",
            "\n",
            "Global step: 318,loss: 0.032535035\n",
            "\n",
            "Global step: 319,loss: 0.029023536\n",
            "\n",
            "Global step: 320,loss: 0.024891013\n",
            "\n",
            "Global step: 321,loss: 0.027868155\n",
            "\n",
            "Global step: 322,loss: 0.023578301\n",
            "\n",
            "Global step: 323,loss: 0.025835492\n",
            "\n",
            "Global step: 324,loss: 0.034126677\n",
            "\n",
            "Global step: 325,loss: 0.026553983\n",
            "\n",
            "Global step: 326,loss: 0.03039585\n",
            "\n",
            "Global step: 327,loss: 0.030510878\n",
            "\n",
            "Global step: 328,loss: 0.02694799\n",
            "\n",
            "Global step: 329,loss: 0.028530568\n",
            "\n",
            "Global step: 330,loss: 0.028135426\n",
            "\n",
            "Global step: 331,loss: 0.022945318\n",
            "\n",
            "Global step: 332,loss: 0.02324137\n",
            "\n",
            "Global step: 333,loss: 0.025054704\n",
            "\n",
            "Global step: 334,loss: 0.024034293\n",
            "\n",
            "Global step: 335,loss: 0.030004162\n",
            "\n",
            "Global step: 336,loss: 0.02927485\n",
            "\n",
            "Global step: 337,loss: 0.029937867\n",
            "\n",
            "Global step: 338,loss: 0.023248574\n",
            "\n",
            "Global step: 339,loss: 0.023925073\n",
            "\n",
            "Global step: 340,loss: 0.03005895\n",
            "\n",
            "Global step: 341,loss: 0.025567643\n",
            "\n",
            "Global step: 342,loss: 0.028352646\n",
            "\n",
            "Global step: 343,loss: 0.027307333\n",
            "\n",
            "Global step: 344,loss: 0.028953921\n",
            "\n",
            "Global step: 345,loss: 0.037796885\n",
            "\n",
            "Global step: 346,loss: 0.025527284\n",
            "\n",
            "Global step: 347,loss: 0.025338095\n",
            "\n",
            "Global step: 348,loss: 0.028869184\n",
            "\n",
            "Global step: 349,loss: 0.026911572\n",
            "\n",
            "Global step: 350,loss: 0.023361709\n",
            "\n",
            "Global step: 351,loss: 0.02646678\n",
            "\n",
            "Global step: 352,loss: 0.02454408\n",
            "\n",
            "Global step: 353,loss: 0.031257644\n",
            "\n",
            "Global step: 354,loss: 0.03142728\n",
            "\n",
            "Global step: 355,loss: 0.03148504\n",
            "\n",
            "Global step: 356,loss: 0.025017954\n",
            "\n",
            "Global step: 357,loss: 0.024817605\n",
            "\n",
            "Global step: 358,loss: 0.02522891\n",
            "\n",
            "Global step: 359,loss: 0.02609862\n",
            "\n",
            "Global step: 360,loss: 0.022226186\n",
            "\n",
            "Global step: 361,loss: 0.029441709\n",
            "\n",
            "Global step: 362,loss: 0.025736332\n",
            "\n",
            "Global step: 363,loss: 0.025896594\n",
            "\n",
            "Global step: 364,loss: 0.02979201\n",
            "\n",
            "Global step: 365,loss: 0.027417522\n",
            "\n",
            "Global step: 366,loss: 0.027287595\n",
            "\n",
            "Global step: 367,loss: 0.031781554\n",
            "\n",
            "Global step: 368,loss: 0.023481756\n",
            "\n",
            "Global step: 369,loss: 0.025224682\n",
            "\n",
            "Global step: 370,loss: 0.021527868\n",
            "\n",
            "Global step: 371,loss: 0.02512924\n",
            "\n",
            "Global step: 372,loss: 0.02985975\n",
            "\n",
            "Global step: 373,loss: 0.027193904\n",
            "\n",
            "Global step: 374,loss: 0.02280955\n",
            "\n",
            "Global step: 375,loss: 0.027196579\n",
            "\n",
            "Global step: 376,loss: 0.025651157\n",
            "\n",
            "Global step: 377,loss: 0.025861394\n",
            "\n",
            "Global step: 378,loss: 0.02792577\n",
            "\n",
            "Global step: 379,loss: 0.024162311\n",
            "\n",
            "Global step: 380,loss: 0.025111264\n",
            "\n",
            "Global step: 381,loss: 0.026789922\n",
            "\n",
            "Global step: 382,loss: 0.025402162\n",
            "\n",
            "Global step: 383,loss: 0.020953014\n",
            "\n",
            "Global step: 384,loss: 0.027938519\n",
            "\n",
            "Global step: 385,loss: 0.02153019\n",
            "\n",
            "Global step: 386,loss: 0.022253431\n",
            "\n",
            "Global step: 387,loss: 0.024414718\n",
            "\n",
            "Global step: 388,loss: 0.021244515\n",
            "\n",
            "Global step: 389,loss: 0.021109674\n",
            "\n",
            "Global step: 390,loss: 0.025759708\n",
            "\n",
            "Global step: 391,loss: 0.022671785\n",
            "\n",
            "Global step: 392,loss: 0.031558193\n",
            "\n",
            "Global step: 393,loss: 0.026429838\n",
            "\n",
            "Global step: 394,loss: 0.02325952\n",
            "\n",
            "Global step: 395,loss: 0.023548432\n",
            "\n",
            "Global step: 396,loss: 0.024094284\n",
            "\n",
            "Global step: 397,loss: 0.035654537\n",
            "\n",
            "Global step: 398,loss: 0.02460723\n",
            "\n",
            "Global step: 399,loss: 0.023440499\n",
            "\n",
            "Global step: 400,loss: 0.031536482\n",
            "\n",
            "Global step: 401,loss: 0.0283272\n",
            "\n",
            "Global step: 402,loss: 0.02921554\n",
            "\n",
            "Global step: 403,loss: 0.023219049\n",
            "\n",
            "Global step: 404,loss: 0.030360606\n",
            "\n",
            "Global step: 405,loss: 0.025174696\n",
            "\n",
            "Global step: 406,loss: 0.025638202\n",
            "\n",
            "Global step: 407,loss: 0.024659203\n",
            "\n",
            "Global step: 408,loss: 0.023756538\n",
            "\n",
            "Global step: 409,loss: 0.021069609\n",
            "\n",
            "Global step: 410,loss: 0.022214666\n",
            "\n",
            "Global step: 411,loss: 0.027266186\n",
            "\n",
            "Global step: 412,loss: 0.026771296\n",
            "\n",
            "Global step: 413,loss: 0.023107886\n",
            "\n",
            "Global step: 414,loss: 0.024344299\n",
            "\n",
            "Global step: 415,loss: 0.024108585\n",
            "\n",
            "Global step: 416,loss: 0.02309502\n",
            "\n",
            "Global step: 417,loss: 0.022783045\n",
            "\n",
            "Global step: 418,loss: 0.024926916\n",
            "\n",
            "Global step: 419,loss: 0.02396913\n",
            "\n",
            "Global step: 420,loss: 0.026101604\n",
            "\n",
            "Global step: 421,loss: 0.024682812\n",
            "\n",
            "Global step: 422,loss: 0.023148838\n",
            "\n",
            "Global step: 423,loss: 0.021748865\n",
            "\n",
            "Global step: 424,loss: 0.021061713\n",
            "\n",
            "Global step: 425,loss: 0.02305979\n",
            "\n",
            "Global step: 426,loss: 0.020557158\n",
            "\n",
            "Global step: 427,loss: 0.024659025\n",
            "\n",
            "Global Step: 427,Val_Loss: 8.371015428565443e-05,  Val_acc: 0.99609375 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:14:10.995931 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 2/15:\n",
            "Global step: 428,loss: 0.022348765\n",
            "\n",
            "Global step: 429,loss: 0.026841827\n",
            "\n",
            "Global step: 430,loss: 0.023042738\n",
            "\n",
            "Global step: 431,loss: 0.029088303\n",
            "\n",
            "Global step: 432,loss: 0.02122798\n",
            "\n",
            "Global step: 433,loss: 0.020311723\n",
            "\n",
            "Global step: 434,loss: 0.023049833\n",
            "\n",
            "Global step: 435,loss: 0.019681761\n",
            "\n",
            "Global step: 436,loss: 0.023952745\n",
            "\n",
            "Global step: 437,loss: 0.01953477\n",
            "\n",
            "Global step: 438,loss: 0.021857515\n",
            "\n",
            "Global step: 439,loss: 0.026445948\n",
            "\n",
            "Global step: 440,loss: 0.020557482\n",
            "\n",
            "Global step: 441,loss: 0.026208322\n",
            "\n",
            "Global step: 442,loss: 0.024561366\n",
            "\n",
            "Global step: 443,loss: 0.019058177\n",
            "\n",
            "Global step: 444,loss: 0.023072958\n",
            "\n",
            "Global step: 445,loss: 0.020436615\n",
            "\n",
            "Global step: 446,loss: 0.023787554\n",
            "\n",
            "Global step: 447,loss: 0.032531776\n",
            "\n",
            "Global step: 448,loss: 0.019587046\n",
            "\n",
            "Global step: 449,loss: 0.028946735\n",
            "\n",
            "Global step: 450,loss: 0.022110429\n",
            "\n",
            "Global step: 451,loss: 0.031790994\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.26875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:14:29.434671 140673493931776 supervisor.py:1099] global_step/sec: 1.26875\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 452,loss: 0.025174584\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 453.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:14:30.422653 140673502324480 supervisor.py:1050] Recording summary at step 453.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 453,loss: 0.02633974\n",
            "\n",
            "Global step: 454,loss: 0.023322858\n",
            "\n",
            "Global step: 455,loss: 0.02322144\n",
            "\n",
            "Global step: 456,loss: 0.018193431\n",
            "\n",
            "Global step: 457,loss: 0.028207596\n",
            "\n",
            "Global step: 458,loss: 0.02423346\n",
            "\n",
            "Global step: 459,loss: 0.022982027\n",
            "\n",
            "Global step: 460,loss: 0.025404658\n",
            "\n",
            "Global step: 461,loss: 0.019305537\n",
            "\n",
            "Global step: 462,loss: 0.017758308\n",
            "\n",
            "Global step: 463,loss: 0.021338493\n",
            "\n",
            "Global step: 464,loss: 0.020394811\n",
            "\n",
            "Global step: 465,loss: 0.01762845\n",
            "\n",
            "Global step: 466,loss: 0.023998786\n",
            "\n",
            "Global step: 467,loss: 0.02145807\n",
            "\n",
            "Global step: 468,loss: 0.021735895\n",
            "\n",
            "Global step: 469,loss: 0.021849284\n",
            "\n",
            "Global step: 470,loss: 0.026144002\n",
            "\n",
            "Global step: 471,loss: 0.021199396\n",
            "\n",
            "Global step: 472,loss: 0.01726219\n",
            "\n",
            "Global step: 473,loss: 0.02157093\n",
            "\n",
            "Global step: 474,loss: 0.023317102\n",
            "\n",
            "Global step: 475,loss: 0.024603609\n",
            "\n",
            "Global step: 476,loss: 0.022948906\n",
            "\n",
            "Global step: 477,loss: 0.026736738\n",
            "\n",
            "Global step: 478,loss: 0.023080893\n",
            "\n",
            "Global step: 479,loss: 0.022932254\n",
            "\n",
            "Global step: 480,loss: 0.018722596\n",
            "\n",
            "Global step: 481,loss: 0.026116654\n",
            "\n",
            "Global step: 482,loss: 0.023120865\n",
            "\n",
            "Global step: 483,loss: 0.020241996\n",
            "\n",
            "Global step: 484,loss: 0.019638237\n",
            "\n",
            "Global step: 485,loss: 0.020350445\n",
            "\n",
            "Global step: 486,loss: 0.022802707\n",
            "\n",
            "Global step: 487,loss: 0.022815317\n",
            "\n",
            "Global step: 488,loss: 0.027161174\n",
            "\n",
            "Global step: 489,loss: 0.02684113\n",
            "\n",
            "Global step: 490,loss: 0.020745516\n",
            "\n",
            "Global step: 491,loss: 0.0251587\n",
            "\n",
            "Global step: 492,loss: 0.023608021\n",
            "\n",
            "Global step: 493,loss: 0.023250721\n",
            "\n",
            "Global step: 494,loss: 0.03144701\n",
            "\n",
            "Global step: 495,loss: 0.025021609\n",
            "\n",
            "Global step: 496,loss: 0.021920133\n",
            "\n",
            "Global step: 497,loss: 0.026261088\n",
            "\n",
            "Global step: 498,loss: 0.020000385\n",
            "\n",
            "Global step: 499,loss: 0.021890352\n",
            "\n",
            "Global step: 500,loss: 0.021061162\n",
            "\n",
            "Global step: 501,loss: 0.018377556\n",
            "\n",
            "Global step: 502,loss: 0.017764742\n",
            "\n",
            "Global step: 503,loss: 0.023675635\n",
            "\n",
            "Global step: 504,loss: 0.022212142\n",
            "\n",
            "Global step: 505,loss: 0.02529453\n",
            "\n",
            "Global step: 506,loss: 0.020027632\n",
            "\n",
            "Global step: 507,loss: 0.022210777\n",
            "\n",
            "Global step: 508,loss: 0.02658387\n",
            "\n",
            "Global step: 509,loss: 0.020227246\n",
            "\n",
            "Global step: 510,loss: 0.020849835\n",
            "\n",
            "Global step: 511,loss: 0.019231344\n",
            "\n",
            "Global step: 512,loss: 0.021373913\n",
            "\n",
            "Global step: 513,loss: 0.02328746\n",
            "\n",
            "Global step: 514,loss: 0.018117363\n",
            "\n",
            "Global step: 515,loss: 0.023695007\n",
            "\n",
            "Global step: 516,loss: 0.021320077\n",
            "\n",
            "Global step: 517,loss: 0.026110968\n",
            "\n",
            "Global step: 518,loss: 0.023758965\n",
            "\n",
            "Global step: 519,loss: 0.020112334\n",
            "\n",
            "Global step: 520,loss: 0.02688067\n",
            "\n",
            "Global step: 521,loss: 0.025426582\n",
            "\n",
            "Global step: 522,loss: 0.024839828\n",
            "\n",
            "Global step: 523,loss: 0.020988408\n",
            "\n",
            "Global step: 524,loss: 0.018863114\n",
            "\n",
            "Global step: 525,loss: 0.020235263\n",
            "\n",
            "Global step: 526,loss: 0.021144623\n",
            "\n",
            "Global step: 527,loss: 0.026500087\n",
            "\n",
            "Global step: 528,loss: 0.020249667\n",
            "\n",
            "Global step: 529,loss: 0.019753274\n",
            "\n",
            "Global step: 530,loss: 0.022575784\n",
            "\n",
            "Global step: 531,loss: 0.027146813\n",
            "\n",
            "Global step: 532,loss: 0.027566811\n",
            "\n",
            "Global step: 533,loss: 0.022091186\n",
            "\n",
            "Global step: 534,loss: 0.025132611\n",
            "\n",
            "Global step: 535,loss: 0.022806805\n",
            "\n",
            "Global step: 536,loss: 0.018805904\n",
            "\n",
            "Global step: 537,loss: 0.020478416\n",
            "\n",
            "Global step: 538,loss: 0.024728708\n",
            "\n",
            "Global step: 539,loss: 0.027825568\n",
            "\n",
            "Global step: 540,loss: 0.022079786\n",
            "\n",
            "Global step: 541,loss: 0.019588875\n",
            "\n",
            "Global step: 542,loss: 0.023361854\n",
            "\n",
            "Global step: 543,loss: 0.02135623\n",
            "\n",
            "Global step: 544,loss: 0.022372894\n",
            "\n",
            "Global step: 545,loss: 0.021847483\n",
            "\n",
            "Global step: 546,loss: 0.02221883\n",
            "\n",
            "Global step: 547,loss: 0.025326695\n",
            "\n",
            "Global step: 548,loss: 0.020101033\n",
            "\n",
            "Global step: 549,loss: 0.017953884\n",
            "\n",
            "Global step: 550,loss: 0.01906058\n",
            "\n",
            "Global step: 551,loss: 0.020096622\n",
            "\n",
            "Global step: 552,loss: 0.01913302\n",
            "\n",
            "Global step: 553,loss: 0.022570698\n",
            "\n",
            "Global step: 554,loss: 0.0205012\n",
            "\n",
            "Global step: 555,loss: 0.020301823\n",
            "\n",
            "Global step: 556,loss: 0.02070678\n",
            "\n",
            "Global step: 557,loss: 0.020796139\n",
            "\n",
            "Global step: 558,loss: 0.01750633\n",
            "\n",
            "Global step: 559,loss: 0.019109473\n",
            "\n",
            "Global step: 560,loss: 0.019734249\n",
            "\n",
            "Global step: 561,loss: 0.021455884\n",
            "\n",
            "Global step: 562,loss: 0.023187142\n",
            "\n",
            "Global step: 563,loss: 0.017530818\n",
            "\n",
            "Global step: 564,loss: 0.020680277\n",
            "\n",
            "Global step: 565,loss: 0.029096652\n",
            "\n",
            "Global step: 566,loss: 0.01861178\n",
            "\n",
            "Global step: 567,loss: 0.023376007\n",
            "\n",
            "Global step: 568,loss: 0.025145277\n",
            "\n",
            "Global step: 569,loss: 0.023871625\n",
            "\n",
            "Global step: 570,loss: 0.019897718\n",
            "\n",
            "Global step: 571,loss: 0.01883919\n",
            "\n",
            "Global step: 572,loss: 0.020076782\n",
            "\n",
            "Global step: 573,loss: 0.023161065\n",
            "\n",
            "Global step: 574,loss: 0.021592513\n",
            "\n",
            "Global step: 575,loss: 0.01837333\n",
            "\n",
            "Global step: 576,loss: 0.019677013\n",
            "\n",
            "Global step: 577,loss: 0.017952718\n",
            "\n",
            "Global step: 578,loss: 0.0252961\n",
            "\n",
            "Global step: 579,loss: 0.021415442\n",
            "\n",
            "Global step: 580,loss: 0.018428713\n",
            "\n",
            "Global step: 581,loss: 0.018712506\n",
            "\n",
            "Global step: 582,loss: 0.024330977\n",
            "\n",
            "Global step: 583,loss: 0.020080222\n",
            "\n",
            "Global step: 584,loss: 0.017981743\n",
            "\n",
            "Global step: 585,loss: 0.022502312\n",
            "\n",
            "Global step: 586,loss: 0.020783965\n",
            "\n",
            "Global step: 587,loss: 0.025199112\n",
            "\n",
            "Global step: 588,loss: 0.017451555\n",
            "\n",
            "Global step: 589,loss: 0.0181478\n",
            "\n",
            "Global step: 590,loss: 0.024012443\n",
            "\n",
            "Global step: 591,loss: 0.021297127\n",
            "\n",
            "Global step: 592,loss: 0.021968868\n",
            "\n",
            "Global step: 593,loss: 0.022045687\n",
            "\n",
            "Global step: 594,loss: 0.018987758\n",
            "\n",
            "Global step: 595,loss: 0.018137336\n",
            "\n",
            "Global step: 596,loss: 0.02006554\n",
            "\n",
            "Global step: 597,loss: 0.018635387\n",
            "\n",
            "Global step: 598,loss: 0.02047367\n",
            "\n",
            "Global step: 599,loss: 0.019125246\n",
            "\n",
            "Global step: 600,loss: 0.025786301\n",
            "\n",
            "Global step: 601,loss: 0.022796817\n",
            "\n",
            "Global step: 602,loss: 0.023343708\n",
            "\n",
            "Global step: 603,loss: 0.026179608\n",
            "\n",
            "Global step: 604,loss: 0.016735487\n",
            "\n",
            "Global step: 605,loss: 0.023126213\n",
            "\n",
            "Global step: 606,loss: 0.019006256\n",
            "\n",
            "Global step: 607,loss: 0.022045694\n",
            "\n",
            "Global step: 608,loss: 0.020201683\n",
            "\n",
            "Global step: 609,loss: 0.017114073\n",
            "\n",
            "Global step: 610,loss: 0.021184307\n",
            "\n",
            "Global step: 611,loss: 0.02315884\n",
            "\n",
            "Global step: 612,loss: 0.021897927\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.34051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:16:29.538594 140673493931776 supervisor.py:1099] global_step/sec: 1.34051\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 613,loss: 0.022944968\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 614.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:16:29.919253 140673502324480 supervisor.py:1050] Recording summary at step 614.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 614,loss: 0.015232541\n",
            "\n",
            "Global step: 615,loss: 0.01952318\n",
            "\n",
            "Global step: 616,loss: 0.015592706\n",
            "\n",
            "Global step: 617,loss: 0.017352473\n",
            "\n",
            "Global step: 618,loss: 0.021686219\n",
            "\n",
            "Global step: 619,loss: 0.023941416\n",
            "\n",
            "Global step: 620,loss: 0.02208805\n",
            "\n",
            "Global step: 621,loss: 0.018576544\n",
            "\n",
            "Global step: 622,loss: 0.020674055\n",
            "\n",
            "Global step: 623,loss: 0.02092519\n",
            "\n",
            "Global step: 624,loss: 0.016722852\n",
            "\n",
            "Global step: 625,loss: 0.017428096\n",
            "\n",
            "Global step: 626,loss: 0.017914299\n",
            "\n",
            "Global step: 627,loss: 0.018519329\n",
            "\n",
            "Global step: 628,loss: 0.02316469\n",
            "\n",
            "Global step: 629,loss: 0.021072483\n",
            "\n",
            "Global step: 630,loss: 0.021511551\n",
            "\n",
            "Global step: 631,loss: 0.017122533\n",
            "\n",
            "Global step: 632,loss: 0.02467218\n",
            "\n",
            "Global step: 633,loss: 0.0226884\n",
            "\n",
            "Global step: 634,loss: 0.016810589\n",
            "\n",
            "Global step: 635,loss: 0.016138012\n",
            "\n",
            "Global step: 636,loss: 0.017448237\n",
            "\n",
            "Global step: 637,loss: 0.02008595\n",
            "\n",
            "Global step: 638,loss: 0.021501655\n",
            "\n",
            "Global step: 639,loss: 0.017249547\n",
            "\n",
            "Global step: 640,loss: 0.01853253\n",
            "\n",
            "Global step: 641,loss: 0.01649199\n",
            "\n",
            "Global Step: 641,Val_Loss: 7.233209161265557e-05,  Val_acc: 0.9971217105263158 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:16:57.005943 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 3/15:\n",
            "Global step: 642,loss: 0.02124837\n",
            "\n",
            "Global step: 643,loss: 0.018682018\n",
            "\n",
            "Global step: 644,loss: 0.015702713\n",
            "\n",
            "Global step: 645,loss: 0.019466462\n",
            "\n",
            "Global step: 646,loss: 0.018345015\n",
            "\n",
            "Global step: 647,loss: 0.019867396\n",
            "\n",
            "Global step: 648,loss: 0.025810473\n",
            "\n",
            "Global step: 649,loss: 0.018639594\n",
            "\n",
            "Global step: 650,loss: 0.019142227\n",
            "\n",
            "Global step: 651,loss: 0.023359898\n",
            "\n",
            "Global step: 652,loss: 0.018217381\n",
            "\n",
            "Global step: 653,loss: 0.023927521\n",
            "\n",
            "Global step: 654,loss: 0.019351877\n",
            "\n",
            "Global step: 655,loss: 0.017385665\n",
            "\n",
            "Global step: 656,loss: 0.019644191\n",
            "\n",
            "Global step: 657,loss: 0.024623426\n",
            "\n",
            "Global step: 658,loss: 0.020358846\n",
            "\n",
            "Global step: 659,loss: 0.021674253\n",
            "\n",
            "Global step: 660,loss: 0.019864034\n",
            "\n",
            "Global step: 661,loss: 0.018436221\n",
            "\n",
            "Global step: 662,loss: 0.01958998\n",
            "\n",
            "Global step: 663,loss: 0.020276891\n",
            "\n",
            "Global step: 664,loss: 0.018075794\n",
            "\n",
            "Global step: 665,loss: 0.017079707\n",
            "\n",
            "Global step: 666,loss: 0.018604571\n",
            "\n",
            "Global step: 667,loss: 0.016039012\n",
            "\n",
            "Global step: 668,loss: 0.015780767\n",
            "\n",
            "Global step: 669,loss: 0.015868388\n",
            "\n",
            "Global step: 670,loss: 0.016904632\n",
            "\n",
            "Global step: 671,loss: 0.019205082\n",
            "\n",
            "Global step: 672,loss: 0.021733772\n",
            "\n",
            "Global step: 673,loss: 0.020808734\n",
            "\n",
            "Global step: 674,loss: 0.022680515\n",
            "\n",
            "Global step: 675,loss: 0.016678236\n",
            "\n",
            "Global step: 676,loss: 0.01977047\n",
            "\n",
            "Global step: 677,loss: 0.016984435\n",
            "\n",
            "Global step: 678,loss: 0.022558924\n",
            "\n",
            "Global step: 679,loss: 0.018491473\n",
            "\n",
            "Global step: 680,loss: 0.018528793\n",
            "\n",
            "Global step: 681,loss: 0.021387588\n",
            "\n",
            "Global step: 682,loss: 0.016121004\n",
            "\n",
            "Global step: 683,loss: 0.022616707\n",
            "\n",
            "Global step: 684,loss: 0.015696442\n",
            "\n",
            "Global step: 685,loss: 0.017916044\n",
            "\n",
            "Global step: 686,loss: 0.020638354\n",
            "\n",
            "Global step: 687,loss: 0.020067645\n",
            "\n",
            "Global step: 688,loss: 0.01695652\n",
            "\n",
            "Global step: 689,loss: 0.019643411\n",
            "\n",
            "Global step: 690,loss: 0.016888088\n",
            "\n",
            "Global step: 691,loss: 0.017866611\n",
            "\n",
            "Global step: 692,loss: 0.017601222\n",
            "\n",
            "Global step: 693,loss: 0.020814922\n",
            "\n",
            "Global step: 694,loss: 0.019867074\n",
            "\n",
            "Global step: 695,loss: 0.017425857\n",
            "\n",
            "Global step: 696,loss: 0.017336553\n",
            "\n",
            "Global step: 697,loss: 0.021995202\n",
            "\n",
            "Global step: 698,loss: 0.021667877\n",
            "\n",
            "Global step: 699,loss: 0.020408858\n",
            "\n",
            "Global step: 700,loss: 0.021914123\n",
            "\n",
            "Global step: 701,loss: 0.021901403\n",
            "\n",
            "Global step: 702,loss: 0.022449449\n",
            "\n",
            "Global step: 703,loss: 0.018851688\n",
            "\n",
            "Global step: 704,loss: 0.020774942\n",
            "\n",
            "Global step: 705,loss: 0.017930433\n",
            "\n",
            "Global step: 706,loss: 0.020687532\n",
            "\n",
            "Global step: 707,loss: 0.017249936\n",
            "\n",
            "Global step: 708,loss: 0.019205648\n",
            "\n",
            "Global step: 709,loss: 0.022358581\n",
            "\n",
            "Global step: 710,loss: 0.018035155\n",
            "\n",
            "Global step: 711,loss: 0.019192597\n",
            "\n",
            "Global step: 712,loss: 0.018101202\n",
            "\n",
            "Global step: 713,loss: 0.020423729\n",
            "\n",
            "Global step: 714,loss: 0.023884188\n",
            "\n",
            "Global step: 715,loss: 0.017726598\n",
            "\n",
            "Global step: 716,loss: 0.022300754\n",
            "\n",
            "Global step: 717,loss: 0.017729841\n",
            "\n",
            "Global step: 718,loss: 0.026673973\n",
            "\n",
            "Global step: 719,loss: 0.019661225\n",
            "\n",
            "Global step: 720,loss: 0.01726676\n",
            "\n",
            "Global step: 721,loss: 0.02335597\n",
            "\n",
            "Global step: 722,loss: 0.017879272\n",
            "\n",
            "Global step: 723,loss: 0.021048516\n",
            "\n",
            "Global step: 724,loss: 0.01689023\n",
            "\n",
            "Global step: 725,loss: 0.01574026\n",
            "\n",
            "Global step: 726,loss: 0.024823394\n",
            "\n",
            "Global step: 727,loss: 0.01693223\n",
            "\n",
            "Global step: 728,loss: 0.020033743\n",
            "\n",
            "Global step: 729,loss: 0.015229544\n",
            "\n",
            "Global step: 730,loss: 0.014486751\n",
            "\n",
            "Global step: 731,loss: 0.020437792\n",
            "\n",
            "Global step: 732,loss: 0.017275114\n",
            "\n",
            "Global step: 733,loss: 0.018674342\n",
            "\n",
            "Global step: 734,loss: 0.0151332095\n",
            "\n",
            "Global step: 735,loss: 0.016048621\n",
            "\n",
            "Global step: 736,loss: 0.018740647\n",
            "\n",
            "Global step: 737,loss: 0.019592986\n",
            "\n",
            "Global step: 738,loss: 0.018107992\n",
            "\n",
            "Global step: 739,loss: 0.02003684\n",
            "\n",
            "Global step: 740,loss: 0.018318625\n",
            "\n",
            "Global step: 741,loss: 0.014587228\n",
            "\n",
            "Global step: 742,loss: 0.019175595\n",
            "\n",
            "Global step: 743,loss: 0.023681624\n",
            "\n",
            "Global step: 744,loss: 0.019435514\n",
            "\n",
            "Global step: 745,loss: 0.01649893\n",
            "\n",
            "Global step: 746,loss: 0.014054441\n",
            "\n",
            "Global step: 747,loss: 0.018050428\n",
            "\n",
            "Global step: 748,loss: 0.016501565\n",
            "\n",
            "Global step: 749,loss: 0.026073508\n",
            "\n",
            "Global step: 750,loss: 0.021400683\n",
            "\n",
            "Global step: 751,loss: 0.020901404\n",
            "\n",
            "Global step: 752,loss: 0.019079864\n",
            "\n",
            "Global step: 753,loss: 0.019392328\n",
            "\n",
            "Global step: 754,loss: 0.020947106\n",
            "\n",
            "Global step: 755,loss: 0.015950818\n",
            "\n",
            "Global step: 756,loss: 0.016233608\n",
            "\n",
            "Global step: 757,loss: 0.016954212\n",
            "\n",
            "Global step: 758,loss: 0.018653687\n",
            "\n",
            "Global step: 759,loss: 0.017512094\n",
            "\n",
            "Global step: 760,loss: 0.018929288\n",
            "\n",
            "Global step: 761,loss: 0.019330654\n",
            "\n",
            "Global step: 762,loss: 0.019013688\n",
            "\n",
            "Global step: 763,loss: 0.01652633\n",
            "\n",
            "Global step: 764,loss: 0.017530764\n",
            "\n",
            "Global step: 765,loss: 0.018062264\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.27629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:18:29.417052 140673493931776 supervisor.py:1099] global_step/sec: 1.27629\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 766,loss: 0.014009414\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 767.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:18:30.516802 140673502324480 supervisor.py:1050] Recording summary at step 767.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 767,loss: 0.01575496\n",
            "\n",
            "Global step: 768,loss: 0.018413905\n",
            "\n",
            "Global step: 769,loss: 0.015743706\n",
            "\n",
            "Global step: 770,loss: 0.022441745\n",
            "\n",
            "Global step: 771,loss: 0.021054141\n",
            "\n",
            "Global step: 772,loss: 0.017517267\n",
            "\n",
            "Global step: 773,loss: 0.021717617\n",
            "\n",
            "Global step: 774,loss: 0.015981045\n",
            "\n",
            "Global step: 775,loss: 0.016149074\n",
            "\n",
            "Global step: 776,loss: 0.016471412\n",
            "\n",
            "Global step: 777,loss: 0.020727651\n",
            "\n",
            "Global step: 778,loss: 0.023132242\n",
            "\n",
            "Global step: 779,loss: 0.016626483\n",
            "\n",
            "Global step: 780,loss: 0.016247097\n",
            "\n",
            "Global step: 781,loss: 0.016298741\n",
            "\n",
            "Global step: 782,loss: 0.019616932\n",
            "\n",
            "Global step: 783,loss: 0.02144374\n",
            "\n",
            "Global step: 784,loss: 0.017521607\n",
            "\n",
            "Global step: 785,loss: 0.01784927\n",
            "\n",
            "Global step: 786,loss: 0.018459115\n",
            "\n",
            "Global step: 787,loss: 0.017047647\n",
            "\n",
            "Global step: 788,loss: 0.018866986\n",
            "\n",
            "Global step: 789,loss: 0.022958929\n",
            "\n",
            "Global step: 790,loss: 0.02005336\n",
            "\n",
            "Global step: 791,loss: 0.016163109\n",
            "\n",
            "Global step: 792,loss: 0.026562108\n",
            "\n",
            "Global step: 793,loss: 0.017523125\n",
            "\n",
            "Global step: 794,loss: 0.015799673\n",
            "\n",
            "Global step: 795,loss: 0.016058637\n",
            "\n",
            "Global step: 796,loss: 0.016186263\n",
            "\n",
            "Global step: 797,loss: 0.02402049\n",
            "\n",
            "Global step: 798,loss: 0.014958275\n",
            "\n",
            "Global step: 799,loss: 0.01860998\n",
            "\n",
            "Global step: 800,loss: 0.015815701\n",
            "\n",
            "Global step: 801,loss: 0.02082989\n",
            "\n",
            "Global step: 802,loss: 0.015617228\n",
            "\n",
            "Global step: 803,loss: 0.018340338\n",
            "\n",
            "Global step: 804,loss: 0.0173198\n",
            "\n",
            "Global step: 805,loss: 0.015680777\n",
            "\n",
            "Global step: 806,loss: 0.017378468\n",
            "\n",
            "Global step: 807,loss: 0.022045553\n",
            "\n",
            "Global step: 808,loss: 0.020951785\n",
            "\n",
            "Global step: 809,loss: 0.015812766\n",
            "\n",
            "Global step: 810,loss: 0.014916462\n",
            "\n",
            "Global step: 811,loss: 0.019689154\n",
            "\n",
            "Global step: 812,loss: 0.019398054\n",
            "\n",
            "Global step: 813,loss: 0.020453097\n",
            "\n",
            "Global step: 814,loss: 0.015999578\n",
            "\n",
            "Global step: 815,loss: 0.016253997\n",
            "\n",
            "Global step: 816,loss: 0.015760908\n",
            "\n",
            "Global step: 817,loss: 0.023471646\n",
            "\n",
            "Global step: 818,loss: 0.021857962\n",
            "\n",
            "Global step: 819,loss: 0.015341464\n",
            "\n",
            "Global step: 820,loss: 0.01869551\n",
            "\n",
            "Global step: 821,loss: 0.017654683\n",
            "\n",
            "Global step: 822,loss: 0.024936302\n",
            "\n",
            "Global step: 823,loss: 0.021148719\n",
            "\n",
            "Global step: 824,loss: 0.016729308\n",
            "\n",
            "Global step: 825,loss: 0.019966964\n",
            "\n",
            "Global step: 826,loss: 0.021240938\n",
            "\n",
            "Global step: 827,loss: 0.02061041\n",
            "\n",
            "Global step: 828,loss: 0.02242321\n",
            "\n",
            "Global step: 829,loss: 0.019191787\n",
            "\n",
            "Global step: 830,loss: 0.01831592\n",
            "\n",
            "Global step: 831,loss: 0.015202773\n",
            "\n",
            "Global step: 832,loss: 0.015750961\n",
            "\n",
            "Global step: 833,loss: 0.020205904\n",
            "\n",
            "Global step: 834,loss: 0.015027063\n",
            "\n",
            "Global step: 835,loss: 0.01758297\n",
            "\n",
            "Global step: 836,loss: 0.016569348\n",
            "\n",
            "Global step: 837,loss: 0.02089538\n",
            "\n",
            "Global step: 838,loss: 0.016691599\n",
            "\n",
            "Global step: 839,loss: 0.015117506\n",
            "\n",
            "Global step: 840,loss: 0.018415116\n",
            "\n",
            "Global step: 841,loss: 0.017202653\n",
            "\n",
            "Global step: 842,loss: 0.014380723\n",
            "\n",
            "Global step: 843,loss: 0.017572714\n",
            "\n",
            "Global step: 844,loss: 0.018212466\n",
            "\n",
            "Global step: 845,loss: 0.021295818\n",
            "\n",
            "Global step: 846,loss: 0.02028347\n",
            "\n",
            "Global step: 847,loss: 0.016712748\n",
            "\n",
            "Global step: 848,loss: 0.017373756\n",
            "\n",
            "Global step: 849,loss: 0.02225129\n",
            "\n",
            "Global step: 850,loss: 0.01823666\n",
            "\n",
            "Global step: 851,loss: 0.018572275\n",
            "\n",
            "Global step: 852,loss: 0.019929405\n",
            "\n",
            "Global step: 853,loss: 0.015022106\n",
            "\n",
            "Global step: 854,loss: 0.01869838\n",
            "\n",
            "Global step: 855,loss: 0.016017368\n",
            "\n",
            "Global Step: 855,Val_Loss: 6.730839729578675e-05,  Val_acc: 0.99609375 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:19:42.710212 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 4/15:\n",
            "Global step: 856,loss: 0.019575704\n",
            "\n",
            "Global step: 857,loss: 0.018307816\n",
            "\n",
            "Global step: 858,loss: 0.014056028\n",
            "\n",
            "Global step: 859,loss: 0.016564533\n",
            "\n",
            "Global step: 860,loss: 0.018994547\n",
            "\n",
            "Global step: 861,loss: 0.016483825\n",
            "\n",
            "Global step: 862,loss: 0.015656661\n",
            "\n",
            "Global step: 863,loss: 0.02132372\n",
            "\n",
            "Global step: 864,loss: 0.018003283\n",
            "\n",
            "Global step: 865,loss: 0.020092584\n",
            "\n",
            "Global step: 866,loss: 0.017284553\n",
            "\n",
            "Global step: 867,loss: 0.0135404635\n",
            "\n",
            "Global step: 868,loss: 0.01695484\n",
            "\n",
            "Global step: 869,loss: 0.020651085\n",
            "\n",
            "Global step: 870,loss: 0.018113155\n",
            "\n",
            "Global step: 871,loss: 0.016147342\n",
            "\n",
            "Global step: 872,loss: 0.014973531\n",
            "\n",
            "Global step: 873,loss: 0.01694397\n",
            "\n",
            "Global step: 874,loss: 0.01645349\n",
            "\n",
            "Global step: 875,loss: 0.017713023\n",
            "\n",
            "Global step: 876,loss: 0.020440437\n",
            "\n",
            "Global step: 877,loss: 0.023099162\n",
            "\n",
            "Global step: 878,loss: 0.013154968\n",
            "\n",
            "Global step: 879,loss: 0.018606119\n",
            "\n",
            "Global step: 880,loss: 0.014605418\n",
            "\n",
            "Global step: 881,loss: 0.017841209\n",
            "\n",
            "Global step: 882,loss: 0.02007687\n",
            "\n",
            "Global step: 883,loss: 0.014047184\n",
            "\n",
            "Global step: 884,loss: 0.016277198\n",
            "\n",
            "Global step: 885,loss: 0.02206193\n",
            "\n",
            "Global step: 886,loss: 0.01851374\n",
            "\n",
            "Global step: 887,loss: 0.01907054\n",
            "\n",
            "Global step: 888,loss: 0.018731795\n",
            "\n",
            "Global step: 889,loss: 0.015156787\n",
            "\n",
            "Global step: 890,loss: 0.015393386\n",
            "\n",
            "Global step: 891,loss: 0.017905513\n",
            "\n",
            "Global step: 892,loss: 0.016241107\n",
            "\n",
            "Global step: 893,loss: 0.016229423\n",
            "\n",
            "Global step: 894,loss: 0.0175065\n",
            "\n",
            "Global step: 895,loss: 0.017171752\n",
            "\n",
            "Global step: 896,loss: 0.016339751\n",
            "\n",
            "Global step: 897,loss: 0.01690878\n",
            "\n",
            "Global step: 898,loss: 0.021163091\n",
            "\n",
            "Global step: 899,loss: 0.017669413\n",
            "\n",
            "Global step: 900,loss: 0.014905469\n",
            "\n",
            "Global step: 901,loss: 0.019339059\n",
            "\n",
            "Global step: 902,loss: 0.015672091\n",
            "\n",
            "Global step: 903,loss: 0.021792911\n",
            "\n",
            "Global step: 904,loss: 0.015506493\n",
            "\n",
            "Global step: 905,loss: 0.018681677\n",
            "\n",
            "Global step: 906,loss: 0.016752087\n",
            "\n",
            "Global step: 907,loss: 0.01722113\n",
            "\n",
            "Global step: 908,loss: 0.018396739\n",
            "\n",
            "Global step: 909,loss: 0.018212533\n",
            "\n",
            "Global step: 910,loss: 0.01905232\n",
            "\n",
            "Global step: 911,loss: 0.015425536\n",
            "\n",
            "Global step: 912,loss: 0.016285481\n",
            "\n",
            "Global step: 913,loss: 0.015229336\n",
            "\n",
            "Global step: 914,loss: 0.022334151\n",
            "\n",
            "Global step: 915,loss: 0.015224813\n",
            "\n",
            "Global step: 916,loss: 0.014182317\n",
            "\n",
            "Global step: 917,loss: 0.018583886\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.26561Global step: 918,loss: 0.018741708\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:20:29.517296 140673493931776 supervisor.py:1099] global_step/sec: 1.26561\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 919.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:20:29.873952 140673502324480 supervisor.py:1050] Recording summary at step 919.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 919,loss: 0.014156283\n",
            "\n",
            "Global step: 920,loss: 0.017571887\n",
            "\n",
            "Global step: 921,loss: 0.015386978\n",
            "\n",
            "Global step: 922,loss: 0.015111324\n",
            "\n",
            "Global step: 923,loss: 0.015297245\n",
            "\n",
            "Global step: 924,loss: 0.019199397\n",
            "\n",
            "Global step: 925,loss: 0.0161075\n",
            "\n",
            "Global step: 926,loss: 0.016387898\n",
            "\n",
            "Global step: 927,loss: 0.015535507\n",
            "\n",
            "Global step: 928,loss: 0.01542991\n",
            "\n",
            "Global step: 929,loss: 0.014111925\n",
            "\n",
            "Global step: 930,loss: 0.01755312\n",
            "\n",
            "Global step: 931,loss: 0.0150033375\n",
            "\n",
            "Global step: 932,loss: 0.019024363\n",
            "\n",
            "Global step: 933,loss: 0.018270338\n",
            "\n",
            "Global step: 934,loss: 0.018456709\n",
            "\n",
            "Global step: 935,loss: 0.015686939\n",
            "\n",
            "Global step: 936,loss: 0.016954359\n",
            "\n",
            "Global step: 937,loss: 0.014824376\n",
            "\n",
            "Global step: 938,loss: 0.01720784\n",
            "\n",
            "Global step: 939,loss: 0.015734442\n",
            "\n",
            "Global step: 940,loss: 0.015268664\n",
            "\n",
            "Global step: 941,loss: 0.018257795\n",
            "\n",
            "Global step: 942,loss: 0.02460479\n",
            "\n",
            "Global step: 943,loss: 0.015348313\n",
            "\n",
            "Global step: 944,loss: 0.017939866\n",
            "\n",
            "Global step: 945,loss: 0.017163454\n",
            "\n",
            "Global step: 946,loss: 0.014493479\n",
            "\n",
            "Global step: 947,loss: 0.014871949\n",
            "\n",
            "Global step: 948,loss: 0.016674113\n",
            "\n",
            "Global step: 949,loss: 0.014802726\n",
            "\n",
            "Global step: 950,loss: 0.012428444\n",
            "\n",
            "Global step: 951,loss: 0.013209784\n",
            "\n",
            "Global step: 952,loss: 0.014374837\n",
            "\n",
            "Global step: 953,loss: 0.016473066\n",
            "\n",
            "Global step: 954,loss: 0.020084094\n",
            "\n",
            "Global step: 955,loss: 0.014638268\n",
            "\n",
            "Global step: 956,loss: 0.015527634\n",
            "\n",
            "Global step: 957,loss: 0.016016385\n",
            "\n",
            "Global step: 958,loss: 0.018075852\n",
            "\n",
            "Global step: 959,loss: 0.014471836\n",
            "\n",
            "Global step: 960,loss: 0.01737626\n",
            "\n",
            "Global step: 961,loss: 0.01637725\n",
            "\n",
            "Global step: 962,loss: 0.016496768\n",
            "\n",
            "Global step: 963,loss: 0.01470491\n",
            "\n",
            "Global step: 964,loss: 0.014379823\n",
            "\n",
            "Global step: 965,loss: 0.01519497\n",
            "\n",
            "Global step: 966,loss: 0.018810235\n",
            "\n",
            "Global step: 967,loss: 0.016301848\n",
            "\n",
            "Global step: 968,loss: 0.016891547\n",
            "\n",
            "Global step: 969,loss: 0.016511895\n",
            "\n",
            "Global step: 970,loss: 0.015085699\n",
            "\n",
            "Global step: 971,loss: 0.016678277\n",
            "\n",
            "Global step: 972,loss: 0.015223034\n",
            "\n",
            "Global step: 973,loss: 0.018246904\n",
            "\n",
            "Global step: 974,loss: 0.02125473\n",
            "\n",
            "Global step: 975,loss: 0.017497582\n",
            "\n",
            "Global step: 976,loss: 0.021238942\n",
            "\n",
            "Global step: 977,loss: 0.015219785\n",
            "\n",
            "Global step: 978,loss: 0.013740859\n",
            "\n",
            "Global step: 979,loss: 0.013579694\n",
            "\n",
            "Global step: 980,loss: 0.018243238\n",
            "\n",
            "Global step: 981,loss: 0.01902479\n",
            "\n",
            "Global step: 982,loss: 0.015118329\n",
            "\n",
            "Global step: 983,loss: 0.01773288\n",
            "\n",
            "Global step: 984,loss: 0.018007241\n",
            "\n",
            "Global step: 985,loss: 0.01630717\n",
            "\n",
            "Global step: 986,loss: 0.01380644\n",
            "\n",
            "Global step: 987,loss: 0.01819295\n",
            "\n",
            "Global step: 988,loss: 0.01885274\n",
            "\n",
            "Global step: 989,loss: 0.01559389\n",
            "\n",
            "Global step: 990,loss: 0.01819111\n",
            "\n",
            "Global step: 991,loss: 0.018965375\n",
            "\n",
            "Global step: 992,loss: 0.01782687\n",
            "\n",
            "Global step: 993,loss: 0.016337685\n",
            "\n",
            "Global step: 994,loss: 0.016675673\n",
            "\n",
            "Global step: 995,loss: 0.014541577\n",
            "\n",
            "Global step: 996,loss: 0.019056408\n",
            "\n",
            "Global step: 997,loss: 0.017137399\n",
            "\n",
            "Global step: 998,loss: 0.01657318\n",
            "\n",
            "Global step: 999,loss: 0.015841667\n",
            "\n",
            "Global step: 1000,loss: 0.014710778\n",
            "\n",
            "Global step: 1001,loss: 0.01508646\n",
            "\n",
            "Global step: 1002,loss: 0.017833238\n",
            "\n",
            "Global step: 1003,loss: 0.013235846\n",
            "\n",
            "Global step: 1004,loss: 0.0141658345\n",
            "\n",
            "Global step: 1005,loss: 0.017042328\n",
            "\n",
            "Global step: 1006,loss: 0.013335969\n",
            "\n",
            "Global step: 1007,loss: 0.017235782\n",
            "\n",
            "Global step: 1008,loss: 0.018056005\n",
            "\n",
            "Global step: 1009,loss: 0.016534938\n",
            "\n",
            "Global step: 1010,loss: 0.013397872\n",
            "\n",
            "Global step: 1011,loss: 0.01979217\n",
            "\n",
            "Global step: 1012,loss: 0.022637315\n",
            "\n",
            "Global step: 1013,loss: 0.015733954\n",
            "\n",
            "Global step: 1014,loss: 0.015988348\n",
            "\n",
            "Global step: 1015,loss: 0.017325455\n",
            "\n",
            "Global step: 1016,loss: 0.016037948\n",
            "\n",
            "Global step: 1017,loss: 0.013638178\n",
            "\n",
            "Global step: 1018,loss: 0.020888085\n",
            "\n",
            "Global step: 1019,loss: 0.017183708\n",
            "\n",
            "Global step: 1020,loss: 0.013670228\n",
            "\n",
            "Global step: 1021,loss: 0.016512047\n",
            "\n",
            "Global step: 1022,loss: 0.01978559\n",
            "\n",
            "Global step: 1023,loss: 0.018586168\n",
            "\n",
            "Global step: 1024,loss: 0.020535171\n",
            "\n",
            "Global step: 1025,loss: 0.01706652\n",
            "\n",
            "Global step: 1026,loss: 0.01711551\n",
            "\n",
            "Global step: 1027,loss: 0.017185388\n",
            "\n",
            "Global step: 1028,loss: 0.015073521\n",
            "\n",
            "Global step: 1029,loss: 0.016078513\n",
            "\n",
            "Global step: 1030,loss: 0.016491443\n",
            "\n",
            "Global step: 1031,loss: 0.017671792\n",
            "\n",
            "Global step: 1032,loss: 0.01447633\n",
            "\n",
            "Global step: 1033,loss: 0.0149053335\n",
            "\n",
            "Global step: 1034,loss: 0.016888913\n",
            "\n",
            "Global step: 1035,loss: 0.020626782\n",
            "\n",
            "Global step: 1036,loss: 0.018268306\n",
            "\n",
            "Global step: 1037,loss: 0.017719235\n",
            "\n",
            "Global step: 1038,loss: 0.01712\n",
            "\n",
            "Global step: 1039,loss: 0.01471304\n",
            "\n",
            "Global step: 1040,loss: 0.015624572\n",
            "\n",
            "Global step: 1041,loss: 0.015377795\n",
            "\n",
            "Global step: 1042,loss: 0.016328452\n",
            "\n",
            "Global step: 1043,loss: 0.014516251\n",
            "\n",
            "Global step: 1044,loss: 0.017732628\n",
            "\n",
            "Global step: 1045,loss: 0.014311442\n",
            "\n",
            "Global step: 1046,loss: 0.020008808\n",
            "\n",
            "Global step: 1047,loss: 0.019818075\n",
            "\n",
            "Global step: 1048,loss: 0.017042095\n",
            "\n",
            "Global step: 1049,loss: 0.015767429\n",
            "\n",
            "Global step: 1050,loss: 0.015166443\n",
            "\n",
            "Global step: 1051,loss: 0.015021969\n",
            "\n",
            "Global step: 1052,loss: 0.018919075\n",
            "\n",
            "Global step: 1053,loss: 0.015543241\n",
            "\n",
            "Global step: 1054,loss: 0.017528944\n",
            "\n",
            "Global step: 1055,loss: 0.014662218\n",
            "\n",
            "Global step: 1056,loss: 0.01613117\n",
            "\n",
            "Global step: 1057,loss: 0.013573413\n",
            "\n",
            "Global step: 1058,loss: 0.0150852045\n",
            "\n",
            "Global step: 1059,loss: 0.01655296\n",
            "\n",
            "Global step: 1060,loss: 0.01509869\n",
            "\n",
            "Global step: 1061,loss: 0.016032148\n",
            "\n",
            "Global step: 1062,loss: 0.016436022\n",
            "\n",
            "Global step: 1063,loss: 0.015892103\n",
            "\n",
            "Global step: 1064,loss: 0.01641884\n",
            "\n",
            "Global step: 1065,loss: 0.01980746\n",
            "\n",
            "Global step: 1066,loss: 0.016021859\n",
            "\n",
            "Global step: 1067,loss: 0.015232527\n",
            "\n",
            "Global step: 1068,loss: 0.017453667\n",
            "\n",
            "Global step: 1069,loss: 0.017226493\n",
            "\n",
            "Global Step: 1069,Val_Loss: 6.304043946377198e-05,  Val_acc: 0.9969161184210527 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:22:27.997200 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 5/15:\n",
            "INFO:tensorflow:global_step/sec: 1.27297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:22:29.708185 140673493931776 supervisor.py:1099] global_step/sec: 1.27297\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1070,loss: 0.014250363\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1072.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:22:30.679590 140673502324480 supervisor.py:1050] Recording summary at step 1072.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1071,loss: 0.013897228\n",
            "\n",
            "Global step: 1072,loss: 0.013285428\n",
            "\n",
            "Global step: 1073,loss: 0.016973425\n",
            "\n",
            "Global step: 1074,loss: 0.014657559\n",
            "\n",
            "Global step: 1075,loss: 0.017650858\n",
            "\n",
            "Global step: 1076,loss: 0.016700326\n",
            "\n",
            "Global step: 1077,loss: 0.013445489\n",
            "\n",
            "Global step: 1078,loss: 0.015018705\n",
            "\n",
            "Global step: 1079,loss: 0.01526362\n",
            "\n",
            "Global step: 1080,loss: 0.014272\n",
            "\n",
            "Global step: 1081,loss: 0.015310266\n",
            "\n",
            "Global step: 1082,loss: 0.014196424\n",
            "\n",
            "Global step: 1083,loss: 0.015145825\n",
            "\n",
            "Global step: 1084,loss: 0.016348526\n",
            "\n",
            "Global step: 1085,loss: 0.017370233\n",
            "\n",
            "Global step: 1086,loss: 0.014572079\n",
            "\n",
            "Global step: 1087,loss: 0.016208366\n",
            "\n",
            "Global step: 1088,loss: 0.016958885\n",
            "\n",
            "Global step: 1089,loss: 0.014510418\n",
            "\n",
            "Global step: 1090,loss: 0.01723094\n",
            "\n",
            "Global step: 1091,loss: 0.015184049\n",
            "\n",
            "Global step: 1092,loss: 0.013697915\n",
            "\n",
            "Global step: 1093,loss: 0.015478653\n",
            "\n",
            "Global step: 1094,loss: 0.015140589\n",
            "\n",
            "Global step: 1095,loss: 0.014544881\n",
            "\n",
            "Global step: 1096,loss: 0.01419564\n",
            "\n",
            "Global step: 1097,loss: 0.012957645\n",
            "\n",
            "Global step: 1098,loss: 0.02420643\n",
            "\n",
            "Global step: 1099,loss: 0.015574345\n",
            "\n",
            "Global step: 1100,loss: 0.016648501\n",
            "\n",
            "Global step: 1101,loss: 0.015371645\n",
            "\n",
            "Global step: 1102,loss: 0.013913302\n",
            "\n",
            "Global step: 1103,loss: 0.01727429\n",
            "\n",
            "Global step: 1104,loss: 0.014604124\n",
            "\n",
            "Global step: 1105,loss: 0.015369771\n",
            "\n",
            "Global step: 1106,loss: 0.0142065985\n",
            "\n",
            "Global step: 1107,loss: 0.014775701\n",
            "\n",
            "Global step: 1108,loss: 0.015344964\n",
            "\n",
            "Global step: 1109,loss: 0.015053432\n",
            "\n",
            "Global step: 1110,loss: 0.015663173\n",
            "\n",
            "Global step: 1111,loss: 0.015021236\n",
            "\n",
            "Global step: 1112,loss: 0.014205093\n",
            "\n",
            "Global step: 1113,loss: 0.014506715\n",
            "\n",
            "Global step: 1114,loss: 0.014427898\n",
            "\n",
            "Global step: 1115,loss: 0.019650187\n",
            "\n",
            "Global step: 1116,loss: 0.017466292\n",
            "\n",
            "Global step: 1117,loss: 0.016233755\n",
            "\n",
            "Global step: 1118,loss: 0.01575363\n",
            "\n",
            "Global step: 1119,loss: 0.014727336\n",
            "\n",
            "Global step: 1120,loss: 0.012644097\n",
            "\n",
            "Global step: 1121,loss: 0.01347194\n",
            "\n",
            "Global step: 1122,loss: 0.012664323\n",
            "\n",
            "Global step: 1123,loss: 0.016565707\n",
            "\n",
            "Global step: 1124,loss: 0.015471334\n",
            "\n",
            "Global step: 1125,loss: 0.0142194005\n",
            "\n",
            "Global step: 1126,loss: 0.020688057\n",
            "\n",
            "Global step: 1127,loss: 0.015685504\n",
            "\n",
            "Global step: 1128,loss: 0.012643304\n",
            "\n",
            "Global step: 1129,loss: 0.014263637\n",
            "\n",
            "Global step: 1130,loss: 0.012357794\n",
            "\n",
            "Global step: 1131,loss: 0.017129274\n",
            "\n",
            "Global step: 1132,loss: 0.019594481\n",
            "\n",
            "Global step: 1133,loss: 0.013362503\n",
            "\n",
            "Global step: 1134,loss: 0.015456304\n",
            "\n",
            "Global step: 1135,loss: 0.0143668065\n",
            "\n",
            "Global step: 1136,loss: 0.014702166\n",
            "\n",
            "Global step: 1137,loss: 0.015914934\n",
            "\n",
            "Global step: 1138,loss: 0.017406244\n",
            "\n",
            "Global step: 1139,loss: 0.01358991\n",
            "\n",
            "Global step: 1140,loss: 0.01465514\n",
            "\n",
            "Global step: 1141,loss: 0.014081537\n",
            "\n",
            "Global step: 1142,loss: 0.015991934\n",
            "\n",
            "Global step: 1143,loss: 0.013626655\n",
            "\n",
            "Global step: 1144,loss: 0.014232363\n",
            "\n",
            "Global step: 1145,loss: 0.017592244\n",
            "\n",
            "Global step: 1146,loss: 0.015918078\n",
            "\n",
            "Global step: 1147,loss: 0.013009642\n",
            "\n",
            "Global step: 1148,loss: 0.013315338\n",
            "\n",
            "Global step: 1149,loss: 0.013522654\n",
            "\n",
            "Global step: 1150,loss: 0.018065836\n",
            "\n",
            "Global step: 1151,loss: 0.013267395\n",
            "\n",
            "Global step: 1152,loss: 0.014699577\n",
            "\n",
            "Global step: 1153,loss: 0.015201554\n",
            "\n",
            "Global step: 1154,loss: 0.014302881\n",
            "\n",
            "Global step: 1155,loss: 0.013417051\n",
            "\n",
            "Global step: 1156,loss: 0.014346656\n",
            "\n",
            "Global step: 1157,loss: 0.014476426\n",
            "\n",
            "Global step: 1158,loss: 0.019451387\n",
            "\n",
            "Global step: 1159,loss: 0.014606476\n",
            "\n",
            "Global step: 1160,loss: 0.015301306\n",
            "\n",
            "Global step: 1161,loss: 0.014880763\n",
            "\n",
            "Global step: 1162,loss: 0.013354501\n",
            "\n",
            "Global step: 1163,loss: 0.014149198\n",
            "\n",
            "Global step: 1164,loss: 0.014222928\n",
            "\n",
            "Global step: 1165,loss: 0.012771902\n",
            "\n",
            "Global step: 1166,loss: 0.017093103\n",
            "\n",
            "Global step: 1167,loss: 0.01366195\n",
            "\n",
            "Global step: 1168,loss: 0.014898759\n",
            "\n",
            "Global step: 1169,loss: 0.013391015\n",
            "\n",
            "Global step: 1170,loss: 0.012813796\n",
            "\n",
            "Global step: 1171,loss: 0.013855219\n",
            "\n",
            "Global step: 1172,loss: 0.014192459\n",
            "\n",
            "Global step: 1173,loss: 0.012901498\n",
            "\n",
            "Global step: 1174,loss: 0.01368811\n",
            "\n",
            "Global step: 1175,loss: 0.014479017\n",
            "\n",
            "Global step: 1176,loss: 0.016167738\n",
            "\n",
            "Global step: 1177,loss: 0.01749105\n",
            "\n",
            "Global step: 1178,loss: 0.017523114\n",
            "\n",
            "Global step: 1179,loss: 0.013209464\n",
            "\n",
            "Global step: 1180,loss: 0.017615374\n",
            "\n",
            "Global step: 1181,loss: 0.01648196\n",
            "\n",
            "Global step: 1182,loss: 0.012361998\n",
            "\n",
            "Global step: 1183,loss: 0.015539432\n",
            "\n",
            "Global step: 1184,loss: 0.014368417\n",
            "\n",
            "Global step: 1185,loss: 0.014020938\n",
            "\n",
            "Global step: 1186,loss: 0.015207004\n",
            "\n",
            "Global step: 1187,loss: 0.015245039\n",
            "\n",
            "Global step: 1188,loss: 0.01557928\n",
            "\n",
            "Global step: 1189,loss: 0.016393833\n",
            "\n",
            "Global step: 1190,loss: 0.013401088\n",
            "\n",
            "Global step: 1191,loss: 0.013556336\n",
            "\n",
            "Global step: 1192,loss: 0.013915653\n",
            "\n",
            "Global step: 1193,loss: 0.014728719\n",
            "\n",
            "Global step: 1194,loss: 0.015878685\n",
            "\n",
            "Global step: 1195,loss: 0.01431897\n",
            "\n",
            "Global step: 1196,loss: 0.017373031\n",
            "\n",
            "Global step: 1197,loss: 0.015142102\n",
            "\n",
            "Global step: 1198,loss: 0.014618937\n",
            "\n",
            "Global step: 1199,loss: 0.01562818\n",
            "\n",
            "Global step: 1200,loss: 0.018227445\n",
            "\n",
            "Global step: 1201,loss: 0.014008857\n",
            "\n",
            "Global step: 1202,loss: 0.017602883\n",
            "\n",
            "Global step: 1203,loss: 0.012888489\n",
            "\n",
            "Global step: 1204,loss: 0.015941989\n",
            "\n",
            "Global step: 1205,loss: 0.013705576\n",
            "\n",
            "Global step: 1206,loss: 0.013433069\n",
            "\n",
            "Global step: 1207,loss: 0.015211271\n",
            "\n",
            "Global step: 1208,loss: 0.015736084\n",
            "\n",
            "Global step: 1209,loss: 0.014986655\n",
            "\n",
            "Global step: 1210,loss: 0.01500425\n",
            "\n",
            "Global step: 1211,loss: 0.015726449\n",
            "\n",
            "Global step: 1212,loss: 0.016453044\n",
            "\n",
            "Global step: 1213,loss: 0.013783013\n",
            "\n",
            "Global step: 1214,loss: 0.013387296\n",
            "\n",
            "Global step: 1215,loss: 0.014454203\n",
            "\n",
            "Global step: 1216,loss: 0.013832125\n",
            "\n",
            "Global step: 1217,loss: 0.013431053\n",
            "\n",
            "Global step: 1218,loss: 0.013980423\n",
            "\n",
            "Global step: 1219,loss: 0.01895618\n",
            "\n",
            "Global step: 1220,loss: 0.017995263\n",
            "\n",
            "Global step: 1221,loss: 0.017266707\n",
            "\n",
            "Global step: 1222,loss: 0.016389016\n",
            "\n",
            "Global step: 1223,loss: 0.014415039\n",
            "\n",
            "Global step: 1224,loss: 0.01413161\n",
            "\n",
            "Global step: 1225,loss: 0.014782446\n",
            "\n",
            "Global step: 1226,loss: 0.015487611\n",
            "\n",
            "Global step: 1227,loss: 0.012462167\n",
            "\n",
            "Global step: 1228,loss: 0.016724134\n",
            "\n",
            "Global step: 1229,loss: 0.018280195\n",
            "\n",
            "Global step: 1230,loss: 0.021739462\n",
            "\n",
            "Global step: 1231,loss: 0.015324263\n",
            "\n",
            "Global step: 1232,loss: 0.014062469\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.35329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:24:29.416751 140673493931776 supervisor.py:1099] global_step/sec: 1.35329\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1233,loss: 0.016476674\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1234.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:24:30.484166 140673502324480 supervisor.py:1050] Recording summary at step 1234.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1234,loss: 0.017160576\n",
            "\n",
            "Global step: 1235,loss: 0.021106109\n",
            "\n",
            "Global step: 1236,loss: 0.014861421\n",
            "\n",
            "Global step: 1237,loss: 0.014635889\n",
            "\n",
            "Global step: 1238,loss: 0.017232826\n",
            "\n",
            "Global step: 1239,loss: 0.017199088\n",
            "\n",
            "Global step: 1240,loss: 0.015012007\n",
            "\n",
            "Global step: 1241,loss: 0.01519239\n",
            "\n",
            "Global step: 1242,loss: 0.01845174\n",
            "\n",
            "Global step: 1243,loss: 0.019005544\n",
            "\n",
            "Global step: 1244,loss: 0.013753632\n",
            "\n",
            "Global step: 1245,loss: 0.013223188\n",
            "\n",
            "Global step: 1246,loss: 0.019362317\n",
            "\n",
            "Global step: 1247,loss: 0.014396256\n",
            "\n",
            "Global step: 1248,loss: 0.016960986\n",
            "\n",
            "Global step: 1249,loss: 0.017587833\n",
            "\n",
            "Global step: 1250,loss: 0.017059715\n",
            "\n",
            "Global step: 1251,loss: 0.023891555\n",
            "\n",
            "Global step: 1252,loss: 0.015028859\n",
            "\n",
            "Global step: 1253,loss: 0.015953748\n",
            "\n",
            "Global step: 1254,loss: 0.013728135\n",
            "\n",
            "Global step: 1255,loss: 0.014027008\n",
            "\n",
            "Global step: 1256,loss: 0.015995821\n",
            "\n",
            "Global step: 1257,loss: 0.014889355\n",
            "\n",
            "Global step: 1258,loss: 0.013896348\n",
            "\n",
            "Global step: 1259,loss: 0.014543563\n",
            "\n",
            "Global step: 1260,loss: 0.017760234\n",
            "\n",
            "Global step: 1261,loss: 0.014730185\n",
            "\n",
            "Global step: 1262,loss: 0.013966771\n",
            "\n",
            "Global step: 1263,loss: 0.016215563\n",
            "\n",
            "Global step: 1264,loss: 0.015232723\n",
            "\n",
            "Global step: 1265,loss: 0.014355287\n",
            "\n",
            "Global step: 1266,loss: 0.014670774\n",
            "\n",
            "Global step: 1267,loss: 0.017024808\n",
            "\n",
            "Global step: 1268,loss: 0.016531318\n",
            "\n",
            "Global step: 1269,loss: 0.015312508\n",
            "\n",
            "Global step: 1270,loss: 0.014076768\n",
            "\n",
            "Global step: 1271,loss: 0.016987987\n",
            "\n",
            "Global step: 1272,loss: 0.014540048\n",
            "\n",
            "Global step: 1273,loss: 0.013701821\n",
            "\n",
            "Global step: 1274,loss: 0.013536746\n",
            "\n",
            "Global step: 1275,loss: 0.015164791\n",
            "\n",
            "Global step: 1276,loss: 0.017285187\n",
            "\n",
            "Global step: 1277,loss: 0.018399095\n",
            "\n",
            "Global step: 1278,loss: 0.013918224\n",
            "\n",
            "Global step: 1279,loss: 0.015836013\n",
            "\n",
            "Global step: 1280,loss: 0.023722637\n",
            "\n",
            "Global step: 1281,loss: 0.012970971\n",
            "\n",
            "Global step: 1282,loss: 0.015518155\n",
            "\n",
            "Global step: 1283,loss: 0.015648628\n",
            "\n",
            "Global Step: 1283,Val_Loss: 5.9391617182164304e-05,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:25:13.484229 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:25:13.912757 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 6/15:\n",
            "Global step: 1284,loss: 0.015420171\n",
            "\n",
            "Global step: 1285,loss: 0.013594461\n",
            "\n",
            "Global step: 1286,loss: 0.015975147\n",
            "\n",
            "Global step: 1287,loss: 0.018709023\n",
            "\n",
            "Global step: 1288,loss: 0.0147644635\n",
            "\n",
            "Global step: 1289,loss: 0.012990797\n",
            "\n",
            "Global step: 1290,loss: 0.013875959\n",
            "\n",
            "Global step: 1291,loss: 0.013733793\n",
            "\n",
            "Global step: 1292,loss: 0.013369814\n",
            "\n",
            "Global step: 1293,loss: 0.016270623\n",
            "\n",
            "Global step: 1294,loss: 0.01507829\n",
            "\n",
            "Global step: 1295,loss: 0.015229562\n",
            "\n",
            "Global step: 1296,loss: 0.014854075\n",
            "\n",
            "Global step: 1297,loss: 0.015038611\n",
            "\n",
            "Global step: 1298,loss: 0.014646846\n",
            "\n",
            "Global step: 1299,loss: 0.014701959\n",
            "\n",
            "Global step: 1300,loss: 0.014330691\n",
            "\n",
            "Global step: 1301,loss: 0.014956119\n",
            "\n",
            "Global step: 1302,loss: 0.019001544\n",
            "\n",
            "Global step: 1303,loss: 0.016446475\n",
            "\n",
            "Global step: 1304,loss: 0.014736989\n",
            "\n",
            "Global step: 1305,loss: 0.015147244\n",
            "\n",
            "Global step: 1306,loss: 0.013330073\n",
            "\n",
            "Global step: 1307,loss: 0.013406526\n",
            "\n",
            "Global step: 1308,loss: 0.014356501\n",
            "\n",
            "Global step: 1309,loss: 0.016323525\n",
            "\n",
            "Global step: 1310,loss: 0.014398771\n",
            "\n",
            "Global step: 1311,loss: 0.017042283\n",
            "\n",
            "Global step: 1312,loss: 0.01763582\n",
            "\n",
            "Global step: 1313,loss: 0.014806591\n",
            "\n",
            "Global step: 1314,loss: 0.014432502\n",
            "\n",
            "Global step: 1315,loss: 0.014100104\n",
            "\n",
            "Global step: 1316,loss: 0.018575162\n",
            "\n",
            "Global step: 1317,loss: 0.012796856\n",
            "\n",
            "Global step: 1318,loss: 0.014492138\n",
            "\n",
            "Global step: 1319,loss: 0.012901925\n",
            "\n",
            "Global step: 1320,loss: 0.016582925\n",
            "\n",
            "Global step: 1321,loss: 0.013371538\n",
            "\n",
            "Global step: 1322,loss: 0.015128316\n",
            "\n",
            "Global step: 1323,loss: 0.013846421\n",
            "\n",
            "Global step: 1324,loss: 0.013425999\n",
            "\n",
            "Global step: 1325,loss: 0.013314575\n",
            "\n",
            "Global step: 1326,loss: 0.012797675\n",
            "\n",
            "Global step: 1327,loss: 0.013193777\n",
            "\n",
            "Global step: 1328,loss: 0.014452763\n",
            "\n",
            "Global step: 1329,loss: 0.0137241315\n",
            "\n",
            "Global step: 1330,loss: 0.01383709\n",
            "\n",
            "Global step: 1331,loss: 0.013758859\n",
            "\n",
            "Global step: 1332,loss: 0.01408785\n",
            "\n",
            "Global step: 1333,loss: 0.013768229\n",
            "\n",
            "Global step: 1334,loss: 0.022235446\n",
            "\n",
            "Global step: 1335,loss: 0.014315058\n",
            "\n",
            "Global step: 1336,loss: 0.013071195\n",
            "\n",
            "Global step: 1337,loss: 0.012591987\n",
            "\n",
            "Global step: 1338,loss: 0.013234936\n",
            "\n",
            "Global step: 1339,loss: 0.012626182\n",
            "\n",
            "Global step: 1340,loss: 0.01468117\n",
            "\n",
            "Global step: 1341,loss: 0.0132660335\n",
            "\n",
            "Global step: 1342,loss: 0.012133712\n",
            "\n",
            "Global step: 1343,loss: 0.0149666555\n",
            "\n",
            "Global step: 1344,loss: 0.013759667\n",
            "\n",
            "Global step: 1345,loss: 0.013774828\n",
            "\n",
            "Global step: 1346,loss: 0.01280254\n",
            "\n",
            "Global step: 1347,loss: 0.013548857\n",
            "\n",
            "Global step: 1348,loss: 0.017499788\n",
            "\n",
            "Global step: 1349,loss: 0.014401022\n",
            "\n",
            "Global step: 1350,loss: 0.013149655\n",
            "\n",
            "Global step: 1351,loss: 0.013636852\n",
            "\n",
            "Global step: 1352,loss: 0.013338417\n",
            "\n",
            "Global step: 1353,loss: 0.015834542\n",
            "\n",
            "Global step: 1354,loss: 0.014946789\n",
            "\n",
            "Global step: 1355,loss: 0.01465982\n",
            "\n",
            "Global step: 1356,loss: 0.016306546\n",
            "\n",
            "Global step: 1357,loss: 0.012936537\n",
            "\n",
            "Global step: 1358,loss: 0.014716927\n",
            "\n",
            "Global step: 1359,loss: 0.016328514\n",
            "\n",
            "Global step: 1360,loss: 0.013465732\n",
            "\n",
            "Global step: 1361,loss: 0.015658882\n",
            "\n",
            "Global step: 1362,loss: 0.014244466\n",
            "\n",
            "Global step: 1363,loss: 0.013374368\n",
            "\n",
            "Global step: 1364,loss: 0.013886771\n",
            "\n",
            "Global step: 1365,loss: 0.014271614\n",
            "\n",
            "Global step: 1366,loss: 0.016256468\n",
            "\n",
            "Global step: 1367,loss: 0.013649947\n",
            "\n",
            "Global step: 1368,loss: 0.013355528\n",
            "\n",
            "Global step: 1369,loss: 0.016009938\n",
            "\n",
            "Global step: 1370,loss: 0.014304213\n",
            "\n",
            "Global step: 1371,loss: 0.014737256\n",
            "\n",
            "Global step: 1372,loss: 0.014797448\n",
            "\n",
            "Global step: 1373,loss: 0.012315227\n",
            "\n",
            "Global step: 1374,loss: 0.013511773\n",
            "\n",
            "Global step: 1375,loss: 0.017825987\n",
            "\n",
            "Global step: 1376,loss: 0.0133490525\n",
            "\n",
            "Global step: 1377,loss: 0.013408044\n",
            "\n",
            "Global step: 1378,loss: 0.013197986\n",
            "\n",
            "Global step: 1379,loss: 0.014904771\n",
            "\n",
            "Global step: 1380,loss: 0.015571924\n",
            "\n",
            "Global step: 1381,loss: 0.012823893\n",
            "\n",
            "Global step: 1382,loss: 0.015334941\n",
            "\n",
            "Global step: 1383,loss: 0.016561832\n",
            "\n",
            "Global step: 1384,loss: 0.012224438\n",
            "\n",
            "INFO:tensorflow:global_step/sec: 1.26667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:26:29.416671 140673493931776 supervisor.py:1099] global_step/sec: 1.26667\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1385,loss: 0.015043743\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1386.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:26:29.956387 140673502324480 supervisor.py:1050] Recording summary at step 1386.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1386,loss: 0.0155576\n",
            "\n",
            "Global step: 1387,loss: 0.017959129\n",
            "\n",
            "Global step: 1388,loss: 0.014408864\n",
            "\n",
            "Global step: 1389,loss: 0.013016608\n",
            "\n",
            "Global step: 1390,loss: 0.014570812\n",
            "\n",
            "Global step: 1391,loss: 0.014541375\n",
            "\n",
            "Global step: 1392,loss: 0.015968814\n",
            "\n",
            "Global step: 1393,loss: 0.01294745\n",
            "\n",
            "Global step: 1394,loss: 0.01354453\n",
            "\n",
            "Global step: 1395,loss: 0.019990636\n",
            "\n",
            "Global step: 1396,loss: 0.01396878\n",
            "\n",
            "Global step: 1397,loss: 0.013926631\n",
            "\n",
            "Global step: 1398,loss: 0.015614705\n",
            "\n",
            "Global step: 1399,loss: 0.012928603\n",
            "\n",
            "Global step: 1400,loss: 0.012704376\n",
            "\n",
            "Global step: 1401,loss: 0.013812354\n",
            "\n",
            "Global step: 1402,loss: 0.012573576\n",
            "\n",
            "Global step: 1403,loss: 0.01483796\n",
            "\n",
            "Global step: 1404,loss: 0.013289022\n",
            "\n",
            "Global step: 1405,loss: 0.014207374\n",
            "\n",
            "Global step: 1406,loss: 0.014432916\n",
            "\n",
            "Global step: 1407,loss: 0.012669531\n",
            "\n",
            "Global step: 1408,loss: 0.015308897\n",
            "\n",
            "Global step: 1409,loss: 0.014809947\n",
            "\n",
            "Global step: 1410,loss: 0.013045261\n",
            "\n",
            "Global step: 1411,loss: 0.013874644\n",
            "\n",
            "Global step: 1412,loss: 0.016876297\n",
            "\n",
            "Global step: 1413,loss: 0.018443152\n",
            "\n",
            "Global step: 1414,loss: 0.013926364\n",
            "\n",
            "Global step: 1415,loss: 0.015277369\n",
            "\n",
            "Global step: 1416,loss: 0.016065596\n",
            "\n",
            "Global step: 1417,loss: 0.01272975\n",
            "\n",
            "Global step: 1418,loss: 0.018961303\n",
            "\n",
            "Global step: 1419,loss: 0.015067675\n",
            "\n",
            "Global step: 1420,loss: 0.014957256\n",
            "\n",
            "Global step: 1421,loss: 0.01334077\n",
            "\n",
            "Global step: 1422,loss: 0.013261422\n",
            "\n",
            "Global step: 1423,loss: 0.01352301\n",
            "\n",
            "Global step: 1424,loss: 0.013261558\n",
            "\n",
            "Global step: 1425,loss: 0.020555697\n",
            "\n",
            "Global step: 1426,loss: 0.013463328\n",
            "\n",
            "Global step: 1427,loss: 0.015987191\n",
            "\n",
            "Global step: 1428,loss: 0.013833579\n",
            "\n",
            "Global step: 1429,loss: 0.012980962\n",
            "\n",
            "Global step: 1430,loss: 0.01576536\n",
            "\n",
            "Global step: 1431,loss: 0.013341148\n",
            "\n",
            "Global step: 1432,loss: 0.014940558\n",
            "\n",
            "Global step: 1433,loss: 0.013240578\n",
            "\n",
            "Global step: 1434,loss: 0.014824842\n",
            "\n",
            "Global step: 1435,loss: 0.013873985\n",
            "\n",
            "Global step: 1436,loss: 0.016763598\n",
            "\n",
            "Global step: 1437,loss: 0.014340913\n",
            "\n",
            "Global step: 1438,loss: 0.01484721\n",
            "\n",
            "Global step: 1439,loss: 0.014927629\n",
            "\n",
            "Global step: 1440,loss: 0.015213439\n",
            "\n",
            "Global step: 1441,loss: 0.01250749\n",
            "\n",
            "Global step: 1442,loss: 0.012667879\n",
            "\n",
            "Global step: 1443,loss: 0.01223315\n",
            "\n",
            "Global step: 1444,loss: 0.01261797\n",
            "\n",
            "Global step: 1445,loss: 0.016482407\n",
            "\n",
            "Global step: 1446,loss: 0.01280269\n",
            "\n",
            "Global step: 1447,loss: 0.013813451\n",
            "\n",
            "Global step: 1448,loss: 0.013105135\n",
            "\n",
            "Global step: 1449,loss: 0.016291257\n",
            "\n",
            "Global step: 1450,loss: 0.012199624\n",
            "\n",
            "Global step: 1451,loss: 0.013326441\n",
            "\n",
            "Global step: 1452,loss: 0.013467019\n",
            "\n",
            "Global step: 1453,loss: 0.014214488\n",
            "\n",
            "Global step: 1454,loss: 0.014851833\n",
            "\n",
            "Global step: 1455,loss: 0.015968716\n",
            "\n",
            "Global step: 1456,loss: 0.015176499\n",
            "\n",
            "Global step: 1457,loss: 0.013921222\n",
            "\n",
            "Global step: 1458,loss: 0.014123115\n",
            "\n",
            "Global step: 1459,loss: 0.012351862\n",
            "\n",
            "Global step: 1460,loss: 0.012406679\n",
            "\n",
            "Global step: 1461,loss: 0.013471173\n",
            "\n",
            "Global step: 1462,loss: 0.0144668\n",
            "\n",
            "Global step: 1463,loss: 0.01752592\n",
            "\n",
            "Global step: 1464,loss: 0.012919733\n",
            "\n",
            "Global step: 1465,loss: 0.012890511\n",
            "\n",
            "Global step: 1466,loss: 0.012576453\n",
            "\n",
            "Global step: 1467,loss: 0.012992874\n",
            "\n",
            "Global step: 1468,loss: 0.013986654\n",
            "\n",
            "Global step: 1469,loss: 0.0136147365\n",
            "\n",
            "Global step: 1470,loss: 0.0131028555\n",
            "\n",
            "Global step: 1471,loss: 0.014663221\n",
            "\n",
            "Global step: 1472,loss: 0.012594733\n",
            "\n",
            "Global step: 1473,loss: 0.0128489705\n",
            "\n",
            "Global step: 1474,loss: 0.016470063\n",
            "\n",
            "Global step: 1475,loss: 0.014526596\n",
            "\n",
            "Global step: 1476,loss: 0.012891829\n",
            "\n",
            "Global step: 1477,loss: 0.012327084\n",
            "\n",
            "Global step: 1478,loss: 0.013016592\n",
            "\n",
            "Global step: 1479,loss: 0.012730596\n",
            "\n",
            "Global step: 1480,loss: 0.014842016\n",
            "\n",
            "Global step: 1481,loss: 0.01522347\n",
            "\n",
            "Global step: 1482,loss: 0.01325905\n",
            "\n",
            "Global step: 1483,loss: 0.0127423555\n",
            "\n",
            "Global step: 1484,loss: 0.01214642\n",
            "\n",
            "Global step: 1485,loss: 0.013582762\n",
            "\n",
            "Global step: 1486,loss: 0.012445626\n",
            "\n",
            "Global step: 1487,loss: 0.01243064\n",
            "\n",
            "Global step: 1488,loss: 0.014274906\n",
            "\n",
            "Global step: 1489,loss: 0.0135337375\n",
            "\n",
            "Global step: 1490,loss: 0.0133591425\n",
            "\n",
            "Global step: 1491,loss: 0.014315314\n",
            "\n",
            "Global step: 1492,loss: 0.012934784\n",
            "\n",
            "Global step: 1493,loss: 0.01602062\n",
            "\n",
            "Global step: 1494,loss: 0.013406941\n",
            "\n",
            "Global step: 1495,loss: 0.013730349\n",
            "\n",
            "Global step: 1496,loss: 0.014154467\n",
            "\n",
            "Global step: 1497,loss: 0.014097267\n",
            "\n",
            "\n",
            "NOT SAVING MODEL!!\n",
            "Global Step: 1497,val_loss: 6.046088770074819e-05\n",
            "\n",
            "Training for epoch 7/15:\n",
            "Global step: 1498,loss: 0.012202379\n",
            "\n",
            "Global step: 1499,loss: 0.013112105\n",
            "\n",
            "Global step: 1500,loss: 0.01447306\n",
            "\n",
            "Global step: 1501,loss: 0.014187539\n",
            "\n",
            "Global step: 1502,loss: 0.012798006\n",
            "\n",
            "Global step: 1503,loss: 0.014154002\n",
            "\n",
            "Global step: 1504,loss: 0.013333858\n",
            "\n",
            "Global step: 1505,loss: 0.013135301\n",
            "\n",
            "Global step: 1506,loss: 0.013497092\n",
            "\n",
            "Global step: 1507,loss: 0.012680095\n",
            "\n",
            "Global step: 1508,loss: 0.015893511\n",
            "\n",
            "Global step: 1509,loss: 0.013745941\n",
            "\n",
            "Global step: 1510,loss: 0.01156893\n",
            "\n",
            "Global step: 1511,loss: 0.013274783\n",
            "\n",
            "Global step: 1512,loss: 0.012392283\n",
            "\n",
            "Global step: 1513,loss: 0.011818443\n",
            "\n",
            "Global step: 1514,loss: 0.012202287\n",
            "\n",
            "Global step: 1515,loss: 0.0122321015\n",
            "\n",
            "Global step: 1516,loss: 0.013222967\n",
            "\n",
            "Global step: 1517,loss: 0.012572578\n",
            "\n",
            "Global step: 1518,loss: 0.012030604\n",
            "\n",
            "Global step: 1519,loss: 0.013979631\n",
            "\n",
            "Global step: 1520,loss: 0.012233392\n",
            "\n",
            "Global step: 1521,loss: 0.012200138\n",
            "\n",
            "Global step: 1522,loss: 0.013455695\n",
            "\n",
            "Global step: 1523,loss: 0.012179956\n",
            "\n",
            "Global step: 1524,loss: 0.012194973\n",
            "\n",
            "Global step: 1525,loss: 0.012412229\n",
            "\n",
            "Global step: 1526,loss: 0.01184216\n",
            "\n",
            "Global step: 1527,loss: 0.012430395\n",
            "\n",
            "Global step: 1528,loss: 0.013005786\n",
            "\n",
            "Global step: 1529,loss: 0.012148624\n",
            "\n",
            "Global step: 1530,loss: 0.012712341\n",
            "\n",
            "Global step: 1531,loss: 0.014511263\n",
            "\n",
            "Global step: 1532,loss: 0.012135095\n",
            "\n",
            "Global step: 1533,loss: 0.014681924\n",
            "\n",
            "Global step: 1534,loss: 0.012807778\n",
            "\n",
            "Global step: 1535,loss: 0.011957258\n",
            "\n",
            "Global step: 1536,loss: 0.013383016\n",
            "\n",
            "Global step: 1537,loss: 0.017812986\n",
            "\n",
            "Global step: 1538,loss: 0.013253429\n",
            "\n",
            "Global step: 1539,loss: 0.016358333\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1540.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:28:29.803266 140673502324480 supervisor.py:1050] Recording summary at step 1540.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1540,loss: 0.013739933\n",
            "\n",
            "Global step: 1541,loss: 0.0132479\n",
            "\n",
            "Global step: 1542,loss: 0.013275119\n",
            "\n",
            "Global step: 1543,loss: 0.0133300545\n",
            "\n",
            "Global step: 1544,loss: 0.012632352\n",
            "\n",
            "Global step: 1545,loss: 0.019618556\n",
            "\n",
            "Global step: 1546,loss: 0.013814053\n",
            "\n",
            "Global step: 1547,loss: 0.015124542\n",
            "\n",
            "Global step: 1548,loss: 0.012549457\n",
            "\n",
            "Global step: 1549,loss: 0.012333259\n",
            "\n",
            "Global step: 1550,loss: 0.01381647\n",
            "\n",
            "Global step: 1551,loss: 0.014764618\n",
            "\n",
            "Global step: 1552,loss: 0.012315668\n",
            "\n",
            "Global step: 1553,loss: 0.013721957\n",
            "\n",
            "Global step: 1554,loss: 0.013501024\n",
            "\n",
            "Global step: 1555,loss: 0.0132411495\n",
            "\n",
            "Global step: 1556,loss: 0.014698723\n",
            "\n",
            "Global step: 1557,loss: 0.0119967535\n",
            "\n",
            "Global step: 1558,loss: 0.014013766\n",
            "\n",
            "Global step: 1559,loss: 0.016187198\n",
            "\n",
            "Global step: 1560,loss: 0.01514476\n",
            "\n",
            "Global step: 1561,loss: 0.013833311\n",
            "\n",
            "Global step: 1562,loss: 0.014951747\n",
            "\n",
            "Global step: 1563,loss: 0.015763955\n",
            "\n",
            "Global step: 1564,loss: 0.014196687\n",
            "\n",
            "Global step: 1565,loss: 0.013757247\n",
            "\n",
            "Global step: 1566,loss: 0.013005364\n",
            "\n",
            "Global step: 1567,loss: 0.012151082\n",
            "\n",
            "Global step: 1568,loss: 0.0143117495\n",
            "\n",
            "Global step: 1569,loss: 0.012960859\n",
            "\n",
            "Global step: 1570,loss: 0.013520423\n",
            "\n",
            "Global step: 1571,loss: 0.0131593235\n",
            "\n",
            "Global step: 1572,loss: 0.014810856\n",
            "\n",
            "Global step: 1573,loss: 0.013192364\n",
            "\n",
            "Global step: 1574,loss: 0.014358476\n",
            "\n",
            "Global step: 1575,loss: 0.013050416\n",
            "\n",
            "Global step: 1576,loss: 0.012621419\n",
            "\n",
            "Global step: 1577,loss: 0.012393762\n",
            "\n",
            "Global step: 1578,loss: 0.014558936\n",
            "\n",
            "Global step: 1579,loss: 0.012654626\n",
            "\n",
            "Global step: 1580,loss: 0.013178615\n",
            "\n",
            "Global step: 1581,loss: 0.012024896\n",
            "\n",
            "Global step: 1582,loss: 0.011303956\n",
            "\n",
            "Global step: 1583,loss: 0.013371391\n",
            "\n",
            "Global step: 1584,loss: 0.013237711\n",
            "\n",
            "Global step: 1585,loss: 0.01590814\n",
            "\n",
            "Global step: 1586,loss: 0.014798044\n",
            "\n",
            "Global step: 1587,loss: 0.012738197\n",
            "\n",
            "Global step: 1588,loss: 0.015250211\n",
            "\n",
            "Global step: 1589,loss: 0.0119205965\n",
            "\n",
            "Global step: 1590,loss: 0.01250733\n",
            "\n",
            "Global step: 1591,loss: 0.013305012\n",
            "\n",
            "Global step: 1592,loss: 0.013059395\n",
            "\n",
            "Global step: 1593,loss: 0.014134903\n",
            "\n",
            "Global step: 1594,loss: 0.01727214\n",
            "\n",
            "Global step: 1595,loss: 0.012229006\n",
            "\n",
            "Global step: 1596,loss: 0.014197495\n",
            "\n",
            "Global step: 1597,loss: 0.015912585\n",
            "\n",
            "Global step: 1598,loss: 0.013790751\n",
            "\n",
            "Global step: 1599,loss: 0.013854998\n",
            "\n",
            "Global step: 1600,loss: 0.012020984\n",
            "\n",
            "Global step: 1601,loss: 0.0132739805\n",
            "\n",
            "Global step: 1602,loss: 0.012777993\n",
            "\n",
            "Global step: 1603,loss: 0.012425544\n",
            "\n",
            "Global step: 1604,loss: 0.011851192\n",
            "\n",
            "Global step: 1605,loss: 0.012756402\n",
            "\n",
            "Global step: 1606,loss: 0.01274091\n",
            "\n",
            "Global step: 1607,loss: 0.014234137\n",
            "\n",
            "Global step: 1608,loss: 0.0129774045\n",
            "\n",
            "Global step: 1609,loss: 0.012841947\n",
            "\n",
            "Global step: 1610,loss: 0.012452997\n",
            "\n",
            "Global step: 1611,loss: 0.014654852\n",
            "\n",
            "Global step: 1612,loss: 0.012687577\n",
            "\n",
            "Global step: 1613,loss: 0.011709204\n",
            "\n",
            "Global step: 1614,loss: 0.015404195\n",
            "\n",
            "Global step: 1615,loss: 0.012790607\n",
            "\n",
            "Global step: 1616,loss: 0.014314378\n",
            "\n",
            "Global step: 1617,loss: 0.0137852235\n",
            "\n",
            "Global step: 1618,loss: 0.013125209\n",
            "\n",
            "Global step: 1619,loss: 0.0133230295\n",
            "\n",
            "Global step: 1620,loss: 0.015344523\n",
            "\n",
            "Global step: 1621,loss: 0.014241083\n",
            "\n",
            "Global step: 1622,loss: 0.014073998\n",
            "\n",
            "Global step: 1623,loss: 0.013764992\n",
            "\n",
            "Global step: 1624,loss: 0.017595962\n",
            "\n",
            "Global step: 1625,loss: 0.013973167\n",
            "\n",
            "Global step: 1626,loss: 0.0151503235\n",
            "\n",
            "Global step: 1627,loss: 0.013670962\n",
            "\n",
            "Global step: 1628,loss: 0.0137052005\n",
            "\n",
            "Global step: 1629,loss: 0.018252898\n",
            "\n",
            "Global step: 1630,loss: 0.012952086\n",
            "\n",
            "Global step: 1631,loss: 0.013578026\n",
            "\n",
            "Global step: 1632,loss: 0.011918144\n",
            "\n",
            "Global step: 1633,loss: 0.013228616\n",
            "\n",
            "Global step: 1634,loss: 0.014316473\n",
            "\n",
            "Global step: 1635,loss: 0.015714766\n",
            "\n",
            "Global step: 1636,loss: 0.012923781\n",
            "\n",
            "Global step: 1637,loss: 0.012195531\n",
            "\n",
            "Global step: 1638,loss: 0.011566449\n",
            "\n",
            "Global step: 1639,loss: 0.0142518785\n",
            "\n",
            "Global step: 1640,loss: 0.013483815\n",
            "\n",
            "Global step: 1641,loss: 0.0125865005\n",
            "\n",
            "Global step: 1642,loss: 0.012017562\n",
            "\n",
            "Global step: 1643,loss: 0.013205136\n",
            "\n",
            "Global step: 1644,loss: 0.011623009\n",
            "\n",
            "Global step: 1645,loss: 0.014789986\n",
            "\n",
            "Global step: 1646,loss: 0.016217114\n",
            "\n",
            "Global step: 1647,loss: 0.011547964\n",
            "\n",
            "Global step: 1648,loss: 0.012758678\n",
            "\n",
            "Global step: 1649,loss: 0.013445963\n",
            "\n",
            "Global step: 1650,loss: 0.013291006\n",
            "\n",
            "Global step: 1651,loss: 0.016019884\n",
            "\n",
            "Global step: 1652,loss: 0.012643126\n",
            "\n",
            "Global step: 1653,loss: 0.013530653\n",
            "\n",
            "Global step: 1654,loss: 0.0125038605\n",
            "\n",
            "Global step: 1655,loss: 0.012716335\n",
            "\n",
            "Global step: 1656,loss: 0.01163968\n",
            "\n",
            "Global step: 1657,loss: 0.015649311\n",
            "\n",
            "Global step: 1658,loss: 0.015335344\n",
            "\n",
            "Global step: 1659,loss: 0.012507197\n",
            "\n",
            "Global step: 1660,loss: 0.014530392\n",
            "\n",
            "Global step: 1661,loss: 0.01355838\n",
            "\n",
            "Global step: 1662,loss: 0.013380584\n",
            "\n",
            "Global step: 1663,loss: 0.0136896875\n",
            "\n",
            "Global step: 1664,loss: 0.012651505\n",
            "\n",
            "Global step: 1665,loss: 0.013276688\n",
            "\n",
            "Global step: 1666,loss: 0.012840254\n",
            "\n",
            "Global step: 1667,loss: 0.012412752\n",
            "\n",
            "Global step: 1668,loss: 0.015970077\n",
            "\n",
            "Global step: 1669,loss: 0.013493246\n",
            "\n",
            "Global step: 1670,loss: 0.0132813845\n",
            "\n",
            "Global step: 1671,loss: 0.012946579\n",
            "\n",
            "Global step: 1672,loss: 0.012637966\n",
            "\n",
            "Global step: 1673,loss: 0.012120311\n",
            "\n",
            "Global step: 1674,loss: 0.013483554\n",
            "\n",
            "Global step: 1675,loss: 0.012575007\n",
            "\n",
            "Global step: 1676,loss: 0.012536856\n",
            "\n",
            "Global step: 1677,loss: 0.012173725\n",
            "\n",
            "Global step: 1678,loss: 0.012615586\n",
            "\n",
            "Global step: 1679,loss: 0.013775684\n",
            "\n",
            "Global step: 1680,loss: 0.01241758\n",
            "\n",
            "Global step: 1681,loss: 0.012276707\n",
            "\n",
            "Global step: 1682,loss: 0.0126460185\n",
            "\n",
            "Global step: 1683,loss: 0.0129328\n",
            "\n",
            "Global step: 1684,loss: 0.013780512\n",
            "\n",
            "Global step: 1685,loss: 0.011949557\n",
            "\n",
            "Global step: 1686,loss: 0.013085227\n",
            "\n",
            "Global step: 1687,loss: 0.012948858\n",
            "\n",
            "Global step: 1688,loss: 0.015069872\n",
            "\n",
            "Global step: 1689,loss: 0.0140490625\n",
            "\n",
            "Global step: 1690,loss: 0.01337268\n",
            "\n",
            "Global step: 1691,loss: 0.011826277\n",
            "\n",
            "Global step: 1692,loss: 0.016602648\n",
            "\n",
            "Global step: 1693,loss: 0.012239309\n",
            "\n",
            "Global step: 1694,loss: 0.012259282\n",
            "\n",
            "Global step: 1695,loss: 0.011825096\n",
            "\n",
            "Global step: 1696,loss: 0.012068029\n",
            "\n",
            "Global step: 1697,loss: 0.012940619\n",
            "\n",
            "Global step: 1698,loss: 0.013915708\n",
            "\n",
            "Global step: 1699,loss: 0.012270026\n",
            "\n",
            "Global step: 1700,loss: 0.014056287\n",
            "\n",
            "Global step: 1701,loss: 0.01200585\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1702.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:30:29.885490 140673502324480 supervisor.py:1050] Recording summary at step 1702.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1702,loss: 0.0162395\n",
            "\n",
            "Global step: 1703,loss: 0.012784677\n",
            "\n",
            "Global step: 1704,loss: 0.012459239\n",
            "\n",
            "Global step: 1705,loss: 0.011853931\n",
            "\n",
            "Global step: 1706,loss: 0.012017581\n",
            "\n",
            "Global step: 1707,loss: 0.011968138\n",
            "\n",
            "Global step: 1708,loss: 0.012074731\n",
            "\n",
            "Global step: 1709,loss: 0.012136573\n",
            "\n",
            "Global step: 1710,loss: 0.01236776\n",
            "\n",
            "Global step: 1711,loss: 0.01256423\n",
            "\n",
            "Global Step: 1711,Val_Loss: 5.6804578267282956e-05,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:30:43.697344 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 8/15:\n",
            "Global step: 1712,loss: 0.0116839195\n",
            "\n",
            "Global step: 1713,loss: 0.013693275\n",
            "\n",
            "Global step: 1714,loss: 0.011875777\n",
            "\n",
            "Global step: 1715,loss: 0.011996902\n",
            "\n",
            "Global step: 1716,loss: 0.01449794\n",
            "\n",
            "Global step: 1717,loss: 0.012942687\n",
            "\n",
            "Global step: 1718,loss: 0.01290903\n",
            "\n",
            "Global step: 1719,loss: 0.013764139\n",
            "\n",
            "Global step: 1720,loss: 0.01197678\n",
            "\n",
            "Global step: 1721,loss: 0.012081116\n",
            "\n",
            "Global step: 1722,loss: 0.011837855\n",
            "\n",
            "Global step: 1723,loss: 0.011795606\n",
            "\n",
            "Global step: 1724,loss: 0.0133974105\n",
            "\n",
            "Global step: 1725,loss: 0.012601215\n",
            "\n",
            "Global step: 1726,loss: 0.013640429\n",
            "\n",
            "Global step: 1727,loss: 0.013462351\n",
            "\n",
            "Global step: 1728,loss: 0.01327639\n",
            "\n",
            "Global step: 1729,loss: 0.012624008\n",
            "\n",
            "Global step: 1730,loss: 0.012209259\n",
            "\n",
            "Global step: 1731,loss: 0.013380797\n",
            "\n",
            "Global step: 1732,loss: 0.013708094\n",
            "\n",
            "Global step: 1733,loss: 0.011916026\n",
            "\n",
            "Global step: 1734,loss: 0.01406276\n",
            "\n",
            "Global step: 1735,loss: 0.011950031\n",
            "\n",
            "Global step: 1736,loss: 0.014480835\n",
            "\n",
            "Global step: 1737,loss: 0.013852163\n",
            "\n",
            "Global step: 1738,loss: 0.013983637\n",
            "\n",
            "Global step: 1739,loss: 0.012493003\n",
            "\n",
            "Global step: 1740,loss: 0.014754076\n",
            "\n",
            "Global step: 1741,loss: 0.0119912075\n",
            "\n",
            "Global step: 1742,loss: 0.011718852\n",
            "\n",
            "Global step: 1743,loss: 0.012668297\n",
            "\n",
            "Global step: 1744,loss: 0.012498528\n",
            "\n",
            "Global step: 1745,loss: 0.012088442\n",
            "\n",
            "Global step: 1746,loss: 0.015787542\n",
            "\n",
            "Global step: 1747,loss: 0.014185209\n",
            "\n",
            "Global step: 1748,loss: 0.012746304\n",
            "\n",
            "Global step: 1749,loss: 0.012937826\n",
            "\n",
            "Global step: 1750,loss: 0.012826194\n",
            "\n",
            "Global step: 1751,loss: 0.012822254\n",
            "\n",
            "Global step: 1752,loss: 0.014349436\n",
            "\n",
            "Global step: 1753,loss: 0.012619974\n",
            "\n",
            "Global step: 1754,loss: 0.011810432\n",
            "\n",
            "Global step: 1755,loss: 0.011401786\n",
            "\n",
            "Global step: 1756,loss: 0.012475209\n",
            "\n",
            "Global step: 1757,loss: 0.013972572\n",
            "\n",
            "Global step: 1758,loss: 0.011541993\n",
            "\n",
            "Global step: 1759,loss: 0.011579924\n",
            "\n",
            "Global step: 1760,loss: 0.0129265515\n",
            "\n",
            "Global step: 1761,loss: 0.0117971385\n",
            "\n",
            "Global step: 1762,loss: 0.011702015\n",
            "\n",
            "Global step: 1763,loss: 0.012532709\n",
            "\n",
            "Global step: 1764,loss: 0.011644256\n",
            "\n",
            "Global step: 1765,loss: 0.013044399\n",
            "\n",
            "Global step: 1766,loss: 0.0129497675\n",
            "\n",
            "Global step: 1767,loss: 0.012453533\n",
            "\n",
            "Global step: 1768,loss: 0.014784845\n",
            "\n",
            "Global step: 1769,loss: 0.015346453\n",
            "\n",
            "Global step: 1770,loss: 0.012943869\n",
            "\n",
            "Global step: 1771,loss: 0.012566373\n",
            "\n",
            "Global step: 1772,loss: 0.012622091\n",
            "\n",
            "Global step: 1773,loss: 0.013318926\n",
            "\n",
            "Global step: 1774,loss: 0.01417638\n",
            "\n",
            "Global step: 1775,loss: 0.012441968\n",
            "\n",
            "Global step: 1776,loss: 0.012106174\n",
            "\n",
            "Global step: 1777,loss: 0.013477153\n",
            "\n",
            "Global step: 1778,loss: 0.012610495\n",
            "\n",
            "Global step: 1779,loss: 0.014011265\n",
            "\n",
            "Global step: 1780,loss: 0.0120240785\n",
            "\n",
            "Global step: 1781,loss: 0.01171911\n",
            "\n",
            "Global step: 1782,loss: 0.011874663\n",
            "\n",
            "Global step: 1783,loss: 0.012026855\n",
            "\n",
            "Global step: 1784,loss: 0.012443231\n",
            "\n",
            "Global step: 1785,loss: 0.0120517835\n",
            "\n",
            "Global step: 1786,loss: 0.012709813\n",
            "\n",
            "Global step: 1787,loss: 0.013970535\n",
            "\n",
            "Global step: 1788,loss: 0.011716344\n",
            "\n",
            "Global step: 1789,loss: 0.012895485\n",
            "\n",
            "Global step: 1790,loss: 0.012011386\n",
            "\n",
            "Global step: 1791,loss: 0.012023388\n",
            "\n",
            "Global step: 1792,loss: 0.011541026\n",
            "\n",
            "Global step: 1793,loss: 0.014447786\n",
            "\n",
            "Global step: 1794,loss: 0.012904847\n",
            "\n",
            "Global step: 1795,loss: 0.01172871\n",
            "\n",
            "Global step: 1796,loss: 0.012432171\n",
            "\n",
            "Global step: 1797,loss: 0.015559327\n",
            "\n",
            "Global step: 1798,loss: 0.012536206\n",
            "\n",
            "Global step: 1799,loss: 0.011403596\n",
            "\n",
            "Global step: 1800,loss: 0.012429861\n",
            "\n",
            "Global step: 1801,loss: 0.011885111\n",
            "\n",
            "Global step: 1802,loss: 0.0117623005\n",
            "\n",
            "Global step: 1803,loss: 0.01161911\n",
            "\n",
            "Global step: 1804,loss: 0.015454549\n",
            "\n",
            "Global step: 1805,loss: 0.016598372\n",
            "\n",
            "Global step: 1806,loss: 0.0135693895\n",
            "\n",
            "Global step: 1807,loss: 0.011791393\n",
            "\n",
            "Global step: 1808,loss: 0.011792927\n",
            "\n",
            "Global step: 1809,loss: 0.013465001\n",
            "\n",
            "Global step: 1810,loss: 0.01316013\n",
            "\n",
            "Global step: 1811,loss: 0.012773746\n",
            "\n",
            "Global step: 1812,loss: 0.011987024\n",
            "\n",
            "Global step: 1813,loss: 0.012931863\n",
            "\n",
            "Global step: 1814,loss: 0.012013882\n",
            "\n",
            "Global step: 1815,loss: 0.013936967\n",
            "\n",
            "Global step: 1816,loss: 0.012681052\n",
            "\n",
            "Global step: 1817,loss: 0.011375065\n",
            "\n",
            "Global step: 1818,loss: 0.011673247\n",
            "\n",
            "Global step: 1819,loss: 0.012633745\n",
            "\n",
            "Global step: 1820,loss: 0.013436215\n",
            "\n",
            "Global step: 1821,loss: 0.012071442\n",
            "\n",
            "Global step: 1822,loss: 0.014072527\n",
            "\n",
            "Global step: 1823,loss: 0.012359895\n",
            "\n",
            "Global step: 1824,loss: 0.01274092\n",
            "\n",
            "Global step: 1825,loss: 0.012358993\n",
            "\n",
            "Global step: 1826,loss: 0.01286485\n",
            "\n",
            "Global step: 1827,loss: 0.012037106\n",
            "\n",
            "Global step: 1828,loss: 0.012146562\n",
            "\n",
            "Global step: 1829,loss: 0.014080666\n",
            "\n",
            "Global step: 1830,loss: 0.012210229\n",
            "\n",
            "Global step: 1831,loss: 0.011887215\n",
            "\n",
            "Global step: 1832,loss: 0.014021978\n",
            "\n",
            "Global step: 1833,loss: 0.012462632\n",
            "\n",
            "Global step: 1834,loss: 0.011783595\n",
            "\n",
            "Global step: 1835,loss: 0.011802797\n",
            "\n",
            "Global step: 1836,loss: 0.014638911\n",
            "\n",
            "Global step: 1837,loss: 0.013240638\n",
            "\n",
            "Global step: 1838,loss: 0.012281323\n",
            "\n",
            "Global step: 1839,loss: 0.012139158\n",
            "\n",
            "Global step: 1840,loss: 0.011903158\n",
            "\n",
            "Global step: 1841,loss: 0.012591409\n",
            "\n",
            "Global step: 1842,loss: 0.012821901\n",
            "\n",
            "Global step: 1843,loss: 0.012652191\n",
            "\n",
            "Global step: 1844,loss: 0.011380047\n",
            "\n",
            "Global step: 1845,loss: 0.013525862\n",
            "\n",
            "Global step: 1846,loss: 0.012031238\n",
            "\n",
            "Global step: 1847,loss: 0.011738094\n",
            "\n",
            "Global step: 1848,loss: 0.012721884\n",
            "\n",
            "Global step: 1849,loss: 0.011668678\n",
            "\n",
            "Global step: 1850,loss: 0.013105324\n",
            "\n",
            "Global step: 1851,loss: 0.012598781\n",
            "\n",
            "Global step: 1852,loss: 0.013035748\n",
            "\n",
            "Global step: 1853,loss: 0.01166124\n",
            "\n",
            "Global step: 1854,loss: 0.013579117\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 1855.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:32:30.315953 140673502324480 supervisor.py:1050] Recording summary at step 1855.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 1855,loss: 0.011429689\n",
            "\n",
            "Global step: 1856,loss: 0.015727542\n",
            "\n",
            "Global step: 1857,loss: 0.0148764225\n",
            "\n",
            "Global step: 1858,loss: 0.01209918\n",
            "\n",
            "Global step: 1859,loss: 0.0125810355\n",
            "\n",
            "Global step: 1860,loss: 0.018546537\n",
            "\n",
            "Global step: 1861,loss: 0.012388331\n",
            "\n",
            "Global step: 1862,loss: 0.012453926\n",
            "\n",
            "Global step: 1863,loss: 0.012063021\n",
            "\n",
            "Global step: 1864,loss: 0.012988577\n",
            "\n",
            "Global step: 1865,loss: 0.015615262\n",
            "\n",
            "Global step: 1866,loss: 0.0143769635\n",
            "\n",
            "Global step: 1867,loss: 0.011205939\n",
            "\n",
            "Global step: 1868,loss: 0.013766331\n",
            "\n",
            "Global step: 1869,loss: 0.012105349\n",
            "\n",
            "Global step: 1870,loss: 0.014896451\n",
            "\n",
            "Global step: 1871,loss: 0.011692007\n",
            "\n",
            "Global step: 1872,loss: 0.012755743\n",
            "\n",
            "Global step: 1873,loss: 0.0138926\n",
            "\n",
            "Global step: 1874,loss: 0.014532617\n",
            "\n",
            "Global step: 1875,loss: 0.012694855\n",
            "\n",
            "Global step: 1876,loss: 0.0120193465\n",
            "\n",
            "Global step: 1877,loss: 0.011773618\n",
            "\n",
            "Global step: 1878,loss: 0.012328302\n",
            "\n",
            "Global step: 1879,loss: 0.012669364\n",
            "\n",
            "Global step: 1880,loss: 0.013835911\n",
            "\n",
            "Global step: 1881,loss: 0.012613042\n",
            "\n",
            "Global step: 1882,loss: 0.012019612\n",
            "\n",
            "Global step: 1883,loss: 0.012658383\n",
            "\n",
            "Global step: 1884,loss: 0.011450062\n",
            "\n",
            "Global step: 1885,loss: 0.012890999\n",
            "\n",
            "Global step: 1886,loss: 0.014443251\n",
            "\n",
            "Global step: 1887,loss: 0.01284258\n",
            "\n",
            "Global step: 1888,loss: 0.012996152\n",
            "\n",
            "Global step: 1889,loss: 0.012354566\n",
            "\n",
            "Global step: 1890,loss: 0.011629603\n",
            "\n",
            "Global step: 1891,loss: 0.013738942\n",
            "\n",
            "Global step: 1892,loss: 0.012914308\n",
            "\n",
            "Global step: 1893,loss: 0.012970951\n",
            "\n",
            "Global step: 1894,loss: 0.014644045\n",
            "\n",
            "Global step: 1895,loss: 0.0122015495\n",
            "\n",
            "Global step: 1896,loss: 0.012844401\n",
            "\n",
            "Global step: 1897,loss: 0.011522082\n",
            "\n",
            "Global step: 1898,loss: 0.013438075\n",
            "\n",
            "Global step: 1899,loss: 0.011719034\n",
            "\n",
            "Global step: 1900,loss: 0.011769786\n",
            "\n",
            "Global step: 1901,loss: 0.012863396\n",
            "\n",
            "Global step: 1902,loss: 0.01331182\n",
            "\n",
            "Global step: 1903,loss: 0.012797035\n",
            "\n",
            "Global step: 1904,loss: 0.011584861\n",
            "\n",
            "Global step: 1905,loss: 0.012732208\n",
            "\n",
            "Global step: 1906,loss: 0.01775458\n",
            "\n",
            "Global step: 1907,loss: 0.011228312\n",
            "\n",
            "Global step: 1908,loss: 0.012046626\n",
            "\n",
            "Global step: 1909,loss: 0.0153184\n",
            "\n",
            "Global step: 1910,loss: 0.012217216\n",
            "\n",
            "Global step: 1911,loss: 0.011191576\n",
            "\n",
            "Global step: 1912,loss: 0.013545845\n",
            "\n",
            "Global step: 1913,loss: 0.010901582\n",
            "\n",
            "Global step: 1914,loss: 0.0114505105\n",
            "\n",
            "Global step: 1915,loss: 0.011602245\n",
            "\n",
            "Global step: 1916,loss: 0.012007678\n",
            "\n",
            "Global step: 1917,loss: 0.011595696\n",
            "\n",
            "Global step: 1918,loss: 0.011471679\n",
            "\n",
            "Global step: 1919,loss: 0.011235438\n",
            "\n",
            "Global step: 1920,loss: 0.01359597\n",
            "\n",
            "Global step: 1921,loss: 0.011381381\n",
            "\n",
            "Global step: 1922,loss: 0.010663393\n",
            "\n",
            "Global step: 1923,loss: 0.011419423\n",
            "\n",
            "Global step: 1924,loss: 0.011017609\n",
            "\n",
            "Global step: 1925,loss: 0.0117416615\n",
            "\n",
            "Global Step: 1925,Val_Loss: 5.223465823770599e-05,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:33:29.648791 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 9/15:\n",
            "Global step: 1926,loss: 0.01355317\n",
            "\n",
            "Global step: 1927,loss: 0.011567762\n",
            "\n",
            "Global step: 1928,loss: 0.0111233825\n",
            "\n",
            "Global step: 1929,loss: 0.011999583\n",
            "\n",
            "Global step: 1930,loss: 0.014888739\n",
            "\n",
            "Global step: 1931,loss: 0.013064841\n",
            "\n",
            "Global step: 1932,loss: 0.011122412\n",
            "\n",
            "Global step: 1933,loss: 0.01105421\n",
            "\n",
            "Global step: 1934,loss: 0.011537882\n",
            "\n",
            "Global step: 1935,loss: 0.011935612\n",
            "\n",
            "Global step: 1936,loss: 0.013033477\n",
            "\n",
            "Global step: 1937,loss: 0.011203443\n",
            "\n",
            "Global step: 1938,loss: 0.011172501\n",
            "\n",
            "Global step: 1939,loss: 0.012465589\n",
            "\n",
            "Global step: 1940,loss: 0.011250464\n",
            "\n",
            "Global step: 1941,loss: 0.012908743\n",
            "\n",
            "Global step: 1942,loss: 0.011833262\n",
            "\n",
            "Global step: 1943,loss: 0.011323182\n",
            "\n",
            "Global step: 1944,loss: 0.014933866\n",
            "\n",
            "Global step: 1945,loss: 0.012046463\n",
            "\n",
            "Global step: 1946,loss: 0.012630772\n",
            "\n",
            "Global step: 1947,loss: 0.011903448\n",
            "\n",
            "Global step: 1948,loss: 0.011357656\n",
            "\n",
            "Global step: 1949,loss: 0.012805792\n",
            "\n",
            "Global step: 1950,loss: 0.010929048\n",
            "\n",
            "Global step: 1951,loss: 0.011163877\n",
            "\n",
            "Global step: 1952,loss: 0.011025306\n",
            "\n",
            "Global step: 1953,loss: 0.011545336\n",
            "\n",
            "Global step: 1954,loss: 0.0130108455\n",
            "\n",
            "Global step: 1955,loss: 0.011425721\n",
            "\n",
            "Global step: 1956,loss: 0.012679277\n",
            "\n",
            "Global step: 1957,loss: 0.0112759555\n",
            "\n",
            "Global step: 1958,loss: 0.011447698\n",
            "\n",
            "Global step: 1959,loss: 0.011806068\n",
            "\n",
            "Global step: 1960,loss: 0.011491546\n",
            "\n",
            "Global step: 1961,loss: 0.011835086\n",
            "\n",
            "Global step: 1962,loss: 0.01223631\n",
            "\n",
            "Global step: 1963,loss: 0.011254825\n",
            "\n",
            "Global step: 1964,loss: 0.011044217\n",
            "\n",
            "Global step: 1965,loss: 0.012521299\n",
            "\n",
            "Global step: 1966,loss: 0.010829767\n",
            "\n",
            "Global step: 1967,loss: 0.012720368\n",
            "\n",
            "Global step: 1968,loss: 0.011642858\n",
            "\n",
            "Global step: 1969,loss: 0.011608781\n",
            "\n",
            "Global step: 1970,loss: 0.011408797\n",
            "\n",
            "Global step: 1971,loss: 0.011219032\n",
            "\n",
            "Global step: 1972,loss: 0.011622996\n",
            "\n",
            "Global step: 1973,loss: 0.012058172\n",
            "\n",
            "Global step: 1974,loss: 0.011099156\n",
            "\n",
            "Global step: 1975,loss: 0.013886745\n",
            "\n",
            "Global step: 1976,loss: 0.011702259\n",
            "\n",
            "Global step: 1977,loss: 0.011433022\n",
            "\n",
            "Global step: 1978,loss: 0.0122447675\n",
            "\n",
            "Global step: 1979,loss: 0.012057327\n",
            "\n",
            "Global step: 1980,loss: 0.011933027\n",
            "\n",
            "Global step: 1981,loss: 0.011522403\n",
            "\n",
            "Global step: 1982,loss: 0.012475094\n",
            "\n",
            "Global step: 1983,loss: 0.013583082\n",
            "\n",
            "Global step: 1984,loss: 0.011917949\n",
            "\n",
            "Global step: 1985,loss: 0.011966847\n",
            "\n",
            "Global step: 1986,loss: 0.012113273\n",
            "\n",
            "Global step: 1987,loss: 0.011639888\n",
            "\n",
            "Global step: 1988,loss: 0.01103147\n",
            "\n",
            "Global step: 1989,loss: 0.011064883\n",
            "\n",
            "Global step: 1990,loss: 0.010602744\n",
            "\n",
            "Global step: 1991,loss: 0.013164731\n",
            "\n",
            "Global step: 1992,loss: 0.011405028\n",
            "\n",
            "Global step: 1993,loss: 0.01182805\n",
            "\n",
            "Global step: 1994,loss: 0.011180487\n",
            "\n",
            "Global step: 1995,loss: 0.01423893\n",
            "\n",
            "Global step: 1996,loss: 0.012262676\n",
            "\n",
            "Global step: 1997,loss: 0.011496648\n",
            "\n",
            "Global step: 1998,loss: 0.011352624\n",
            "\n",
            "Global step: 1999,loss: 0.011467172\n",
            "\n",
            "Global step: 2000,loss: 0.011974197\n",
            "\n",
            "Global step: 2001,loss: 0.011976513\n",
            "\n",
            "Global step: 2002,loss: 0.011048814\n",
            "\n",
            "Global step: 2003,loss: 0.011988482\n",
            "\n",
            "Global step: 2004,loss: 0.011536292\n",
            "\n",
            "Global step: 2005,loss: 0.011583628\n",
            "\n",
            "Global step: 2006,loss: 0.011241167\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2007.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:34:30.455948 140673502324480 supervisor.py:1050] Recording summary at step 2007.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2007,loss: 0.011471494\n",
            "\n",
            "Global step: 2008,loss: 0.013086525\n",
            "\n",
            "Global step: 2009,loss: 0.012163261\n",
            "\n",
            "Global step: 2010,loss: 0.011439067\n",
            "\n",
            "Global step: 2011,loss: 0.010986935\n",
            "\n",
            "Global step: 2012,loss: 0.015869653\n",
            "\n",
            "Global step: 2013,loss: 0.010870781\n",
            "\n",
            "Global step: 2014,loss: 0.01145801\n",
            "\n",
            "Global step: 2015,loss: 0.011422713\n",
            "\n",
            "Global step: 2016,loss: 0.011250845\n",
            "\n",
            "Global step: 2017,loss: 0.012453914\n",
            "\n",
            "Global step: 2018,loss: 0.012679146\n",
            "\n",
            "Global step: 2019,loss: 0.011633771\n",
            "\n",
            "Global step: 2020,loss: 0.013109157\n",
            "\n",
            "Global step: 2021,loss: 0.013995785\n",
            "\n",
            "Global step: 2022,loss: 0.0129393805\n",
            "\n",
            "Global step: 2023,loss: 0.011350325\n",
            "\n",
            "Global step: 2024,loss: 0.012320801\n",
            "\n",
            "Global step: 2025,loss: 0.012092714\n",
            "\n",
            "Global step: 2026,loss: 0.011404707\n",
            "\n",
            "Global step: 2027,loss: 0.01135333\n",
            "\n",
            "Global step: 2028,loss: 0.011595173\n",
            "\n",
            "Global step: 2029,loss: 0.011343117\n",
            "\n",
            "Global step: 2030,loss: 0.011429899\n",
            "\n",
            "Global step: 2031,loss: 0.0114504285\n",
            "\n",
            "Global step: 2032,loss: 0.0116033815\n",
            "\n",
            "Global step: 2033,loss: 0.012928745\n",
            "\n",
            "Global step: 2034,loss: 0.011832175\n",
            "\n",
            "Global step: 2035,loss: 0.011547756\n",
            "\n",
            "Global step: 2036,loss: 0.010974027\n",
            "\n",
            "Global step: 2037,loss: 0.011279967\n",
            "\n",
            "Global step: 2038,loss: 0.015227033\n",
            "\n",
            "Global step: 2039,loss: 0.010567803\n",
            "\n",
            "Global step: 2040,loss: 0.01262097\n",
            "\n",
            "Global step: 2041,loss: 0.011806541\n",
            "\n",
            "Global step: 2042,loss: 0.011331588\n",
            "\n",
            "Global step: 2043,loss: 0.011756562\n",
            "\n",
            "Global step: 2044,loss: 0.011187006\n",
            "\n",
            "Global step: 2045,loss: 0.011348738\n",
            "\n",
            "Global step: 2046,loss: 0.012639789\n",
            "\n",
            "Global step: 2047,loss: 0.011530637\n",
            "\n",
            "Global step: 2048,loss: 0.012880937\n",
            "\n",
            "Global step: 2049,loss: 0.011528132\n",
            "\n",
            "Global step: 2050,loss: 0.013417628\n",
            "\n",
            "Global step: 2051,loss: 0.012069498\n",
            "\n",
            "Global step: 2052,loss: 0.011263723\n",
            "\n",
            "Global step: 2053,loss: 0.011318017\n",
            "\n",
            "Global step: 2054,loss: 0.011475088\n",
            "\n",
            "Global step: 2055,loss: 0.011628576\n",
            "\n",
            "Global step: 2056,loss: 0.011421048\n",
            "\n",
            "Global step: 2057,loss: 0.011023511\n",
            "\n",
            "Global step: 2058,loss: 0.0109635135\n",
            "\n",
            "Global step: 2059,loss: 0.012834242\n",
            "\n",
            "Global step: 2060,loss: 0.013203796\n",
            "\n",
            "Global step: 2061,loss: 0.011180589\n",
            "\n",
            "Global step: 2062,loss: 0.011768609\n",
            "\n",
            "Global step: 2063,loss: 0.011464784\n",
            "\n",
            "Global step: 2064,loss: 0.012564598\n",
            "\n",
            "Global step: 2065,loss: 0.011231961\n",
            "\n",
            "Global step: 2066,loss: 0.013233578\n",
            "\n",
            "Global step: 2067,loss: 0.011239473\n",
            "\n",
            "Global step: 2068,loss: 0.010976739\n",
            "\n",
            "Global step: 2069,loss: 0.011713211\n",
            "\n",
            "Global step: 2070,loss: 0.010799188\n",
            "\n",
            "Global step: 2071,loss: 0.015269753\n",
            "\n",
            "Global step: 2072,loss: 0.0119657265\n",
            "\n",
            "Global step: 2073,loss: 0.012287155\n",
            "\n",
            "Global step: 2074,loss: 0.011504912\n",
            "\n",
            "Global step: 2075,loss: 0.011256223\n",
            "\n",
            "Global step: 2076,loss: 0.012329781\n",
            "\n",
            "Global step: 2077,loss: 0.012906989\n",
            "\n",
            "Global step: 2078,loss: 0.013265938\n",
            "\n",
            "Global step: 2079,loss: 0.01142346\n",
            "\n",
            "Global step: 2080,loss: 0.011730017\n",
            "\n",
            "Global step: 2081,loss: 0.010992129\n",
            "\n",
            "Global step: 2082,loss: 0.01223603\n",
            "\n",
            "Global step: 2083,loss: 0.010929491\n",
            "\n",
            "Global step: 2084,loss: 0.010697596\n",
            "\n",
            "Global step: 2085,loss: 0.013281522\n",
            "\n",
            "Global step: 2086,loss: 0.011699809\n",
            "\n",
            "Global step: 2087,loss: 0.011638553\n",
            "\n",
            "Global step: 2088,loss: 0.012459257\n",
            "\n",
            "Global step: 2089,loss: 0.011124764\n",
            "\n",
            "Global step: 2090,loss: 0.014371231\n",
            "\n",
            "Global step: 2091,loss: 0.011276478\n",
            "\n",
            "Global step: 2092,loss: 0.014575278\n",
            "\n",
            "Global step: 2093,loss: 0.01045619\n",
            "\n",
            "Global step: 2094,loss: 0.011128126\n",
            "\n",
            "Global step: 2095,loss: 0.011322038\n",
            "\n",
            "Global step: 2096,loss: 0.012610143\n",
            "\n",
            "Global step: 2097,loss: 0.011225505\n",
            "\n",
            "Global step: 2098,loss: 0.013843058\n",
            "\n",
            "Global step: 2099,loss: 0.014251698\n",
            "\n",
            "Global step: 2100,loss: 0.012158303\n",
            "\n",
            "Global step: 2101,loss: 0.012392772\n",
            "\n",
            "Global step: 2102,loss: 0.011993115\n",
            "\n",
            "Global step: 2103,loss: 0.01098071\n",
            "\n",
            "Global step: 2104,loss: 0.013049574\n",
            "\n",
            "Global step: 2105,loss: 0.011372678\n",
            "\n",
            "Global step: 2106,loss: 0.0109583335\n",
            "\n",
            "Global step: 2107,loss: 0.010515705\n",
            "\n",
            "Global step: 2108,loss: 0.0137312375\n",
            "\n",
            "Global step: 2109,loss: 0.010730653\n",
            "\n",
            "Global step: 2110,loss: 0.01165945\n",
            "\n",
            "Global step: 2111,loss: 0.013206794\n",
            "\n",
            "Global step: 2112,loss: 0.011090374\n",
            "\n",
            "Global step: 2113,loss: 0.011761739\n",
            "\n",
            "Global step: 2114,loss: 0.014503177\n",
            "\n",
            "Global step: 2115,loss: 0.011748533\n",
            "\n",
            "Global step: 2116,loss: 0.012632224\n",
            "\n",
            "Global step: 2117,loss: 0.01238979\n",
            "\n",
            "Global step: 2118,loss: 0.011330422\n",
            "\n",
            "Global step: 2119,loss: 0.012165786\n",
            "\n",
            "Global step: 2120,loss: 0.015395078\n",
            "\n",
            "Global step: 2121,loss: 0.012984914\n",
            "\n",
            "Global step: 2122,loss: 0.010356453\n",
            "\n",
            "Global step: 2123,loss: 0.011028285\n",
            "\n",
            "Global step: 2124,loss: 0.01108613\n",
            "\n",
            "Global step: 2125,loss: 0.012500292\n",
            "\n",
            "Global step: 2126,loss: 0.011264764\n",
            "\n",
            "Global step: 2127,loss: 0.0117826\n",
            "\n",
            "Global step: 2128,loss: 0.010732553\n",
            "\n",
            "Global step: 2129,loss: 0.012223339\n",
            "\n",
            "Global step: 2130,loss: 0.011412684\n",
            "\n",
            "Global step: 2131,loss: 0.011903935\n",
            "\n",
            "Global step: 2132,loss: 0.0112003265\n",
            "\n",
            "Global step: 2133,loss: 0.011409862\n",
            "\n",
            "Global step: 2134,loss: 0.011799449\n",
            "\n",
            "Global step: 2135,loss: 0.01130938\n",
            "\n",
            "Global step: 2136,loss: 0.011195141\n",
            "\n",
            "Global step: 2137,loss: 0.011597929\n",
            "\n",
            "Global step: 2138,loss: 0.0114343185\n",
            "\n",
            "Global step: 2139,loss: 0.010922651\n",
            "\n",
            "Global Step: 2139,Val_Loss: 5.221964410769655e-05,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:36:15.735255 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 10/15:\n",
            "Global step: 2140,loss: 0.011206053\n",
            "\n",
            "Global step: 2141,loss: 0.012106178\n",
            "\n",
            "Global step: 2142,loss: 0.010485995\n",
            "\n",
            "Global step: 2143,loss: 0.011850453\n",
            "\n",
            "Global step: 2144,loss: 0.010794065\n",
            "\n",
            "Global step: 2145,loss: 0.011825696\n",
            "\n",
            "Global step: 2146,loss: 0.012563148\n",
            "\n",
            "Global step: 2147,loss: 0.010588079\n",
            "\n",
            "Global step: 2148,loss: 0.010390218\n",
            "\n",
            "Global step: 2149,loss: 0.011855979\n",
            "\n",
            "Global step: 2150,loss: 0.010611599\n",
            "\n",
            "Global step: 2151,loss: 0.011663336\n",
            "\n",
            "Global step: 2152,loss: 0.0107488\n",
            "\n",
            "Global step: 2153,loss: 0.011596963\n",
            "\n",
            "Global step: 2154,loss: 0.012086675\n",
            "\n",
            "Global step: 2155,loss: 0.011235986\n",
            "\n",
            "Global step: 2156,loss: 0.012082846\n",
            "\n",
            "Global step: 2157,loss: 0.010688496\n",
            "\n",
            "Global step: 2158,loss: 0.012538721\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2159.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:36:30.412280 140673502324480 supervisor.py:1050] Recording summary at step 2159.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2159,loss: 0.010965086\n",
            "\n",
            "Global step: 2160,loss: 0.011966512\n",
            "\n",
            "Global step: 2161,loss: 0.012019298\n",
            "\n",
            "Global step: 2162,loss: 0.0120601365\n",
            "\n",
            "Global step: 2163,loss: 0.012247283\n",
            "\n",
            "Global step: 2164,loss: 0.010958745\n",
            "\n",
            "Global step: 2165,loss: 0.010558902\n",
            "\n",
            "Global step: 2166,loss: 0.013028558\n",
            "\n",
            "Global step: 2167,loss: 0.0119281765\n",
            "\n",
            "Global step: 2168,loss: 0.012802512\n",
            "\n",
            "Global step: 2169,loss: 0.01102286\n",
            "\n",
            "Global step: 2170,loss: 0.012388931\n",
            "\n",
            "Global step: 2171,loss: 0.011701789\n",
            "\n",
            "Global step: 2172,loss: 0.011915875\n",
            "\n",
            "Global step: 2173,loss: 0.010745528\n",
            "\n",
            "Global step: 2174,loss: 0.0107435025\n",
            "\n",
            "Global step: 2175,loss: 0.011231078\n",
            "\n",
            "Global step: 2176,loss: 0.0107821245\n",
            "\n",
            "Global step: 2177,loss: 0.010854896\n",
            "\n",
            "Global step: 2178,loss: 0.014971678\n",
            "\n",
            "Global step: 2179,loss: 0.013241289\n",
            "\n",
            "Global step: 2180,loss: 0.0106558725\n",
            "\n",
            "Global step: 2181,loss: 0.01088098\n",
            "\n",
            "Global step: 2182,loss: 0.012880312\n",
            "\n",
            "Global step: 2183,loss: 0.011099162\n",
            "\n",
            "Global step: 2184,loss: 0.010257306\n",
            "\n",
            "Global step: 2185,loss: 0.010374239\n",
            "\n",
            "Global step: 2186,loss: 0.01159937\n",
            "\n",
            "Global step: 2187,loss: 0.01107335\n",
            "\n",
            "Global step: 2188,loss: 0.014723714\n",
            "\n",
            "Global step: 2189,loss: 0.011955178\n",
            "\n",
            "Global step: 2190,loss: 0.013574138\n",
            "\n",
            "Global step: 2191,loss: 0.011338348\n",
            "\n",
            "Global step: 2192,loss: 0.011393332\n",
            "\n",
            "Global step: 2193,loss: 0.011252229\n",
            "\n",
            "Global step: 2194,loss: 0.010958505\n",
            "\n",
            "Global step: 2195,loss: 0.010691661\n",
            "\n",
            "Global step: 2196,loss: 0.011487639\n",
            "\n",
            "Global step: 2197,loss: 0.012340918\n",
            "\n",
            "Global step: 2198,loss: 0.010934158\n",
            "\n",
            "Global step: 2199,loss: 0.010796884\n",
            "\n",
            "Global step: 2200,loss: 0.011889147\n",
            "\n",
            "Global step: 2201,loss: 0.010798044\n",
            "\n",
            "Global step: 2202,loss: 0.010713426\n",
            "\n",
            "Global step: 2203,loss: 0.010773832\n",
            "\n",
            "Global step: 2204,loss: 0.010254599\n",
            "\n",
            "Global step: 2205,loss: 0.010521043\n",
            "\n",
            "Global step: 2206,loss: 0.011015728\n",
            "\n",
            "Global step: 2207,loss: 0.010263584\n",
            "\n",
            "Global step: 2208,loss: 0.010834072\n",
            "\n",
            "Global step: 2209,loss: 0.012132253\n",
            "\n",
            "Global step: 2210,loss: 0.012299675\n",
            "\n",
            "Global step: 2211,loss: 0.013184179\n",
            "\n",
            "Global step: 2212,loss: 0.010606505\n",
            "\n",
            "Global step: 2213,loss: 0.010478402\n",
            "\n",
            "Global step: 2214,loss: 0.0105511425\n",
            "\n",
            "Global step: 2215,loss: 0.010894715\n",
            "\n",
            "Global step: 2216,loss: 0.0107108345\n",
            "\n",
            "Global step: 2217,loss: 0.01060763\n",
            "\n",
            "Global step: 2218,loss: 0.010421016\n",
            "\n",
            "Global step: 2219,loss: 0.010998403\n",
            "\n",
            "Global step: 2220,loss: 0.011204916\n",
            "\n",
            "Global step: 2221,loss: 0.011964755\n",
            "\n",
            "Global step: 2222,loss: 0.01107549\n",
            "\n",
            "Global step: 2223,loss: 0.010808535\n",
            "\n",
            "Global step: 2224,loss: 0.014975077\n",
            "\n",
            "Global step: 2225,loss: 0.012592658\n",
            "\n",
            "Global step: 2226,loss: 0.010815324\n",
            "\n",
            "Global step: 2227,loss: 0.012532928\n",
            "\n",
            "Global step: 2228,loss: 0.010939173\n",
            "\n",
            "Global step: 2229,loss: 0.0108651295\n",
            "\n",
            "Global step: 2230,loss: 0.010485415\n",
            "\n",
            "Global step: 2231,loss: 0.010642982\n",
            "\n",
            "Global step: 2232,loss: 0.0112023465\n",
            "\n",
            "Global step: 2233,loss: 0.010233795\n",
            "\n",
            "Global step: 2234,loss: 0.010511334\n",
            "\n",
            "Global step: 2235,loss: 0.010234718\n",
            "\n",
            "Global step: 2236,loss: 0.010073106\n",
            "\n",
            "Global step: 2237,loss: 0.010661664\n",
            "\n",
            "Global step: 2238,loss: 0.011799524\n",
            "\n",
            "Global step: 2239,loss: 0.010737275\n",
            "\n",
            "Global step: 2240,loss: 0.010804551\n",
            "\n",
            "Global step: 2241,loss: 0.011340902\n",
            "\n",
            "Global step: 2242,loss: 0.013279453\n",
            "\n",
            "Global step: 2243,loss: 0.0112419855\n",
            "\n",
            "Global step: 2244,loss: 0.0114614675\n",
            "\n",
            "Global step: 2245,loss: 0.010927629\n",
            "\n",
            "Global step: 2246,loss: 0.010189633\n",
            "\n",
            "Global step: 2247,loss: 0.01696815\n",
            "\n",
            "Global step: 2248,loss: 0.010699072\n",
            "\n",
            "Global step: 2249,loss: 0.010364438\n",
            "\n",
            "Global step: 2250,loss: 0.01234148\n",
            "\n",
            "Global step: 2251,loss: 0.012092765\n",
            "\n",
            "Global step: 2252,loss: 0.010282158\n",
            "\n",
            "Global step: 2253,loss: 0.011637623\n",
            "\n",
            "Global step: 2254,loss: 0.0110110855\n",
            "\n",
            "Global step: 2255,loss: 0.011305853\n",
            "\n",
            "Global step: 2256,loss: 0.012202312\n",
            "\n",
            "Global step: 2257,loss: 0.010782329\n",
            "\n",
            "Global step: 2258,loss: 0.01059389\n",
            "\n",
            "Global step: 2259,loss: 0.010749928\n",
            "\n",
            "Global step: 2260,loss: 0.012508434\n",
            "\n",
            "Global step: 2261,loss: 0.010901156\n",
            "\n",
            "Global step: 2262,loss: 0.010779864\n",
            "\n",
            "Global step: 2263,loss: 0.011582367\n",
            "\n",
            "Global step: 2264,loss: 0.010697314\n",
            "\n",
            "Global step: 2265,loss: 0.011836644\n",
            "\n",
            "Global step: 2266,loss: 0.013531923\n",
            "\n",
            "Global step: 2267,loss: 0.011134552\n",
            "\n",
            "Global step: 2268,loss: 0.011042281\n",
            "\n",
            "Global step: 2269,loss: 0.0117968125\n",
            "\n",
            "Global step: 2270,loss: 0.010773627\n",
            "\n",
            "Global step: 2271,loss: 0.011063915\n",
            "\n",
            "Global step: 2272,loss: 0.011418035\n",
            "\n",
            "Global step: 2273,loss: 0.010239409\n",
            "\n",
            "Global step: 2274,loss: 0.010335159\n",
            "\n",
            "Global step: 2275,loss: 0.011086626\n",
            "\n",
            "Global step: 2276,loss: 0.010665708\n",
            "\n",
            "Global step: 2277,loss: 0.013085849\n",
            "\n",
            "Global step: 2278,loss: 0.012039775\n",
            "\n",
            "Global step: 2279,loss: 0.010023098\n",
            "\n",
            "Global step: 2280,loss: 0.011792016\n",
            "\n",
            "Global step: 2281,loss: 0.010734142\n",
            "\n",
            "Global step: 2282,loss: 0.011269189\n",
            "\n",
            "Global step: 2283,loss: 0.010704978\n",
            "\n",
            "Global step: 2284,loss: 0.011124144\n",
            "\n",
            "Global step: 2285,loss: 0.012822683\n",
            "\n",
            "Global step: 2286,loss: 0.014039073\n",
            "\n",
            "Global step: 2287,loss: 0.011769071\n",
            "\n",
            "Global step: 2288,loss: 0.010965302\n",
            "\n",
            "Global step: 2289,loss: 0.011134552\n",
            "\n",
            "Global step: 2290,loss: 0.01131645\n",
            "\n",
            "Global step: 2291,loss: 0.0109749995\n",
            "\n",
            "Global step: 2292,loss: 0.011502778\n",
            "\n",
            "Global step: 2293,loss: 0.0104546435\n",
            "\n",
            "Global step: 2294,loss: 0.010453458\n",
            "\n",
            "Global step: 2295,loss: 0.010697636\n",
            "\n",
            "Global step: 2296,loss: 0.012851589\n",
            "\n",
            "Global step: 2297,loss: 0.011970607\n",
            "\n",
            "Global step: 2298,loss: 0.0109187765\n",
            "\n",
            "Global step: 2299,loss: 0.010949397\n",
            "\n",
            "Global step: 2300,loss: 0.010419321\n",
            "\n",
            "Global step: 2301,loss: 0.011403677\n",
            "\n",
            "Global step: 2302,loss: 0.010642631\n",
            "\n",
            "Global step: 2303,loss: 0.009948922\n",
            "\n",
            "Global step: 2304,loss: 0.011246483\n",
            "\n",
            "Global step: 2305,loss: 0.010846222\n",
            "\n",
            "Global step: 2306,loss: 0.010930342\n",
            "\n",
            "Global step: 2307,loss: 0.010687619\n",
            "\n",
            "Global step: 2308,loss: 0.011481764\n",
            "\n",
            "Global step: 2309,loss: 0.01257308\n",
            "\n",
            "Global step: 2310,loss: 0.01040338\n",
            "\n",
            "Global step: 2311,loss: 0.010144452\n",
            "\n",
            "Global step: 2312,loss: 0.010490959\n",
            "\n",
            "Global step: 2313,loss: 0.010703584\n",
            "\n",
            "Global step: 2314,loss: 0.0110450685\n",
            "\n",
            "Global step: 2315,loss: 0.010537688\n",
            "\n",
            "Global step: 2316,loss: 0.011289397\n",
            "\n",
            "Global step: 2317,loss: 0.011856645\n",
            "\n",
            "Global step: 2318,loss: 0.010763169\n",
            "\n",
            "Global step: 2319,loss: 0.01289629\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2320.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:38:30.250483 140673502324480 supervisor.py:1050] Recording summary at step 2320.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2320,loss: 0.010473156\n",
            "\n",
            "Global step: 2321,loss: 0.01074989\n",
            "\n",
            "Global step: 2322,loss: 0.011466277\n",
            "\n",
            "Global step: 2323,loss: 0.013028648\n",
            "\n",
            "Global step: 2324,loss: 0.010942153\n",
            "\n",
            "Global step: 2325,loss: 0.010466263\n",
            "\n",
            "Global step: 2326,loss: 0.011332623\n",
            "\n",
            "Global step: 2327,loss: 0.0110095795\n",
            "\n",
            "Global step: 2328,loss: 0.012279904\n",
            "\n",
            "Global step: 2329,loss: 0.012119478\n",
            "\n",
            "Global step: 2330,loss: 0.010904337\n",
            "\n",
            "Global step: 2331,loss: 0.010514353\n",
            "\n",
            "Global step: 2332,loss: 0.010604297\n",
            "\n",
            "Global step: 2333,loss: 0.010169849\n",
            "\n",
            "Global step: 2334,loss: 0.0145918345\n",
            "\n",
            "Global step: 2335,loss: 0.010427242\n",
            "\n",
            "Global step: 2336,loss: 0.0115597695\n",
            "\n",
            "Global step: 2337,loss: 0.010206144\n",
            "\n",
            "Global step: 2338,loss: 0.011108456\n",
            "\n",
            "Global step: 2339,loss: 0.010242156\n",
            "\n",
            "Global step: 2340,loss: 0.010480491\n",
            "\n",
            "Global step: 2341,loss: 0.0111027565\n",
            "\n",
            "Global step: 2342,loss: 0.010393631\n",
            "\n",
            "Global step: 2343,loss: 0.011054309\n",
            "\n",
            "Global step: 2344,loss: 0.012307273\n",
            "\n",
            "Global step: 2345,loss: 0.009959932\n",
            "\n",
            "Global step: 2346,loss: 0.010617441\n",
            "\n",
            "Global step: 2347,loss: 0.010279238\n",
            "\n",
            "Global step: 2348,loss: 0.010431585\n",
            "\n",
            "Global step: 2349,loss: 0.010349574\n",
            "\n",
            "Global step: 2350,loss: 0.01190418\n",
            "\n",
            "Global step: 2351,loss: 0.01039348\n",
            "\n",
            "Global step: 2352,loss: 0.011431565\n",
            "\n",
            "Global step: 2353,loss: 0.010438977\n",
            "\n",
            "Global Step: 2353,Val_Loss: 5.105317675196076e-05,  Val_acc: 0.9977384868421053 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:39:02.180705 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 11/15:\n",
            "Global step: 2354,loss: 0.012330567\n",
            "\n",
            "Global step: 2355,loss: 0.010409407\n",
            "\n",
            "Global step: 2356,loss: 0.0104854265\n",
            "\n",
            "Global step: 2357,loss: 0.012086294\n",
            "\n",
            "Global step: 2358,loss: 0.011110914\n",
            "\n",
            "Global step: 2359,loss: 0.011418522\n",
            "\n",
            "Global step: 2360,loss: 0.010540645\n",
            "\n",
            "Global step: 2361,loss: 0.010325905\n",
            "\n",
            "Global step: 2362,loss: 0.011180734\n",
            "\n",
            "Global step: 2363,loss: 0.010665762\n",
            "\n",
            "Global step: 2364,loss: 0.010728235\n",
            "\n",
            "Global step: 2365,loss: 0.010409744\n",
            "\n",
            "Global step: 2366,loss: 0.010511759\n",
            "\n",
            "Global step: 2367,loss: 0.010311047\n",
            "\n",
            "Global step: 2368,loss: 0.010234909\n",
            "\n",
            "Global step: 2369,loss: 0.010386287\n",
            "\n",
            "Global step: 2370,loss: 0.010355942\n",
            "\n",
            "Global step: 2371,loss: 0.010116372\n",
            "\n",
            "Global step: 2372,loss: 0.010004769\n",
            "\n",
            "Global step: 2373,loss: 0.010136713\n",
            "\n",
            "Global step: 2374,loss: 0.010116747\n",
            "\n",
            "Global step: 2375,loss: 0.009848451\n",
            "\n",
            "Global step: 2376,loss: 0.012086315\n",
            "\n",
            "Global step: 2377,loss: 0.0110929925\n",
            "\n",
            "Global step: 2378,loss: 0.0112224305\n",
            "\n",
            "Global step: 2379,loss: 0.011245501\n",
            "\n",
            "Global step: 2380,loss: 0.010119939\n",
            "\n",
            "Global step: 2381,loss: 0.010618416\n",
            "\n",
            "Global step: 2382,loss: 0.009929464\n",
            "\n",
            "Global step: 2383,loss: 0.011883306\n",
            "\n",
            "Global step: 2384,loss: 0.0103636505\n",
            "\n",
            "Global step: 2385,loss: 0.010485577\n",
            "\n",
            "Global step: 2386,loss: 0.010134587\n",
            "\n",
            "Global step: 2387,loss: 0.010145571\n",
            "\n",
            "Global step: 2388,loss: 0.010413585\n",
            "\n",
            "Global step: 2389,loss: 0.009897003\n",
            "\n",
            "Global step: 2390,loss: 0.011199426\n",
            "\n",
            "Global step: 2391,loss: 0.010687315\n",
            "\n",
            "Global step: 2392,loss: 0.010413103\n",
            "\n",
            "Global step: 2393,loss: 0.010366224\n",
            "\n",
            "Global step: 2394,loss: 0.010474531\n",
            "\n",
            "Global step: 2395,loss: 0.010601621\n",
            "\n",
            "Global step: 2396,loss: 0.010798998\n",
            "\n",
            "Global step: 2397,loss: 0.011299592\n",
            "\n",
            "Global step: 2398,loss: 0.010313264\n",
            "\n",
            "Global step: 2399,loss: 0.011174003\n",
            "\n",
            "Global step: 2400,loss: 0.010728035\n",
            "\n",
            "Global step: 2401,loss: 0.010256333\n",
            "\n",
            "Global step: 2402,loss: 0.010280331\n",
            "\n",
            "Global step: 2403,loss: 0.010286022\n",
            "\n",
            "Global step: 2404,loss: 0.010871505\n",
            "\n",
            "Global step: 2405,loss: 0.011580382\n",
            "\n",
            "Global step: 2406,loss: 0.009807732\n",
            "\n",
            "Global step: 2407,loss: 0.010062764\n",
            "\n",
            "Global step: 2408,loss: 0.00981309\n",
            "\n",
            "Global step: 2409,loss: 0.01049692\n",
            "\n",
            "Global step: 2410,loss: 0.010226063\n",
            "\n",
            "Global step: 2411,loss: 0.010750064\n",
            "\n",
            "Global step: 2412,loss: 0.010648608\n",
            "\n",
            "Global step: 2413,loss: 0.011427944\n",
            "\n",
            "Global step: 2414,loss: 0.011657721\n",
            "\n",
            "Global step: 2415,loss: 0.010078685\n",
            "\n",
            "Global step: 2416,loss: 0.011869702\n",
            "\n",
            "Global step: 2417,loss: 0.010416603\n",
            "\n",
            "Global step: 2418,loss: 0.010532479\n",
            "\n",
            "Global step: 2419,loss: 0.009953528\n",
            "\n",
            "Global step: 2420,loss: 0.010173957\n",
            "\n",
            "Global step: 2421,loss: 0.010841827\n",
            "\n",
            "Global step: 2422,loss: 0.010441656\n",
            "\n",
            "Global step: 2423,loss: 0.010238757\n",
            "\n",
            "Global step: 2424,loss: 0.009620427\n",
            "\n",
            "Global step: 2425,loss: 0.010209538\n",
            "\n",
            "Global step: 2426,loss: 0.010630868\n",
            "\n",
            "Global step: 2427,loss: 0.010724346\n",
            "\n",
            "Global step: 2428,loss: 0.010180345\n",
            "\n",
            "Global step: 2429,loss: 0.011369901\n",
            "\n",
            "Global step: 2430,loss: 0.011862837\n",
            "\n",
            "Global step: 2431,loss: 0.009928755\n",
            "\n",
            "Global step: 2432,loss: 0.010344048\n",
            "\n",
            "Global step: 2433,loss: 0.013637579\n",
            "\n",
            "Global step: 2434,loss: 0.010773577\n",
            "\n",
            "Global step: 2435,loss: 0.010541892\n",
            "\n",
            "Global step: 2436,loss: 0.011454142\n",
            "\n",
            "Global step: 2437,loss: 0.011336778\n",
            "\n",
            "Global step: 2438,loss: 0.010053824\n",
            "\n",
            "Global step: 2439,loss: 0.011596377\n",
            "\n",
            "Global step: 2440,loss: 0.0105993645\n",
            "\n",
            "Global step: 2441,loss: 0.011091195\n",
            "\n",
            "Global step: 2442,loss: 0.010844026\n",
            "\n",
            "Global step: 2443,loss: 0.009955989\n",
            "\n",
            "Global step: 2444,loss: 0.011830368\n",
            "\n",
            "Global step: 2445,loss: 0.010492358\n",
            "\n",
            "Global step: 2446,loss: 0.010072225\n",
            "\n",
            "Global step: 2447,loss: 0.010149111\n",
            "\n",
            "Global step: 2448,loss: 0.009895054\n",
            "\n",
            "Global step: 2449,loss: 0.010662418\n",
            "\n",
            "Global step: 2450,loss: 0.011363378\n",
            "\n",
            "Global step: 2451,loss: 0.011281971\n",
            "\n",
            "Global step: 2452,loss: 0.010023839\n",
            "\n",
            "Global step: 2453,loss: 0.010143092\n",
            "\n",
            "Global step: 2454,loss: 0.009555719\n",
            "\n",
            "Global step: 2455,loss: 0.009946837\n",
            "\n",
            "Global step: 2456,loss: 0.011235435\n",
            "\n",
            "Global step: 2457,loss: 0.010055893\n",
            "\n",
            "Global step: 2458,loss: 0.010266668\n",
            "\n",
            "Global step: 2459,loss: 0.009985311\n",
            "\n",
            "Global step: 2460,loss: 0.010976929\n",
            "\n",
            "Global step: 2461,loss: 0.009836429\n",
            "\n",
            "Global step: 2462,loss: 0.010500096\n",
            "\n",
            "Global step: 2463,loss: 0.010723996\n",
            "\n",
            "Global step: 2464,loss: 0.010415306\n",
            "\n",
            "Global step: 2465,loss: 0.010624056\n",
            "\n",
            "Global step: 2466,loss: 0.011604682\n",
            "\n",
            "Global step: 2467,loss: 0.010518234\n",
            "\n",
            "Global step: 2468,loss: 0.011245538\n",
            "\n",
            "Global step: 2469,loss: 0.010421857\n",
            "\n",
            "Global step: 2470,loss: 0.010444975\n",
            "\n",
            "Global step: 2471,loss: 0.010823213\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2472.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:40:29.781475 140673502324480 supervisor.py:1050] Recording summary at step 2472.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2472,loss: 0.010309349\n",
            "\n",
            "Global step: 2473,loss: 0.010960108\n",
            "\n",
            "Global step: 2474,loss: 0.0108657675\n",
            "\n",
            "Global step: 2475,loss: 0.010554348\n",
            "\n",
            "Global step: 2476,loss: 0.010005856\n",
            "\n",
            "Global step: 2477,loss: 0.010909967\n",
            "\n",
            "Global step: 2478,loss: 0.010317191\n",
            "\n",
            "Global step: 2479,loss: 0.011032596\n",
            "\n",
            "Global step: 2480,loss: 0.0100726355\n",
            "\n",
            "Global step: 2481,loss: 0.01028313\n",
            "\n",
            "Global step: 2482,loss: 0.010212275\n",
            "\n",
            "Global step: 2483,loss: 0.0104426835\n",
            "\n",
            "Global step: 2484,loss: 0.012185825\n",
            "\n",
            "Global step: 2485,loss: 0.010169125\n",
            "\n",
            "Global step: 2486,loss: 0.009969932\n",
            "\n",
            "Global step: 2487,loss: 0.013263871\n",
            "\n",
            "Global step: 2488,loss: 0.010278395\n",
            "\n",
            "Global step: 2489,loss: 0.010365339\n",
            "\n",
            "Global step: 2490,loss: 0.010002386\n",
            "\n",
            "Global step: 2491,loss: 0.009331932\n",
            "\n",
            "Global step: 2492,loss: 0.011152427\n",
            "\n",
            "Global step: 2493,loss: 0.009875211\n",
            "\n",
            "Global step: 2494,loss: 0.01086293\n",
            "\n",
            "Global step: 2495,loss: 0.010117737\n",
            "\n",
            "Global step: 2496,loss: 0.009997946\n",
            "\n",
            "Global step: 2497,loss: 0.010034049\n",
            "\n",
            "Global step: 2498,loss: 0.0100752\n",
            "\n",
            "Global step: 2499,loss: 0.009760672\n",
            "\n",
            "Global step: 2500,loss: 0.010845964\n",
            "\n",
            "Global step: 2501,loss: 0.010272627\n",
            "\n",
            "Global step: 2502,loss: 0.010296635\n",
            "\n",
            "Global step: 2503,loss: 0.010587845\n",
            "\n",
            "Global step: 2504,loss: 0.010145319\n",
            "\n",
            "Global step: 2505,loss: 0.010356553\n",
            "\n",
            "Global step: 2506,loss: 0.010465292\n",
            "\n",
            "Global step: 2507,loss: 0.0094018625\n",
            "\n",
            "Global step: 2508,loss: 0.00984163\n",
            "\n",
            "Global step: 2509,loss: 0.010388663\n",
            "\n",
            "Global step: 2510,loss: 0.009840066\n",
            "\n",
            "Global step: 2511,loss: 0.009824111\n",
            "\n",
            "Global step: 2512,loss: 0.011073029\n",
            "\n",
            "Global step: 2513,loss: 0.010382201\n",
            "\n",
            "Global step: 2514,loss: 0.010335444\n",
            "\n",
            "Global step: 2515,loss: 0.011695396\n",
            "\n",
            "Global step: 2516,loss: 0.009769228\n",
            "\n",
            "Global step: 2517,loss: 0.01019205\n",
            "\n",
            "Global step: 2518,loss: 0.009875596\n",
            "\n",
            "Global step: 2519,loss: 0.010110591\n",
            "\n",
            "Global step: 2520,loss: 0.011632202\n",
            "\n",
            "Global step: 2521,loss: 0.013506143\n",
            "\n",
            "Global step: 2522,loss: 0.010124815\n",
            "\n",
            "Global step: 2523,loss: 0.011161379\n",
            "\n",
            "Global step: 2524,loss: 0.011003094\n",
            "\n",
            "Global step: 2525,loss: 0.01067351\n",
            "\n",
            "Global step: 2526,loss: 0.009846337\n",
            "\n",
            "Global step: 2527,loss: 0.009954756\n",
            "\n",
            "Global step: 2528,loss: 0.009895186\n",
            "\n",
            "Global step: 2529,loss: 0.010527975\n",
            "\n",
            "Global step: 2530,loss: 0.010033532\n",
            "\n",
            "Global step: 2531,loss: 0.010290548\n",
            "\n",
            "Global step: 2532,loss: 0.0098483395\n",
            "\n",
            "Global step: 2533,loss: 0.009851738\n",
            "\n",
            "Global step: 2534,loss: 0.0107239485\n",
            "\n",
            "Global step: 2535,loss: 0.010074539\n",
            "\n",
            "Global step: 2536,loss: 0.010240135\n",
            "\n",
            "Global step: 2537,loss: 0.010391152\n",
            "\n",
            "Global step: 2538,loss: 0.009935594\n",
            "\n",
            "Global step: 2539,loss: 0.009773592\n",
            "\n",
            "Global step: 2540,loss: 0.010726029\n",
            "\n",
            "Global step: 2541,loss: 0.009900123\n",
            "\n",
            "Global step: 2542,loss: 0.010023321\n",
            "\n",
            "Global step: 2543,loss: 0.010574677\n",
            "\n",
            "Global step: 2544,loss: 0.0101461\n",
            "\n",
            "Global step: 2545,loss: 0.012017008\n",
            "\n",
            "Global step: 2546,loss: 0.0100708\n",
            "\n",
            "Global step: 2547,loss: 0.012966724\n",
            "\n",
            "Global step: 2548,loss: 0.010152328\n",
            "\n",
            "Global step: 2549,loss: 0.009728981\n",
            "\n",
            "Global step: 2550,loss: 0.0115406895\n",
            "\n",
            "Global step: 2551,loss: 0.010158282\n",
            "\n",
            "Global step: 2552,loss: 0.009882902\n",
            "\n",
            "Global step: 2553,loss: 0.0103001725\n",
            "\n",
            "Global step: 2554,loss: 0.0107695125\n",
            "\n",
            "Global step: 2555,loss: 0.010173053\n",
            "\n",
            "Global step: 2556,loss: 0.010056191\n",
            "\n",
            "Global step: 2557,loss: 0.01060847\n",
            "\n",
            "Global step: 2558,loss: 0.0100466525\n",
            "\n",
            "Global step: 2559,loss: 0.010433063\n",
            "\n",
            "Global step: 2560,loss: 0.010313901\n",
            "\n",
            "Global step: 2561,loss: 0.01104964\n",
            "\n",
            "Global step: 2562,loss: 0.009835956\n",
            "\n",
            "Global step: 2563,loss: 0.010028655\n",
            "\n",
            "Global step: 2564,loss: 0.011128124\n",
            "\n",
            "Global step: 2565,loss: 0.011462928\n",
            "\n",
            "Global step: 2566,loss: 0.010852447\n",
            "\n",
            "Global step: 2567,loss: 0.009705281\n",
            "\n",
            "Global Step: 2567,Val_Loss: 4.912961039257138e-05,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:41:47.191648 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 12/15:\n",
            "Global step: 2568,loss: 0.009440824\n",
            "\n",
            "Global step: 2569,loss: 0.009551721\n",
            "\n",
            "Global step: 2570,loss: 0.010125664\n",
            "\n",
            "Global step: 2571,loss: 0.009646959\n",
            "\n",
            "Global step: 2572,loss: 0.010347785\n",
            "\n",
            "Global step: 2573,loss: 0.009856392\n",
            "\n",
            "Global step: 2574,loss: 0.009994051\n",
            "\n",
            "Global step: 2575,loss: 0.0093943775\n",
            "\n",
            "Global step: 2576,loss: 0.011056443\n",
            "\n",
            "Global step: 2577,loss: 0.009683514\n",
            "\n",
            "Global step: 2578,loss: 0.009899123\n",
            "\n",
            "Global step: 2579,loss: 0.009835173\n",
            "\n",
            "Global step: 2580,loss: 0.010184518\n",
            "\n",
            "Global step: 2581,loss: 0.010160006\n",
            "\n",
            "Global step: 2582,loss: 0.01011026\n",
            "\n",
            "Global step: 2583,loss: 0.009564151\n",
            "\n",
            "Global step: 2584,loss: 0.010134679\n",
            "\n",
            "Global step: 2585,loss: 0.0099679455\n",
            "\n",
            "Global step: 2586,loss: 0.009783595\n",
            "\n",
            "Global step: 2587,loss: 0.010136846\n",
            "\n",
            "Global step: 2588,loss: 0.010544371\n",
            "\n",
            "Global step: 2589,loss: 0.009920271\n",
            "\n",
            "Global step: 2590,loss: 0.0096499985\n",
            "\n",
            "Global step: 2591,loss: 0.011074336\n",
            "\n",
            "Global step: 2592,loss: 0.009868635\n",
            "\n",
            "Global step: 2593,loss: 0.010387223\n",
            "\n",
            "Global step: 2594,loss: 0.010098031\n",
            "\n",
            "Global step: 2595,loss: 0.009239266\n",
            "\n",
            "Global step: 2596,loss: 0.010117793\n",
            "\n",
            "Global step: 2597,loss: 0.01001757\n",
            "\n",
            "Global step: 2598,loss: 0.009927395\n",
            "\n",
            "Global step: 2599,loss: 0.010793183\n",
            "\n",
            "Global step: 2600,loss: 0.010393953\n",
            "\n",
            "Global step: 2601,loss: 0.009948254\n",
            "\n",
            "Global step: 2602,loss: 0.009833034\n",
            "\n",
            "Global step: 2603,loss: 0.009905915\n",
            "\n",
            "Global step: 2604,loss: 0.010051959\n",
            "\n",
            "Global step: 2605,loss: 0.010085938\n",
            "\n",
            "Global step: 2606,loss: 0.010902595\n",
            "\n",
            "Global step: 2607,loss: 0.010224561\n",
            "\n",
            "Global step: 2608,loss: 0.011532281\n",
            "\n",
            "Global step: 2609,loss: 0.010387756\n",
            "\n",
            "Global step: 2610,loss: 0.010728911\n",
            "\n",
            "Global step: 2611,loss: 0.010430892\n",
            "\n",
            "Global step: 2612,loss: 0.010021214\n",
            "\n",
            "Global step: 2613,loss: 0.009893225\n",
            "\n",
            "Global step: 2614,loss: 0.009379649\n",
            "\n",
            "Global step: 2615,loss: 0.010638398\n",
            "\n",
            "Global step: 2616,loss: 0.009694161\n",
            "\n",
            "Global step: 2617,loss: 0.009841383\n",
            "\n",
            "Global step: 2618,loss: 0.009822624\n",
            "\n",
            "Global step: 2619,loss: 0.010599244\n",
            "\n",
            "Global step: 2620,loss: 0.009936145\n",
            "\n",
            "Global step: 2621,loss: 0.0098836655\n",
            "\n",
            "Global step: 2622,loss: 0.009616655\n",
            "\n",
            "Global step: 2623,loss: 0.009724042\n",
            "\n",
            "Global step: 2624,loss: 0.010387674\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2625.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:42:30.056004 140673502324480 supervisor.py:1050] Recording summary at step 2625.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2625,loss: 0.00980393\n",
            "\n",
            "Global step: 2626,loss: 0.0098646255\n",
            "\n",
            "Global step: 2627,loss: 0.009938781\n",
            "\n",
            "Global step: 2628,loss: 0.010660401\n",
            "\n",
            "Global step: 2629,loss: 0.009693265\n",
            "\n",
            "Global step: 2630,loss: 0.009709893\n",
            "\n",
            "Global step: 2631,loss: 0.010058582\n",
            "\n",
            "Global step: 2632,loss: 0.00940416\n",
            "\n",
            "Global step: 2633,loss: 0.009700672\n",
            "\n",
            "Global step: 2634,loss: 0.010136993\n",
            "\n",
            "Global step: 2635,loss: 0.010120466\n",
            "\n",
            "Global step: 2636,loss: 0.009381127\n",
            "\n",
            "Global step: 2637,loss: 0.0111881625\n",
            "\n",
            "Global step: 2638,loss: 0.008962593\n",
            "\n",
            "Global step: 2639,loss: 0.009949798\n",
            "\n",
            "Global step: 2640,loss: 0.009971123\n",
            "\n",
            "Global step: 2641,loss: 0.00944476\n",
            "\n",
            "Global step: 2642,loss: 0.010713944\n",
            "\n",
            "Global step: 2643,loss: 0.009783714\n",
            "\n",
            "Global step: 2644,loss: 0.011974531\n",
            "\n",
            "Global step: 2645,loss: 0.009656101\n",
            "\n",
            "Global step: 2646,loss: 0.0101248585\n",
            "\n",
            "Global step: 2647,loss: 0.010569544\n",
            "\n",
            "Global step: 2648,loss: 0.009842629\n",
            "\n",
            "Global step: 2649,loss: 0.0096871015\n",
            "\n",
            "Global step: 2650,loss: 0.009341513\n",
            "\n",
            "Global step: 2651,loss: 0.010289335\n",
            "\n",
            "Global step: 2652,loss: 0.0093111815\n",
            "\n",
            "Global step: 2653,loss: 0.00966068\n",
            "\n",
            "Global step: 2654,loss: 0.009736382\n",
            "\n",
            "Global step: 2655,loss: 0.009728255\n",
            "\n",
            "Global step: 2656,loss: 0.009354826\n",
            "\n",
            "Global step: 2657,loss: 0.010183369\n",
            "\n",
            "Global step: 2658,loss: 0.009193183\n",
            "\n",
            "Global step: 2659,loss: 0.009427227\n",
            "\n",
            "Global step: 2660,loss: 0.009601283\n",
            "\n",
            "Global step: 2661,loss: 0.009809967\n",
            "\n",
            "Global step: 2662,loss: 0.009954366\n",
            "\n",
            "Global step: 2663,loss: 0.009417938\n",
            "\n",
            "Global step: 2664,loss: 0.010164306\n",
            "\n",
            "Global step: 2665,loss: 0.00988258\n",
            "\n",
            "Global step: 2666,loss: 0.010607868\n",
            "\n",
            "Global step: 2667,loss: 0.01014942\n",
            "\n",
            "Global step: 2668,loss: 0.00997548\n",
            "\n",
            "Global step: 2669,loss: 0.00988475\n",
            "\n",
            "Global step: 2670,loss: 0.00953116\n",
            "\n",
            "Global step: 2671,loss: 0.010135095\n",
            "\n",
            "Global step: 2672,loss: 0.009519149\n",
            "\n",
            "Global step: 2673,loss: 0.009300984\n",
            "\n",
            "Global step: 2674,loss: 0.00961959\n",
            "\n",
            "Global step: 2675,loss: 0.009431032\n",
            "\n",
            "Global step: 2676,loss: 0.012245055\n",
            "\n",
            "Global step: 2677,loss: 0.009984116\n",
            "\n",
            "Global step: 2678,loss: 0.010814259\n",
            "\n",
            "Global step: 2679,loss: 0.009698318\n",
            "\n",
            "Global step: 2680,loss: 0.009331189\n",
            "\n",
            "Global step: 2681,loss: 0.009924979\n",
            "\n",
            "Global step: 2682,loss: 0.01000322\n",
            "\n",
            "Global step: 2683,loss: 0.009234181\n",
            "\n",
            "Global step: 2684,loss: 0.009855944\n",
            "\n",
            "Global step: 2685,loss: 0.00965752\n",
            "\n",
            "Global step: 2686,loss: 0.010981622\n",
            "\n",
            "Global step: 2687,loss: 0.009630215\n",
            "\n",
            "Global step: 2688,loss: 0.010390305\n",
            "\n",
            "Global step: 2689,loss: 0.00951368\n",
            "\n",
            "Global step: 2690,loss: 0.009902856\n",
            "\n",
            "Global step: 2691,loss: 0.010840053\n",
            "\n",
            "Global step: 2692,loss: 0.00936637\n",
            "\n",
            "Global step: 2693,loss: 0.008920726\n",
            "\n",
            "Global step: 2694,loss: 0.010963117\n",
            "\n",
            "Global step: 2695,loss: 0.009551197\n",
            "\n",
            "Global step: 2696,loss: 0.009701477\n",
            "\n",
            "Global step: 2697,loss: 0.009786678\n",
            "\n",
            "Global step: 2698,loss: 0.00951489\n",
            "\n",
            "Global step: 2699,loss: 0.009925632\n",
            "\n",
            "Global step: 2700,loss: 0.009441498\n",
            "\n",
            "Global step: 2701,loss: 0.00962401\n",
            "\n",
            "Global step: 2702,loss: 0.009010666\n",
            "\n",
            "Global step: 2703,loss: 0.009881055\n",
            "\n",
            "Global step: 2704,loss: 0.009342183\n",
            "\n",
            "Global step: 2705,loss: 0.00992071\n",
            "\n",
            "Global step: 2706,loss: 0.009185966\n",
            "\n",
            "Global step: 2707,loss: 0.009191686\n",
            "\n",
            "Global step: 2708,loss: 0.009719283\n",
            "\n",
            "Global step: 2709,loss: 0.00931843\n",
            "\n",
            "Global step: 2710,loss: 0.009938228\n",
            "\n",
            "Global step: 2711,loss: 0.009595564\n",
            "\n",
            "Global step: 2712,loss: 0.009327597\n",
            "\n",
            "Global step: 2713,loss: 0.009673785\n",
            "\n",
            "Global step: 2714,loss: 0.010830467\n",
            "\n",
            "Global step: 2715,loss: 0.009356684\n",
            "\n",
            "Global step: 2716,loss: 0.009883149\n",
            "\n",
            "Global step: 2717,loss: 0.010230255\n",
            "\n",
            "Global step: 2718,loss: 0.010435424\n",
            "\n",
            "Global step: 2719,loss: 0.010073145\n",
            "\n",
            "Global step: 2720,loss: 0.009602937\n",
            "\n",
            "Global step: 2721,loss: 0.009236671\n",
            "\n",
            "Global step: 2722,loss: 0.009321208\n",
            "\n",
            "Global step: 2723,loss: 0.009891548\n",
            "\n",
            "Global step: 2724,loss: 0.0094770165\n",
            "\n",
            "Global step: 2725,loss: 0.009506101\n",
            "\n",
            "Global step: 2726,loss: 0.010311443\n",
            "\n",
            "Global step: 2727,loss: 0.011868813\n",
            "\n",
            "Global step: 2728,loss: 0.009772858\n",
            "\n",
            "Global step: 2729,loss: 0.009681911\n",
            "\n",
            "Global step: 2730,loss: 0.009393481\n",
            "\n",
            "Global step: 2731,loss: 0.009171224\n",
            "\n",
            "Global step: 2732,loss: 0.01012465\n",
            "\n",
            "Global step: 2733,loss: 0.0128819235\n",
            "\n",
            "Global step: 2734,loss: 0.008970531\n",
            "\n",
            "Global step: 2735,loss: 0.010873382\n",
            "\n",
            "Global step: 2736,loss: 0.009538811\n",
            "\n",
            "Global step: 2737,loss: 0.013391284\n",
            "\n",
            "Global step: 2738,loss: 0.009741133\n",
            "\n",
            "Global step: 2739,loss: 0.009808053\n",
            "\n",
            "Global step: 2740,loss: 0.009114395\n",
            "\n",
            "Global step: 2741,loss: 0.0099182045\n",
            "\n",
            "Global step: 2742,loss: 0.009735675\n",
            "\n",
            "Global step: 2743,loss: 0.009961885\n",
            "\n",
            "Global step: 2744,loss: 0.01074703\n",
            "\n",
            "Global step: 2745,loss: 0.009682187\n",
            "\n",
            "Global step: 2746,loss: 0.009133674\n",
            "\n",
            "Global step: 2747,loss: 0.010095886\n",
            "\n",
            "Global step: 2748,loss: 0.012266403\n",
            "\n",
            "Global step: 2749,loss: 0.009557897\n",
            "\n",
            "Global step: 2750,loss: 0.010242919\n",
            "\n",
            "Global step: 2751,loss: 0.009543782\n",
            "\n",
            "Global step: 2752,loss: 0.010050352\n",
            "\n",
            "Global step: 2753,loss: 0.0092162965\n",
            "\n",
            "Global step: 2754,loss: 0.010247285\n",
            "\n",
            "Global step: 2755,loss: 0.009784053\n",
            "\n",
            "Global step: 2756,loss: 0.00952865\n",
            "\n",
            "Global step: 2757,loss: 0.009219254\n",
            "\n",
            "Global step: 2758,loss: 0.010188572\n",
            "\n",
            "Global step: 2759,loss: 0.009337359\n",
            "\n",
            "Global step: 2760,loss: 0.009522757\n",
            "\n",
            "Global step: 2761,loss: 0.009134346\n",
            "\n",
            "Global step: 2762,loss: 0.009243054\n",
            "\n",
            "Global step: 2763,loss: 0.010702112\n",
            "\n",
            "Global step: 2764,loss: 0.010548278\n",
            "\n",
            "Global step: 2765,loss: 0.011599411\n",
            "\n",
            "Global step: 2766,loss: 0.010015903\n",
            "\n",
            "Global step: 2767,loss: 0.009651849\n",
            "\n",
            "Global step: 2768,loss: 0.009856661\n",
            "\n",
            "Global step: 2769,loss: 0.010255699\n",
            "\n",
            "Global step: 2770,loss: 0.009725755\n",
            "\n",
            "Global step: 2771,loss: 0.009397172\n",
            "\n",
            "Global step: 2772,loss: 0.009313658\n",
            "\n",
            "Global step: 2773,loss: 0.009888498\n",
            "\n",
            "Global step: 2774,loss: 0.009727802\n",
            "\n",
            "Global step: 2775,loss: 0.008938302\n",
            "\n",
            "Global step: 2776,loss: 0.009192349\n",
            "\n",
            "Global step: 2777,loss: 0.009387947\n",
            "\n",
            "Global step: 2778,loss: 0.009391295\n",
            "\n",
            "Global step: 2779,loss: 0.00951582\n",
            "\n",
            "Global step: 2780,loss: 0.010897034\n",
            "\n",
            "Global step: 2781,loss: 0.009647183\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2782.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:44:29.974615 140673502324480 supervisor.py:1050] Recording summary at step 2782.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global Step: 2781,Val_Loss: 4.735760548531911e-05,  Val_acc: 0.997327302631579 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:44:33.274272 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 13/15:\n",
            "Global step: 2782,loss: 0.010531023\n",
            "\n",
            "Global step: 2783,loss: 0.009442536\n",
            "\n",
            "Global step: 2784,loss: 0.00939994\n",
            "\n",
            "Global step: 2785,loss: 0.009889138\n",
            "\n",
            "Global step: 2786,loss: 0.009114982\n",
            "\n",
            "Global step: 2787,loss: 0.009572682\n",
            "\n",
            "Global step: 2788,loss: 0.009637888\n",
            "\n",
            "Global step: 2789,loss: 0.009551615\n",
            "\n",
            "Global step: 2790,loss: 0.009510763\n",
            "\n",
            "Global step: 2791,loss: 0.008920759\n",
            "\n",
            "Global step: 2792,loss: 0.009354671\n",
            "\n",
            "Global step: 2793,loss: 0.009382425\n",
            "\n",
            "Global step: 2794,loss: 0.009374427\n",
            "\n",
            "Global step: 2795,loss: 0.008917833\n",
            "\n",
            "Global step: 2796,loss: 0.009474965\n",
            "\n",
            "Global step: 2797,loss: 0.010824542\n",
            "\n",
            "Global step: 2798,loss: 0.009541348\n",
            "\n",
            "Global step: 2799,loss: 0.008974611\n",
            "\n",
            "Global step: 2800,loss: 0.009350664\n",
            "\n",
            "Global step: 2801,loss: 0.009376098\n",
            "\n",
            "Global step: 2802,loss: 0.009184862\n",
            "\n",
            "Global step: 2803,loss: 0.009952123\n",
            "\n",
            "Global step: 2804,loss: 0.009465937\n",
            "\n",
            "Global step: 2805,loss: 0.00932431\n",
            "\n",
            "Global step: 2806,loss: 0.009452366\n",
            "\n",
            "Global step: 2807,loss: 0.0092557315\n",
            "\n",
            "Global step: 2808,loss: 0.009132162\n",
            "\n",
            "Global step: 2809,loss: 0.009344707\n",
            "\n",
            "Global step: 2810,loss: 0.009318811\n",
            "\n",
            "Global step: 2811,loss: 0.008778686\n",
            "\n",
            "Global step: 2812,loss: 0.008991161\n",
            "\n",
            "Global step: 2813,loss: 0.011091561\n",
            "\n",
            "Global step: 2814,loss: 0.009979341\n",
            "\n",
            "Global step: 2815,loss: 0.010485009\n",
            "\n",
            "Global step: 2816,loss: 0.009261983\n",
            "\n",
            "Global step: 2817,loss: 0.009143935\n",
            "\n",
            "Global step: 2818,loss: 0.009424666\n",
            "\n",
            "Global step: 2819,loss: 0.009273959\n",
            "\n",
            "Global step: 2820,loss: 0.009169009\n",
            "\n",
            "Global step: 2821,loss: 0.009555539\n",
            "\n",
            "Global step: 2822,loss: 0.009559392\n",
            "\n",
            "Global step: 2823,loss: 0.009435675\n",
            "\n",
            "Global step: 2824,loss: 0.009944175\n",
            "\n",
            "Global step: 2825,loss: 0.009418467\n",
            "\n",
            "Global step: 2826,loss: 0.010028172\n",
            "\n",
            "Global step: 2827,loss: 0.009951894\n",
            "\n",
            "Global step: 2828,loss: 0.009201409\n",
            "\n",
            "Global step: 2829,loss: 0.009284188\n",
            "\n",
            "Global step: 2830,loss: 0.00943725\n",
            "\n",
            "Global step: 2831,loss: 0.010537785\n",
            "\n",
            "Global step: 2832,loss: 0.009369881\n",
            "\n",
            "Global step: 2833,loss: 0.009174259\n",
            "\n",
            "Global step: 2834,loss: 0.009229499\n",
            "\n",
            "Global step: 2835,loss: 0.009611279\n",
            "\n",
            "Global step: 2836,loss: 0.00928117\n",
            "\n",
            "Global step: 2837,loss: 0.009470442\n",
            "\n",
            "Global step: 2838,loss: 0.009260139\n",
            "\n",
            "Global step: 2839,loss: 0.010165834\n",
            "\n",
            "Global step: 2840,loss: 0.008727169\n",
            "\n",
            "Global step: 2841,loss: 0.010012742\n",
            "\n",
            "Global step: 2842,loss: 0.009304647\n",
            "\n",
            "Global step: 2843,loss: 0.010568216\n",
            "\n",
            "Global step: 2844,loss: 0.010738589\n",
            "\n",
            "Global step: 2845,loss: 0.00920643\n",
            "\n",
            "Global step: 2846,loss: 0.00966624\n",
            "\n",
            "Global step: 2847,loss: 0.009337528\n",
            "\n",
            "Global step: 2848,loss: 0.009302966\n",
            "\n",
            "Global step: 2849,loss: 0.00925565\n",
            "\n",
            "Global step: 2850,loss: 0.009311581\n",
            "\n",
            "Global step: 2851,loss: 0.009457298\n",
            "\n",
            "Global step: 2852,loss: 0.009772051\n",
            "\n",
            "Global step: 2853,loss: 0.009244174\n",
            "\n",
            "Global step: 2854,loss: 0.0093434155\n",
            "\n",
            "Global step: 2855,loss: 0.00938642\n",
            "\n",
            "Global step: 2856,loss: 0.009665637\n",
            "\n",
            "Global step: 2857,loss: 0.009140889\n",
            "\n",
            "Global step: 2858,loss: 0.0093613835\n",
            "\n",
            "Global step: 2859,loss: 0.009798586\n",
            "\n",
            "Global step: 2860,loss: 0.00915095\n",
            "\n",
            "Global step: 2861,loss: 0.008647914\n",
            "\n",
            "Global step: 2862,loss: 0.010485754\n",
            "\n",
            "Global step: 2863,loss: 0.009621468\n",
            "\n",
            "Global step: 2864,loss: 0.009478918\n",
            "\n",
            "Global step: 2865,loss: 0.009070416\n",
            "\n",
            "Global step: 2866,loss: 0.009491049\n",
            "\n",
            "Global step: 2867,loss: 0.009299049\n",
            "\n",
            "Global step: 2868,loss: 0.01005321\n",
            "\n",
            "Global step: 2869,loss: 0.009825327\n",
            "\n",
            "Global step: 2870,loss: 0.00919172\n",
            "\n",
            "Global step: 2871,loss: 0.008990781\n",
            "\n",
            "Global step: 2872,loss: 0.009242372\n",
            "\n",
            "Global step: 2873,loss: 0.009167036\n",
            "\n",
            "Global step: 2874,loss: 0.008970414\n",
            "\n",
            "Global step: 2875,loss: 0.009362677\n",
            "\n",
            "Global step: 2876,loss: 0.009274433\n",
            "\n",
            "Global step: 2877,loss: 0.009409859\n",
            "\n",
            "Global step: 2878,loss: 0.009341894\n",
            "\n",
            "Global step: 2879,loss: 0.009425583\n",
            "\n",
            "Global step: 2880,loss: 0.009443443\n",
            "\n",
            "Global step: 2881,loss: 0.008935856\n",
            "\n",
            "Global step: 2882,loss: 0.009358477\n",
            "\n",
            "Global step: 2883,loss: 0.009228294\n",
            "\n",
            "Global step: 2884,loss: 0.011579285\n",
            "\n",
            "Global step: 2885,loss: 0.011143314\n",
            "\n",
            "Global step: 2886,loss: 0.0104311975\n",
            "\n",
            "Global step: 2887,loss: 0.009252313\n",
            "\n",
            "Global step: 2888,loss: 0.009799445\n",
            "\n",
            "Global step: 2889,loss: 0.009860051\n",
            "\n",
            "Global step: 2890,loss: 0.009559423\n",
            "\n",
            "Global step: 2891,loss: 0.009562203\n",
            "\n",
            "Global step: 2892,loss: 0.009058193\n",
            "\n",
            "Global step: 2893,loss: 0.009634329\n",
            "\n",
            "Global step: 2894,loss: 0.009586772\n",
            "\n",
            "Global step: 2895,loss: 0.009300349\n",
            "\n",
            "Global step: 2896,loss: 0.009181695\n",
            "\n",
            "Global step: 2897,loss: 0.009057044\n",
            "\n",
            "Global step: 2898,loss: 0.009347865\n",
            "\n",
            "Global step: 2899,loss: 0.009689218\n",
            "\n",
            "Global step: 2900,loss: 0.009376786\n",
            "\n",
            "Global step: 2901,loss: 0.009159863\n",
            "\n",
            "Global step: 2902,loss: 0.008760587\n",
            "\n",
            "Global step: 2903,loss: 0.00949745\n",
            "\n",
            "Global step: 2904,loss: 0.009274611\n",
            "\n",
            "Global step: 2905,loss: 0.009024306\n",
            "\n",
            "Global step: 2906,loss: 0.009431351\n",
            "\n",
            "Global step: 2907,loss: 0.011631484\n",
            "\n",
            "Global step: 2908,loss: 0.009142165\n",
            "\n",
            "Global step: 2909,loss: 0.009192349\n",
            "\n",
            "Global step: 2910,loss: 0.009236503\n",
            "\n",
            "Global step: 2911,loss: 0.01049144\n",
            "\n",
            "Global step: 2912,loss: 0.009144697\n",
            "\n",
            "Global step: 2913,loss: 0.009048083\n",
            "\n",
            "Global step: 2914,loss: 0.009004341\n",
            "\n",
            "Global step: 2915,loss: 0.009286282\n",
            "\n",
            "Global step: 2916,loss: 0.008949269\n",
            "\n",
            "Global step: 2917,loss: 0.009627205\n",
            "\n",
            "Global step: 2918,loss: 0.009064892\n",
            "\n",
            "Global step: 2919,loss: 0.0100752255\n",
            "\n",
            "Global step: 2920,loss: 0.009246408\n",
            "\n",
            "Global step: 2921,loss: 0.009312253\n",
            "\n",
            "Global step: 2922,loss: 0.009047987\n",
            "\n",
            "Global step: 2923,loss: 0.009447796\n",
            "\n",
            "Global step: 2924,loss: 0.010302521\n",
            "\n",
            "Global step: 2925,loss: 0.009340518\n",
            "\n",
            "Global step: 2926,loss: 0.009167819\n",
            "\n",
            "Global step: 2927,loss: 0.009754216\n",
            "\n",
            "Global step: 2928,loss: 0.00949048\n",
            "\n",
            "Global step: 2929,loss: 0.008971075\n",
            "\n",
            "Global step: 2930,loss: 0.00918538\n",
            "\n",
            "Global step: 2931,loss: 0.009403721\n",
            "\n",
            "Global step: 2932,loss: 0.008976763\n",
            "\n",
            "Global step: 2933,loss: 0.010024385\n",
            "\n",
            "Global step: 2934,loss: 0.00909625\n",
            "\n",
            "Global step: 2935,loss: 0.00986042\n",
            "\n",
            "Global step: 2936,loss: 0.008972386\n",
            "\n",
            "Global step: 2937,loss: 0.00885653\n",
            "\n",
            "Global step: 2938,loss: 0.009460936\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 2939.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:46:30.317093 140673502324480 supervisor.py:1050] Recording summary at step 2939.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 2939,loss: 0.009832891\n",
            "\n",
            "Global step: 2940,loss: 0.009285934\n",
            "\n",
            "Global step: 2941,loss: 0.009284047\n",
            "\n",
            "Global step: 2942,loss: 0.009034082\n",
            "\n",
            "Global step: 2943,loss: 0.010311094\n",
            "\n",
            "Global step: 2944,loss: 0.009089896\n",
            "\n",
            "Global step: 2945,loss: 0.009450887\n",
            "\n",
            "Global step: 2946,loss: 0.009437134\n",
            "\n",
            "Global step: 2947,loss: 0.011187041\n",
            "\n",
            "Global step: 2948,loss: 0.009060583\n",
            "\n",
            "Global step: 2949,loss: 0.009047712\n",
            "\n",
            "Global step: 2950,loss: 0.009008894\n",
            "\n",
            "Global step: 2951,loss: 0.0092387\n",
            "\n",
            "Global step: 2952,loss: 0.009346851\n",
            "\n",
            "Global step: 2953,loss: 0.0091856625\n",
            "\n",
            "Global step: 2954,loss: 0.009284162\n",
            "\n",
            "Global step: 2955,loss: 0.009375962\n",
            "\n",
            "Global step: 2956,loss: 0.010998086\n",
            "\n",
            "Global step: 2957,loss: 0.009938732\n",
            "\n",
            "Global step: 2958,loss: 0.009442843\n",
            "\n",
            "Global step: 2959,loss: 0.008901633\n",
            "\n",
            "Global step: 2960,loss: 0.009160559\n",
            "\n",
            "Global step: 2961,loss: 0.009485955\n",
            "\n",
            "Global step: 2962,loss: 0.009409383\n",
            "\n",
            "Global step: 2963,loss: 0.010355369\n",
            "\n",
            "Global step: 2964,loss: 0.009199521\n",
            "\n",
            "Global step: 2965,loss: 0.009227372\n",
            "\n",
            "Global step: 2966,loss: 0.009252881\n",
            "\n",
            "Global step: 2967,loss: 0.008899989\n",
            "\n",
            "Global step: 2968,loss: 0.0100656515\n",
            "\n",
            "Global step: 2969,loss: 0.00956611\n",
            "\n",
            "Global step: 2970,loss: 0.009353195\n",
            "\n",
            "Global step: 2971,loss: 0.009756779\n",
            "\n",
            "Global step: 2972,loss: 0.009160272\n",
            "\n",
            "Global step: 2973,loss: 0.008840443\n",
            "\n",
            "Global step: 2974,loss: 0.009231496\n",
            "\n",
            "Global step: 2975,loss: 0.009189168\n",
            "\n",
            "Global step: 2976,loss: 0.009426383\n",
            "\n",
            "Global step: 2977,loss: 0.009243903\n",
            "\n",
            "Global step: 2978,loss: 0.009126157\n",
            "\n",
            "Global step: 2979,loss: 0.009131135\n",
            "\n",
            "Global step: 2980,loss: 0.009006255\n",
            "\n",
            "Global step: 2981,loss: 0.009220208\n",
            "\n",
            "Global step: 2982,loss: 0.009045509\n",
            "\n",
            "Global step: 2983,loss: 0.0090890955\n",
            "\n",
            "Global step: 2984,loss: 0.009181457\n",
            "\n",
            "Global step: 2985,loss: 0.009044052\n",
            "\n",
            "Global step: 2986,loss: 0.009014997\n",
            "\n",
            "Global step: 2987,loss: 0.009200206\n",
            "\n",
            "Global step: 2988,loss: 0.008819486\n",
            "\n",
            "Global step: 2989,loss: 0.009067186\n",
            "\n",
            "Global step: 2990,loss: 0.009182836\n",
            "\n",
            "Global step: 2991,loss: 0.008434043\n",
            "\n",
            "Global step: 2992,loss: 0.0093095265\n",
            "\n",
            "Global step: 2993,loss: 0.009799829\n",
            "\n",
            "Global step: 2994,loss: 0.00934876\n",
            "\n",
            "Global step: 2995,loss: 0.009466635\n",
            "\n",
            "Global Step: 2995,Val_Loss: 4.6846901616510494e-05,  Val_acc: 0.9979440789473685 Improved\n",
            "\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 08:47:19.078539 140676699535232 meta_graph.py:448] Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for epoch 14/15:\n",
            "Global step: 2996,loss: 0.009638526\n",
            "\n",
            "Global step: 2997,loss: 0.009503998\n",
            "\n",
            "Global step: 2998,loss: 0.008656599\n",
            "\n",
            "Global step: 2999,loss: 0.00879398\n",
            "\n",
            "Global step: 3000,loss: 0.008895697\n",
            "\n",
            "Global step: 3001,loss: 0.009996958\n",
            "\n",
            "Global step: 3002,loss: 0.008762717\n",
            "\n",
            "Global step: 3003,loss: 0.009229903\n",
            "\n",
            "Global step: 3004,loss: 0.009073148\n",
            "\n",
            "Global step: 3005,loss: 0.009345242\n",
            "\n",
            "Global step: 3006,loss: 0.009776029\n",
            "\n",
            "Global step: 3007,loss: 0.008562061\n",
            "\n",
            "Global step: 3008,loss: 0.009325418\n",
            "\n",
            "Global step: 3009,loss: 0.009460176\n",
            "\n",
            "Global step: 3010,loss: 0.009268786\n",
            "\n",
            "Global step: 3011,loss: 0.008904117\n",
            "\n",
            "Global step: 3012,loss: 0.009262163\n",
            "\n",
            "Global step: 3013,loss: 0.008749719\n",
            "\n",
            "Global step: 3014,loss: 0.010701381\n",
            "\n",
            "Global step: 3015,loss: 0.0091178045\n",
            "\n",
            "Global step: 3016,loss: 0.009201943\n",
            "\n",
            "Global step: 3017,loss: 0.008802116\n",
            "\n",
            "Global step: 3018,loss: 0.009302959\n",
            "\n",
            "Global step: 3019,loss: 0.009153678\n",
            "\n",
            "Global step: 3020,loss: 0.008882879\n",
            "\n",
            "Global step: 3021,loss: 0.008904146\n",
            "\n",
            "Global step: 3022,loss: 0.008687039\n",
            "\n",
            "Global step: 3023,loss: 0.008924546\n",
            "\n",
            "Global step: 3024,loss: 0.008954947\n",
            "\n",
            "Global step: 3025,loss: 0.008712126\n",
            "\n",
            "Global step: 3026,loss: 0.009198565\n",
            "\n",
            "Global step: 3027,loss: 0.00904121\n",
            "\n",
            "Global step: 3028,loss: 0.008922003\n",
            "\n",
            "Global step: 3029,loss: 0.008820091\n",
            "\n",
            "Global step: 3030,loss: 0.009408293\n",
            "\n",
            "Global step: 3031,loss: 0.009180949\n",
            "\n",
            "Global step: 3032,loss: 0.008930074\n",
            "\n",
            "Global step: 3033,loss: 0.008948645\n",
            "\n",
            "Global step: 3034,loss: 0.009387961\n",
            "\n",
            "Global step: 3035,loss: 0.009376653\n",
            "\n",
            "Global step: 3036,loss: 0.010082768\n",
            "\n",
            "Global step: 3037,loss: 0.009014105\n",
            "\n",
            "Global step: 3038,loss: 0.00986052\n",
            "\n",
            "Global step: 3039,loss: 0.009095306\n",
            "\n",
            "Global step: 3040,loss: 0.008988596\n",
            "\n",
            "Global step: 3041,loss: 0.008710787\n",
            "\n",
            "Global step: 3042,loss: 0.009000881\n",
            "\n",
            "Global step: 3043,loss: 0.0088276155\n",
            "\n",
            "Global step: 3044,loss: 0.008803295\n",
            "\n",
            "Global step: 3045,loss: 0.008934702\n",
            "\n",
            "Global step: 3046,loss: 0.008573822\n",
            "\n",
            "Global step: 3047,loss: 0.009791209\n",
            "\n",
            "Global step: 3048,loss: 0.008536752\n",
            "\n",
            "Global step: 3049,loss: 0.011117824\n",
            "\n",
            "Global step: 3050,loss: 0.009206913\n",
            "\n",
            "Global step: 3051,loss: 0.009082783\n",
            "\n",
            "Global step: 3052,loss: 0.008760491\n",
            "\n",
            "Global step: 3053,loss: 0.009253182\n",
            "\n",
            "Global step: 3054,loss: 0.009175719\n",
            "\n",
            "Global step: 3055,loss: 0.008893937\n",
            "\n",
            "Global step: 3056,loss: 0.00922185\n",
            "\n",
            "Global step: 3057,loss: 0.009008309\n",
            "\n",
            "Global step: 3058,loss: 0.009053315\n",
            "\n",
            "Global step: 3059,loss: 0.0091088405\n",
            "\n",
            "Global step: 3060,loss: 0.008483161\n",
            "\n",
            "Global step: 3061,loss: 0.010037807\n",
            "\n",
            "Global step: 3062,loss: 0.008928363\n",
            "\n",
            "Global step: 3063,loss: 0.009030509\n",
            "\n",
            "Global step: 3064,loss: 0.008675167\n",
            "\n",
            "Global step: 3065,loss: 0.008874611\n",
            "\n",
            "Global step: 3066,loss: 0.008816714\n",
            "\n",
            "Global step: 3067,loss: 0.008748062\n",
            "\n",
            "Global step: 3068,loss: 0.00938384\n",
            "\n",
            "Global step: 3069,loss: 0.008498289\n",
            "\n",
            "Global step: 3070,loss: 0.009826558\n",
            "\n",
            "Global step: 3071,loss: 0.010121336\n",
            "\n",
            "Global step: 3072,loss: 0.008811938\n",
            "\n",
            "Global step: 3073,loss: 0.008657123\n",
            "\n",
            "Global step: 3074,loss: 0.008695945\n",
            "\n",
            "Global step: 3075,loss: 0.00927485\n",
            "\n",
            "Global step: 3076,loss: 0.008791711\n",
            "\n",
            "Global step: 3077,loss: 0.009242962\n",
            "\n",
            "Global step: 3078,loss: 0.009440532\n",
            "\n",
            "Global step: 3079,loss: 0.008885001\n",
            "\n",
            "Global step: 3080,loss: 0.008517665\n",
            "\n",
            "Global step: 3081,loss: 0.009133078\n",
            "\n",
            "Global step: 3082,loss: 0.00873971\n",
            "\n",
            "Global step: 3083,loss: 0.008612812\n",
            "\n",
            "Global step: 3084,loss: 0.008916317\n",
            "\n",
            "Global step: 3085,loss: 0.00855373\n",
            "\n",
            "Global step: 3086,loss: 0.008939682\n",
            "\n",
            "Global step: 3087,loss: 0.008219755\n",
            "\n",
            "Global step: 3088,loss: 0.008951352\n",
            "\n",
            "Global step: 3089,loss: 0.008915031\n",
            "\n",
            "Global step: 3090,loss: 0.008701798\n",
            "\n",
            "INFO:tensorflow:Recording summary at step 3091.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:48:29.927980 140673502324480 supervisor.py:1050] Recording summary at step 3091.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Global step: 3091,loss: 0.008923375\n",
            "\n",
            "Global step: 3092,loss: 0.008809091\n",
            "\n",
            "Global step: 3093,loss: 0.009266822\n",
            "\n",
            "Global step: 3094,loss: 0.008865616\n",
            "\n",
            "Global step: 3095,loss: 0.008728172\n",
            "\n",
            "Global step: 3096,loss: 0.010107173\n",
            "\n",
            "Global step: 3097,loss: 0.008620101\n",
            "\n",
            "Global step: 3098,loss: 0.008353379\n",
            "\n",
            "Global step: 3099,loss: 0.009046188\n",
            "\n",
            "Global step: 3100,loss: 0.008897182\n",
            "\n",
            "Global step: 3101,loss: 0.008920638\n",
            "\n",
            "Global step: 3102,loss: 0.008834421\n",
            "\n",
            "Global step: 3103,loss: 0.010035458\n",
            "\n",
            "Global step: 3104,loss: 0.008861159\n",
            "\n",
            "Global step: 3105,loss: 0.010052921\n",
            "\n",
            "Global step: 3106,loss: 0.008836701\n",
            "\n",
            "Global step: 3107,loss: 0.008824679\n",
            "\n",
            "Global step: 3108,loss: 0.00805367\n",
            "\n",
            "Global step: 3109,loss: 0.010024061\n",
            "\n",
            "Global step: 3110,loss: 0.008486049\n",
            "\n",
            "Global step: 3111,loss: 0.008972074\n",
            "\n",
            "Global step: 3112,loss: 0.009165628\n",
            "\n",
            "Global step: 3113,loss: 0.008975331\n",
            "\n",
            "Global step: 3114,loss: 0.008955605\n",
            "\n",
            "Global step: 3115,loss: 0.009176104\n",
            "\n",
            "Global step: 3116,loss: 0.00882808\n",
            "\n",
            "Global step: 3117,loss: 0.00838685\n",
            "\n",
            "Global step: 3118,loss: 0.009099994\n",
            "\n",
            "Global step: 3119,loss: 0.008335158\n",
            "\n",
            "Global step: 3120,loss: 0.008813993\n",
            "\n",
            "Global step: 3121,loss: 0.008726185\n",
            "\n",
            "Global step: 3122,loss: 0.008410524\n",
            "\n",
            "Global step: 3123,loss: 0.008733555\n",
            "\n",
            "Global step: 3124,loss: 0.009185917\n",
            "\n",
            "Global step: 3125,loss: 0.009081359\n",
            "\n",
            "Global step: 3126,loss: 0.009035145\n",
            "\n",
            "Global step: 3127,loss: 0.008908029\n",
            "\n",
            "Global step: 3128,loss: 0.00867227\n",
            "\n",
            "Global step: 3129,loss: 0.009236292\n",
            "\n",
            "Global step: 3130,loss: 0.008855118\n",
            "\n",
            "Global step: 3131,loss: 0.008563854\n",
            "\n",
            "Global step: 3132,loss: 0.009550728\n",
            "\n",
            "Global step: 3133,loss: 0.00890579\n",
            "\n",
            "Global step: 3134,loss: 0.009258306\n",
            "\n",
            "Global step: 3135,loss: 0.00901509\n",
            "\n",
            "Global step: 3136,loss: 0.008454698\n",
            "\n",
            "Global step: 3137,loss: 0.0091454005\n",
            "\n",
            "Global step: 3138,loss: 0.0088062985\n",
            "\n",
            "Global step: 3139,loss: 0.008951689\n",
            "\n",
            "Global step: 3140,loss: 0.008349562\n",
            "\n",
            "Global step: 3141,loss: 0.008884146\n",
            "\n",
            "Global step: 3142,loss: 0.009215057\n",
            "\n",
            "Global step: 3143,loss: 0.009828244\n",
            "\n",
            "Global step: 3144,loss: 0.00863053\n",
            "\n",
            "Global step: 3145,loss: 0.00881067\n",
            "\n",
            "Global step: 3146,loss: 0.008510469\n",
            "\n",
            "Global step: 3147,loss: 0.008941239\n",
            "\n",
            "Global step: 3148,loss: 0.009393291\n",
            "\n",
            "Global step: 3149,loss: 0.008963352\n",
            "\n",
            "Global step: 3150,loss: 0.00881545\n",
            "\n",
            "Global step: 3151,loss: 0.0089186365\n",
            "\n",
            "Global step: 3152,loss: 0.008760138\n",
            "\n",
            "Global step: 3153,loss: 0.008252469\n",
            "\n",
            "Global step: 3154,loss: 0.008569556\n",
            "\n",
            "Global step: 3155,loss: 0.009053201\n",
            "\n",
            "Global step: 3156,loss: 0.008393736\n",
            "\n",
            "Global step: 3157,loss: 0.008543027\n",
            "\n",
            "Global step: 3158,loss: 0.008481947\n",
            "\n",
            "Global step: 3159,loss: 0.008955627\n",
            "\n",
            "Global step: 3160,loss: 0.00868637\n",
            "\n",
            "Global step: 3161,loss: 0.008731252\n",
            "\n",
            "Global step: 3162,loss: 0.008863046\n",
            "\n",
            "Global step: 3163,loss: 0.008982216\n",
            "\n",
            "Global step: 3164,loss: 0.008899961\n",
            "\n",
            "Global step: 3165,loss: 0.008807885\n",
            "\n",
            "Global step: 3166,loss: 0.008390791\n",
            "\n",
            "Global step: 3167,loss: 0.0089067295\n",
            "\n",
            "Global step: 3168,loss: 0.008976964\n",
            "\n",
            "Global step: 3169,loss: 0.0087091625\n",
            "\n",
            "Global step: 3170,loss: 0.0085073495\n",
            "\n",
            "Global step: 3171,loss: 0.008782779\n",
            "\n",
            "Global step: 3172,loss: 0.009185471\n",
            "\n",
            "Global step: 3173,loss: 0.00893042\n",
            "\n",
            "Global step: 3174,loss: 0.008735487\n",
            "\n",
            "Global step: 3175,loss: 0.008676492\n",
            "\n",
            "Global step: 3176,loss: 0.009006162\n",
            "\n",
            "Global step: 3177,loss: 0.009138974\n",
            "\n",
            "Global step: 3178,loss: 0.008504165\n",
            "\n",
            "Global step: 3179,loss: 0.008796442\n",
            "\n",
            "Global step: 3180,loss: 0.0086102355\n",
            "\n",
            "Global step: 3181,loss: 0.008794686\n",
            "\n",
            "Global step: 3182,loss: 0.008692368\n",
            "\n",
            "Global step: 3183,loss: 0.008979751\n",
            "\n",
            "Global step: 3184,loss: 0.008807632\n",
            "\n",
            "Global step: 3185,loss: 0.008361638\n",
            "\n",
            "Global step: 3186,loss: 0.008529455\n",
            "\n",
            "Global step: 3187,loss: 0.008245349\n",
            "\n",
            "Global step: 3188,loss: 0.008942614\n",
            "\n",
            "Global step: 3189,loss: 0.009047187\n",
            "\n",
            "Global step: 3190,loss: 0.0084809605\n",
            "\n",
            "Global step: 3191,loss: 0.008593607\n",
            "\n",
            "Global step: 3192,loss: 0.008834036\n",
            "\n",
            "Global step: 3193,loss: 0.0091747\n",
            "\n",
            "Global step: 3194,loss: 0.008115249\n",
            "\n",
            "Global step: 3195,loss: 0.008432899\n",
            "\n",
            "Global step: 3196,loss: 0.008577838\n",
            "\n",
            "Global step: 3197,loss: 0.008650905\n",
            "\n",
            "Global step: 3198,loss: 0.008326917\n",
            "\n",
            "Global step: 3199,loss: 0.009045985\n",
            "\n",
            "Global step: 3200,loss: 0.008626996\n",
            "\n",
            "Global step: 3201,loss: 0.008682199\n",
            "\n",
            "Global step: 3202,loss: 0.010924239\n",
            "\n",
            "Global step: 3203,loss: 0.008487764\n",
            "\n",
            "Global step: 3204,loss: 0.009786998\n",
            "\n",
            "Global step: 3205,loss: 0.009238141\n",
            "\n",
            "Global step: 3206,loss: 0.008476577\n",
            "\n",
            "Global step: 3207,loss: 0.009173306\n",
            "\n",
            "Global step: 3208,loss: 0.008980518\n",
            "\n",
            "Global step: 3209,loss: 0.008759864\n",
            "\n",
            "\n",
            "NOT SAVING MODEL!!\n",
            "Global Step: 3209,val_loss: 4.6987152592536354e-05\n",
            "\n",
            "INFO:tensorflow:Training done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 08:50:03.985588 140676699535232 <ipython-input-10-fda5935e9628>:14] Training done\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spYdQXFr1UbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "c73e5609-74ba-44f5-e967-04e2b48cc061"
      },
      "source": [
        "cfg.is_training=False\n",
        "\n",
        "def main(_):\n",
        "    tf.logging.info(' Loading Graph...')\n",
        "    num_label = 10\n",
        "    model = CapsNet()\n",
        "    tf.logging.info(' Graph loaded')\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n",
        "\n",
        "    if cfg.is_training:\n",
        "        tf.logging.info(' Start training...')\n",
        "        train(model, sv, num_label)\n",
        "        tf.logging.info('Training done')\n",
        "    else:\n",
        "        evaluation(model, sv, num_label)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.app.run()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Loading Graph...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:26.869243 140676699535232 <ipython-input-11-2d49ea40900d>:4]  Loading Graph...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:28.507539 140676699535232 <ipython-input-6-4752a8a30df1>:46] Seting up the main structure\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow: Graph loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:28.510647 140676699535232 <ipython-input-11-2d49ea40900d>:7]  Graph loaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0013_step_2995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:28.969423 140676699535232 saver.py:1284] Restoring parameters from logdir/model_epoch_0013_step_2995\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1104 09:26:29.309622 140676699535232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:29.326611 140676699535232 session_manager.py:500] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:26:29.357829 140676699535232 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting standard services.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:27:06.302939 140676699535232 supervisor.py:737] Starting standard services.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:27:07.081838 140676699535232 supervisor.py:743] Starting queue runners.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logdir/model_epoch_0013_step_2995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:27:07.100440 140676699535232 saver.py:1284] Restoring parameters from logdir/model_epoch_0013_step_2995\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Model restored!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:27:08.605688 140676699535232 <ipython-input-9-f2e4e6322322>:116] Model restored!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy is 0.7943359375:\n",
            "\n",
            "Test accuracy has been saved to results/test_acc\n",
            "INFO:tensorflow:Recording summary at step 2996.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1104 09:27:21.690150 140673443575552 supervisor.py:1050] Recording summary at step 2996.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W8Z-9sVTMtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}